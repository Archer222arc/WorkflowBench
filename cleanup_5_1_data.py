#!/usr/bin/env python3
"""
5.1åŸºå‡†æµ‹è¯•æ•°æ®æ¸…ç†è„šæœ¬
æ¸…ç†æ‰€æœ‰æ¨¡å‹çš„ optimal prompt + easyéš¾åº¦ + 0.8å·¥å…·æˆåŠŸç‡ çš„æµ‹è¯•æ•°æ®
"""

import json
import sys
from pathlib import Path

def cleanup_5_1_data():
    """æ¸…ç†5.1åŸºå‡†æµ‹è¯•æ•°æ®"""
    db_path = Path("pilot_bench_cumulative_results/master_database.json")
    
    if not db_path.exists():
        print("âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨")
        return False
    
    print("ğŸ”„ åŠ è½½æ•°æ®åº“...")
    with open(db_path, 'r', encoding='utf-8') as f:
        db = json.load(f)
    
    models = db.get('models', {})
    cleaned_count = 0
    cleaned_models = []
    
    print("\nğŸ“Š å¼€å§‹æ¸…ç†5.1æ•°æ® (optimal + easy + 0.8)...")
    print("=" * 60)
    
    for model_name in list(models.keys()):
        model_data = models[model_name]
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨optimal promptç±»å‹
        by_prompt = model_data.get('by_prompt_type', {})
        if 'optimal' not in by_prompt:
            continue
            
        optimal_data = by_prompt['optimal']
        by_rate = optimal_data.get('by_tool_success_rate', {})
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨0.8å·¥å…·æˆåŠŸç‡
        if '0.8' not in by_rate:
            continue
            
        rate_08_data = by_rate['0.8']
        by_difficulty = rate_08_data.get('by_difficulty', {})
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨easyéš¾åº¦
        if 'easy' not in by_difficulty:
            continue
            
        easy_data = by_difficulty['easy']
        by_task_type = easy_data.get('by_task_type', {})
        
        if by_task_type:
            # è®¡ç®—è¦æ¸…ç†çš„æ•°æ®æ•°é‡
            total_tests = sum(task_data.get('total', 0) for task_data in by_task_type.values())
            if total_tests > 0:
                print(f"ğŸ—‘ï¸  {model_name}: æ¸…ç† {total_tests} ä¸ªæµ‹è¯•è®°å½•")
                cleaned_count += total_tests
                cleaned_models.append(model_name)
                
                # æ¸…ç†easyéš¾åº¦ä¸‹çš„æ‰€æœ‰ä»»åŠ¡ç±»å‹æ•°æ®
                by_difficulty['easy'] = {'by_task_type': {}}
                
                # å¦‚æœeasyæ˜¯å”¯ä¸€çš„éš¾åº¦ï¼Œæ¸…ç†æ•´ä¸ª0.8å±‚çº§
                if list(by_difficulty.keys()) == ['easy']:
                    by_rate['0.8'] = {'by_difficulty': {}}
                    
                    # å¦‚æœ0.8æ˜¯å”¯ä¸€çš„å·¥å…·æˆåŠŸç‡ï¼Œæ¸…ç†æ•´ä¸ªoptimalå±‚çº§
                    if list(by_rate.keys()) == ['0.8']:
                        by_prompt['optimal'] = {'by_tool_success_rate': {}}
                        
                        # å¦‚æœoptimalæ˜¯å”¯ä¸€çš„promptç±»å‹ï¼Œæ¸…ç†æ•´ä¸ªæ¨¡å‹æ•°æ®
                        if list(by_prompt.keys()) == ['optimal']:
                            models[model_name] = {'by_prompt_type': {}}
    
    print(f"\nğŸ“ˆ æ¸…ç†ç»Ÿè®¡:")
    print(f"  æ¸…ç†æ¨¡å‹æ•°: {len(cleaned_models)}")
    print(f"  æ¸…ç†æµ‹è¯•æ•°: {cleaned_count}")
    print(f"  æ¶‰åŠæ¨¡å‹: {', '.join(cleaned_models[:3])}{'...' if len(cleaned_models) > 3 else ''}")
    
    if cleaned_count == 0:
        print("âš ï¸  æ²¡æœ‰æ‰¾åˆ°5.1æ•°æ®å¯æ¸…ç†")
        return True
    
    # é‡æ–°è®¡ç®—summaryç»Ÿè®¡
    print("\nğŸ”„ é‡æ–°è®¡ç®—summaryç»Ÿè®¡...")
    total_tests = 0
    total_success = 0
    total_partial = 0
    total_failure = 0
    
    for model_name, model_data in models.items():
        by_prompt = model_data.get('by_prompt_type', {})
        for prompt_type, prompt_data in by_prompt.items():
            by_rate = prompt_data.get('by_tool_success_rate', {})
            for rate, rate_data in by_rate.items():
                by_difficulty = rate_data.get('by_difficulty', {})
                for difficulty, diff_data in by_difficulty.items():
                    by_task_type = diff_data.get('by_task_type', {})
                    for task_type, task_data in by_task_type.items():
                        total_tests += task_data.get('total', 0)
                        total_success += task_data.get('successful', 0)
                        total_partial += task_data.get('partial', 0)
                        total_failure += task_data.get('failed', 0)
    
    db['summary'] = {
        'total_tests': total_tests,
        'total_success': total_success,
        'total_partial': total_partial, 
        'total_failure': total_failure,
        'models_tested': list(models.keys()),
        'last_test_time': db.get('summary', {}).get('last_test_time', '')
    }
    
    # ä¿å­˜æ¸…ç†åçš„æ•°æ®åº“
    print("\nğŸ’¾ ä¿å­˜æ¸…ç†åçš„æ•°æ®åº“...")
    with open(db_path, 'w', encoding='utf-8') as f:
        json.dump(db, f, indent=2, ensure_ascii=False)
    
    print(f"âœ… 5.1æ•°æ®æ¸…ç†å®Œæˆï¼æ¸…ç†äº† {cleaned_count} ä¸ªæµ‹è¯•è®°å½•")
    return True

if __name__ == '__main__':
    if len(sys.argv) > 1 and sys.argv[1] == '--confirm':
        cleanup_5_1_data()
    else:
        print("ğŸš¨ è¿™å°†æ¸…ç†æ‰€æœ‰5.1åŸºå‡†æµ‹è¯•æ•°æ® (optimal + easy + 0.8)")
        print("ğŸ“‹ æ¶‰åŠçš„é…ç½®:")
        print("   - Promptç±»å‹: optimal")
        print("   - éš¾åº¦: easy")  
        print("   - å·¥å…·æˆåŠŸç‡: 0.8")
        print("\nâš ï¸  è¯·ç¡®è®¤è¦ç»§ç»­æ¸…ç†ï¼Œä½¿ç”¨: python cleanup_5_1_data.py --confirm")