#!/usr/bin/env python3
"""
æ™ºèƒ½æ¸è¿›å¼è®­ç»ƒè„šæœ¬ - æ”¯æŒæ¨¡å‹æ¶æ„åŠ¨æ€å˜åŒ–
é€šè¿‡çŸ¥è¯†è’¸é¦å’Œæƒé‡è¿ç§»å®ç°ä¸åŒå¤§å°æ¨¡å‹é—´çš„å¹³æ»‘è¿‡æ¸¡
"""

import os
import sys
import json
import time
import torch
import torch.nn as nn
import numpy as np
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, Optional, Tuple, List
import logging
import argparse
from collections import OrderedDict

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class WeightTransferManager:
    """æ™ºèƒ½æƒé‡è¿ç§»ç®¡ç†å™¨"""
    
    def __init__(self):
        self.transfer_stats = {
            'direct_copy': 0,
            'resized': 0,
            'partially_transferred': 0,
            'new_init': 0
        }
    
    def transfer_linear_weights(self, old_weight: torch.Tensor, new_weight: torch.Tensor) -> torch.Tensor:
        """è¿ç§»Linearå±‚æƒé‡"""
        old_out, old_in = old_weight.shape
        new_out, new_in = new_weight.shape
        
        # åˆ›å»ºæ–°æƒé‡å¼ é‡
        transferred = new_weight.clone()
        
        # å¦‚æœæ–°æ¨¡å‹æ›´å¤§ï¼Œå…ˆç”¨Xavieråˆå§‹åŒ–
        if new_out > old_out or new_in > old_in:
            nn.init.xavier_uniform_(transferred)
        
        # å¤åˆ¶é‡å éƒ¨åˆ†
        min_out = min(old_out, new_out)
        min_in = min(old_in, new_in)
        transferred[:min_out, :min_in] = old_weight[:min_out, :min_in]
        
        # å¦‚æœè¾“å…¥ç»´åº¦å¢åŠ ï¼Œå¯¹æ–°å¢éƒ¨åˆ†ä½¿ç”¨å‡å€¼å¡«å……
        if new_in > old_in:
            # ä½¿ç”¨å·²æœ‰æƒé‡çš„ç»Ÿè®¡ä¿¡æ¯åˆå§‹åŒ–æ–°å¢éƒ¨åˆ†
            mean_val = old_weight[:min_out, :].mean().item()
            std_val = old_weight[:min_out, :].std().item()
            transferred[:min_out, old_in:new_in] = torch.randn(min_out, new_in - old_in) * std_val + mean_val
        
        # å¦‚æœè¾“å‡ºç»´åº¦å¢åŠ ï¼Œä½¿ç”¨ç±»ä¼¼çš„ç­–ç•¥
        if new_out > old_out:
            for i in range(old_out, new_out):
                # ä½¿ç”¨å·²æœ‰ç¥ç»å…ƒçš„çº¿æ€§ç»„åˆ
                idx1, idx2 = np.random.choice(old_out, 2, replace=False)
                alpha = np.random.uniform(0.3, 0.7)
                transferred[i, :min_in] = alpha * old_weight[idx1, :min_in] + (1 - alpha) * old_weight[idx2, :min_in]
        
        return transferred
    
    def transfer_conv_weights(self, old_weight: torch.Tensor, new_weight: torch.Tensor) -> torch.Tensor:
        """è¿ç§»å·ç§¯å±‚æƒé‡"""
        # å¯¹äºTransformeræ¶æ„ï¼Œé€šå¸¸ä¸éœ€è¦ï¼Œä½†ä¿ç•™ä»¥å¤‡ç”¨
        return new_weight
    
    def transfer_norm_weights(self, old_weight: torch.Tensor, new_weight: torch.Tensor) -> torch.Tensor:
        """è¿ç§»å½’ä¸€åŒ–å±‚æƒé‡"""
        old_size = old_weight.shape[0]
        new_size = new_weight.shape[0]
        
        transferred = new_weight.clone()
        min_size = min(old_size, new_size)
        
        # ç›´æ¥å¤åˆ¶é‡å éƒ¨åˆ†
        transferred[:min_size] = old_weight[:min_size]
        
        # æ–°å¢éƒ¨åˆ†ä½¿ç”¨å·²æœ‰å‚æ•°çš„å‡å€¼
        if new_size > old_size:
            transferred[old_size:] = old_weight.mean()
        
        return transferred
    
    def intelligent_weight_transfer(self, old_state_dict: Dict[str, torch.Tensor], 
                                   new_model: nn.Module) -> Dict[str, torch.Tensor]:
        """æ‰§è¡Œæ™ºèƒ½æƒé‡è¿ç§»"""
        new_state_dict = new_model.state_dict()
        transferred_dict = OrderedDict()
        
        logger.info("\n" + "="*60)
        logger.info("ğŸ§  Starting Intelligent Weight Transfer")
        logger.info("="*60)
        
        for key, new_param in new_state_dict.items():
            if key in old_state_dict:
                old_param = old_state_dict[key]
                
                # å®Œå…¨åŒ¹é… - ç›´æ¥å¤åˆ¶
                if old_param.shape == new_param.shape:
                    transferred_dict[key] = old_param
                    self.transfer_stats['direct_copy'] += 1
                    logger.debug(f"âœ“ Direct copy: {key} {old_param.shape}")
                
                # éœ€è¦è°ƒæ•´å¤§å°
                else:
                    logger.info(f"ğŸ”„ Resizing: {key} from {old_param.shape} to {new_param.shape}")
                    
                    # Linearå±‚æƒé‡
                    if 'weight' in key and len(old_param.shape) == 2:
                        transferred_dict[key] = self.transfer_linear_weights(old_param, new_param)
                        self.transfer_stats['resized'] += 1
                    
                    # åç½®æˆ–å½’ä¸€åŒ–å‚æ•°
                    elif len(old_param.shape) == 1:
                        transferred_dict[key] = self.transfer_norm_weights(old_param, new_param)
                        self.transfer_stats['resized'] += 1
                    
                    # å¤šå¤´æ³¨æ„åŠ›çš„ç‰¹æ®Šå¤„ç†
                    elif 'attention' in key and len(old_param.shape) > 2:
                        # é‡å¡‘å¹¶è¿ç§»
                        old_shape = old_param.shape
                        new_shape = new_param.shape
                        
                        if len(old_shape) == len(new_shape):
                            # å°è¯•é€ç»´åº¦è¿ç§»
                            transferred = new_param.clone()
                            for dim in range(len(old_shape)):
                                min_size = min(old_shape[dim], new_shape[dim])
                                slices = [slice(None)] * len(old_shape)
                                slices[dim] = slice(0, min_size)
                                transferred[tuple(slices)] = old_param[tuple(slices)]
                            transferred_dict[key] = transferred
                            self.transfer_stats['partially_transferred'] += 1
                        else:
                            transferred_dict[key] = new_param
                            self.transfer_stats['new_init'] += 1
                    
                    else:
                        # æ— æ³•æ™ºèƒ½è¿ç§»ï¼Œä½¿ç”¨æ–°åˆå§‹åŒ–
                        transferred_dict[key] = new_param
                        self.transfer_stats['new_init'] += 1
                        logger.warning(f"âš ï¸ Cannot transfer {key}, using new initialization")
            
            else:
                # æ–°å‚æ•°
                transferred_dict[key] = new_param
                self.transfer_stats['new_init'] += 1
                logger.debug(f"âœ— New parameter: {key}")
        
        # æ‰“å°è¿ç§»ç»Ÿè®¡
        logger.info("\nğŸ“Š Transfer Statistics:")
        logger.info(f"  â€¢ Direct copies: {self.transfer_stats['direct_copy']}")
        logger.info(f"  â€¢ Resized: {self.transfer_stats['resized']}")
        logger.info(f"  â€¢ Partially transferred: {self.transfer_stats['partially_transferred']}")
        logger.info(f"  â€¢ New initializations: {self.transfer_stats['new_init']}")
        logger.info("="*60 + "\n")
        
        return transferred_dict


class ProgressiveStage:
    """è®­ç»ƒé˜¶æ®µé…ç½®"""
    
    def __init__(self, name: str, difficulty: str, episodes: int, config: Dict[str, Any]):
        self.name = name
        self.difficulty = difficulty
        self.episodes = episodes
        self.config = config
        self.start_time = None
        self.end_time = None
        self.metrics = {}


class SmartProgressiveTrainer:
    """æ™ºèƒ½æ¸è¿›å¼è®­ç»ƒå™¨"""
    
    def __init__(self, base_output_dir: str = "smart_progressive_training"):
        self.base_output_dir = Path(base_output_dir)
        self.base_output_dir.mkdir(exist_ok=True)
        
        # æƒé‡è¿ç§»ç®¡ç†å™¨
        self.weight_transfer_manager = WeightTransferManager()
        
        # åŸºç¡€é…ç½®
        self.base_config = {
            "algorithm": "ppo",
            "device": "cuda",
            "use_mixed_precision": True,
            "use_task_aware_buffer": True,
            "use_curriculum": True,
            "use_lr_scheduler": True,
            "lr_schedule": "cosine",
            "use_auxiliary_tasks": True,
            "use_curiosity": True,
            "num_heads": 4,
            "dropout": 0.1,
            "activation": "gelu",
            "use_layer_norm": True,
            "n_epochs": 10,
            "gamma": 0.99,
            "gae_lambda": 0.95,
            "max_grad_norm": 0.5,
        }
        
        # è®­ç»ƒé˜¶æ®µå®šä¹‰
        self.stages = [
            ProgressiveStage(
                name="ğŸŒ± Foundation (Very Easy)",
                difficulty="very_easy",
                episodes=400,
                config={
                    "hidden_dim": 256,
                    "num_layers": 2,
                    "learning_rate": 0.0003,
                    "batch_size": 128,
                    "mini_batch_size": 32,
                    "ent_coef": 0.02,
                    "clip_range": 0.3,
                    "n_steps": 1024,
                    "intrinsic_reward_scale": 0.2,
                    "dropout": 0.15,
                }
            ),
            ProgressiveStage(
                name="ğŸŒ¿ Skill Building (Easy)",
                difficulty="easy",
                episodes=400,
                config={
                    "hidden_dim": 384,  # æ¶æ„å¢é•¿ï¼
                    "num_layers": 3,    # å±‚æ•°å¢åŠ ï¼
                    "learning_rate": 0.0002,
                    "batch_size": 192,
                    "mini_batch_size": 48,
                    "ent_coef": 0.015,
                    "clip_range": 0.25,
                    "n_steps": 1536,
                    "intrinsic_reward_scale": 0.15,
                    "dropout": 0.12,
                }
            ),
            ProgressiveStage(
                name="ğŸŒ³ Competence (Medium)",
                difficulty="medium",
                episodes=600,
                config={
                    "hidden_dim": 512,  # ç»§ç»­å¢é•¿ï¼
                    "num_layers": 3,
                    "learning_rate": 0.00015,
                    "batch_size": 256,
                    "mini_batch_size": 64,
                    "ent_coef": 0.01,
                    "clip_range": 0.2,
                    "n_steps": 2048,
                    "intrinsic_reward_scale": 0.1,
                    "dropout": 0.1,
                    "l2_reg": 0.0001,
                }
            ),
            ProgressiveStage(
                name="ğŸ’ª Mastery (Hard)",
                difficulty="hard",
                episodes=400,
                config={
                    "hidden_dim": 512,  # ä¿æŒç¨³å®š
                    "num_layers": 3,
                    "learning_rate": 0.0001,
                    "batch_size": 256,
                    "mini_batch_size": 64,
                    "ent_coef": 0.005,
                    "clip_range": 0.15,
                    "n_steps": 2048,
                    "intrinsic_reward_scale": 0.05,
                    "dropout": 0.08,
                    "l2_reg": 0.0002,
                    "gradient_penalty": 0.001,
                }
            ),
            ProgressiveStage(
                name="ğŸ† Expert (Mixed)",
                difficulty="all_difficulties",
                episodes=600,
                config={
                    "hidden_dim": 512,
                    "num_layers": 3,
                    "learning_rate": 0.00005,
                    "batch_size": 256,
                    "mini_batch_size": 64,
                    "ent_coef": 0.001,
                    "clip_range": 0.1,
                    "n_steps": 2048,
                    "intrinsic_reward_scale": 0.01,
                    "dropout": 0.05,
                    "l2_reg": 0.0003,
                    "gradient_penalty": 0.001,
                    "lr_warmup_steps": 50,
                }
            )
        ]
        
        # è®­ç»ƒå†å²
        self.training_history = []
        self.current_checkpoint = None
    
    def create_stage_config(self, stage: ProgressiveStage) -> Path:
        """åˆ›å»ºé˜¶æ®µé…ç½®æ–‡ä»¶"""
        # åˆå¹¶åŸºç¡€é…ç½®å’Œé˜¶æ®µé…ç½®
        full_config = {**self.base_config, **stage.config}
        
        # ä¿å­˜é…ç½®
        stage_dir = self.base_output_dir / f"stage_{len(self.training_history)+1}_{stage.difficulty}"
        stage_dir.mkdir(exist_ok=True)
        
        config_path = stage_dir / "config.json"
        with open(config_path, 'w') as f:
            json.dump(full_config, f, indent=2)
        
        return config_path, stage_dir
    
    def load_and_transfer_model(self, old_checkpoint_path: Path, new_config: Dict[str, Any]) -> Optional[Path]:
        """åŠ è½½æ—§æ¨¡å‹å¹¶è¿ç§»åˆ°æ–°æ¶æ„"""
        try:
            # åŠ è½½æ—§checkpoint
            old_checkpoint = torch.load(old_checkpoint_path, map_location='cpu')
            old_config = old_checkpoint.get('config', {})
            
            # æ£€æŸ¥æ˜¯å¦éœ€è¦æ¶æ„è¿ç§»
            architecture_changed = (
                old_config.get('hidden_dim') != new_config.get('hidden_dim') or
                old_config.get('num_layers') != new_config.get('num_layers') or
                old_config.get('num_heads') != new_config.get('num_heads')
            )
            
            if not architecture_changed:
                logger.info("âœ“ Architecture unchanged, direct resume possible")
                return old_checkpoint_path
            
            logger.info("\nğŸ”„ Architecture changed, performing intelligent transfer...")
            logger.info(f"  Old: hidden_dim={old_config.get('hidden_dim')}, layers={old_config.get('num_layers')}")
            logger.info(f"  New: hidden_dim={new_config.get('hidden_dim')}, layers={new_config.get('num_layers')}")
            
            # åˆ›å»ºä¸´æ—¶çš„è®­ç»ƒç®¡ç†å™¨æ¥æ„å»ºæ–°æ¨¡å‹
            sys.path.append(str(Path(__file__).parent))
            from unified_training_manager import UnifiedTrainingManager
            
            # åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶
            temp_config_path = self.base_output_dir / "temp_config.json"
            with open(temp_config_path, 'w') as f:
                json.dump(new_config, f)
            
            # åˆ›å»ºæ–°æ¶æ„çš„ç®¡ç†å™¨
            temp_manager = UnifiedTrainingManager(
                config_path=str(temp_config_path),
                algorithm=new_config.get('algorithm', 'ppo')
            )
            
            # è®¾ç½®ç¯å¢ƒä»¥è·å¾—æ­£ç¡®çš„æ¨¡å‹
            if not temp_manager.setup_environment():
                raise RuntimeError("Failed to setup environment for new model")
            
            # è·å–æ—§çš„state_dict
            old_state_dict = None
            if 'network_state_dict' in old_checkpoint:
                old_state_dict = old_checkpoint['network_state_dict']
            elif 'model_state_dict' in old_checkpoint:
                old_state_dict = old_checkpoint['model_state_dict']
            elif 'q_network_state_dict' in old_checkpoint:
                old_state_dict = old_checkpoint['q_network_state_dict']
            
            if old_state_dict is None:
                logger.error("Could not find model state dict in checkpoint!")
                return None
            
            # æ‰§è¡Œæ™ºèƒ½æƒé‡è¿ç§»
            new_model = temp_manager.trainer.network if hasattr(temp_manager.trainer, 'network') else temp_manager.trainer.q_network
            transferred_state_dict = self.weight_transfer_manager.intelligent_weight_transfer(
                old_state_dict,
                new_model
            )
            
            # åŠ è½½è¿ç§»åçš„æƒé‡
            new_model.load_state_dict(transferred_state_dict)
            
            # åˆ›å»ºæ–°çš„checkpoint
            new_checkpoint = {
                'algorithm': new_config.get('algorithm', 'ppo'),
                'state_dim': temp_manager.env.get_state_dim(),
                'action_dim': temp_manager.env.num_actions,
                'config': new_config,
                'episode': old_checkpoint.get('episode', 0),
                'timestamp': datetime.now().isoformat(),
                'transferred_from': str(old_checkpoint_path),
                'architecture_migration': True
            }
            
            # ä¿å­˜ç½‘ç»œçŠ¶æ€
            if hasattr(temp_manager.trainer, 'network'):
                new_checkpoint['network_state_dict'] = new_model.state_dict()
            else:
                new_checkpoint['q_network_state_dict'] = new_model.state_dict()
            
            # ä¿å­˜è®­ç»ƒå†å²ï¼ˆå¦‚æœæœ‰ï¼‰
            if 'training_history' in old_checkpoint:
                new_checkpoint['training_history'] = old_checkpoint['training_history']
            
            # ä¿å­˜è¿ç§»åçš„checkpoint
            migrated_checkpoint_path = self.base_output_dir / f"migrated_checkpoint_stage_{len(self.training_history)+1}.pt"
            torch.save(new_checkpoint, migrated_checkpoint_path)
            
            logger.info(f"âœ… Model successfully migrated and saved to {migrated_checkpoint_path}")
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            temp_config_path.unlink()
            
            return migrated_checkpoint_path
            
        except Exception as e:
            logger.error(f"âŒ Model migration failed: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def run_stage(self, stage: ProgressiveStage, resume_checkpoint: Optional[Path] = None) -> Tuple[bool, Optional[Path]]:
        """è¿è¡Œå•ä¸ªè®­ç»ƒé˜¶æ®µ"""
        stage.start_time = time.time()
        
        logger.info("\n" + "="*70)
        logger.info(f"ğŸš€ Starting {stage.name}")
        logger.info(f"ğŸ“Š Episodes: {stage.episodes}")
        logger.info(f"ğŸ§  Architecture: hidden_dim={stage.config['hidden_dim']}, layers={stage.config['num_layers']}")
        logger.info("="*70 + "\n")
        
        # åˆ›å»ºé˜¶æ®µé…ç½®
        config_path, stage_dir = self.create_stage_config(stage)
        
        # å¤„ç†æ¨¡å‹è¿ç§»ï¼ˆå¦‚æœéœ€è¦ï¼‰
        actual_resume_checkpoint = None
        if resume_checkpoint:
            migrated_checkpoint = self.load_and_transfer_model(resume_checkpoint, {**self.base_config, **stage.config})
            if migrated_checkpoint:
                actual_resume_checkpoint = migrated_checkpoint
            else:
                logger.warning("âš ï¸ Model migration failed, starting fresh for this stage")
        
        # å‡†å¤‡è®­ç»ƒå‘½ä»¤
        cmd = [
            sys.executable,  # ä½¿ç”¨å½“å‰Pythonè§£é‡Šå™¨
            "gpu_training_script.py",
            "--episodes", str(stage.episodes),
            "--config", str(config_path),
        ]
        
        # å¦‚æœæœ‰å¯ç”¨çš„checkpointï¼Œæ·»åŠ resumeå‚æ•°
        if actual_resume_checkpoint:
            cmd.extend(["--resume", "--checkpoint", str(actual_resume_checkpoint)])
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        env = os.environ.copy()
        
        # è®¾ç½®ä»»åŠ¡åº“è·¯å¾„
        if stage.difficulty == "all_difficulties":
            task_library = "mcp_generated_library/task_library_all_difficulties.json"
        else:
            task_library = f"mcp_generated_library/difficulty_versions/task_library_enhanced_v3_{stage.difficulty}.json"
        
        env['TASK_LIBRARY_PATH'] = task_library
        env['TRAINING_STAGE_DIR'] = str(stage_dir)
        
        # è¿è¡Œè®­ç»ƒ
        logger.info(f"ğŸ“ Executing: {' '.join(cmd)}")
        logger.info(f"ğŸ“š Task library: {task_library}")
        
        try:
            import subprocess
            result = subprocess.run(cmd, env=env, check=True)
            
            # æŸ¥æ‰¾ç”Ÿæˆçš„checkpoint
            checkpoint_pattern = stage_dir / "checkpoints" / "*.pt"
            checkpoints = list(Path(stage_dir).glob("checkpoints/*.pt"))
            
            if not checkpoints:
                # å°è¯•é»˜è®¤ä½ç½®
                default_checkpoint = Path("gpu_4070_training/checkpoints/final_gpu_model.pt")
                if default_checkpoint.exists():
                    checkpoints = [default_checkpoint]
            
            if checkpoints:
                # é€‰æ‹©æœ€æ–°çš„checkpoint
                latest_checkpoint = max(checkpoints, key=lambda p: p.stat().st_mtime)
                stage.end_time = time.time()
                stage.metrics['duration'] = stage.end_time - stage.start_time
                stage.metrics['success'] = True
                
                logger.info(f"âœ… Stage completed in {stage.metrics['duration']/60:.1f} minutes")
                return True, latest_checkpoint
            else:
                logger.error("âŒ No checkpoint found after training!")
                return False, None
                
        except subprocess.CalledProcessError as e:
            logger.error(f"âŒ Training failed with error code {e.returncode}")
            return False, None
        except Exception as e:
            logger.error(f"âŒ Unexpected error: {e}")
            import traceback
            traceback.print_exc()
            return False, None
    
    def run_progressive_training(self, start_stage: int = 0):
        """è¿è¡Œå®Œæ•´çš„æ¸è¿›å¼è®­ç»ƒ"""
        logger.info("\n" + "ğŸŒŸ"*30)
        logger.info("ğŸ¯ SMART PROGRESSIVE TRAINING")
        logger.info("ğŸ§  With Intelligent Weight Transfer")
        logger.info("ğŸŒŸ"*30 + "\n")
        
        total_start_time = time.time()
        current_checkpoint = self.current_checkpoint
        
        for i, stage in enumerate(self.stages[start_stage:], start=start_stage):
            success, checkpoint_path = self.run_stage(stage, current_checkpoint)
            
            if success:
                self.training_history.append({
                    'stage': i,
                    'name': stage.name,
                    'difficulty': stage.difficulty,
                    'episodes': stage.episodes,
                    'checkpoint': str(checkpoint_path),
                    'metrics': stage.metrics
                })
                current_checkpoint = checkpoint_path
                self.current_checkpoint = checkpoint_path
                
                # ä¿å­˜è®­ç»ƒå†å²
                self.save_training_history()
                
                # é˜¶æ®µé—´ä¼‘æ¯
                if i < len(self.stages) - 1:
                    logger.info("\nâ¸ï¸  Resting before next stage (10 seconds)...")
                    time.sleep(10)
            else:
                logger.error(f"âŒ Stage {stage.name} failed! Stopping training.")
                break
        
        # è®­ç»ƒæ€»ç»“
        total_time = time.time() - total_start_time
        total_episodes = sum(s.episodes for s in self.stages[:len(self.training_history)])
        
        logger.info("\n" + "="*70)
        logger.info("ğŸ“Š TRAINING SUMMARY")
        logger.info("="*70)
        logger.info(f"âœ… Completed stages: {len(self.training_history)}/{len(self.stages)}")
        logger.info(f"â±ï¸  Total time: {total_time/3600:.2f} hours")
        logger.info(f"ğŸ® Total episodes: {total_episodes}")
        
        if self.current_checkpoint:
            logger.info(f"ğŸ’¾ Final model: {self.current_checkpoint}")
        
        # åˆ†ææ¶æ„å˜åŒ–çš„å½±å“
        self.analyze_architecture_progression()
    
    def save_training_history(self):
        """ä¿å­˜è®­ç»ƒå†å²"""
        history_path = self.base_output_dir / "training_history.json"
        with open(history_path, 'w') as f:
            json.dump(self.training_history, f, indent=2)
    
    def analyze_architecture_progression(self):
        """åˆ†ææ¶æ„å˜åŒ–å¯¹æ€§èƒ½çš„å½±å“"""
        logger.info("\nğŸ“ˆ Architecture Progression Analysis:")
        
        for i, record in enumerate(self.training_history):
            stage_config = self.stages[i].config
            logger.info(f"\nStage {i+1}: {record['name']}")
            logger.info(f"  â€¢ Hidden dim: {stage_config.get('hidden_dim')}")
            logger.info(f"  â€¢ Layers: {stage_config.get('num_layers')}")
            logger.info(f"  â€¢ Duration: {record['metrics'].get('duration', 0)/60:.1f} min")
    
    def create_final_ensemble(self):
        """åˆ›å»ºæœ€ç»ˆçš„é›†æˆæ¨¡å‹ï¼ˆå¯é€‰ï¼‰"""
        logger.info("\nğŸ­ Creating ensemble from all stages...")
        # è¿™é‡Œå¯ä»¥å®ç°æ¨¡å‹é›†æˆé€»è¾‘
        pass


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="Smart Progressive Training with Architecture Evolution"
    )
    parser.add_argument(
        "--start-stage", 
        type=int, 
        default=0, 
        help="Start from specific stage (0-indexed)"
    )
    parser.add_argument(
        "--output-dir", 
        type=str, 
        default="smart_progressive_training",
        help="Output directory for all stages"
    )
    parser.add_argument(
        "--resume", 
        type=str, 
        help="Resume from specific checkpoint path"
    )
    parser.add_argument(
        "--test-transfer", 
        action="store_true",
        help="Test weight transfer without training"
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = SmartProgressiveTrainer(base_output_dir=args.output_dir)
    
    # å¦‚æœæŒ‡å®šäº†æ¢å¤checkpoint
    if args.resume:
        trainer.current_checkpoint = Path(args.resume)
        logger.info(f"ğŸ“‚ Resuming from checkpoint: {args.resume}")
    
    # æµ‹è¯•æ¨¡å¼
    if args.test_transfer:
        logger.info("ğŸ§ª Running in test mode - weight transfer only")
        # è¿™é‡Œå¯ä»¥æ·»åŠ æµ‹è¯•æƒé‡è¿ç§»çš„ä»£ç 
        return
    
    # è¿è¡Œæ¸è¿›å¼è®­ç»ƒ
    trainer.run_progressive_training(start_stage=args.start_stage)


if __name__ == "__main__":
    main()