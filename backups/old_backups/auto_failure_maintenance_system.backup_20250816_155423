#!/usr/bin/env python3
"""
è‡ªåŠ¨å¤±è´¥ç»´æŠ¤ç³»ç»Ÿ
- è‡ªåŠ¨æ£€æµ‹å’Œç»´æŠ¤å¤±è´¥è®°å½•
- æ™ºèƒ½é‡æµ‹ç­–ç•¥
- åŸºäºç°æœ‰è¿›åº¦çš„å¢é‡é‡æµ‹
- ä¸ç°æœ‰ç³»ç»Ÿæ— ç¼é›†æˆ
"""

import json
import os
import sys
import time
import subprocess
import threading
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Set, Tuple
from dataclasses import dataclass, asdict
from contextlib import contextmanager
from concurrent.futures import ThreadPoolExecutor
from collections import defaultdict

sys.path.insert(0, str(Path(__file__).parent))
from enhanced_failed_tests_manager import EnhancedFailedTestsManager
from enhanced_progress_manager import EnhancedProgressManager
from database_utils import load_database, get_completed_count, get_all_models_from_database, get_model_test_summary

@dataclass
class RetryStrategy:
    """é‡è¯•ç­–ç•¥é…ç½®"""
    max_retries: int = 3
    retry_delay: int = 300  # 5åˆ†é’Ÿ
    backoff_multiplier: float = 1.5
    priority_models: List[str] = None
    skip_models: List[str] = None
    retry_on_timeout: bool = True
    retry_on_api_error: bool = True
    retry_on_format_error: bool = False
    
    def __post_init__(self):
        if self.priority_models is None:
            self.priority_models = []
        if self.skip_models is None:
            self.skip_models = []

@dataclass
class AutoRetestConfig:
    """è‡ªåŠ¨é‡æµ‹é…ç½®"""
    enabled: bool = True
    include_partial_failures: bool = True
    retest_timeout_failures: bool = True
    retest_api_failures: bool = True
    retest_format_failures: bool = False
    # æ–°çš„é‡æµ‹é€»è¾‘ï¼šåŸºäºå®Œå…¨å¤±è´¥ + å¹³å‡æ‰§è¡Œæ—¶é—´è¾¾åˆ°ä¸Šé™
    complete_failure_threshold: float = 0.0  # æˆåŠŸç‡ä¸º0ï¼ˆå®Œå…¨å¤±è´¥ï¼‰
    max_execution_time: float = 300.0        # æœ€å¤§æ‰§è¡Œæ—¶é—´é˜ˆå€¼ï¼ˆç§’ï¼‰
    cooldown_hours: int = 2                  # é‡æµ‹å†·å´æœŸ
    batch_size: int = 5                      # é‡æµ‹æ‰¹æ¬¡å¤§å°
    # ä¿ç•™æ—§é…ç½®ä»¥å‘åå…¼å®¹ï¼ˆå·²å¼ƒç”¨ï¼‰
    minimum_completion_rate: float = 0.8     # å·²å¼ƒç”¨
    maximum_failure_rate: float = 0.3        # å·²å¼ƒç”¨
    
class AutoFailureMaintenanceSystem:
    """è‡ªåŠ¨å¤±è´¥ç»´æŠ¤ç³»ç»Ÿ"""
    
    def __init__(self, 
                 config_file: str = "auto_maintenance_config.json",
                 enable_auto_retry: bool = True):
        self.config_file = Path(config_file)
        self.failed_manager = EnhancedFailedTestsManager()
        self.progress_manager = EnhancedProgressManager()
        self.enable_auto_retry = enable_auto_retry
        self.lock = threading.Lock()
        self._load_config()
        
        # ç›‘æ§çº¿ç¨‹
        self._monitoring = False
        self._monitor_thread = None
        
    def _load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if self.config_file.exists():
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    config_data = json.load(f)
                self.retry_strategy = RetryStrategy(**config_data.get("retry_strategy", {}))
                self.auto_retest_config = AutoRetestConfig(**config_data.get("auto_retest", {}))
            except Exception as e:
                print(f"åŠ è½½é…ç½®å¤±è´¥: {e}")
                self._create_default_config()
        else:
            self._create_default_config()
    
    def _create_default_config(self):
        """åˆ›å»ºé»˜è®¤é…ç½®"""
        self.retry_strategy = RetryStrategy()
        self.auto_retest_config = AutoRetestConfig()
        self._save_config()
    
    def _save_config(self):
        """ä¿å­˜é…ç½®æ–‡ä»¶"""
        config_data = {
            "version": "1.0",
            "last_updated": datetime.now().isoformat(),
            "retry_strategy": asdict(self.retry_strategy),
            "auto_retest": asdict(self.auto_retest_config)
        }
        
        with open(self.config_file, 'w', encoding='utf-8') as f:
            json.dump(config_data, f, indent=2, ensure_ascii=False)
    
    def analyze_test_completion(self, models: List[str] = None) -> Dict[str, Any]:
        """åˆ†ææµ‹è¯•å®Œæˆæƒ…å†µ"""
        db = load_database()
        analysis = {
            "timestamp": datetime.now().isoformat(),
            "models_analyzed": [],
            "completion_summary": {},
            "failure_patterns": {},
            "retry_recommendations": []
        }
        
        if not models:
            models = list(db.get("models", {}).keys())
        
        for model in models:
            model_analysis = self._analyze_model_completion(model, db)
            analysis["models_analyzed"].append(model)
            analysis["completion_summary"][model] = model_analysis
            
            # æ£€æµ‹å¤±è´¥æ¨¡å¼
            failure_pattern = self._detect_failure_patterns(model, model_analysis)
            if failure_pattern:
                analysis["failure_patterns"][model] = failure_pattern
            
            # ç”Ÿæˆé‡è¯•å»ºè®®
            retry_rec = self._generate_retry_recommendation(model, model_analysis)
            if retry_rec:
                analysis["retry_recommendations"].append(retry_rec)
        
        return analysis
    
    def _analyze_model_completion(self, model: str, db: Dict) -> Dict[str, Any]:
        """åˆ†æå•ä¸ªæ¨¡å‹çš„å®Œæˆæƒ…å†µ"""
        model_data = db.get("models", {}).get(model, {})
        
        if not model_data:
            return {"status": "no_data", "completion_rate": 0.0}
        
        # ç»Ÿè®¡å„ä¸ªç»´åº¦çš„å®Œæˆæƒ…å†µå’Œæ‰§è¡Œæ—¶é—´
        total_tests = 0
        completed_tests = 0
        failed_tests = 0
        total_execution_time = 0.0
        execution_time_count = 0
        complete_failure_configs = []
        high_execution_time_configs = []
        
        # V3æ•°æ®åº“ç»“æ„åˆ†æ
        if "by_prompt_type" in model_data:
            for prompt_type, prompt_data in model_data["by_prompt_type"].items():
                if "by_tool_success_rate" in prompt_data:
                    for rate, rate_data in prompt_data["by_tool_success_rate"].items():
                        if "by_difficulty" in rate_data:
                            for difficulty, diff_data in rate_data["by_difficulty"].items():
                                if "by_task_type" in diff_data:
                                    for task_type, task_data in diff_data["by_task_type"].items():
                                        total = task_data.get("total", 0)
                                        success = task_data.get("success", 0)
                                        avg_exec_time = task_data.get("avg_execution_time", 0.0)
                                        
                                        total_tests += total
                                        completed_tests += success
                                        failed_tests += (total - success)
                                        
                                        # ç»Ÿè®¡æ‰§è¡Œæ—¶é—´
                                        if avg_exec_time > 0:
                                            total_execution_time += avg_exec_time * total
                                            execution_time_count += total
                                        
                                        # æ£€æŸ¥å®Œå…¨å¤±è´¥ï¼ˆæˆåŠŸç‡ä¸º0ï¼‰
                                        if total > 0 and success == 0:
                                            config_id = f"{prompt_type}/{rate}/{difficulty}/{task_type}"
                                            complete_failure_configs.append({
                                                "config": config_id,
                                                "total_tests": total,
                                                "avg_execution_time": avg_exec_time
                                            })
                                        
                                        # æ£€æŸ¥é«˜æ‰§è¡Œæ—¶é—´
                                        if avg_exec_time >= self.auto_retest_config.max_execution_time:
                                            config_id = f"{prompt_type}/{rate}/{difficulty}/{task_type}"
                                            high_execution_time_configs.append({
                                                "config": config_id,
                                                "total_tests": total,
                                                "success_tests": success,
                                                "avg_execution_time": avg_exec_time
                                            })
        
        completion_rate = completed_tests / total_tests if total_tests > 0 else 0.0
        failure_rate = failed_tests / total_tests if total_tests > 0 else 0.0
        avg_execution_time = total_execution_time / execution_time_count if execution_time_count > 0 else 0.0
        
        # æ–°çš„é‡æµ‹é€»è¾‘ï¼šåŸºäºå®Œå…¨å¤±è´¥ + å¹³å‡æ‰§è¡Œæ—¶é—´è¾¾åˆ°ä¸Šé™
        needs_retry_complete_failure = len(complete_failure_configs) > 0
        needs_retry_high_execution_time = avg_execution_time >= self.auto_retest_config.max_execution_time
        needs_retry = needs_retry_complete_failure or needs_retry_high_execution_time
        
        return {
            "status": "analyzed",
            "total_tests": total_tests,
            "completed_tests": completed_tests,
            "failed_tests": failed_tests,
            "completion_rate": completion_rate,
            "failure_rate": failure_rate,
            "avg_execution_time": avg_execution_time,
            "complete_failure_configs": complete_failure_configs,
            "high_execution_time_configs": high_execution_time_configs,
            "needs_retry": needs_retry,
            "needs_retry_complete_failure": needs_retry_complete_failure,
            "needs_retry_high_execution_time": needs_retry_high_execution_time,
            # ä¿ç•™æ—§çš„é€»è¾‘ä»¥å‘åå…¼å®¹
            "needs_retry_legacy": completion_rate < self.auto_retest_config.minimum_completion_rate or 
                                 failure_rate > self.auto_retest_config.maximum_failure_rate
        }
    
    def _detect_failure_patterns(self, model: str, analysis: Dict) -> Optional[Dict]:
        """æ£€æµ‹å¤±è´¥æ¨¡å¼ï¼ˆæ–°é€»è¾‘ï¼šåŸºäºå®Œå…¨å¤±è´¥+é«˜æ‰§è¡Œæ—¶é—´ï¼‰"""
        if analysis.get("status") != "analyzed":
            return None
        
        patterns = {}
        
        # å®Œå…¨å¤±è´¥æ¨¡å¼
        if analysis.get("needs_retry_complete_failure", False):
            patterns["complete_failure"] = {
                "description": "å­˜åœ¨å®Œå…¨å¤±è´¥çš„é…ç½®ï¼ˆæˆåŠŸç‡ä¸º0ï¼‰",
                "configs": analysis.get("complete_failure_configs", []),
                "severity": "high",
                "recommendation": "éœ€è¦é‡æµ‹å®Œå…¨å¤±è´¥çš„é…ç½®"
            }
        
        # é«˜æ‰§è¡Œæ—¶é—´æ¨¡å¼
        if analysis.get("needs_retry_high_execution_time", False):
            patterns["high_execution_time"] = {
                "description": f"å¹³å‡æ‰§è¡Œæ—¶é—´è¶…è¿‡é˜ˆå€¼ï¼ˆ{self.auto_retest_config.max_execution_time}ç§’ï¼‰",
                "avg_execution_time": analysis.get("avg_execution_time", 0),
                "configs": analysis.get("high_execution_time_configs", []),
                "severity": "medium",
                "recommendation": "å¯èƒ½å­˜åœ¨æ€§èƒ½é—®é¢˜æˆ–è¶…æ—¶é—®é¢˜ï¼Œå»ºè®®é‡æµ‹"
            }
        
        # ä¿ç•™æ—§çš„å¤±è´¥ç‡æ£€æŸ¥ä»¥å‘åå…¼å®¹
        if analysis["failure_rate"] > self.auto_retest_config.maximum_failure_rate:
            patterns["high_failure_rate_legacy"] = {
                "severity": "high",
                "failure_rate": analysis["failure_rate"],
                "threshold": self.auto_retest_config.maximum_failure_rate,
                "description": "é—ç•™é€»è¾‘ï¼šé«˜å¤±è´¥ç‡æ¨¡å¼ï¼ˆå·²å¼ƒç”¨ï¼‰",
                "severity": "low"
            }
        
        # ä½å®Œæˆç‡æ¨¡å¼
        if analysis["completion_rate"] < self.auto_retest_config.minimum_completion_rate:
            patterns["low_completion_rate"] = {
                "severity": "medium",
                "completion_rate": analysis["completion_rate"],
                "threshold": self.auto_retest_config.minimum_completion_rate
            }
        
        return patterns if patterns else None
    
    def _generate_retry_recommendation(self, model: str, analysis: Dict) -> Optional[Dict]:
        """ç”Ÿæˆé‡è¯•å»ºè®®"""
        if not analysis.get("needs_retry"):
            return None
        
        recommendation = {
            "model": model,
            "priority": "normal",
            "reason": [],
            "suggested_configs": []
        }
        
        # ä¼˜å…ˆçº§åˆ¤æ–­
        if model in self.retry_strategy.priority_models:
            recommendation["priority"] = "high"
        
        # åˆ¤æ–­é‡è¯•åŸå› 
        if analysis["completion_rate"] < self.auto_retest_config.minimum_completion_rate:
            recommendation["reason"].append(f"ä½å®Œæˆç‡: {analysis['completion_rate']:.1%}")
        
        if analysis["failure_rate"] > self.auto_retest_config.maximum_failure_rate:
            recommendation["reason"].append(f"é«˜å¤±è´¥ç‡: {analysis['failure_rate']:.1%}")
        
        # å»ºè®®é…ç½®
        recommendation["suggested_configs"] = self._suggest_retry_configs(model, analysis)
        
        return recommendation
    
    def _suggest_retry_configs(self, model: str, analysis: Dict) -> List[Dict]:
        """å»ºè®®é‡è¯•é…ç½®"""
        configs = []
        
        # åŸºç¡€é‡è¯•é…ç½®
        base_config = {
            "model": model,
            "prompt_types": "baseline",
            "difficulty": "easy",
            "task_types": "all",
            "num_instances": 20,
            "tool_success_rate": 0.8
        }
        
        # å¦‚æœå¤±è´¥ç‡å¾ˆé«˜ï¼Œé™ä½éš¾åº¦
        if analysis["failure_rate"] > 0.5:
            base_config["difficulty"] = "easy"
            base_config["num_instances"] = 10
        
        configs.append(base_config)
        
        # å¦‚æœæ˜¯é‡è¦æ¨¡å‹ï¼Œæ·»åŠ æ›´å…¨é¢çš„æµ‹è¯•
        if model in self.retry_strategy.priority_models:
            comprehensive_config = base_config.copy()
            comprehensive_config.update({
                "prompt_types": "baseline,cot,optimal",
                "num_instances": 50
            })
            configs.append(comprehensive_config)
        
        return configs
    
    def get_incomplete_tests(self, models: List[str] = None) -> Dict[str, List[Dict]]:
        """è·å–æœªå®Œæˆçš„æµ‹è¯•é…ç½®"""
        incomplete_tests = defaultdict(list)
        
        if not models:
            db = load_database()
            models = list(db.get("models", {}).keys())
        
        # æ ‡å‡†æµ‹è¯•é…ç½®
        standard_configs = [
            {"prompt_type": "baseline", "difficulty": "easy", "task_type": "simple_task", "num_instances": 20},
            {"prompt_type": "baseline", "difficulty": "easy", "task_type": "basic_task", "num_instances": 20},
            {"prompt_type": "cot", "difficulty": "easy", "task_type": "simple_task", "num_instances": 20},
            {"prompt_type": "optimal", "difficulty": "easy", "task_type": "simple_task", "num_instances": 20},
        ]
        
        for model in models:
            for config in standard_configs:
                completed = get_completed_count(
                    model=model,
                    prompt_type=config["prompt_type"],
                    difficulty=config["difficulty"],
                    task_type=config["task_type"]
                )
                
                if completed < config["num_instances"]:
                    missing = config["num_instances"] - completed
                    test_config = config.copy()
                    test_config.update({
                        "model": model,
                        "missing_count": missing,
                        "completed_count": completed
                    })
                    incomplete_tests[model].append(test_config)
        
        return dict(incomplete_tests)
    
    def auto_maintain_failures(self) -> Dict[str, Any]:
        """è‡ªåŠ¨ç»´æŠ¤å¤±è´¥è®°å½•"""
        if not self.auto_retest_config.enabled:
            return {"status": "disabled"}
        
        print("ğŸ”§ å¼€å§‹è‡ªåŠ¨å¤±è´¥ç»´æŠ¤...")
        
        # åˆ†æå½“å‰çŠ¶æ€
        analysis = self.analyze_test_completion()
        
        # è·å–æœªå®Œæˆæµ‹è¯•
        incomplete_tests = self.get_incomplete_tests()
        
        # ç”Ÿæˆç»´æŠ¤è®¡åˆ’
        maintenance_plan = {
            "timestamp": datetime.now().isoformat(),
            "analysis_summary": {
                "models_analyzed": len(analysis["models_analyzed"]),
                "models_with_failures": len(analysis["failure_patterns"]),
                "retry_recommendations": len(analysis["retry_recommendations"])
            },
            "actions_taken": [],
            "incomplete_tests": incomplete_tests
        }
        
        # æ‰§è¡Œè‡ªåŠ¨é‡è¯•ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.enable_auto_retry and analysis["retry_recommendations"]:
            retry_results = self._execute_auto_retries(analysis["retry_recommendations"])
            maintenance_plan["actions_taken"].extend(retry_results)
        
        # æ›´æ–°å¤±è´¥è®°å½•
        self._update_failure_records(analysis, incomplete_tests)
        
        print(f"âœ… è‡ªåŠ¨ç»´æŠ¤å®Œæˆï¼Œåˆ†æäº† {len(analysis['models_analyzed'])} ä¸ªæ¨¡å‹")
        if analysis["retry_recommendations"]:
            print(f"ğŸ“‹ ç”Ÿæˆäº† {len(analysis['retry_recommendations'])} ä¸ªé‡è¯•å»ºè®®")
        
        return maintenance_plan
    
    def _execute_auto_retries(self, recommendations: List[Dict]) -> List[Dict]:
        """æ‰§è¡Œè‡ªåŠ¨é‡è¯•"""
        results = []
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        recommendations.sort(key=lambda x: 0 if x["priority"] == "high" else 1)
        
        for rec in recommendations[:self.auto_retest_config.batch_size]:
            if rec["model"] in self.retry_strategy.skip_models:
                continue
            
            for config in rec["suggested_configs"]:
                result = self._execute_single_retry(config)
                results.append(result)
                
                # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡è½½
                time.sleep(self.retry_strategy.retry_delay)
        
        return results
    
    def _execute_single_retry(self, config: Dict) -> Dict:
        """æ‰§è¡Œå•ä¸ªé‡è¯•"""
        print(f"ğŸ”„ æ‰§è¡Œé‡è¯•: {config['model']}")
        
        # æ„å»ºå‘½ä»¤
        cmd = [
            "python", "smart_batch_runner.py",
            "--model", config["model"],
            "--prompt-types", config["prompt_types"],
            "--difficulty", config["difficulty"],
            "--task-types", config["task_types"],
            "--num-instances", str(config["num_instances"]),
            "--tool-success-rate", str(config.get("tool_success_rate", 0.8)),
            "--max-workers", "1",
            "--no-save-logs"
        ]
        
        start_time = datetime.now()
        try:
            # è®°å½•é‡è¯•å¼€å§‹
            self.failed_manager.mark_test_for_retry(
                config["model"], 
                f"{config['prompt_types']}_{config['difficulty']}"
            )
            
            # æ‰§è¡Œæµ‹è¯•
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=1800)
            
            success = result.returncode == 0
            end_time = datetime.now()
            duration = (end_time - start_time).total_seconds()
            
            # è®°å½•ç»“æœ
            if success:
                self.failed_manager.record_test_success(
                    config["model"],
                    f"{config['prompt_types']}_{config['difficulty']}",
                    was_retry=True
                )
            
            return {
                "model": config["model"],
                "config": config,
                "success": success,
                "duration": duration,
                "stdout": result.stdout[-500:] if result.stdout else "",
                "stderr": result.stderr[-500:] if result.stderr else ""
            }
            
        except subprocess.TimeoutExpired:
            return {
                "model": config["model"],
                "config": config,
                "success": False,
                "duration": 1800,
                "error": "timeout"
            }
        except Exception as e:
            return {
                "model": config["model"],
                "config": config,
                "success": False,
                "duration": 0,
                "error": str(e)
            }
    
    def _update_failure_records(self, analysis: Dict, incomplete_tests: Dict):
        """æ›´æ–°å¤±è´¥è®°å½•"""
        for model, patterns in analysis["failure_patterns"].items():
            for pattern_type, pattern_data in patterns.items():
                self.failed_manager.record_test_failure(
                    model=model,
                    group_name=f"auto_detected_{pattern_type}",
                    prompt_types="multiple",
                    test_type="pattern_detection",
                    failure_reason=f"{pattern_type}: {pattern_data}",
                    context={"analysis": analysis, "incomplete": incomplete_tests.get(model, [])}
                )
    
    def generate_retest_script(self, models: List[str] = None, 
                             output_file: str = "auto_retest_incomplete.sh") -> str:
        """ç”Ÿæˆé‡æµ‹è„šæœ¬"""
        incomplete_tests = self.get_incomplete_tests(models)
        
        if not incomplete_tests:
            print("æ²¡æœ‰æœªå®Œæˆçš„æµ‹è¯•éœ€è¦é‡æµ‹")
            return None
        
        script_content = f'''#!/bin/bash

# è‡ªåŠ¨ç”Ÿæˆçš„é‡æµ‹è„šæœ¬ - åŸºäºç°æœ‰è¿›åº¦
# ç”Ÿæˆæ—¶é—´: {datetime.now().isoformat()}
# æ¶‰åŠæ¨¡å‹: {len(incomplete_tests)}

set -e

echo "å¼€å§‹åŸºäºç°æœ‰è¿›åº¦çš„è‡ªåŠ¨é‡æµ‹..."
echo "æ¶‰åŠ {len(incomplete_tests)} ä¸ªæ¨¡å‹"
echo ""

'''
        
        test_count = 0
        for model, tests in incomplete_tests.items():
            if model in self.retry_strategy.skip_models:
                continue
            
            script_content += f'''
echo "=== å¤„ç†æ¨¡å‹: {model} ==="
echo "æœªå®Œæˆæµ‹è¯•: {len(tests)} ä¸ªé…ç½®"
'''
            
            for test_config in tests:
                test_count += 1
                missing = test_config["missing_count"]
                
                script_content += f'''
echo "  é…ç½® {test_count}: {test_config['prompt_type']} / {test_config['task_type']}"
echo "    éœ€è¦è¡¥å……: {missing} ä¸ªæµ‹è¯•"

python smart_batch_runner.py \\
    --model "{model}" \\
    --prompt-types "{test_config['prompt_type']}" \\
    --difficulty "{test_config['difficulty']}" \\
    --task-types "{test_config['task_type']}" \\
    --num-instances {missing} \\
    --tool-success-rate 0.8 \\
    --max-workers 2 \\
    --adaptive \\
    --no-save-logs

echo ""
'''
        
        script_content += f'''
echo "é‡æµ‹å®Œæˆï¼"
echo "æ€»å…±å¤„ç†äº† {test_count} ä¸ªæµ‹è¯•é…ç½®"

# ç”ŸæˆçŠ¶æ€æŠ¥å‘Š
echo ""
echo "=== æœ€ç»ˆçŠ¶æ€æŠ¥å‘Š ==="
python auto_failure_maintenance_system.py status
'''
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(script_content)
        
        os.chmod(output_file, 0o755)
        print(f"âœ… ç”Ÿæˆé‡æµ‹è„šæœ¬: {output_file}")
        print(f"ğŸ“Š æ¶‰åŠ {len(incomplete_tests)} ä¸ªæ¨¡å‹ï¼Œ{test_count} ä¸ªæµ‹è¯•é…ç½®")
        
        return output_file
    
    def start_monitoring(self, interval: int = 3600):
        """å¯åŠ¨è‡ªåŠ¨ç›‘æ§"""
        if self._monitoring:
            print("ç›‘æ§å·²ç»åœ¨è¿è¡Œ")
            return
        
        self._monitoring = True
        self._monitor_thread = threading.Thread(
            target=self._monitoring_loop,
            args=(interval,),
            daemon=True
        )
        self._monitor_thread.start()
        print(f"ğŸ” å¯åŠ¨è‡ªåŠ¨ç›‘æ§ï¼Œé—´éš” {interval//60} åˆ†é’Ÿ")
    
    def stop_monitoring(self):
        """åœæ­¢è‡ªåŠ¨ç›‘æ§"""
        self._monitoring = False
        if self._monitor_thread:
            self._monitor_thread.join(timeout=5)
        print("ğŸ›‘ åœæ­¢è‡ªåŠ¨ç›‘æ§")
    
    def _monitoring_loop(self, interval: int):
        """ç›‘æ§å¾ªç¯"""
        while self._monitoring:
            try:
                print(f"ğŸ” æ‰§è¡Œå®šæœŸæ£€æŸ¥ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                maintenance_result = self.auto_maintain_failures()
                
                # å¦‚æœå‘ç°éœ€è¦å¤„ç†çš„é—®é¢˜ï¼Œç”ŸæˆæŠ¥å‘Š
                if maintenance_result.get("retry_recommendations"):
                    self._generate_monitoring_report(maintenance_result)
                
            except Exception as e:
                print(f"ç›‘æ§æ£€æŸ¥å‡ºé”™: {e}")
            
            time.sleep(interval)
    
    def _generate_monitoring_report(self, maintenance_result: Dict):
        """ç”Ÿæˆç›‘æ§æŠ¥å‘Š"""
        report_file = f"auto_maintenance_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(maintenance_result, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“‹ ç”Ÿæˆç›‘æ§æŠ¥å‘Š: {report_file}")
    
    def show_status(self):
        """æ˜¾ç¤ºç³»ç»ŸçŠ¶æ€"""
        print("=" * 60)
        print("ğŸ”§ è‡ªåŠ¨å¤±è´¥ç»´æŠ¤ç³»ç»ŸçŠ¶æ€")
        print("=" * 60)
        
        # æ˜¾ç¤ºé…ç½®
        print("âš™ï¸  é…ç½®ä¿¡æ¯:")
        print(f"  è‡ªåŠ¨é‡è¯•: {'å¯ç”¨' if self.enable_auto_retry else 'ç¦ç”¨'}")
        print(f"  è‡ªåŠ¨é‡æµ‹: {'å¯ç”¨' if self.auto_retest_config.enabled else 'ç¦ç”¨'}")
        print(f"  æœ€å°å®Œæˆç‡é˜ˆå€¼: {self.auto_retest_config.minimum_completion_rate:.1%}")
        print(f"  æœ€å¤§å¤±è´¥ç‡é˜ˆå€¼: {self.auto_retest_config.maximum_failure_rate:.1%}")
        print(f"  é‡è¯•æ‰¹æ¬¡å¤§å°: {self.auto_retest_config.batch_size}")
        print()
        
        # åˆ†æå½“å‰çŠ¶æ€
        analysis = self.analyze_test_completion()
        print("ğŸ“Š å½“å‰çŠ¶æ€åˆ†æ:")
        print(f"  å·²åˆ†ææ¨¡å‹: {len(analysis['models_analyzed'])}")
        print(f"  å‘ç°å¤±è´¥æ¨¡å¼: {len(analysis['failure_patterns'])}")
        print(f"  é‡è¯•å»ºè®®: {len(analysis['retry_recommendations'])}")
        print()
        
        # æ˜¾ç¤ºè¯¦ç»†åˆ†æ
        if analysis["failure_patterns"]:
            print("ğŸ”´ æ£€æµ‹åˆ°çš„å¤±è´¥æ¨¡å¼:")
            for model, patterns in analysis["failure_patterns"].items():
                print(f"  {model}:")
                for pattern_type, data in patterns.items():
                    print(f"    - {pattern_type}: {data}")
        
        # æ˜¾ç¤ºé‡è¯•å»ºè®®
        if analysis["retry_recommendations"]:
            print()
            print("ğŸ“‹ é‡è¯•å»ºè®®:")
            for rec in analysis["retry_recommendations"]:
                print(f"  {rec['model']} ({rec['priority']}): {', '.join(rec['reason'])}")
        
        # æ˜¾ç¤ºæœªå®Œæˆæµ‹è¯•
        incomplete = self.get_incomplete_tests()
        if incomplete:
            print()
            print("â³ æœªå®Œæˆçš„æµ‹è¯•:")
            for model, tests in incomplete.items():
                total_missing = sum(t["missing_count"] for t in tests)
                print(f"  {model}: {len(tests)} ä¸ªé…ç½®ï¼Œç¼ºå°‘ {total_missing} ä¸ªæµ‹è¯•")
        
        print()
        print("ğŸ’¡ å¯ç”¨æ“ä½œ:")
        print("  python auto_failure_maintenance_system.py maintain   # æ‰§è¡Œè‡ªåŠ¨ç»´æŠ¤")
        print("  python auto_failure_maintenance_system.py retest    # ç”Ÿæˆé‡æµ‹è„šæœ¬") 
        print("  python auto_failure_maintenance_system.py monitor   # å¯åŠ¨ç›‘æ§")

def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    import sys
    
    system = AutoFailureMaintenanceSystem()
    
    if len(sys.argv) < 2:
        system.show_status()
        return
    
    command = sys.argv[1]
    
    if command == "status":
        system.show_status()
    elif command == "maintain":
        result = system.auto_maintain_failures()
        print(f"ç»´æŠ¤å®Œæˆï¼Œå¤„ç†äº† {len(result.get('actions_taken', []))} ä¸ªæ“ä½œ")
    elif command == "retest":
        models = sys.argv[2:] if len(sys.argv) > 2 else None
        script = system.generate_retest_script(models)
        if script:
            print(f"è¿è¡Œé‡æµ‹è„šæœ¬: ./{script}")
    elif command == "monitor":
        try:
            system.start_monitoring()
            print("ç›‘æ§å·²å¯åŠ¨ï¼ŒæŒ‰ Ctrl+C åœæ­¢")
            while True:
                time.sleep(1)
        except KeyboardInterrupt:
            system.stop_monitoring()
    elif command == "incomplete":
        incomplete = system.get_incomplete_tests()
        for model, tests in incomplete.items():
            total_missing = sum(t["missing_count"] for t in tests)
            print(f"{model}: {len(tests)} ä¸ªé…ç½®ï¼Œç¼ºå°‘ {total_missing} ä¸ªæµ‹è¯•")
    else:
        print(f"æœªçŸ¥å‘½ä»¤: {command}")
        print("å¯ç”¨å‘½ä»¤: status, maintain, retest, monitor, incomplete")

if __name__ == "__main__":
    main()