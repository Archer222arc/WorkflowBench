#!/usr/bin/env python3
"""
Provider-Aware Parallel Batch Runner
=====================================
åŸºäºAPIæä¾›å•†çº§åˆ«é€Ÿç‡é™åˆ¶çš„å¹¶è¡Œæ‰¹æµ‹è¯•è¿è¡Œå™¨
æ ¹æ®æµ‹è¯•å‘ç°ï¼Œé€Ÿç‡é™åˆ¶æ˜¯æä¾›å•†çº§åˆ«è€Œéæ¨¡å‹çº§åˆ«
"""

import json
import sys
import time
from pathlib import Path
from typing import List, Dict, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
from collections import defaultdict
from datetime import datetime
import threading

sys.path.insert(0, str(Path(__file__).parent))

from batch_test_runner import BatchTestRunner, TestTask
from enhanced_cumulative_manager import EnhancedCumulativeManager
from cumulative_test_manager import TestRecord
from api_client_manager import APIClientManager, MODEL_PROVIDER_MAP


class ProviderParallelRunner:
    """åŸºäºæä¾›å•†çš„å¹¶è¡Œæµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self, debug: bool = False, silent: bool = False, 
                 save_logs: bool = True, use_ai_classification: bool = False):
        """
        åˆå§‹åŒ–è¿è¡Œå™¨
        
        Args:
            debug: è°ƒè¯•æ¨¡å¼
            silent: é™é»˜æ¨¡å¼
            save_logs: æ˜¯å¦ä¿å­˜æ—¥å¿—
            use_ai_classification: æ˜¯å¦ä½¿ç”¨AIåˆ†ç±»
        """
        self.debug = debug
        self.silent = silent
        self.save_logs = save_logs
        self.use_ai_classification = use_ai_classification
        
        # æä¾›å•†é…ç½®
        self.provider_configs = {
            'azure': {
                'models': ['gpt-4o-mini'],
                'max_parallel': 50,  # Azureæ”¯æŒé«˜å¹¶å‘
                'initial_qps': 100.0,
                'description': 'Azure OpenAI (é«˜æ€§èƒ½)'
            },
            'user_azure': {
                'models': ['gpt-5-nano', 'DeepSeek-R1-0528', 'DeepSeek-V3-0324',
                          'gpt-5-mini', 'gpt-oss-120b', 'grok-3', 'Llama-3.3-70B-Instruct'],
                'max_parallel': 30,  # ç”¨æˆ·Azureç«¯ç‚¹
                'initial_qps': 50.0,
                'description': 'ç”¨æˆ·Azureç«¯ç‚¹'
            },
            'idealab': {
                'models': [],  # å°†ä»MODEL_PROVIDER_MAPåŠ¨æ€å¡«å……
                'max_parallel': 5,  # IdealLabé™åˆ¶æ›´ä¸¥æ ¼
                'initial_qps': 10.0,
                'description': 'IdealLab API (å…±äº«é€Ÿç‡é™åˆ¶)'
            }
        }
        
        # åŠ¨æ€å¡«å……IdealLabæ¨¡å‹åˆ—è¡¨
        for model, provider in MODEL_PROVIDER_MAP.items():
            if provider == 'idealab' and model not in self.provider_configs['idealab']['models']:
                self.provider_configs['idealab']['models'].append(model)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_tests': 0,
            'total_success': 0,
            'total_failed': 0,
            'provider_stats': defaultdict(lambda: {'tests': 0, 'success': 0, 'failed': 0, 'time': 0})
        }
        
        # é”ç”¨äºçº¿ç¨‹å®‰å…¨
        self.stats_lock = threading.Lock()
    
    def group_tasks_by_provider(self, tasks: List[TestTask]) -> Dict[str, List[TestTask]]:
        """
        å°†ä»»åŠ¡æŒ‰æä¾›å•†åˆ†ç»„
        
        Args:
            tasks: æµ‹è¯•ä»»åŠ¡åˆ—è¡¨
            
        Returns:
            æŒ‰æä¾›å•†åˆ†ç»„çš„ä»»åŠ¡å­—å…¸
        """
        provider_tasks = defaultdict(list)
        
        for task in tasks:
            model = task.model
            provider = MODEL_PROVIDER_MAP.get(model, 'idealab')
            provider_tasks[provider].append(task)
        
        return dict(provider_tasks)
    
    def run_provider_batch(self, provider: str, tasks: List[TestTask], 
                           max_workers: int = None, qps: float = None) -> List[Dict]:
        """
        è¿è¡Œå•ä¸ªæä¾›å•†çš„æ‰¹é‡æµ‹è¯•
        
        Args:
            provider: æä¾›å•†åç§°
            tasks: è¯¥æä¾›å•†çš„ä»»åŠ¡åˆ—è¡¨
            max_workers: æœ€å¤§å¹¶å‘æ•°
            qps: QPSé™åˆ¶
            
        Returns:
            æµ‹è¯•ç»“æœåˆ—è¡¨
        """
        config = self.provider_configs.get(provider, {})
        max_workers = max_workers or config.get('max_parallel', 10)
        qps = qps or config.get('initial_qps', 20.0)
        
        print(f"\n{'='*60}")
        print(f"ğŸš€ å¼€å§‹ {provider} æä¾›å•†æµ‹è¯•")
        print(f"   æ¨¡å‹: {', '.join(set(t.model for t in tasks))}")
        print(f"   ä»»åŠ¡æ•°: {len(tasks)}")
        print(f"   å¹¶å‘: {max_workers}, QPS: {qps}")
        print(f"{'='*60}")
        
        start_time = time.time()
        
        # åˆ›å»ºBatchTestRunnerå®ä¾‹
        runner = BatchTestRunner(
            debug=self.debug,
            silent=self.silent,
            save_logs=self.save_logs,
            use_adaptive=True,  # ä½¿ç”¨è‡ªé€‚åº”æ¨¡å¼
            enable_database_updates=False,  # æ‰¹é‡æ¨¡å¼ï¼Œç¨åç»Ÿä¸€æ›´æ–°
            use_ai_classification=self.use_ai_classification
        )
        
        # è¿è¡Œæµ‹è¯•
        try:
            results = runner.run_adaptive_concurrent_batch(
                tasks,
                initial_workers=max_workers,
                initial_qps=qps
            )
        except Exception as e:
            print(f"âŒ {provider} æä¾›å•†æµ‹è¯•å¤±è´¥: {e}")
            results = []
        
        elapsed = time.time() - start_time
        success_count = sum(1 for r in results if r and r.get('success', False))
        
        # æ›´æ–°ç»Ÿè®¡
        with self.stats_lock:
            self.stats['provider_stats'][provider]['tests'] += len(tasks)
            self.stats['provider_stats'][provider]['success'] += success_count
            self.stats['provider_stats'][provider]['failed'] += len(tasks) - success_count
            self.stats['provider_stats'][provider]['time'] += elapsed
        
        print(f"\nâœ… {provider} å®Œæˆ: {success_count}/{len(tasks)} æˆåŠŸ (è€—æ—¶ {elapsed:.1f}ç§’)")
        
        return results
    
    def run_parallel_by_provider(self, tasks: List[TestTask]) -> Tuple[List[Dict], Dict]:
        """
        æŒ‰æä¾›å•†å¹¶è¡Œè¿è¡Œæ‰¹é‡æµ‹è¯•
        
        Args:
            tasks: æ‰€æœ‰æµ‹è¯•ä»»åŠ¡
            
        Returns:
            (æ‰€æœ‰ç»“æœ, ç»Ÿè®¡ä¿¡æ¯)
        """
        print(f"\n{'='*70}")
        print(f"ğŸ¯ åŸºäºæä¾›å•†çš„å¹¶è¡Œæ‰¹æµ‹è¯•")
        print(f"   æ€»ä»»åŠ¡æ•°: {len(tasks)}")
        print(f"{'='*70}")
        
        # æŒ‰æä¾›å•†åˆ†ç»„ä»»åŠ¡
        provider_tasks = self.group_tasks_by_provider(tasks)
        
        print(f"\nğŸ“Š ä»»åŠ¡åˆ†å¸ƒ:")
        for provider, ptasks in provider_tasks.items():
            config = self.provider_configs.get(provider, {})
            print(f"   {provider}: {len(ptasks)} ä¸ªä»»åŠ¡ ({config.get('description', '')})")
        
        # å¹¶è¡Œè¿è¡Œä¸åŒæä¾›å•†
        all_results = []
        start_time = time.time()
        
        with ThreadPoolExecutor(max_workers=len(provider_tasks)) as executor:
            future_to_provider = {}
            
            for provider, ptasks in provider_tasks.items():
                if ptasks:  # åªè¿è¡Œæœ‰ä»»åŠ¡çš„æä¾›å•†
                    config = self.provider_configs.get(provider, {})
                    future = executor.submit(
                        self.run_provider_batch,
                        provider,
                        ptasks,
                        config.get('max_parallel'),
                        config.get('initial_qps')
                    )
                    future_to_provider[future] = provider
            
            # æ”¶é›†ç»“æœ
            for future in as_completed(future_to_provider):
                provider = future_to_provider[future]
                try:
                    results = future.result()
                    all_results.extend(results)
                except Exception as e:
                    print(f"âŒ {provider} æä¾›å•†æ‰§è¡Œå¤±è´¥: {e}")
        
        total_time = time.time() - start_time
        
        # è®¡ç®—æ€»ç»Ÿè®¡
        with self.stats_lock:
            self.stats['total_tests'] = len(tasks)
            self.stats['total_success'] = sum(
                s['success'] for s in self.stats['provider_stats'].values()
            )
            self.stats['total_failed'] = sum(
                s['failed'] for s in self.stats['provider_stats'].values()
            )
            self.stats['total_time'] = total_time
        
        # æ‰“å°æœ€ç»ˆç»Ÿè®¡
        self._print_final_stats()
        
        return all_results, self.stats
    
    def _print_final_stats(self):
        """æ‰“å°æœ€ç»ˆç»Ÿè®¡ä¿¡æ¯"""
        print(f"\n{'='*70}")
        print(f"ğŸ“ˆ æœ€ç»ˆç»Ÿè®¡")
        print(f"{'='*70}")
        
        print(f"\næ€»ä½“:")
        print(f"  æ€»æµ‹è¯•: {self.stats['total_tests']}")
        print(f"  æˆåŠŸ: {self.stats['total_success']} ({self.stats['total_success']/max(1, self.stats['total_tests'])*100:.1f}%)")
        print(f"  å¤±è´¥: {self.stats['total_failed']}")
        print(f"  æ€»è€—æ—¶: {self.stats['total_time']:.1f}ç§’")
        
        if self.stats['total_time'] > 0:
            qps = self.stats['total_tests'] / self.stats['total_time']
            print(f"  å¹³å‡QPS: {qps:.2f}")
        
        print(f"\næŒ‰æä¾›å•†:")
        for provider, pstats in self.stats['provider_stats'].items():
            if pstats['tests'] > 0:
                success_rate = pstats['success'] / pstats['tests'] * 100
                avg_time = pstats['time'] / pstats['tests']
                print(f"\n  {provider}:")
                print(f"    æµ‹è¯•: {pstats['tests']}")
                print(f"    æˆåŠŸ: {pstats['success']} ({success_rate:.1f}%)")
                print(f"    å¤±è´¥: {pstats['failed']}")
                print(f"    è€—æ—¶: {pstats['time']:.1f}ç§’")
                print(f"    å¹³å‡: {avg_time:.2f}ç§’/æµ‹è¯•")
                if pstats['time'] > 0:
                    provider_qps = pstats['tests'] / pstats['time']
                    print(f"    QPS: {provider_qps:.2f}")
    
    def save_results_to_database(self, results: List[Dict], model: str, difficulty: str):
        """
        ä¿å­˜ç»“æœåˆ°æ•°æ®åº“
        
        Args:
            results: æµ‹è¯•ç»“æœåˆ—è¡¨
            model: æ¨¡å‹åç§°
            difficulty: éš¾åº¦çº§åˆ«
        """
        if not results:
            return
        
        manager = EnhancedCumulativeManager(use_ai_classification=self.use_ai_classification)
        success_count = 0
        
        for result in results:
            if result and not result.get('_saved', False):
                record = TestRecord(
                    model=model,
                    task_type=result.get("task_type", "unknown"),
                    prompt_type=result.get("prompt_type", "baseline"),
                    difficulty=result.get("difficulty", difficulty)
                )
                
                # è®¾ç½®å…¶ä»–å­—æ®µ
                for field in ['timestamp', 'success', 'success_level', 'execution_time', 'turns', 
                            'tool_calls', 'workflow_score', 'phase2_score', 'quality_score', 
                            'final_score', 'error_type', 'tool_success_rate', 'is_flawed', 
                            'flaw_type', 'format_error_count', 'api_issues', 'executed_tools', 
                            'required_tools', 'tool_coverage_rate', 'task_instance', 'execution_history']:
                    if field in result:
                        setattr(record, field, result[field])
                
                try:
                    manager.add_test_result_with_classification(record)
                    result['_saved'] = True
                    success_count += 1
                except Exception as e:
                    print(f"âš ï¸ ä¿å­˜å¤±è´¥: {e}")
        
        # å¼ºåˆ¶åˆ·æ–°
        if hasattr(manager, '_flush_buffer'):
            manager._flush_buffer()
        
        if success_count > 0:
            print(f"âœ… å·²ä¿å­˜ {success_count} ä¸ªæµ‹è¯•ç»“æœåˆ°æ•°æ®åº“")


def test_provider_parallel():
    """æµ‹è¯•å‡½æ•°"""
    print("Provider-Aware Parallel Runner Test")
    print("="*70)
    
    # åˆ›å»ºæµ‹è¯•ä»»åŠ¡
    test_tasks = []
    
    # Azureä»»åŠ¡
    for i in range(3):
        test_tasks.append(TestTask(
            model='gpt-4o-mini',
            task_type='simple_task',
            prompt_type='baseline',
            difficulty='easy',
            tool_success_rate=0.8
        ))
    
    # IdealLabä»»åŠ¡
    for model in ['qwen2.5-3b-instruct', 'DeepSeek-V3-671B']:
        for i in range(2):
            test_tasks.append(TestTask(
                model=model,
                task_type='simple_task',
                prompt_type='baseline',
                difficulty='easy',
                tool_success_rate=0.8
            ))
    
    # è¿è¡Œæµ‹è¯•
    runner = ProviderParallelRunner(debug=False, silent=False)
    results, stats = runner.run_parallel_by_provider(test_tasks)
    
    print(f"\næµ‹è¯•å®Œæˆ!")
    print(f"æ€»ç»“æœæ•°: {len(results)}")
    
    # ä¿å­˜ç»“æœ
    print("\nä¿å­˜ç»“æœåˆ°æ•°æ®åº“...")
    for model in set(t.model for t in test_tasks):
        model_results = [r for r in results if r.get('model') == model]
        if model_results:
            runner.save_results_to_database(model_results, model, 'easy')


if __name__ == "__main__":
    test_provider_parallel()