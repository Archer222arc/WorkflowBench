#!/usr/bin/env python3
"""
æ™ºèƒ½æ‰¹æµ‹è¯•è¿è¡Œå™¨ - å¢å¼ºç‰ˆ
ä¿æŒåŸæœ‰æ¥å£ï¼Œä½†å†…éƒ¨ä½¿ç”¨BatchTestRunnerä»¥è·å¾—å®Œæ•´åŠŸèƒ½
æ”¯æŒç¼“å­˜æ¨¡å¼é¿å…å¹¶å‘ç«äº‰æ¡ä»¶
æ–°å¢ï¼šè‡ªåŠ¨å¤±è´¥ç»´æŠ¤å’ŒåŸºäºè¿›åº¦çš„å¢é‡é‡æµ‹
"""
import json
import os
import sys
import subprocess
import argparse
import tempfile
import shutil
from typing import List, Dict, Optional, Any
from pathlib import Path
from datetime import datetime
import time

sys.path.insert(0, str(Path(__file__).parent))
from batch_test_runner import BatchTestRunner, TestTask
from enhanced_cumulative_manager import EnhancedCumulativeManager
from cumulative_test_manager import TestRecord
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from collections import defaultdict
from api_client_manager import MODEL_PROVIDER_MAP
from database_utils import load_database, get_completed_count


def _save_results_to_database(results, model, difficulty, use_ai_classification=False, result_suffix=''):
    """ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ•°æ®åº“"""
    if not results:
        return 0
    
    manager = EnhancedCumulativeManager(use_ai_classification=use_ai_classification, db_suffix=result_suffix)
    success_commit = 0
    
    for result in results:
        if result and not result.get('_saved', False):
            record = TestRecord(
                model=model,
                task_type=result.get("task_type", "unknown"),
                prompt_type=result.get("prompt_type", "baseline"),
                difficulty=result.get("difficulty", difficulty)
            )
            # è®¾ç½®å…¶ä»–å­—æ®µ
            for field in ['timestamp', 'success', 'success_level', 'execution_time', 'turns', 
                        'tool_calls', 'workflow_score', 'phase2_score', 'quality_score', 
                        'final_score', 'error_type', 'tool_success_rate', 'is_flawed', 
                        'flaw_type', 'format_error_count', 'api_issues', 'executed_tools', 
                        'required_tools', 'tool_coverage_rate', 'task_instance', 'execution_history']:
                if field in result:
                    setattr(record, field, result[field])
            
            try:
                manager.add_test_result_with_classification(record)
                result['_saved'] = True  # æ ‡è®°å·²ä¿å­˜
                success_commit += 1
            except Exception as e:
                print(f"âš ï¸  æäº¤å¤±è´¥: {e}")
    
    # å¼ºåˆ¶åˆ·æ–°ç¼“å†²åŒº
    if hasattr(manager, '_flush_buffer'):
        manager._flush_buffer()
    
    if success_commit > 0:
        print(f"âœ… å·²ä¿å­˜ {success_commit} ä¸ªæµ‹è¯•ç»“æœåˆ°æ•°æ®åº“")
    
    return success_commit

# load_databaseå‡½æ•°å·²ç§»åŠ¨åˆ°database_utils.py

def get_incomplete_tests_for_model(model: str) -> Dict[str, List[Dict]]:
    """è·å–æŒ‡å®šæ¨¡å‹æœªå®Œæˆçš„æµ‹è¯•é…ç½®"""
    # é¿å…å¾ªç¯å¯¼å…¥ï¼Œåœ¨å‡½æ•°å†…éƒ¨å¯¼å…¥
    from auto_failure_maintenance_system import AutoFailureMaintenanceSystem
    maintenance_system = AutoFailureMaintenanceSystem(enable_auto_retry=False)
    incomplete_tests = maintenance_system.get_incomplete_tests([model])
    return incomplete_tests.get(model, [])

# get_completed_countå‡½æ•°å·²ç§»åŠ¨åˆ°database_utils.py

def run_batch_test_smart(model: str, prompt_types: str, difficulty: str, 
                         task_types: str, num_instances: int, adaptive: bool = True, 
                         tool_success_rate: float = 0.8, batch_commit: bool = False, 
                         checkpoint_interval: int = 20, use_provider_parallel: bool = False, **kwargs):
    """
    æ™ºèƒ½è¿è¡Œæ‰¹æµ‹è¯•ï¼Œè‡ªåŠ¨è®¡ç®—éœ€è¦è¡¥å……çš„æµ‹è¯•æ•°é‡
    ä¿æŒåŸæœ‰æ¥å£ï¼Œä½†ä½¿ç”¨BatchTestRunneræ‰§è¡Œ
    æ”¯æŒbatch_commitæ‰¹é‡æäº¤é¿å…å¹¶å‘ç«äº‰
    æ”¯æŒcheckpoint_intervalä¸­é—´ä¿å­˜ï¼ˆæ¯Nä¸ªæµ‹è¯•ä¿å­˜ä¸€æ¬¡ï¼‰
    æ”¯æŒå¤šprompt typeså¹¶è¡Œæ‰§è¡Œ
    """
    # è§£æprompt types - æ”¯æŒå¤šä¸ªprompt types
    if prompt_types == "all":
        prompt_list = ["baseline", "cot", "optimal"]
    elif prompt_types == "all_with_flawed":
        prompt_list = ["baseline", "cot", "optimal"] + [
            f"flawed_{ft}" for ft in [
                "sequence_disorder", "tool_misuse", "parameter_error",
                "missing_step", "redundant_operations", 
                "logical_inconsistency", "semantic_drift"
            ]
        ]
    elif "," in prompt_types:
        # æ”¯æŒé€—å·åˆ†éš”çš„å¤šä¸ªprompt types
        prompt_list = [p.strip() for p in prompt_types.split(",")]
    else:
        # å•ä¸ªprompt type
        prompt_list = [prompt_types]
    
    # è§£æä»»åŠ¡ç±»å‹
    if task_types == "all":
        task_list = ["simple_task", "basic_task", "data_pipeline", 
                    "api_integration", "multi_stage_pipeline"]
    else:
        task_list = task_types.split(",")
    
    # åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨å¤špromptå¹¶è¡Œ
    use_prompt_parallel = len(prompt_list) > 1 and kwargs.get('prompt_parallel', True)
    provider = MODEL_PROVIDER_MAP.get(model, 'idealab')
    silent = kwargs.get('silent', False)
    
    if not silent:
        print(f"\n{'='*60}")
        print(f"æ™ºèƒ½æ‰¹æµ‹è¯•: {model} ({provider})")
        print(f"Prompt types: {prompt_list}")
        print(f"éš¾åº¦: {difficulty}")
        print(f"ç›®æ ‡: æ¯ç§é…ç½® {num_instances} ä¸ªå®ä¾‹")
        if use_prompt_parallel:
            if provider in ['azure', 'user_azure']:
                print(f"ç­–ç•¥: æ‰€æœ‰{len(prompt_list)}ä¸ªprompt typesåŒæ—¶å¹¶è¡Œï¼ˆé«˜å¹¶å‘ï¼‰")
            else:
                print(f"ç­–ç•¥: {len(prompt_list)}ä¸ªprompt typesä½¿ç”¨ä¸åŒAPI keyså¹¶è¡Œ")
        print(f"{'='*60}")
    
    # å¦‚æœä½¿ç”¨å¤špromptå¹¶è¡Œï¼Œç›´æ¥æ„å»ºæ‰€æœ‰ä»»åŠ¡
    if use_prompt_parallel:
        return _run_multi_prompt_parallel(
            model=model,
            prompt_list=prompt_list,
            task_list=task_list,
            difficulty=difficulty,
            num_instances=num_instances,
            tool_success_rate=tool_success_rate,
            provider=provider,
            adaptive=adaptive,
            batch_commit=batch_commit,
            checkpoint_interval=checkpoint_interval,
            **kwargs
        )
    
    # å•prompt typeçš„åŸæœ‰é€»è¾‘
    prompt_type = prompt_list[0]
    
    # æ£€æŸ¥æ¯ç§ä»»åŠ¡ç±»å‹çš„å®Œæˆæƒ…å†µ
    total_needed = 0
    tasks_to_run = []
    
    # åˆ¤æ–­æ˜¯å¦æ˜¯ç¼ºé™·æµ‹è¯•
    is_flawed = prompt_type.startswith("flawed_")
    flaw_type = prompt_type.replace("flawed_", "") if is_flawed else None
    
    for task_type in task_list:
        completed = get_completed_count(model, prompt_type, difficulty, task_type, tool_success_rate)
        needed = max(0, num_instances - completed)
        total_needed += needed
        
        status_symbol = "âœ“" if needed == 0 else "â—‹"
        if not silent:
            print(f"{status_symbol} {task_type:20s}: {completed:3d}/{num_instances:3d} å·²å®Œæˆ", end="")
            if needed > 0:
                print(f" (éœ€è¦è¡¥å…… {needed} ä¸ª)")
            else:
                print(" [è·³è¿‡]")
        
        if needed > 0:
            # è®°å½•éœ€è¦è¿è¡Œçš„ä»»åŠ¡
            tasks_to_run.append({
                'task_type': task_type,
                'count': needed
            })
    
    if total_needed == 0:
        if not silent:
            print(f"\nâœ… æ‰€æœ‰æµ‹è¯•å·²å®Œæˆï¼Œè·³è¿‡æ­¤é…ç½®")
        return True
    
    if not silent:
        print(f"\nâ³ éœ€è¦è¿è¡Œ {total_needed} ä¸ªæ–°æµ‹è¯•")
    
    # æ„å»ºæ‰€æœ‰æµ‹è¯•ä»»åŠ¡
    all_tasks = []
    for task_info in tasks_to_run:
        task_type = task_info['task_type']
        count = task_info['count']
        
        if not silent:
            print(f"\nâ–¶ å‡†å¤‡ {task_type} ({count} ä¸ªå®ä¾‹)...")
        
        for _ in range(count):
            task = TestTask(
                model=model,
                task_type=task_type,
                prompt_type="flawed" if is_flawed else prompt_type,
                difficulty=difficulty,
                is_flawed=is_flawed,
                flaw_type=flaw_type,
                tool_success_rate=tool_success_rate
            )
            all_tasks.append(task)
    
    # è¿è¡Œæ‰¹é‡æµ‹è¯•
    if not silent:
        print(f"\nâ–¶ å¼€å§‹æ‰§è¡Œ {len(all_tasks)} ä¸ªæµ‹è¯•...")
        if batch_commit:
            if checkpoint_interval > 0:
                print(f"ğŸ“¦ æ‰¹é‡æäº¤æ¨¡å¼ï¼šæ¯{checkpoint_interval}ä¸ªæµ‹è¯•ä¿å­˜ä¸€æ¬¡")
            else:
                print("ğŸ“¦ æ‰¹é‡æäº¤æ¨¡å¼ï¼šæµ‹è¯•å®Œæˆåç»Ÿä¸€å†™å…¥æ•°æ®åº“")
    
    save_logs = kwargs.get('save_logs', True)
    workers = kwargs.get('max_workers', 10)
    qps = kwargs.get('qps', 20.0)
    
    # å¦‚æœä½¿ç”¨åŸºäºæä¾›å•†çš„å¹¶è¡Œç­–ç•¥
    if use_provider_parallel:
        if not silent:
            print("\nğŸš€ ä½¿ç”¨åŸºäºAPIæä¾›å•†çš„å¹¶è¡Œç­–ç•¥ï¼ˆé›†æˆç‰ˆï¼‰")
        
        # æŒ‰æä¾›å•†åˆ†ç»„ä»»åŠ¡
        provider_tasks = defaultdict(list)
        for task in all_tasks:
            provider = MODEL_PROVIDER_MAP.get(task.model, 'idealab')
            provider_tasks[provider].append(task)
        
        if not silent:
            print(f"\nğŸ“¦ ä»»åŠ¡åˆ†å¸ƒï¼š")
            for provider, tasks in provider_tasks.items():
                print(f"  {provider}: {len(tasks)} ä¸ªä»»åŠ¡")
        
        # è®¾ç½®æä¾›å•†çº§åˆ«çš„å¹¶å‘å‚æ•°
        provider_configs = {
            'azure': {'max_workers': 100, 'initial_qps': 200.0},  # æé«˜åˆ°100 workersï¼Œæ”¯æŒ20*5ä¸€æ¬¡æ€§å®Œæˆ
            'user_azure': {'max_workers': 50, 'initial_qps': 100.0},  # ä¹Ÿç›¸åº”æé«˜
            'idealab': {'max_workers': 24, 'initial_qps': 30.0}  # 3 keys Ã— 8 å¹¶å‘
        }
        
        all_results = []
        
        # å¹¶è¡Œè¿è¡Œä¸åŒæä¾›å•†çš„ä»»åŠ¡
        with ProcessPoolExecutor(max_workers=len(provider_tasks)) as executor:
            futures = []
            
            for provider, tasks in provider_tasks.items():
                config = provider_configs.get(provider, {'max_workers': 10, 'initial_qps': 20.0})
                
                # åˆ›å»ºä¸“é—¨çš„runner
                future = executor.submit(
                    _run_provider_tasks,
                    tasks,
                    config['max_workers'],
                    config['initial_qps'],
                    adaptive,
                    save_logs,
                    silent,
                    not batch_commit,
                    kwargs.get('ai_classification', False),
                    checkpoint_interval if batch_commit else 0
                )
                futures.append((provider, future))
            
            # æ”¶é›†ç»“æœ
            for provider, future in futures:
                try:
                    provider_results = future.result()
                    all_results.extend(provider_results)
                    if not silent:
                        print(f"\nâœ… {provider} æä¾›å•†ä»»åŠ¡å®Œæˆï¼š{len(provider_results)} ä¸ªç»“æœ")
                except Exception as e:
                    if not silent:
                        print(f"\nâŒ {provider} æä¾›å•†ä»»åŠ¡å¤±è´¥ï¼š{e}")
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in all_results if r and r.get('success', False))
        if not silent:
            print(f"\nâœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆ")
            print(f"   æˆåŠŸ: {success_count}/{len(all_results)}")
            print(f"   å¤±è´¥: {len(all_results) - success_count}/{len(all_results)}")
        
        # ä¿å­˜ç»“æœ
        if batch_commit and all_results:
            unsaved_results = [r for r in all_results if r and not r.get('_saved', False)]
            if unsaved_results:
                if not silent:
                    print(f"\nğŸ“¤ ä¿å­˜{len(unsaved_results)}ä¸ªæµ‹è¯•ç»“æœ...")
                _save_results_to_database(unsaved_results, model, difficulty, kwargs.get('ai_classification', False), kwargs.get('result_suffix', ''))
        
        return True
    
    # æ ¹æ®æ¨¡å‹å’Œæ¨¡å¼è°ƒæ•´å‚æ•°
    if any(x in model.lower() for x in ['qwen', 'llama-4-scout', 'o1']):
        # IdealLab API: é€‚åº¦ä¿å®ˆä½†ä¸è¦å¤ªä¿å®ˆ
        if adaptive:
            workers = min(workers, 5)  # æé«˜åˆ°5
            qps = min(qps, 10.0)  # æé«˜åˆ°10
        else:
            # éadaptiveæ¨¡å¼ä¹Ÿå¯ä»¥ç¨å¾®æ¿€è¿›ä¸€ç‚¹
            workers = 3  # ä»1æé«˜åˆ°3
            qps = 5.0   # ä»2æé«˜åˆ°5
        if not silent:
            print(f"âš ï¸  æ£€æµ‹åˆ°idealab APIï¼Œè°ƒæ•´å¹¶å‘: workers={workers}, qps={qps}")
    elif any(x in model.lower() for x in ['deepseek', 'llama-3.3', 'gpt-4o-mini', 'gpt-5']):
        # Azure API: éå¸¸æ¿€è¿›ï¼Œæ”¯æŒ100å¹¶å‘ä¸€æ¬¡æ€§å®Œæˆ20*5æµ‹è¯•
        if adaptive:
            workers = max(workers, 100)  # ç›´æ¥ä»100å¼€å§‹ï¼Œæ”¯æŒ20*5ä¸€æ¬¡æ€§å¹¶è¡Œ
            qps = max(qps, 200.0)  # ç›´æ¥ä»200å¼€å§‹
        else:
            # éadaptiveæ¨¡å¼ä½¿ç”¨æ›´é«˜çš„å›ºå®šå€¼
            workers = 100  # æé«˜åˆ°100
            qps = 200.0   # æé«˜åˆ°200
        if not silent:
            print(f"ğŸš€ æ£€æµ‹åˆ°Azure APIï¼Œä½¿ç”¨è¶…é«˜å¹¶å‘: workers={workers}, qps={qps}")
    else:
        # é»˜è®¤æ¨¡å‹ï¼šä¹Ÿæ›´æ¿€è¿›ä¸€äº›
        if not adaptive:
            workers = min(workers, 20)  # æé«˜åˆ°20
            qps = min(qps, 30.0)  # æé«˜åˆ°30
    
    # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
    runner = BatchTestRunner(
        debug=False,
        silent=silent,
        use_adaptive=adaptive,
        save_logs=save_logs,
        enable_database_updates=not batch_commit,  # batchæ¨¡å¼ç¦ç”¨å®æ—¶å†™å…¥
        use_ai_classification=kwargs.get('ai_classification', False),  # AIåˆ†ç±»æ”¯æŒ
        checkpoint_interval=checkpoint_interval if batch_commit else 0  # ä¸­é—´ä¿å­˜é—´éš”
    )
    
    try:
        if adaptive:
            results = runner.run_adaptive_concurrent_batch(
                all_tasks, 
                initial_workers=workers, 
                initial_qps=qps
            )
        else:
            results = runner.run_concurrent_batch(
                all_tasks, 
                workers=workers, 
                qps=qps
            )
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if r and r.get('success', False))
        if not silent:
            print(f"\nâœ… æ‰¹æµ‹è¯•å®Œæˆ")
            print(f"   æˆåŠŸ: {success_count}/{len(results)}")
            print(f"   å¤±è´¥: {len(results) - success_count}/{len(results)}")
        
        # å¦‚æœæ˜¯æ‰¹é‡æäº¤æ¨¡å¼ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æœªä¿å­˜çš„ç»“æœ
        if batch_commit:
            # è·å–runnerä¸­çš„pending_resultsï¼ˆå¦‚æœæœ‰ï¼‰
            pending_results = getattr(runner, 'pending_results', [])
            if pending_results:
                if not silent:
                    print(f"\nğŸ“¤ ä¿å­˜{len(pending_results)}ä¸ªæœªæäº¤çš„æµ‹è¯•ç»“æœ...")
                _save_results_to_database(pending_results, model, difficulty, kwargs.get('ai_classification', False), kwargs.get('result_suffix', ''))
                runner.pending_results = []  # æ¸…ç©º
            
            # å¦‚æœè¿˜æœ‰å…¶ä»–æœªå¤„ç†çš„results
            if results:
                unsaved_results = [r for r in results if r and not r.get('_saved', False)]
                if unsaved_results:
                    if not silent:
                        print(f"\nğŸ“¤ æœ€ç»ˆä¿å­˜{len(unsaved_results)}ä¸ªæµ‹è¯•ç»“æœ...")
                    _save_results_to_database(unsaved_results, model, difficulty, kwargs.get('ai_classification', False), kwargs.get('result_suffix', ''))
        
        return True
        
    except Exception as e:
        if not silent:
            print(f"\nâœ— æ‰¹æµ‹è¯•å¤±è´¥: {e}")
        return False

def run_auto_maintenance_mode(models: Optional[List[str]] = None) -> bool:
    """
    è‡ªåŠ¨ç»´æŠ¤æ¨¡å¼ï¼šåˆ†æå¤±è´¥ï¼Œç”Ÿæˆé‡æµ‹è®¡åˆ’å¹¶æ‰§è¡Œ
    """
    print("ğŸ”§ è¿›å…¥è‡ªåŠ¨ç»´æŠ¤æ¨¡å¼...")
    
    # é¿å…å¾ªç¯å¯¼å…¥
    from auto_failure_maintenance_system import AutoFailureMaintenanceSystem
    # åˆå§‹åŒ–ç»´æŠ¤ç³»ç»Ÿ
    maintenance_system = AutoFailureMaintenanceSystem(enable_auto_retry=True)
    
    # æ‰§è¡Œè‡ªåŠ¨ç»´æŠ¤
    maintenance_result = maintenance_system.auto_maintain_failures()
    
    # è·å–æœªå®Œæˆçš„æµ‹è¯•
    incomplete_tests = maintenance_system.get_incomplete_tests(models)
    
    if not incomplete_tests:
        print("âœ… æ‰€æœ‰æ¨¡å‹æµ‹è¯•å®Œæ•´ï¼Œæ— éœ€è¡¥å……")
        return True
    
    print(f"\nğŸ“‹ å‘ç° {len(incomplete_tests)} ä¸ªæ¨¡å‹æœ‰æœªå®Œæˆæµ‹è¯•")
    
    # ç”Ÿæˆå¹¶æ‰§è¡Œé‡æµ‹è„šæœ¬
    total_executed = 0
    for model, configs in incomplete_tests.items():
        print(f"\nâ–¶ å¤„ç†æ¨¡å‹: {model}")
        
        for config in configs:
            if config["missing_count"] <= 0:
                continue
                
            print(f"  è¡¥å…… {config['prompt_type']} / {config['task_type']}: {config['missing_count']} ä¸ªæµ‹è¯•")
            
            # æ‰§è¡Œå•ä¸ªé…ç½®çš„è¡¥å……æµ‹è¯•
            success = run_batch_test_smart(
                model=model,
                prompt_types=config["prompt_type"],
                difficulty=config["difficulty"],
                task_types=config["task_type"],
                num_instances=config["missing_count"],
                adaptive=True,
                tool_success_rate=0.8,
                batch_commit=True,
                checkpoint_interval=10,
                max_workers=5,
                save_logs=False,
                silent=True
            )
            
            if success:
                total_executed += config["missing_count"]
                print(f"    âœ… å®Œæˆ {config['missing_count']} ä¸ªæµ‹è¯•")
            else:
                print(f"    âŒ æµ‹è¯•å¤±è´¥")
                # è®°å½•å¤±è´¥
                maintenance_system.failed_manager.record_test_failure(
                    model=model,
                    group_name=f"{config['prompt_type']}_{config['task_type']}",
                    prompt_types=config["prompt_type"],
                    test_type="auto_retest",
                    failure_reason="Auto retest failed"
                )
    
    print(f"\nğŸ¯ è‡ªåŠ¨ç»´æŠ¤å®Œæˆï¼Œæ‰§è¡Œäº† {total_executed} ä¸ªæµ‹è¯•")
    return True

def run_incremental_retest_mode(models: Optional[List[str]] = None, 
                               completion_threshold: float = 0.8) -> bool:
    """
    å¢é‡é‡æµ‹æ¨¡å¼ï¼šåŸºäºç°æœ‰è¿›åº¦ï¼Œåªæµ‹è¯•ç¼ºå¤±çš„éƒ¨åˆ†
    """
    print(f"ğŸ”„ è¿›å…¥å¢é‡é‡æµ‹æ¨¡å¼ï¼ˆå®Œæˆç‡é˜ˆå€¼: {completion_threshold:.1%}ï¼‰...")
    
    # é¿å…å¾ªç¯å¯¼å…¥
    from auto_failure_maintenance_system import AutoFailureMaintenanceSystem
    maintenance_system = AutoFailureMaintenanceSystem(enable_auto_retry=False)
    
    # åˆ†æå®Œæˆæƒ…å†µ
    analysis = maintenance_system.analyze_test_completion(models)
    
    models_to_retest = []
    for model in analysis["models_analyzed"]:
        model_analysis = analysis["completion_summary"][model]
        if (model_analysis.get("status") == "analyzed" and 
            model_analysis.get("completion_rate", 0) < completion_threshold):
            models_to_retest.append(model)
    
    if not models_to_retest:
        print(f"âœ… æ‰€æœ‰æ¨¡å‹å®Œæˆç‡éƒ½è¶…è¿‡ {completion_threshold:.1%}ï¼Œæ— éœ€é‡æµ‹")
        return True
    
    print(f"\nğŸ“‹ éœ€è¦é‡æµ‹çš„æ¨¡å‹: {models_to_retest}")
    
    # å¯¹æ¯ä¸ªéœ€è¦é‡æµ‹çš„æ¨¡å‹æ‰§è¡Œå¢é‡æµ‹è¯•
    for model in models_to_retest:
        model_analysis = analysis["completion_summary"][model]
        print(f"\nâ–¶ é‡æµ‹æ¨¡å‹: {model}")
        print(f"   å½“å‰å®Œæˆç‡: {model_analysis.get('completion_rate', 0):.1%}")
        print(f"   å¤±è´¥ç‡: {model_analysis.get('failure_rate', 0):.1%}")
        
        # è·å–è¯¥æ¨¡å‹çš„æœªå®Œæˆæµ‹è¯•
        incomplete_configs = get_incomplete_tests_for_model(model)
        
        for config in incomplete_configs:
            if config["missing_count"] <= 0:
                continue
                
            print(f"    è¡¥å……: {config['prompt_type']} / {config['task_type']} ({config['missing_count']} ä¸ª)")
            
            success = run_batch_test_smart(
                model=model,
                prompt_types=config["prompt_type"],
                difficulty=config["difficulty"],
                task_types=config["task_type"],
                num_instances=config["missing_count"],
                adaptive=True,
                tool_success_rate=0.8,
                batch_commit=True,
                checkpoint_interval=5,
                max_workers=3,
                save_logs=False,
                silent=True
            )
            
            if success:
                print(f"      âœ… å®Œæˆ")
            else:
                print(f"      âŒ å¤±è´¥")
    
    print("\nğŸ¯ å¢é‡é‡æµ‹å®Œæˆ")
    return True

def main():
    parser = argparse.ArgumentParser(description='æ™ºèƒ½æ‰¹æµ‹è¯•è¿è¡Œå™¨ - å¢å¼ºç‰ˆ')
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--model', help='æ¨¡å‹åç§°')
    parser.add_argument('--prompt-types', help='Promptç±»å‹(é€—å·åˆ†éš”æˆ–all)')
    parser.add_argument('--difficulty', default='easy', help='éš¾åº¦')
    parser.add_argument('--task-types', default='all', help='ä»»åŠ¡ç±»å‹')
    parser.add_argument('--num-instances', type=int, default=20, help='æ¯ç§ä»»åŠ¡çš„å®ä¾‹æ•°')
    parser.add_argument('--tool-success-rate', type=float, help='å·¥å…·æˆåŠŸç‡')
    
    # è¿è¡Œå‚æ•°
    parser.add_argument('--max-workers', type=int, default=20, help='æœ€å¤§å¹¶å‘æ•°ï¼ˆAzureæ¨¡å‹è‡ªåŠ¨ä½¿ç”¨50+ï¼‰')
    parser.add_argument('--adaptive', action='store_true', default=True, help='ä½¿ç”¨adaptiveæ¨¡å¼ï¼ˆé»˜è®¤å¼€å¯ï¼‰')
    parser.add_argument('--no-adaptive', dest='adaptive', action='store_false', help='ç¦ç”¨adaptiveæ¨¡å¼')
    parser.add_argument('--save-logs', action='store_true', default=True, help='ä¿å­˜è¯¦ç»†æ—¥å¿—ï¼ˆé»˜è®¤å¼€å¯ï¼‰')
    parser.add_argument('--no-save-logs', dest='save_logs', action='store_false', help='ç¦ç”¨æ—¥å¿—ä¿å­˜')
    parser.add_argument('--silent', action='store_true', help='é™é»˜æ¨¡å¼')
    parser.add_argument('--qps', type=float, default=20.0, help='QPSé™åˆ¶ï¼ˆéadaptiveæ¨¡å¼ä¸‹ä½¿ç”¨ï¼‰')
    parser.add_argument('--batch-commit', action='store_true', help='æ‰¹é‡æäº¤æ¨¡å¼é¿å…å¹¶å‘ç«äº‰')
    parser.add_argument('--checkpoint-interval', type=int, default=20, help='ä¸­é—´ä¿å­˜é—´éš”(æ¯Nä¸ªæµ‹è¯•ä¿å­˜ä¸€æ¬¡ï¼Œé»˜è®¤20)')
    parser.add_argument('--ai-classification', action='store_true', help='å¯ç”¨AIé”™è¯¯åˆ†ç±»(ä½¿ç”¨gpt-5-nano)')
    parser.add_argument('--provider-parallel', action='store_true', help='ä½¿ç”¨åŸºäºAPIæä¾›å•†çš„å¹¶è¡Œç­–ç•¥(è·¨æä¾›å•†å¹¶è¡Œ)')
    parser.add_argument('--prompt-parallel', action='store_true', help='å¹¶è¡Œè¿è¡Œå¤šä¸ªprompt types(Azureç›´æ¥å¹¶è¡Œ,IdealLabä½¿ç”¨ä¸åŒkeys)')
    parser.add_argument('--result-suffix', default='', help='ç»“æœæ–‡ä»¶åç¼€(ç”¨äºåŒºåˆ†é—­æº/å¼€æºæ¨¡å‹)')
    
    # æ–°å¢ï¼šè‡ªåŠ¨ç»´æŠ¤æ¨¡å¼
    parser.add_argument('--auto-maintain', action='store_true', help='è‡ªåŠ¨ç»´æŠ¤æ¨¡å¼ï¼šè‡ªåŠ¨æ£€æµ‹å¤±è´¥å¹¶é‡æµ‹')
    parser.add_argument('--incremental-retest', action='store_true', help='å¢é‡é‡æµ‹æ¨¡å¼ï¼šåŸºäºç°æœ‰è¿›åº¦è¡¥å……ç¼ºå¤±æµ‹è¯•')
    parser.add_argument('--completion-threshold', type=float, default=0.8, help='å¢é‡é‡æµ‹çš„å®Œæˆç‡é˜ˆå€¼(é»˜è®¤80%%)')
    parser.add_argument('--models', nargs='*', help='æŒ‡å®šè¦å¤„ç†çš„æ¨¡å‹åˆ—è¡¨(ç”¨äºè‡ªåŠ¨ç»´æŠ¤æ¨¡å¼)')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ˜¯å¦è¿›å…¥ç‰¹æ®Šæ¨¡å¼
    if args.auto_maintain:
        print("ğŸ”§ å¯åŠ¨è‡ªåŠ¨ç»´æŠ¤æ¨¡å¼")
        success = run_auto_maintenance_mode(args.models)
        sys.exit(0 if success else 1)
    
    if args.incremental_retest:
        print("ğŸ”„ å¯åŠ¨å¢é‡é‡æµ‹æ¨¡å¼")
        success = run_incremental_retest_mode(args.models, args.completion_threshold)
        sys.exit(0 if success else 1)
    
    # éªŒè¯å¿…éœ€å‚æ•°
    if not args.model or not args.prompt_types:
        print("âŒ å¸¸è§„æ¨¡å¼éœ€è¦æŒ‡å®š --model å’Œ --prompt-types å‚æ•°")
        print("ğŸ’¡ æˆ–ä½¿ç”¨ä»¥ä¸‹ç‰¹æ®Šæ¨¡å¼ï¼š")
        print("   --auto-maintain          è‡ªåŠ¨ç»´æŠ¤æ¨¡å¼")
        print("   --incremental-retest     å¢é‡é‡æµ‹æ¨¡å¼")
        print("")
        print("ğŸ“– ç¤ºä¾‹ç”¨æ³•ï¼š")
        print("   # å¸¸è§„æµ‹è¯•")
        print("   python smart_batch_runner.py --model gpt-4o-mini --prompt-types baseline")
        print("   # è‡ªåŠ¨ç»´æŠ¤æ‰€æœ‰æ¨¡å‹")
        print("   python smart_batch_runner.py --auto-maintain")
        print("   # å¢é‡é‡æµ‹ç‰¹å®šæ¨¡å‹")
        print("   python smart_batch_runner.py --incremental-retest --models gpt-4o-mini claude-3-sonnet")
        sys.exit(1)
    
    # å¸¸è§„æµ‹è¯•æ¨¡å¼
    success = run_batch_test_smart(
        model=args.model,
        prompt_types=args.prompt_types,
        difficulty=args.difficulty,
        task_types=args.task_types,
        num_instances=args.num_instances,
        adaptive=args.adaptive,
        tool_success_rate=args.tool_success_rate if args.tool_success_rate else 0.8,
        batch_commit=args.batch_commit,
        checkpoint_interval=args.checkpoint_interval,
        use_provider_parallel=args.provider_parallel,
        max_workers=args.max_workers,
        save_logs=args.save_logs,
        silent=args.silent,
        qps=args.qps,
        ai_classification=args.ai_classification,
        prompt_parallel=args.prompt_parallel,  # ä¼ é€’prompt_parallelå‚æ•°
        result_suffix=args.result_suffix  # ä¼ é€’ç»“æœæ–‡ä»¶åç¼€
    )
    
    sys.exit(0 if success else 1)

def _run_multi_prompt_parallel(model: str, prompt_list: List[str], task_list: List[str],
                               difficulty: str, num_instances: int, tool_success_rate: float,
                               provider: str, adaptive: bool, batch_commit: bool,
                               checkpoint_interval: int, **kwargs):
    """
    å¹¶è¡Œè¿è¡Œå¤šä¸ªprompt types
    Azureæ¨¡å‹ï¼šæ‰€æœ‰prompt typesåŒæ—¶å¹¶è¡Œ
    IdealLabæ¨¡å‹ï¼šæ¯ä¸ªprompt typeä½¿ç”¨ä¸åŒçš„API keyå¹¶è¡Œ
    """
    # æ„å»ºæ‰€æœ‰æµ‹è¯•ä»»åŠ¡
    all_tasks = []
    task_groups = {}  # æŒ‰prompt_typeåˆ†ç»„
    
    for prompt_type in prompt_list:
        task_groups[prompt_type] = []
        is_flawed = prompt_type.startswith("flawed_")
        flaw_type = prompt_type.replace("flawed_", "") if is_flawed else None
        
        for task_type in task_list:
            # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆ
            completed = get_completed_count(model, prompt_type, difficulty, task_type, tool_success_rate)
            needed = max(0, num_instances - completed)
            
            if needed > 0:
                for _ in range(needed):
                    task = TestTask(
                        model=model,
                        task_type=task_type,
                        prompt_type="flawed" if is_flawed else prompt_type,
                        difficulty=difficulty,
                        is_flawed=is_flawed,
                        flaw_type=flaw_type,
                        tool_success_rate=tool_success_rate
                    )
                    all_tasks.append(task)
                    task_groups[prompt_type].append(task)
    
    if not all_tasks:
        print(f"\nâœ… æ‰€æœ‰æµ‹è¯•å·²å®Œæˆï¼Œè·³è¿‡æ­¤é…ç½®")
        return True
    
    print(f"\næ€»è®¡: {len(all_tasks)} ä¸ªæµ‹è¯•ä»»åŠ¡")
    for pt, tasks in task_groups.items():
        if tasks:
            print(f"  {pt}: {len(tasks)} ä¸ªä»»åŠ¡")
    
    # æ ¹æ®providerå†³å®šå¹¶è¡Œç­–ç•¥
    if provider in ['azure', 'user_azure']:
        # Azureï¼šç›´æ¥å¹¶è¡Œæ‰€æœ‰ä»»åŠ¡
        return _run_azure_parallel_tasks(all_tasks, model, difficulty, provider, 
                                        adaptive, batch_commit, checkpoint_interval, **kwargs)
    else:
        # IdealLabï¼šæŒ‰prompt typeåˆ†ç»„å¹¶è¡Œ
        return _run_idealab_parallel_tasks(task_groups, model, difficulty,
                                          adaptive, batch_commit, checkpoint_interval, **kwargs)

def _run_azure_parallel_tasks(all_tasks: List, model: str, difficulty: str, provider: str,
                              adaptive: bool, batch_commit: bool, checkpoint_interval: int, **kwargs):
    """Azureæ¨¡å‹çš„å¹¶è¡Œç­–ç•¥ï¼šæ‰€æœ‰prompt typesåŒæ—¶è¿è¡Œ"""
    # è®¾ç½®Azureçš„æ¿€è¿›å¹¶å‘å‚æ•° - æ”¯æŒ100å¹¶å‘ä¸€æ¬¡æ€§å®Œæˆ20*5æµ‹è¯•
    if provider == 'azure':
        max_workers = max(kwargs.get('max_workers', 100), 100)  # æé«˜åˆ°100
        initial_qps = max(kwargs.get('qps', 200.0), 200.0)  # æé«˜åˆ°200
    else:  # user_azure
        max_workers = max(kwargs.get('max_workers', 50), 50)  # ä¹Ÿç›¸åº”æé«˜
        initial_qps = max(kwargs.get('qps', 100.0), 100.0)
    
    print(f"\nâš¡ ä½¿ç”¨é«˜å¹¶å‘å‚æ•°: workers={max_workers}, QPS={initial_qps}")
    
    # è¿è¡Œæ‰¹é‡æµ‹è¯•
    runner = BatchTestRunner(
        debug=False,
        silent=kwargs.get('silent', False),
        use_adaptive=adaptive,
        save_logs=kwargs.get('save_logs', True),
        enable_database_updates=not batch_commit,
        use_ai_classification=kwargs.get('ai_classification', False),
        checkpoint_interval=checkpoint_interval if batch_commit else 0
    )
    
    if adaptive:
        results = runner.run_adaptive_concurrent_batch(
            all_tasks,
            initial_workers=max_workers,
            initial_qps=initial_qps
        )
    else:
        results = runner.run_concurrent_batch(
            all_tasks,
            workers=max_workers,
            qps=initial_qps
        )
    
    # ç»Ÿè®¡ç»“æœ
    success_count = sum(1 for r in results if r and r.get('success', False))
    print(f"\nâœ… æ‰¹æµ‹è¯•å®Œæˆ")
    print(f"   æˆåŠŸ: {success_count}/{len(results)}")
    print(f"   å¤±è´¥: {len(results) - success_count}/{len(results)}")
    
    # å¦‚æœæ˜¯æ‰¹é‡æäº¤æ¨¡å¼ï¼Œä¿å­˜ç»“æœ
    if batch_commit:
        unsaved_results = [r for r in results if r and not r.get('_saved', False)]
        if unsaved_results:
            print(f"\nğŸ“¤ ä¿å­˜{len(unsaved_results)}ä¸ªæµ‹è¯•ç»“æœ...")
            _save_results_to_database(unsaved_results, model, difficulty, 
                                    kwargs.get('ai_classification', False),
                                    kwargs.get('result_suffix', ''))
    
    return True

def _run_idealab_parallel_tasks(task_groups: Dict, model: str, difficulty: str,
                                adaptive: bool, batch_commit: bool, checkpoint_interval: int, **kwargs):
    """IdealLabæ¨¡å‹çš„å¹¶è¡Œç­–ç•¥ï¼šæ¯ä¸ªprompt typeä½¿ç”¨ä¸åŒçš„API keyå¹¶è¡Œ"""
    from concurrent.futures import ThreadPoolExecutor, as_completed
    import time
    
    # API keyåˆ†é…è¯´æ˜
    print(f"\nğŸ“¦ IdealLabå¹¶è¡Œç­–ç•¥ï¼š")
    for i, pt in enumerate(task_groups.keys()):
        if pt in ['baseline', 'cot', 'optimal']:
            print(f"  {pt} â†’ API Key {i+1}")
        else:
            print(f"  {pt} â†’ è½®è¯¢ä½¿ç”¨3ä¸ªkeys")
    
    print(f"\nâš¡ å¯åŠ¨{len(task_groups)}ä¸ªå¹¶è¡Œä»»åŠ¡...")
    start_time = time.time()
    
    with ThreadPoolExecutor(max_workers=len(task_groups)) as executor:
        futures = []
        
        for prompt_type, tasks in task_groups.items():
            if not tasks:
                continue
                
            # ä¸ºæ¯ä¸ªprompt typeæäº¤ä¸€ä¸ªä»»åŠ¡
            future = executor.submit(
                _run_single_prompt_tasks,
                tasks, model, prompt_type, difficulty,
                adaptive, batch_commit, **kwargs
            )
            futures.append((prompt_type, future))
        
        # æ”¶é›†ç»“æœ
        all_results = []
        prompt_results = {}
        
        for prompt_type, future in futures:
            try:
                results = future.result(timeout=600)  # 10åˆ†é’Ÿè¶…æ—¶
                all_results.extend(results)
                success_count = sum(1 for r in results if r and r.get('success', False))
                prompt_results[prompt_type] = {
                    'success': True,
                    'count': len(results),
                    'success_count': success_count
                }
                print(f"âœ… {prompt_type} å®Œæˆ: {success_count}/{len(results)} æˆåŠŸ")
            except Exception as e:
                print(f"âŒ {prompt_type} å¤±è´¥: {e}")
                prompt_results[prompt_type] = {
                    'success': False,
                    'error': str(e)
                }
    
    elapsed = time.time() - start_time
    
    # ç»Ÿè®¡æ€»ç»“æœ
    total_success = sum(1 for r in all_results if r and r.get('success', False))
    print(f"\nâœ… æ‰€æœ‰prompt typesæµ‹è¯•å®Œæˆ")
    print(f"   æ€»æ—¶é—´: {elapsed:.2f}ç§’")
    print(f"   æ€»æµ‹è¯•: {len(all_results)}")
    print(f"   æˆåŠŸ: {total_success}")
    print(f"   å¤±è´¥: {len(all_results) - total_success}")
    
    # å¦‚æœæ˜¯æ‰¹é‡æäº¤æ¨¡å¼ï¼Œä¿å­˜ç»“æœ
    if batch_commit and all_results:
        unsaved_results = [r for r in all_results if r and not r.get('_saved', False)]
        if unsaved_results:
            print(f"\nğŸ“¤ ä¿å­˜{len(unsaved_results)}ä¸ªæµ‹è¯•ç»“æœ...")
            _save_results_to_database(unsaved_results, model, difficulty,
                                    kwargs.get('ai_classification', False),
                                    kwargs.get('result_suffix', ''))
    
    return True

def _run_single_prompt_tasks(tasks: List, model: str, prompt_type: str, difficulty: str,
                            adaptive: bool, batch_commit: bool, **kwargs):
    """è¿è¡Œå•ä¸ªprompt typeçš„æ‰€æœ‰ä»»åŠ¡"""
    # IdealLabä½¿ç”¨ä¿å®ˆå‚æ•°
    max_workers = min(kwargs.get('max_workers', 5), 5)
    initial_qps = min(kwargs.get('qps', 10.0), 10.0)
    
    # åˆ›å»ºrunnerå¹¶è¿è¡Œ
    runner = BatchTestRunner(
        debug=False,
        silent=True,  # å‡å°‘è¾“å‡ºé¿å…æ··ä¹±
        use_adaptive=adaptive,
        save_logs=kwargs.get('save_logs', True),
        enable_database_updates=not batch_commit,
        use_ai_classification=kwargs.get('ai_classification', False),
        checkpoint_interval=0  # å•ä¸ªprompt typeä¸éœ€è¦checkpoint
    )
    
    if adaptive:
        results = runner.run_adaptive_concurrent_batch(
            tasks,
            initial_workers=max_workers,
            initial_qps=initial_qps
        )
    else:
        results = runner.run_concurrent_batch(
            tasks,
            workers=max_workers,
            qps=initial_qps
        )
    
    return results

def _run_provider_tasks(tasks, max_workers, initial_qps, adaptive, save_logs, silent, 
                        enable_database_updates, use_ai_classification, checkpoint_interval):
    """è¿è¡Œå•ä¸ªæä¾›å•†çš„ä»»åŠ¡ï¼ˆç”¨äºè¿›ç¨‹æ± ï¼‰"""
    runner = BatchTestRunner(
        debug=False,
        silent=silent,
        use_adaptive=adaptive,
        save_logs=save_logs,
        enable_database_updates=enable_database_updates,
        use_ai_classification=use_ai_classification,
        checkpoint_interval=checkpoint_interval
    )
    
    if adaptive:
        return runner.run_adaptive_concurrent_batch(
            tasks,
            initial_workers=max_workers,
            initial_qps=initial_qps
        )
    else:
        return runner.run_concurrent_batch(
            tasks,
            workers=max_workers,
            qps=initial_qps
        )

if __name__ == "__main__":
    main()