#!/usr/bin/env python3
"""
Ultra Parallel Runner - æœ€å¤§åŒ–å¹¶è¡Œåº¦çš„æµ‹è¯•æ‰§è¡Œå™¨
===============================================

æ ¸å¿ƒè®¾è®¡ï¼š
1. æ™ºèƒ½å®ä¾‹æ± ç®¡ç†ï¼šç»Ÿä¸€è°ƒåº¦9ä¸ªAzureå®ä¾‹
2. ä»»åŠ¡æ™ºèƒ½åˆ†ç‰‡ï¼šå°†å¤§ä»»åŠ¡æ‹†åˆ†åˆ°å¤šå®ä¾‹
3. åŠ¨æ€è´Ÿè½½å‡è¡¡ï¼šæ ¹æ®å®ä¾‹æ€§èƒ½è‡ªé€‚åº”åˆ†é…
4. èšåˆç»“æœç®¡ç†ï¼šç»Ÿä¸€æ”¶é›†å’Œå­˜å‚¨ç»“æœ

ç›®æ ‡ï¼šå°†èµ„æºåˆ©ç”¨ç‡ä»11%æå‡åˆ°90%+
"""

import asyncio
import concurrent.futures
import json
import time
import logging
from dataclasses import dataclass
from typing import List, Dict, Set, Optional, Tuple
from pathlib import Path
import subprocess
import threading
from queue import Queue, Empty

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class InstanceConfig:
    """Azureå®ä¾‹é…ç½®"""
    name: str
    model_family: str  # "deepseek-v3", "deepseek-r1", "llama-3.3"
    max_workers: int = 100
    max_qps: float = 200.0
    is_busy: bool = False
    current_load: int = 0
    performance_score: float = 1.0  # æ€§èƒ½è¯„åˆ†ï¼ŒåŠ¨æ€è°ƒæ•´

@dataclass
class TaskShard:
    """ä»»åŠ¡åˆ†ç‰‡"""
    shard_id: str
    model: str
    prompt_types: str
    difficulty: str
    task_types: str
    num_instances: int
    instance_name: str
    tool_success_rate: float = 0.8

class UltraParallelRunner:
    """è¶…é«˜å¹¶è¡Œåº¦æµ‹è¯•æ‰§è¡Œå™¨"""
    
    def __init__(self):
        self.instance_pool = self._initialize_instance_pool()
        self.active_tasks: Set[str] = set()
        self.task_queue = Queue()
        self.results_lock = threading.Lock()
        self.performance_stats = {}
        
    def _initialize_instance_pool(self) -> Dict[str, InstanceConfig]:
        """åˆå§‹åŒ–Azureå®ä¾‹æ± """
        instances = {}
        
        # 6ä¸ªDeepSeekå®ä¾‹
        deepseek_v3_instances = [
            "DeepSeek-V3-0324",
            "deepseek-v3-0324-2", 
            "deepseek-v3-0324-3"
        ]
        
        deepseek_r1_instances = [
            "DeepSeek-R1-0528",
            "deepseek-r1-0528-2",
            "deepseek-r1-0528-3"
        ]
        
        # 3ä¸ªLlamaå®ä¾‹  
        llama_instances = [
            "Llama-3.3-70B-Instruct",
            "llama-3.3-70b-instruct-2",
            "llama-3.3-70b-instruct-3"
        ]
        
        # 3ä¸ªIdealLab API Keyå¯¹åº”çš„è™šæ‹Ÿå®ä¾‹
        # æ¯ä¸ªkeyå¯ä»¥æ”¯æŒå¤šä¸ªQwenæ¨¡å‹ï¼Œä½†æœ‰å…±åŒçš„rate limit
        ideallab_qwen_instances = [
            "qwen2.5-72b-instruct",      # key0 ä¸»åŠ›
            "qwen2.5-32b-instruct",      # key1 ä¸»åŠ›
            "qwen2.5-14b-instruct"       # key2 ä¸»åŠ›
        ]
        
        # æ³¨å†Œæ‰€æœ‰å®ä¾‹
        for name in deepseek_v3_instances:
            instances[name] = InstanceConfig(
                name=name,
                model_family="deepseek-v3",
                max_workers=100,
                max_qps=200.0
            )
            
        for name in deepseek_r1_instances:
            instances[name] = InstanceConfig(
                name=name,
                model_family="deepseek-r1", 
                max_workers=100,
                max_qps=200.0
            )
            
        for name in llama_instances:
            instances[name] = InstanceConfig(
                name=name,
                model_family="llama-3.3",
                max_workers=100,
                max_qps=200.0
            )
            
        # æ³¨å†ŒIdealLabå®ä¾‹ (API Keyçº§åˆ«å¹¶è¡Œ)
        for name in ideallab_qwen_instances:
            instances[name] = InstanceConfig(
                name=name,
                model_family="qwen",
                max_workers=5,   # IdealLabå•keyé™åˆ¶è¾ƒä½
                max_qps=10.0     # ä¿å®ˆè®¾ç½®é¿å…é™æµ
            )
        
        # æ·»åŠ é—­æºAzureæ¨¡å‹å®ä¾‹ (åªæœ‰ä¸€ä¸ªdeploymentï¼Œä½†å¯ä»¥model levelå¹¶å‘)
        azure_closed_models = [
            "gpt-4o-mini",
            "gpt-5-mini"
        ]
        
        for model in azure_closed_models:
            # é—­æºæ¨¡å‹åªæœ‰å•ä¸ªdeploymentï¼Œä½†å¯ä»¥ç”¨æ›´é«˜å¹¶å‘
            instances[model] = InstanceConfig(
                name=model,
                model_family=f"azure-{model}",
                max_workers=200,  # å•å®ä¾‹æ›´é«˜å¹¶å‘
                max_qps=400.0
            )
        
        # æ·»åŠ IdealLabé—­æºæ¨¡å‹å®ä¾‹ (åªæœ‰ä¸€ä¸ªAPI Keyå¯ç”¨ï¼Œä½†å¯ä»¥model levelå¹¶å‘)
        ideallab_closed_models = [
            "o3-0416-global",
            "gemini-2.5-flash-06-17", 
            "kimi-k2",
            "claude_sonnet4"
        ]
        
        for model in ideallab_closed_models:
            # é—­æºæ¨¡å‹åªèƒ½ç”¨ä¸€ä¸ªAPI Keyï¼Œä¸”æœ‰ä¸¥æ ¼é€Ÿç‡é™åˆ¶
            instances[model] = InstanceConfig(
                name=model,
                model_family=f"ideallab-{model}",
                max_workers=5,   # IdealLabé™åˆ¶ï¼Œæœ€å¤š5å¹¶å‘
                max_qps=10.0
            )
            
        logger.info(f"åˆå§‹åŒ–å®ä¾‹æ± : {len(instances)}ä¸ªå®ä¾‹ ({len([i for i in instances.values() if 'azure' in i.model_family])}ä¸ªAzure + {len([i for i in instances.values() if 'ideallab' in i.model_family or i.model_family == 'qwen'])}ä¸ªIdealLab)")
        return instances
        
    def get_available_instances(self, model_family: str) -> List[InstanceConfig]:
        """è·å–æŒ‡å®šæ¨¡å‹æ—çš„å¯ç”¨å®ä¾‹"""
        available = []
        for instance in self.instance_pool.values():
            if (instance.model_family == model_family and 
                not instance.is_busy and 
                instance.current_load < instance.max_workers * 0.8):
                available.append(instance)
        
        # æŒ‰æ€§èƒ½è¯„åˆ†æ’åºï¼Œä¼˜å…ˆä½¿ç”¨é«˜æ€§èƒ½å®ä¾‹
        available.sort(key=lambda x: x.performance_score, reverse=True)
        return available
        
    def create_task_shards(self, model: str, prompt_types: str, difficulty: str, 
                          task_types: str, num_instances: int, tool_success_rate: float = 0.8) -> List[TaskShard]:
        """æ™ºèƒ½åˆ›å»ºä»»åŠ¡åˆ†ç‰‡"""
        
        # ç¡®å®šæ¨¡å‹æ—
        if "deepseek-v3" in model.lower():
            model_family = "deepseek-v3"
            base_model = "DeepSeek-V3-0324"
        elif "deepseek-r1" in model.lower():
            model_family = "deepseek-r1"
            base_model = "DeepSeek-R1-0528"
        elif "llama" in model.lower():
            model_family = "llama-3.3"
            base_model = "Llama-3.3-70B-Instruct"
        elif "qwen" in model.lower():
            model_family = "qwen"
            base_model = "qwen2.5-72b-instruct"  # ä½¿ç”¨æœ€å¼ºçš„ä½œä¸ºbase
        # Azureé—­æºæ¨¡å‹
        elif model == "gpt-4o-mini":
            model_family = "azure-gpt-4o-mini"
            base_model = "gpt-4o-mini"
        elif model == "gpt-5-mini":
            model_family = "azure-gpt-5-mini"
            base_model = "gpt-5-mini"
        # IdealLabé—­æºæ¨¡å‹
        elif model == "o3-0416-global":
            model_family = "ideallab-o3-0416-global"
            base_model = "o3-0416-global"
        elif model == "gemini-2.5-flash-06-17":
            model_family = "ideallab-gemini-2.5-flash-06-17"
            base_model = "gemini-2.5-flash-06-17"
        elif model == "kimi-k2":
            model_family = "ideallab-kimi-k2"
            base_model = "kimi-k2"
        elif model == "claude_sonnet4":
            model_family = "ideallab-claude_sonnet4"
            base_model = "claude_sonnet4"
        else:
            logger.warning(f"æœªçŸ¥æ¨¡å‹æ—: {model}")
            return []
            
        # è·å–å¯ç”¨å®ä¾‹
        available_instances = self.get_available_instances(model_family)
        
        if not available_instances:
            logger.warning(f"æ²¡æœ‰å¯ç”¨çš„{model_family}å®ä¾‹")
            return []
            
        # è®¡ç®—åˆ†ç‰‡ç­–ç•¥
        shards = []
        
        # å¯¹äºé—­æºæ¨¡å‹ï¼Œä½¿ç”¨ç‰¹æ®Šçš„åˆ†ç‰‡ç­–ç•¥
        if model_family.startswith("ideallab-"):
            instances_to_use = 1  # IdealLabé—­æºæ¨¡å‹åªèƒ½ä½¿ç”¨å•åˆ†ç‰‡é¿å…API Keyå†²çª
            logger.info(f"IdealLabé—­æºæ¨¡å‹ {model} ä½¿ç”¨å•åˆ†ç‰‡ç­–ç•¥ï¼ˆé¿å…API Keyå†²çªï¼‰")
        elif model_family.startswith("azure-"):
            instances_to_use = 1  # Azureé—­æºæ¨¡å‹ä½¿ç”¨å•åˆ†ç‰‡é«˜å¹¶å‘ç­–ç•¥
            logger.info(f"Azureé—­æºæ¨¡å‹ {model} ä½¿ç”¨å•åˆ†ç‰‡é«˜å¹¶å‘ç­–ç•¥ï¼ˆå•deploymentä¼˜åŒ–ï¼‰")
        else:
            instances_to_use = min(len(available_instances), 3)  # å¼€æºæ¨¡å‹æœ€å¤šç”¨3ä¸ªå®ä¾‹
            
        instances_per_shard = max(1, num_instances // instances_to_use)
        
        logger.info(f"åˆ›å»ºä»»åŠ¡åˆ†ç‰‡: {instances_to_use}ä¸ªå®ä¾‹å¹¶è¡Œ")
        
        for i in range(instances_to_use):
            instance = available_instances[i]
            shard_instances = instances_per_shard
            
            # æœ€åä¸€ä¸ªåˆ†ç‰‡å¤„ç†ä½™æ•°
            if i == instances_to_use - 1:
                shard_instances += num_instances % instances_to_use
                
            shard = TaskShard(
                shard_id=f"{model}_{difficulty}_{i}",
                model=instance.name,  # ä½¿ç”¨å…·ä½“å®ä¾‹å
                prompt_types=prompt_types,
                difficulty=difficulty,
                task_types=task_types,
                num_instances=shard_instances,
                instance_name=instance.name,
                tool_success_rate=tool_success_rate
            )
            shards.append(shard)
            
        return shards
        
    def execute_shard_async(self, shard: TaskShard, rate_mode: str = "adaptive", result_suffix: str = "", silent: bool = False) -> subprocess.Popen:
        """å¼‚æ­¥æ‰§è¡Œä»»åŠ¡åˆ†ç‰‡
        
        Args:
            shard: ä»»åŠ¡åˆ†ç‰‡
            rate_mode: é€Ÿç‡æ¨¡å¼ - "adaptive" æˆ– "fixed"
        """
        
        # æ ‡è®°å®ä¾‹ä¸ºå¿™ç¢Œ
        if shard.instance_name in self.instance_pool:
            self.instance_pool[shard.instance_name].is_busy = True
            self.instance_pool[shard.instance_name].current_load += shard.num_instances
            
        # è®¡ç®—promptæ•°é‡ï¼ˆç”¨äºåŠ¨æ€è°ƒæ•´workersï¼‰
        prompt_count = len(shard.prompt_types.split(",")) if "," in shard.prompt_types else 1
        use_prompt_parallel = "--prompt-parallel" if prompt_count > 1 else ""
        
        # æ ¹æ®æ¨¡å‹ç±»å‹å’Œrate_modeè°ƒæ•´å‚æ•°
        if shard.instance_name in self.instance_pool:
            instance = self.instance_pool[shard.instance_name]
            
            # IdealLabå¼€æºæ¨¡å‹ï¼ˆqwenç³»åˆ—ï¼‰
            if instance.model_family == "qwen":
                if rate_mode == "fixed":
                    max_workers = 5  # å›ºå®šæ¨¡å¼ï¼šä¿å®ˆå‚æ•°
                    qps = 10
                else:
                    max_workers = 10  # è‡ªé€‚åº”æ¨¡å¼ï¼šå°è¯•æ›´é«˜å¹¶å‘
                    qps = None  # adaptiveä¸éœ€è¦QPS
            # IdealLabé—­æºæ¨¡å‹ï¼ˆå•API Keyé™åˆ¶ï¼‰
            elif instance.model_family.startswith("ideallab-"):
                if rate_mode == "fixed":
                    max_workers = 5  # å›ºå®šæ¨¡å¼ï¼šç»Ÿä¸€ä½¿ç”¨5ä¸ªworkers
                    qps = 5
                else:
                    max_workers = 5  # è‡ªé€‚åº”æ¨¡å¼ï¼šç»Ÿä¸€ä½¿ç”¨5ä¸ªworkers
                    qps = None
                logger.info(f"  IdealLabé—­æºæ¨¡å‹: max_workers={max_workers} (å•API Keyé™åˆ¶)")
            # Azureé—­æºæ¨¡å‹ï¼ˆå•deploymentä½†æ”¯æŒé«˜å¹¶å‘ï¼‰
            elif instance.model_family.startswith("azure-"):
                if rate_mode == "fixed":
                    # é—­æºæ¨¡å‹å›ºå®šæ¨¡å¼ï¼šå•deploymenté«˜å¹¶å‘
                    base_workers = 100  # æ›´é«˜çš„åŸºç¡€å¹¶å‘
                    max_workers = base_workers * prompt_count if prompt_count > 1 else base_workers
                    qps = 200
                    logger.info(f"  Azureé—­æºæ¨¡å‹å›ºå®šæ¨¡å¼: {prompt_count}ä¸ªprompt Ã— {base_workers} = {max_workers} workers")
                else:
                    # é—­æºæ¨¡å‹è‡ªé€‚åº”æ¨¡å¼ï¼šå•deploymentè¶…é«˜å¹¶å‘
                    base_workers = 200  # è¶…é«˜åŸºç¡€å¹¶å‘
                    max_workers = base_workers * prompt_count if prompt_count > 1 else base_workers
                    qps = None
                    logger.info(f"  Azureé—­æºæ¨¡å‹è‡ªé€‚åº”æ¨¡å¼: {prompt_count}ä¸ªprompt Ã— {base_workers} = {max_workers} workers")
            else:
                # Azureå¼€æºæ¨¡å‹ - æ”¯æŒpromptå¹¶å‘å€å¢
                if rate_mode == "fixed":
                    # å›ºå®šæ¨¡å¼ï¼šæ¯ä¸ªprompt 50 workers
                    base_workers = 50
                    max_workers = base_workers * prompt_count if prompt_count > 1 else base_workers
                    qps = 100
                    logger.info(f"  Azureå›ºå®šæ¨¡å¼: {prompt_count}ä¸ªprompt Ã— {base_workers} = {max_workers} workers")
                else:
                    # è‡ªé€‚åº”æ¨¡å¼ï¼šæ¯ä¸ªprompt 100 workers
                    base_workers = 100
                    max_workers = base_workers * prompt_count if prompt_count > 1 else base_workers
                    qps = None  # adaptiveä¸éœ€è¦QPS
                    logger.info(f"  Azureè‡ªé€‚åº”æ¨¡å¼: {prompt_count}ä¸ªprompt Ã— {base_workers} = {max_workers} workers")
        else:
            # é»˜è®¤é…ç½®
            if rate_mode == "fixed":
                base_workers = 30
                max_workers = base_workers * prompt_count if prompt_count > 1 else base_workers
                qps = 50
            else:
                base_workers = 50
                max_workers = base_workers * prompt_count if prompt_count > 1 else base_workers
                qps = None
        
        # æ„å»ºå‘½ä»¤
        cmd = [
            "python", "smart_batch_runner.py",
            "--model", shard.model,
            "--prompt-types", shard.prompt_types,
            "--difficulty", shard.difficulty,
            "--task-types", shard.task_types,
            "--num-instances", str(shard.num_instances),
            "--max-workers", str(max_workers),
            "--tool-success-rate", str(shard.tool_success_rate),
            "--batch-commit",
            "--checkpoint-interval", "20",
            "--ai-classification",
            "--no-save-logs"  # é¿å…æ—¥å¿—å†²çª
        ]
        
        # æ·»åŠ é™é»˜æ¨¡å¼å‚æ•°
        if silent:
            cmd.append("--silent")
        
        # æ ¹æ®rate_modeæ·»åŠ å‚æ•°
        if rate_mode == "fixed":
            cmd.extend(["--no-adaptive", "--qps", str(qps)])
        else:
            cmd.append("--adaptive")
        
        # æœ‰å¤šä¸ªpromptæ—¶æ‰æ·»åŠ prompt-parallel
        if use_prompt_parallel:
            cmd.append(use_prompt_parallel)
        
        # æ·»åŠ ç»“æœæ–‡ä»¶åç¼€
        if result_suffix:
            cmd.extend(["--result-suffix", result_suffix])
        
        logger.info(f"ğŸš€ å¯åŠ¨åˆ†ç‰‡ {shard.shard_id}: {shard.instance_name}")
        logger.info(f"   å®ä¾‹æ•°: {shard.num_instances}, æ¨¡å‹: {shard.model}")
        
        # å¯åŠ¨è¿›ç¨‹ï¼ˆå°†è¾“å‡ºé‡å®šå‘åˆ°DEVNULLä»¥é¿å…æ˜¾ç¤ºï¼‰
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.DEVNULL,  # ä¸æ•è·è¾“å‡ºï¼Œé¿å…ç¼“å†²åŒºé—®é¢˜
            stderr=subprocess.DEVNULL,  # ä¸æ•è·é”™è¯¯ï¼Œé¿å…ç¼“å†²åŒºé—®é¢˜
            text=True
        )
        
        self.active_tasks.add(shard.shard_id)
        return process
        
    def run_ultra_parallel_test(self, model: str, prompt_types: str, difficulty: str,
                               task_types: str = "all", num_instances: int = 20,
                               rate_mode: str = "adaptive", result_suffix: str = "",
                               silent: bool = False, tool_success_rate: float = 0.8) -> bool:
        """è¿è¡Œè¶…é«˜å¹¶è¡Œåº¦æµ‹è¯•
        
        Args:
            model: æ¨¡å‹åç§°
            prompt_types: æç¤ºç±»å‹
            difficulty: éš¾åº¦
            task_types: ä»»åŠ¡ç±»å‹
            num_instances: å®ä¾‹æ•°
            rate_mode: é€Ÿç‡æ¨¡å¼ - "adaptive" æˆ– "fixed"
        """
        
        logger.info(f"\nğŸ”¥ å¯åŠ¨è¶…é«˜å¹¶è¡Œæµ‹è¯•")
        logger.info(f"   æ¨¡å‹: {model}")
        logger.info(f"   Promptç±»å‹: {prompt_types}") 
        logger.info(f"   éš¾åº¦: {difficulty}")
        logger.info(f"   å®ä¾‹æ•°: {num_instances}")
        logger.info(f"   é€Ÿç‡æ¨¡å¼: {rate_mode}")
        
        # åˆ›å»ºä»»åŠ¡åˆ†ç‰‡
        shards = self.create_task_shards(model, prompt_types, difficulty, 
                                        task_types, num_instances, tool_success_rate)
        
        if not shards:
            logger.error("æ— æ³•åˆ›å»ºä»»åŠ¡åˆ†ç‰‡")
            return False
            
        logger.info(f"ğŸ“Š åˆ›å»ºäº† {len(shards)} ä¸ªå¹¶è¡Œåˆ†ç‰‡")
        
        # å¹¶è¡Œå¯åŠ¨æ‰€æœ‰åˆ†ç‰‡ï¼ˆé”™å¼€å¯åŠ¨é¿å…workflowç”Ÿæˆå†²çªï¼‰
        processes = []
        start_time = time.time()
        
        # æ™ºèƒ½åˆ†ç»„å¯åŠ¨ï¼Œå¹³è¡¡å¹¶å‘æ€§å’Œç¨³å®šæ€§
        # ç­–ç•¥ï¼šç¬¬ä¸€ä¸ªåˆ†ç‰‡ç«‹å³å¯åŠ¨ï¼Œåç»­åˆ†ç‰‡å»¶è¿Ÿå¯åŠ¨
        # è¿™æ ·ç¬¬ä¸€ä¸ªåˆ†ç‰‡çš„workflowç”Ÿæˆå¯ä»¥ä¸åç»­åˆ†ç‰‡çš„å¯åŠ¨å¹¶è¡Œ
        for i, shard in enumerate(shards):
            if i == 0:
                # ç¬¬ä¸€ä¸ªåˆ†ç‰‡ç«‹å³å¯åŠ¨
                process = self.execute_shard_async(shard, rate_mode=rate_mode, result_suffix=result_suffix, silent=silent)
                processes.append((shard, process))
                logger.info(f"ğŸš€ ç¬¬ä¸€ä¸ªåˆ†ç‰‡ {shard.shard_id} ç«‹å³å¯åŠ¨")
            elif i == 1:
                # ç¬¬äºŒä¸ªåˆ†ç‰‡å»¶è¿Ÿ30ç§’ï¼ˆè®©ç¬¬ä¸€ä¸ªåˆ†ç‰‡å®Œæˆå¤§éƒ¨åˆ†workflowç”Ÿæˆï¼‰
                logger.info(f"â±ï¸  å»¶è¿Ÿ30ç§’åå¯åŠ¨ç¬¬äºŒä¸ªåˆ†ç‰‡...")
                time.sleep(30)
                process = self.execute_shard_async(shard, rate_mode=rate_mode, result_suffix=result_suffix, silent=silent)
                processes.append((shard, process))
            else:
                # ç¬¬ä¸‰ä¸ªåŠåç»­åˆ†ç‰‡å»¶è¿Ÿ20ç§’ï¼ˆworkflowç”Ÿæˆé«˜å³°å·²è¿‡ï¼‰
                logger.info(f"â±ï¸  å»¶è¿Ÿ20ç§’åå¯åŠ¨åˆ†ç‰‡ {i+1}...")
                time.sleep(20)
                process = self.execute_shard_async(shard, rate_mode=rate_mode, result_suffix=result_suffix, silent=silent)
                processes.append((shard, process))
            
        # å¹¶å‘ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆï¼ˆçœŸæ­£çš„å¹¶å‘ï¼ï¼‰
        logger.info(f"â³ å¹¶å‘ç­‰å¾… {len(processes)} ä¸ªåˆ†ç‰‡å®Œæˆ...")
        
        success_count = 0
        failed_shards = []
        completed_shards = set()
        
        # ä½¿ç”¨è½®è¯¢æ–¹å¼æ£€æŸ¥æ‰€æœ‰è¿›ç¨‹çŠ¶æ€ï¼ˆéé˜»å¡ï¼‰
        while len(completed_shards) < len(processes):
            for shard, process in processes:
                if shard.shard_id in completed_shards:
                    continue
                    
                # éé˜»å¡æ£€æŸ¥è¿›ç¨‹çŠ¶æ€
                poll_result = process.poll()
                if poll_result is not None:  # è¿›ç¨‹å·²ç»“æŸ
                    completed_shards.add(shard.shard_id)
                    
                    if poll_result == 0:
                        logger.info(f"âœ… åˆ†ç‰‡ {shard.shard_id} å®Œæˆ")
                        success_count += 1
                        self._update_performance_score(shard.instance_name, True)
                    else:
                        logger.error(f"âŒ åˆ†ç‰‡ {shard.shard_id} å¤±è´¥ (é€€å‡ºç : {poll_result})")
                        failed_shards.append(shard.shard_id)
                        self._update_performance_score(shard.instance_name, False)
                    
                    # é‡Šæ”¾å®ä¾‹
                    if shard.instance_name in self.instance_pool:
                        instance = self.instance_pool[shard.instance_name]
                        instance.is_busy = False
                        instance.current_load = max(0, instance.current_load - shard.num_instances)
                    
                    self.active_tasks.discard(shard.shard_id)
            
            # çŸ­æš‚ä¼‘çœ é¿å…CPUå ç”¨è¿‡é«˜
            if len(completed_shards) < len(processes):
                time.sleep(1)
                
        end_time = time.time()
        duration = end_time - start_time
        
        # æŠ¥å‘Šç»“æœ
        logger.info(f"\nğŸ“Š å¹¶è¡Œæµ‹è¯•å®Œæˆ")
        logger.info(f"   æˆåŠŸ: {success_count}/{len(shards)} ä¸ªåˆ†ç‰‡")
        logger.info(f"   æ€»è€—æ—¶: {duration:.1f}ç§’")
        logger.info(f"   ç†è®ºåŠ é€Ÿæ¯”: {len(shards)}x")
        
        if failed_shards:
            logger.warning(f"   å¤±è´¥åˆ†ç‰‡: {failed_shards}")
            
        return len(failed_shards) == 0
        
    def _update_performance_score(self, instance_name: str, success: bool):
        """æ›´æ–°å®ä¾‹æ€§èƒ½è¯„åˆ†"""
        if instance_name not in self.performance_stats:
            self.performance_stats[instance_name] = {'success': 0, 'total': 0}
            
        stats = self.performance_stats[instance_name]
        stats['total'] += 1
        if success:
            stats['success'] += 1
            
        # è®¡ç®—æˆåŠŸç‡ä½œä¸ºæ€§èƒ½è¯„åˆ†
        success_rate = stats['success'] / stats['total']
        self.instance_pool[instance_name].performance_score = success_rate
        
    def get_resource_utilization(self) -> Dict:
        """è·å–èµ„æºåˆ©ç”¨ç‡ç»Ÿè®¡"""
        total_capacity = sum(inst.max_workers for inst in self.instance_pool.values())
        current_load = sum(inst.current_load for inst in self.instance_pool.values())
        busy_instances = sum(1 for inst in self.instance_pool.values() if inst.is_busy)
        
        return {
            "total_instances": len(self.instance_pool),
            "busy_instances": busy_instances,
            "total_capacity": total_capacity,
            "current_load": current_load,
            "utilization_rate": current_load / total_capacity if total_capacity > 0 else 0,
            "active_tasks": len(self.active_tasks)
        }

def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    import argparse
    import os
    
    parser = argparse.ArgumentParser(description="Ultra Parallel Test Runner")
    parser.add_argument("--model", required=True, help="æ¨¡å‹åç§°")
    parser.add_argument("--prompt-types", required=True, help="Promptç±»å‹")
    parser.add_argument("--difficulty", default="easy", help="éš¾åº¦")
    parser.add_argument("--task-types", default="all", help="ä»»åŠ¡ç±»å‹")
    parser.add_argument("--num-instances", type=int, default=20, help="å®ä¾‹æ•°é‡")
    parser.add_argument("--rate-mode", default="adaptive", 
                       choices=["adaptive", "fixed"],
                       help="é€Ÿç‡æ¨¡å¼: adaptive(åŠ¨æ€è°ƒæ•´) æˆ– fixed(å›ºå®šé€Ÿç‡)")
    parser.add_argument("--result-suffix", default="", 
                       help="ç»“æœæ–‡ä»¶åç¼€(ç”¨äºåŒºåˆ†é—­æº/å¼€æºæ¨¡å‹)")
    parser.add_argument("--silent", action="store_true",
                       help="é™é»˜æ¨¡å¼ï¼Œå‡å°‘è¾“å‡º")
    parser.add_argument("--tool-success-rate", type=float, default=0.8,
                       help="å·¥å…·æˆåŠŸç‡(0.0-1.0)")
    
    args = parser.parse_args()
    
    # ä¹Ÿå¯ä»¥ä»ç¯å¢ƒå˜é‡è¯»å–rate_mode
    rate_mode = args.rate_mode
    if os.environ.get("RATE_MODE"):
        rate_mode = os.environ.get("RATE_MODE")
        logger.info(f"ä½¿ç”¨ç¯å¢ƒå˜é‡ RATE_MODE: {rate_mode}")
    
    # æ³¨æ„ï¼šä¿ç•™loggerçš„INFOçº§åˆ«ä»¥æ˜¾ç¤ºé«˜å±‚çº§è¿›åº¦ä¿¡æ¯
    # silentå‚æ•°åªæ§åˆ¶è¯¦ç»†çš„æ‰§è¡Œè¾“å‡ºï¼Œä¸å½±å“ä¸»è¦è¿›åº¦æ˜¾ç¤º
    
    runner = UltraParallelRunner()
    
    # æ˜¾ç¤ºèµ„æºçŠ¶æ€
    util = runner.get_resource_utilization()
    logger.info(f"èµ„æºæ± çŠ¶æ€: {util['total_instances']}ä¸ªå®ä¾‹, å®¹é‡{util['total_capacity']}")
    
    # æ‰§è¡Œæµ‹è¯•
    success = runner.run_ultra_parallel_test(
        model=args.model,
        prompt_types=args.prompt_types,
        difficulty=args.difficulty,
        task_types=args.task_types,
        num_instances=args.num_instances,
        rate_mode=rate_mode,
        result_suffix=args.result_suffix,
        silent=args.silent,
        tool_success_rate=args.tool_success_rate
    )
    
    # æœ€ç»ˆç»Ÿè®¡
    final_util = runner.get_resource_utilization()
    logger.info(f"æœ€ç»ˆåˆ©ç”¨ç‡: {final_util['utilization_rate']:.1%}")
    
    return 0 if success else 1

if __name__ == "__main__":
    exit(main())