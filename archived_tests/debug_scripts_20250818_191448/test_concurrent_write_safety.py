#!/usr/bin/env python3
"""
æµ‹è¯•å¹¶å‘å†™å…¥æ•°æ®åº“çš„å®‰å…¨æ€§
éªŒè¯æ–‡ä»¶é”æœºåˆ¶å’Œäº‹åŠ¡å¤„ç†
"""

import json
import time
import threading
import multiprocessing
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import random

class ConcurrentWriteTester:
    """å¹¶å‘å†™å…¥æµ‹è¯•å™¨"""
    
    def __init__(self):
        self.db_path = Path("pilot_bench_cumulative_results/master_database.json")
        self.backup_path = None
        self.results = {
            'conflicts': 0,
            'successes': 0,
            'failures': 0,
            'data_losses': 0
        }
        self.lock = threading.Lock()
        
    def backup_database(self):
        """å¤‡ä»½åŸå§‹æ•°æ®åº“"""
        if self.db_path.exists():
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            self.backup_path = self.db_path.with_suffix(f'.backup_concurrent_test_{timestamp}.json')
            
            with open(self.db_path, 'r') as f:
                data = json.load(f)
            
            with open(self.backup_path, 'w') as f:
                json.dump(data, f, indent=2)
            
            print(f"âœ… å·²å¤‡ä»½æ•°æ®åº“åˆ°: {self.backup_path}")
            return data
        return {}
    
    def restore_database(self):
        """æ¢å¤æ•°æ®åº“"""
        if self.backup_path and self.backup_path.exists():
            with open(self.backup_path, 'r') as f:
                data = json.load(f)
            
            with open(self.db_path, 'w') as f:
                json.dump(data, f, indent=2)
            
            print(f"âœ… å·²æ¢å¤æ•°æ®åº“")
    
    def simulate_write_operation(self, worker_id, operation_id):
        """æ¨¡æ‹Ÿå†™å…¥æ“ä½œ"""
        try:
            # ä½¿ç”¨enhanced_cumulative_managerè¿›è¡Œå†™å…¥
            from enhanced_cumulative_manager import EnhancedCumulativeManager
            from cumulative_test_manager import TestRecord
            
            manager = EnhancedCumulativeManager()
            
            # åˆ›å»ºæµ‹è¯•è®°å½•
            test_model = f"concurrent-test-model-{worker_id}"
            record = TestRecord(
                model=test_model,
                task_type=f'task_{operation_id}',
                prompt_type='baseline',
                difficulty='easy'
            )
            record.tool_success_rate = 0.8
            record.success = random.choice([True, False])
            record.execution_time = random.uniform(1.0, 5.0)
            record.turns = random.randint(1, 10)
            record.tool_calls = random.randint(1, 5)
            
            # å°è¯•å†™å…¥
            start_time = time.time()
            success = manager.add_test_result_with_classification(record)
            elapsed = time.time() - start_time
            
            # è®°å½•ç»“æœ
            with self.lock:
                if success:
                    self.results['successes'] += 1
                else:
                    self.results['failures'] += 1
            
            return {
                'worker_id': worker_id,
                'operation_id': operation_id,
                'success': success,
                'elapsed': elapsed
            }
            
        except Exception as e:
            with self.lock:
                self.results['failures'] += 1
            return {
                'worker_id': worker_id,
                'operation_id': operation_id,
                'success': False,
                'error': str(e)
            }
    
    def test_thread_concurrency(self, num_threads=10, operations_per_thread=5):
        """æµ‹è¯•çº¿ç¨‹å¹¶å‘"""
        print(f"\nğŸ§µ æµ‹è¯•çº¿ç¨‹å¹¶å‘ ({num_threads} çº¿ç¨‹, æ¯çº¿ç¨‹ {operations_per_thread} æ“ä½œ)...")
        
        with ThreadPoolExecutor(max_workers=num_threads) as executor:
            futures = []
            for thread_id in range(num_threads):
                for op_id in range(operations_per_thread):
                    future = executor.submit(
                        self.simulate_write_operation,
                        f"thread_{thread_id}",
                        op_id
                    )
                    futures.append(future)
            
            # ç­‰å¾…æ‰€æœ‰æ“ä½œå®Œæˆ
            results = [f.result() for f in futures]
        
        # åˆ†æç»“æœ
        successful = sum(1 for r in results if r.get('success'))
        failed = sum(1 for r in results if not r.get('success'))
        avg_time = sum(r.get('elapsed', 0) for r in results) / len(results)
        
        print(f"  æˆåŠŸ: {successful}/{len(results)}")
        print(f"  å¤±è´¥: {failed}/{len(results)}")
        print(f"  å¹³å‡è€—æ—¶: {avg_time:.3f}ç§’")
        
        return results
    
    def test_process_concurrency(self, num_processes=5, operations_per_process=3):
        """æµ‹è¯•è¿›ç¨‹å¹¶å‘"""
        print(f"\nğŸ”§ æµ‹è¯•è¿›ç¨‹å¹¶å‘ ({num_processes} è¿›ç¨‹, æ¯è¿›ç¨‹ {operations_per_process} æ“ä½œ)...")
        
        # æ³¨æ„ï¼šProcessPoolExecutoréœ€è¦å¯åºåˆ—åŒ–çš„å‡½æ•°
        # è¿™é‡Œç®€åŒ–ä¸ºé¡ºåºæ‰§è¡Œå¤šä¸ªè¿›ç¨‹çš„æµ‹è¯•
        results = []
        for proc_id in range(num_processes):
            for op_id in range(operations_per_process):
                result = self.simulate_write_operation(f"process_{proc_id}", op_id)
                results.append(result)
        
        # åˆ†æç»“æœ
        successful = sum(1 for r in results if r.get('success'))
        failed = sum(1 for r in results if not r.get('success'))
        
        print(f"  æˆåŠŸ: {successful}/{len(results)}")
        print(f"  å¤±è´¥: {failed}/{len(results)}")
        
        return results
    
    def verify_data_integrity(self, original_data):
        """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
        print("\nğŸ” éªŒè¯æ•°æ®å®Œæ•´æ€§...")
        
        # è¯»å–å½“å‰æ•°æ®åº“
        with open(self.db_path, 'r') as f:
            current_data = json.load(f)
        
        # æ£€æŸ¥åŸå§‹æ•°æ®æ˜¯å¦ä¿ç•™
        original_models = set(original_data.get('models', {}).keys())
        current_models = set(current_data.get('models', {}).keys())
        
        lost_models = original_models - current_models
        new_models = current_models - original_models
        
        print(f"  åŸå§‹æ¨¡å‹æ•°: {len(original_models)}")
        print(f"  å½“å‰æ¨¡å‹æ•°: {len(current_models)}")
        print(f"  æ–°å¢æ¨¡å‹: {len(new_models)}")
        
        if lost_models:
            print(f"  âŒ ä¸¢å¤±çš„æ¨¡å‹: {lost_models}")
            self.results['data_losses'] += len(lost_models)
        else:
            print(f"  âœ… æ‰€æœ‰åŸå§‹æ•°æ®ä¿ç•™å®Œæ•´")
        
        # æ£€æŸ¥æµ‹è¯•æ•°æ®æ˜¯å¦æ­£ç¡®å†™å…¥
        test_models = [m for m in current_models if m.startswith('concurrent-test-model')]
        print(f"  æµ‹è¯•æ¨¡å‹å†™å…¥: {len(test_models)}")
        
        return len(lost_models) == 0
    
    def run_comprehensive_test(self):
        """è¿è¡Œç»¼åˆæµ‹è¯•"""
        print("=" * 60)
        print("å¹¶å‘å†™å…¥å®‰å…¨æ€§æµ‹è¯•")
        print("=" * 60)
        
        # 1. å¤‡ä»½åŸå§‹æ•°æ®
        original_data = self.backup_database()
        
        try:
            # 2. æµ‹è¯•çº¿ç¨‹å¹¶å‘
            thread_results = self.test_thread_concurrency(
                num_threads=10,
                operations_per_thread=5
            )
            
            # 3. æµ‹è¯•è¿›ç¨‹å¹¶å‘
            process_results = self.test_process_concurrency(
                num_processes=3,
                operations_per_process=5
            )
            
            # 4. éªŒè¯æ•°æ®å®Œæ•´æ€§
            integrity_ok = self.verify_data_integrity(original_data)
            
            # 5. ç”ŸæˆæŠ¥å‘Š
            self.generate_report(thread_results, process_results, integrity_ok)
            
        finally:
            # 6. æ¢å¤æ•°æ®åº“
            self.restore_database()
    
    def generate_report(self, thread_results, process_results, integrity_ok):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "=" * 60)
        print("æµ‹è¯•æŠ¥å‘Š")
        print("=" * 60)
        
        total_operations = len(thread_results) + len(process_results)
        
        print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡:")
        print(f"  æ€»æ“ä½œæ•°: {total_operations}")
        print(f"  æˆåŠŸå†™å…¥: {self.results['successes']}")
        print(f"  å¤±è´¥å†™å…¥: {self.results['failures']}")
        print(f"  æ•°æ®ä¸¢å¤±: {self.results['data_losses']}")
        
        success_rate = (self.results['successes'] / total_operations * 100) if total_operations > 0 else 0
        print(f"  æˆåŠŸç‡: {success_rate:.1f}%")
        
        print(f"\nğŸ”’ å¹¶å‘å®‰å…¨æ€§:")
        if self.results['data_losses'] == 0:
            print("  âœ… æœªæ£€æµ‹åˆ°æ•°æ®ä¸¢å¤±")
        else:
            print(f"  âŒ æ£€æµ‹åˆ° {self.results['data_losses']} ä¸ªæ•°æ®ä¸¢å¤±")
        
        if self.results['conflicts'] == 0:
            print("  âœ… æœªæ£€æµ‹åˆ°å†™å…¥å†²çª")
        else:
            print(f"  âš ï¸ æ£€æµ‹åˆ° {self.results['conflicts']} ä¸ªå†™å…¥å†²çª")
        
        if integrity_ok:
            print("  âœ… æ•°æ®å®Œæ•´æ€§éªŒè¯é€šè¿‡")
        else:
            print("  âŒ æ•°æ®å®Œæ•´æ€§éªŒè¯å¤±è´¥")
        
        print(f"\nğŸ“ ç»“è®º:")
        if success_rate > 95 and self.results['data_losses'] == 0 and integrity_ok:
            print("  âœ… å¹¶å‘å†™å…¥æœºåˆ¶å®‰å…¨å¯é ")
        elif success_rate > 80 and self.results['data_losses'] == 0:
            print("  âš ï¸ å¹¶å‘å†™å…¥åŸºæœ¬å®‰å…¨ï¼Œä½†æœ‰æ”¹è¿›ç©ºé—´")
        else:
            print("  âŒ å¹¶å‘å†™å…¥å­˜åœ¨é—®é¢˜ï¼Œéœ€è¦ä¿®å¤")

def main():
    """ä¸»å‡½æ•°"""
    tester = ConcurrentWriteTester()
    tester.run_comprehensive_test()
    
    return 0

if __name__ == "__main__":
    exit(main())