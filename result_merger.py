#!/usr/bin/env python3
"""
ç»“æœåˆå¹¶å™¨ - ResultCollectorçš„æ ¸å¿ƒç»„ä»¶
è´Ÿè´£ä»ä¸´æ—¶æ–‡ä»¶æ”¶é›†ç»“æœå¹¶ç»Ÿä¸€å†™å…¥æ•°æ®åº“
"""

import os
import json
import time
import threading
import logging
from pathlib import Path
from typing import Dict, List, Optional
from datetime import datetime
from collections import defaultdict
from merger_lock import get_merger_lock

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å¯¼å…¥å¿…è¦çš„æ¨¡å—
from cumulative_test_manager import TestRecord
from enhanced_cumulative_manager import EnhancedCumulativeManager

# é¿å…å¾ªç¯å¯¼å…¥ï¼Œæœ¬åœ°å®ç°managerè·å–
def _get_or_create_manager(use_ai_classification=True):
    """è·å–æˆ–åˆ›å»ºmanagerå®ä¾‹"""
    return EnhancedCumulativeManager(use_ai_classification=use_ai_classification)


class ResultMerger:
    """
    ç»“æœåˆå¹¶å™¨ - è´Ÿè´£æ”¶é›†å’Œåˆå¹¶æ‰€æœ‰ä¸´æ—¶ç»“æœæ–‡ä»¶
    å•ä¾‹æ¨¡å¼ï¼Œç¡®ä¿åªæœ‰ä¸€ä¸ªè¿›ç¨‹å†™å…¥æ•°æ®åº“
    """
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not hasattr(self, '_initialized'):
            self.temp_dir = Path("temp_results")
            self.temp_dir.mkdir(exist_ok=True)
            self.processed_files = set()  # å·²å¤„ç†çš„æ–‡ä»¶
            self.merge_interval = 10  # åˆå¹¶é—´éš”ï¼ˆç§’ï¼‰
            self.is_running = False
            self.merge_thread = None
            self._initialized = True
            logger.info("ResultMergeråˆå§‹åŒ–å®Œæˆ")
    
    def start(self, interval: int = 10):
        """
        å¯åŠ¨åå°åˆå¹¶çº¿ç¨‹
        
        Args:
            interval: åˆå¹¶é—´éš”ï¼ˆç§’ï¼‰
        """
        if self.is_running:
            logger.warning("åˆå¹¶çº¿ç¨‹å·²åœ¨è¿è¡Œ")
            return
        
        # å°è¯•è·å–åˆå¹¶å™¨é”
        lock = get_merger_lock()
        if not lock.acquire():
            logger.warning(f"å¦ä¸€ä¸ªåˆå¹¶å™¨å·²åœ¨è¿è¡Œ (PID: {lock.get_lock_owner()})")
            return
        
        logger.info(f"ğŸš€ å¯åŠ¨ResultMergerï¼Œåˆå¹¶é—´éš”: {interval}ç§’")
        self.merge_interval = interval
        self.is_running = True
        self.merge_thread = threading.Thread(target=self._merge_loop, daemon=True)
        self.merge_thread.start()
        logger.info(f"âœ… ResultMergeråå°çº¿ç¨‹å·²å¯åŠ¨ï¼Œæ”¯æŒæ™ºèƒ½åœæ­¢æœºåˆ¶")
    
    def stop(self):
        """åœæ­¢åˆå¹¶çº¿ç¨‹"""
        if not self.is_running:
            logger.info("åˆå¹¶çº¿ç¨‹å·²ç»åœæ­¢ï¼Œæ— éœ€é‡å¤æ“ä½œ")
            return
        
        logger.info("ğŸ›‘ æ‰‹åŠ¨åœæ­¢ResultMerger...")
        self.is_running = False
        
        if self.merge_thread:
            logger.info("ç­‰å¾…åˆå¹¶çº¿ç¨‹ç»“æŸ...")
            # ğŸ”§ ä¿®å¤ï¼šç¼©çŸ­joinè¶…æ—¶æ—¶é—´ï¼Œé¿å…é•¿æ—¶é—´ç­‰å¾…
            self.merge_thread.join(timeout=2)
            if self.merge_thread.is_alive():
                logger.info("âš ï¸ åˆå¹¶çº¿ç¨‹ä»åœ¨è¿è¡Œï¼Œä½†ç”±äºä½¿ç”¨daemonçº¿ç¨‹ï¼Œä¸»ç¨‹åºé€€å‡ºæ—¶ä¼šè‡ªåŠ¨æ¸…ç†")
            else:
                logger.info("âœ… åˆå¹¶çº¿ç¨‹å·²æ­£å¸¸ç»“æŸ")
        
        # é‡Šæ”¾åˆå¹¶å™¨é”
        try:
            lock = get_merger_lock()
            lock.release()
            logger.info("ğŸ”“ åˆå¹¶å™¨é”å·²é‡Šæ”¾")
        except Exception as e:
            logger.warning(f"âš ï¸ é‡Šæ”¾åˆå¹¶å™¨é”æ—¶å‡ºç°é—®é¢˜: {e}")
        
        logger.info("ğŸ ResultMergerå·²å®Œå…¨åœæ­¢")
    
    def _merge_loop(self):
        """åˆå¹¶å¾ªç¯ - å¢å¼ºç‰ˆï¼šæ”¯æŒæ™ºèƒ½åœæ­¢é˜²æ­¢hangä½"""
        consecutive_empty_rounds = 0
        max_empty_rounds = 3  # è¿ç»­3è½®æ²¡æœ‰æ–‡ä»¶å°±è‡ªåŠ¨åœæ­¢
        
        logger.info(f"ResultMergerå¼€å§‹è¿è¡Œï¼Œæ™ºèƒ½åœæ­¢é˜ˆå€¼: {max_empty_rounds}è½®")
        
        while self.is_running:
            try:
                processed_files = self.merge_once()
                
                if processed_files == 0:
                    consecutive_empty_rounds += 1
                    logger.debug(f"æœ¬è½®æ— æ–‡ä»¶å¤„ç†ï¼Œè¿ç»­ç©ºè½®æ¬¡: {consecutive_empty_rounds}/{max_empty_rounds}")
                    
                    if consecutive_empty_rounds >= max_empty_rounds:
                        logger.info(f"ğŸ›‘ è¿ç»­{max_empty_rounds}è½®æ— æ–°æ–‡ä»¶ï¼Œè‡ªåŠ¨åœæ­¢åˆå¹¶å™¨é˜²æ­¢hangä½")
                        self.is_running = False
                        break
                else:
                    if consecutive_empty_rounds > 0:
                        logger.info(f"âœ… å¤„ç†äº†{processed_files}ä¸ªæ–‡ä»¶ï¼Œé‡ç½®ç©ºè½®æ¬¡è®¡æ•°å™¨")
                    consecutive_empty_rounds = 0  # é‡ç½®è®¡æ•°å™¨
                    
            except Exception as e:
                logger.error(f"åˆå¹¶å‡ºé”™: {e}")
                # å¼‚å¸¸æ—¶ä¸å¢åŠ ç©ºè½®æ¬¡è®¡æ•°ï¼Œé¿å…æ„å¤–é€€å‡º
            
            # ç­‰å¾…ä¸‹ä¸€æ¬¡åˆå¹¶
            if self.is_running:  # åªæœ‰åœ¨è¿˜éœ€è¦è¿è¡Œæ—¶æ‰ç­‰å¾…
                for _ in range(self.merge_interval):
                    if not self.is_running:
                        break
                    time.sleep(1)
        
        logger.info("ğŸ ResultMergeråˆå¹¶å¾ªç¯å·²ç»“æŸ")
    
    def merge_once(self) -> int:
        """
        æ‰§è¡Œä¸€æ¬¡åˆå¹¶æ“ä½œ
        
        Returns:
            åˆå¹¶çš„æ–‡ä»¶æ•°é‡
        """
        # æŸ¥æ‰¾æ‰€æœ‰ä¸´æ—¶ç»“æœæ–‡ä»¶
        temp_files = list(self.temp_dir.glob("*.json"))
        new_files = [f for f in temp_files if f not in self.processed_files]
        
        if not new_files:
            return 0
        
        logger.info(f"å‘ç°{len(new_files)}ä¸ªæ–°çš„ç»“æœæ–‡ä»¶")
        
        # æŒ‰æ¨¡å‹åˆ†ç»„
        results_by_model = defaultdict(list)
        
        for file_path in new_files:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä»ç„¶å­˜åœ¨ï¼ˆå¯èƒ½è¢«å…¶ä»–è¿›ç¨‹åˆ é™¤ï¼‰
            if not file_path.exists():
                logger.debug(f"æ–‡ä»¶å·²è¢«å…¶ä»–è¿›ç¨‹å¤„ç†: {file_path}")
                self.processed_files.add(file_path)
                continue
            
            try:
                # å°è¯•è¯»å–æ–‡ä»¶å†…å®¹
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                except FileNotFoundError:
                    # æ–‡ä»¶åœ¨æ‰“å¼€æ—¶è¢«åˆ é™¤äº†
                    logger.debug(f"æ–‡ä»¶åœ¨è¯»å–æ—¶è¢«åˆ é™¤: {file_path}")
                    self.processed_files.add(file_path)
                    continue
                except json.JSONDecodeError as e:
                    logger.error(f"JSONè§£æå¤±è´¥ {file_path}: {e}")
                    # ä»ç„¶æ ‡è®°ä¸ºå·²å¤„ç†ï¼Œé¿å…é‡å¤å°è¯•
                    self.processed_files.add(file_path)
                    continue
                
                model = data.get('model', 'unknown')
                results = data.get('results', [])
                
                # è½¬æ¢ä¸ºTestRecordå¯¹è±¡
                for result in results:
                    if result and not result.get('_merged', False):
                        record = self._create_test_record(model, result)
                        if record:
                            results_by_model[model].append(record)
                
                # æ ‡è®°ä¸ºå·²å¤„ç†
                self.processed_files.add(file_path)
                
                # å¤„ç†ååˆ é™¤æ–‡ä»¶ï¼ˆé¿å…å ç”¨ç©ºé—´ï¼‰
                try:
                    if file_path.exists():  # å†æ¬¡æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                        file_path.unlink()
                        logger.debug(f"åˆ é™¤å·²å¤„ç†æ–‡ä»¶: {file_path}")
                except FileNotFoundError:
                    pass  # æ–‡ä»¶å·²è¢«å…¶ä»–è¿›ç¨‹åˆ é™¤ï¼Œè¿™æ˜¯æ­£å¸¸çš„
                except Exception as e:
                    logger.warning(f"æ— æ³•åˆ é™¤æ–‡ä»¶{file_path}: {e}")
                
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶{file_path}å¤±è´¥: {e}")
                continue
        
        # æ‰¹é‡ä¿å­˜åˆ°æ•°æ®åº“
        total_saved = 0
        for model, records in results_by_model.items():
            saved = self._save_to_database(model, records)
            total_saved += saved
        
        logger.info(f"åˆå¹¶å®Œæˆï¼Œå…±å¤„ç†{len(new_files)}ä¸ªæ–‡ä»¶ï¼Œä¿å­˜{total_saved}æ¡è®°å½•")
        return len(new_files)
    
    def _create_test_record(self, model: str, result: Dict) -> Optional[TestRecord]:
        """
        ä»ç»“æœå­—å…¸åˆ›å»ºTestRecordå¯¹è±¡
        
        Args:
            model: æ¨¡å‹åç§°
            result: ç»“æœå­—å…¸
            
        Returns:
            TestRecordå¯¹è±¡æˆ–None
        """
        try:
            record = TestRecord(
                model=model,
                task_type=result.get("task_type", "unknown"),
                prompt_type=result.get("prompt_type", "baseline"),
                difficulty=result.get("difficulty", "easy")
            )
            
            # è®¾ç½®å…¶ä»–å­—æ®µ
            for field in ['timestamp', 'success', 'success_level', 'execution_time', 
                         'turns', 'tool_calls', 'workflow_score', 'phase2_score', 
                         'quality_score', 'final_score', 'error_type', 'error_message',
                         'tool_success_rate', 'is_flawed', 'flaw_type', 'execution_status',
                         'ai_error_category', 'tool_coverage_rate', 'format_turns',
                         'assisted_turns', 'assisted_success']:
                if field in result:
                    setattr(record, field, result[field])
            
            return record
            
        except Exception as e:
            logger.error(f"åˆ›å»ºTestRecordå¤±è´¥: {e}")
            return None
    
    def _save_to_database(self, model: str, records: List[TestRecord]) -> int:
        """
        ä¿å­˜è®°å½•åˆ°æ•°æ®åº“
        
        Args:
            model: æ¨¡å‹åç§°
            records: è®°å½•åˆ—è¡¨
            
        Returns:
            æˆåŠŸä¿å­˜çš„æ•°é‡
        """
        if not records:
            return 0
        
        try:
            # è·å–æˆ–åˆ›å»ºmanagerï¼ˆå•ä¾‹ï¼‰ - å¢å¼ºæ•°æ®ä¿æŠ¤
            manager = _get_or_create_manager(use_ai_classification=True)
            
            # ğŸ”§ æ•°æ®ä¿æŠ¤ä¿®å¤ï¼šä½¿ç”¨æ™ºèƒ½åˆå¹¶è€Œä¸æ˜¯å¼ºåˆ¶é‡æ–°åŠ è½½
            # ç§»é™¤å¼ºåˆ¶é‡æ–°åŠ è½½ï¼Œè®©managerä½¿ç”¨å…¶å†…ç½®çš„æ–‡ä»¶é”å’Œåˆå¹¶æœºåˆ¶
            logger.info(f"[MERGER_PROTECTION] ä½¿ç”¨managerå†…ç½®çš„å®‰å…¨åˆå¹¶æœºåˆ¶")
            
            success_count = 0
            for record in records:
                try:
                    if manager.append_test_result(record):
                        success_count += 1
                except Exception as e:
                    logger.error(f"ä¿å­˜è®°å½•å¤±è´¥: {e}")
                    continue
            
            logger.info(f"æ¨¡å‹{model}ä¿å­˜{success_count}/{len(records)}æ¡è®°å½•")
            return success_count
            
        except Exception as e:
            logger.error(f"æ•°æ®åº“ä¿å­˜å¤±è´¥: {e}")
            return 0
    
    def force_merge_all(self) -> int:
        """
        å¼ºåˆ¶åˆå¹¶æ‰€æœ‰æœªå¤„ç†çš„æ–‡ä»¶
        
        Returns:
            åˆå¹¶çš„æ–‡ä»¶æ•°é‡
        """
        # é‡ç½®å·²å¤„ç†æ–‡ä»¶åˆ—è¡¨ï¼Œå¼ºåˆ¶å¤„ç†æ‰€æœ‰æ–‡ä»¶
        self.processed_files.clear()
        return self.merge_once()
    
    def cleanup_old_files(self, days: int = 7):
        """
        æ¸…ç†æ—§çš„ä¸´æ—¶æ–‡ä»¶
        
        Args:
            days: ä¿ç•™å¤©æ•°
        """
        cutoff_time = time.time() - (days * 24 * 60 * 60)
        cleaned = 0
        
        for file_path in self.temp_dir.glob("*.json"):
            if file_path.stat().st_mtime < cutoff_time:
                try:
                    file_path.unlink()
                    cleaned += 1
                    if file_path in self.processed_files:
                        self.processed_files.remove(file_path)
                except Exception as e:
                    logger.error(f"åˆ é™¤æ–‡ä»¶{file_path}å¤±è´¥: {e}")
        
        if cleaned > 0:
            logger.info(f"æ¸…ç†äº†{cleaned}ä¸ªæ—§æ–‡ä»¶")
        
        return cleaned


# å…¨å±€åˆå¹¶å™¨å®ä¾‹
_global_merger = None

def get_merger() -> ResultMerger:
    """è·å–å…¨å±€åˆå¹¶å™¨å®ä¾‹"""
    global _global_merger
    if _global_merger is None:
        _global_merger = ResultMerger()
    return _global_merger


def start_auto_merge(interval: int = 10):
    """
    å¯åŠ¨è‡ªåŠ¨åˆå¹¶
    
    Args:
        interval: åˆå¹¶é—´éš”ï¼ˆç§’ï¼‰
    """
    merger = get_merger()
    merger.start(interval)
    return merger


def stop_auto_merge():
    """åœæ­¢è‡ªåŠ¨åˆå¹¶"""
    merger = get_merger()
    merger.stop()


def force_merge():
    """å¼ºåˆ¶æ‰§è¡Œä¸€æ¬¡åˆå¹¶"""
    merger = get_merger()
    return merger.force_merge_all()


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("å¯åŠ¨ResultMergeræµ‹è¯•...")
    
    # å¯åŠ¨è‡ªåŠ¨åˆå¹¶
    merger = start_auto_merge(interval=5)
    
    print("åˆå¹¶å™¨å·²å¯åŠ¨ï¼Œæ¯5ç§’æ£€æŸ¥ä¸€æ¬¡")
    print("æŒ‰Ctrl+Cåœæ­¢...")
    
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("\nåœæ­¢åˆå¹¶å™¨...")
        stop_auto_merge()
        
        # æœ€åå¼ºåˆ¶åˆå¹¶ä¸€æ¬¡
        print("æ‰§è¡Œæœ€ç»ˆåˆå¹¶...")
        count = force_merge()
        print(f"æœ€ç»ˆåˆå¹¶äº†{count}ä¸ªæ–‡ä»¶")