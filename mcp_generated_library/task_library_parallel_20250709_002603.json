{
  "tasks": [
    {
      "instance_id": "task_a7df66d2",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format.",
      "inputs": {
        "data": {
          "values": [
            96,
            5,
            88,
            99,
            41
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:16.668250",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d2348100",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria, such as removing any entries that do not meet certain conditions. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            20,
            78,
            49,
            99,
            57
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 9.36305627044685
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:19.024100",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_ea2581de",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to filter the validated data based on specified criteria to reduce the dataset size. Step 4: Use data_processing_transformer to transform the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            19,
            83,
            18,
            24,
            46
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:21.030005",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_92f586b2",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria, reducing the dataset to only relevant entries. Step 4: Use data_processing_transformer to transform the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            31,
            87,
            93,
            51,
            77
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:23.069307",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_efe660bb",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to filter the validated data based on specified criteria, retaining only the relevant records. Step 4: Use the data_processing_transformer tool to convert the filtered data into JSON format for further processing.",
      "inputs": {
        "data": {
          "values": [
            22,
            87,
            41,
            3,
            59
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:25.348848",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_53ca00ec",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 3: Use data_processing_filter to selectively filter the valid data based on specified criteria. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further processing.",
      "inputs": {
        "data": {
          "values": [
            91,
            33,
            25,
            44,
            61
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:27.378785",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_643dec2d",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure it meets the requirements. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 4: Use the data_processing_transformer tool to convert the filtered data from JSON format to XML format.",
      "inputs": {
        "data": {
          "values": [
            79,
            4,
            74,
            83,
            59
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 5.85369463933485,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:29.835945",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_31f3c7b2",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 3: Use the data_processing_filter to filter the validated data based on specified criteria, narrowing down the dataset to relevant entries. Step 4: Use the data_processing_transformer to convert the filtered data from JSON format to XML format for compatibility with other systems.",
      "inputs": {
        "data": {
          "values": [
            7,
            26,
            49,
            8,
            40
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:32.223293",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_74b45e20",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 4: Use the data_processing_transformer tool to transform the filtered data from JSON format to XML format for further usage.",
      "inputs": {
        "data": {
          "values": [
            8,
            84,
            33,
            81,
            72
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 1.3897774858961893
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:34.211894",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_72013492",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to filter the validated data based on specified criteria, selecting only the relevant entries. Step 4: Use the data_processing_transformer tool to transform the filtered data from JSON format to XML format for further usage.",
      "inputs": {
        "data": {
          "values": [
            26,
            56,
            30,
            98,
            60
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:36.421049",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4085c91d",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to filter the validated data based on specified criteria to reduce the dataset size. Step 4: Finally, use the data_processing_transformer tool to transform the filtered data from JSON format to XML format.",
      "inputs": {
        "data": {
          "values": [
            92,
            74,
            30,
            51,
            95
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 3.9433429539069342
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:16.592980",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5e0d5593",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. Step 2: Next, use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: After the data is validated, use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 4: Finally, use the data_processing_transformer tool to transform the filtered data from JSON format to XML format.",
      "inputs": {
        "data": {
          "values": [
            28,
            81,
            45,
            52,
            75
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:18.694658",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_76606f88",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 4: Use the data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            36,
            58,
            7,
            53,
            64
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 6.719093178468598,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:20.794149",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e6249468",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria, reducing the dataset to only the necessary entries. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format, ensuring it meets the required output specifications.",
      "inputs": {
        "data": {
          "values": [
            71,
            3,
            96,
            34,
            65
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 8.78300321757319
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:22.935690",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_7ce78edd",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 4: Use the data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            69,
            72,
            72,
            3,
            21
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "low"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:25.171250",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_44eb58fe",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 4: Use data_processing_transformer to transform the filtered data from JSON format to XML format for further processing.",
      "inputs": {
        "data": {
          "values": [
            19,
            17,
            98,
            67,
            19
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:27.463559",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_adcd6f15",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 4: Use the data_processing_transformer tool to convert the filtered data from JSON format to XML format.",
      "inputs": {
        "data": {
          "values": [
            82,
            33,
            95,
            18,
            93
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 3.602333055464526
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:29.677335",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_92eaba81",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria, keeping only the necessary records. Step 4: Use data_processing_transformer to transform the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            2,
            42,
            43,
            23,
            79
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:32.076572",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_7f7a4a28",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 4: Use the data_processing_transformer tool to transform the filtered data from JSON format to XML format.",
      "inputs": {
        "data": {
          "values": [
            36,
            63,
            89,
            45,
            41
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:34.111116",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_90aa88a1",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria, such as removing entries that do not meet certain conditions. Step 4: Use the data_processing_transformer to convert the filtered data from the structured format into a JSON format for easier consumption.",
      "inputs": {
        "data": {
          "values": [
            92,
            78,
            30,
            44,
            70
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:36.447318",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_45d9c1f0",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to filter the validated data based on specific criteria, such as removing entries that do not meet a certain condition. Step 4: Use data_processing_transformer to transform the filtered data from JSON format to XML format for further processing.",
      "inputs": {
        "data": {
          "values": [
            71,
            16,
            18,
            26,
            71
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:16.597464",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_66ac81cf",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 4: Use data_processing_transformer to transform the filtered data from JSON format to XML format.",
      "inputs": {
        "data": {
          "values": [
            84,
            31,
            85,
            13,
            77
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:18.629773",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c2bf6d02",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 4: Use data_processing_transformer to transform the filtered data from JSON format to XML format.",
      "inputs": {
        "data": {
          "values": [
            74,
            46,
            57,
            29,
            50
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:20.362680",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9b469098",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 4: Use the data_processing_transformer to transform the filtered data from JSON format to XML format, if necessary.",
      "inputs": {
        "data": {
          "values": [
            89,
            46,
            36,
            74,
            63
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:22.260790",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_208f239a",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format.",
      "inputs": {
        "data": {
          "values": [
            69,
            66,
            10,
            92,
            18
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 5.794475417616016
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:24.969417",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_aef9d05d",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to filter the validated data based on specified criteria to extract only the relevant information. Step 4: Use data_processing_transformer to transform the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            60,
            63,
            5,
            78,
            68
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 1.9830933082597768,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:27.107486",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_12fe6c71",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. This will organize the data for further processing. Step 2: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. This step checks if the data meets the schema requirements. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant information. Step 4: Use the data_processing_transformer tool to convert the filtered data from JSON format to XML format, preparing it for final output.",
      "inputs": {
        "data": {
          "values": [
            86,
            17,
            9,
            18,
            30
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:29.760611",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_70bbfe0b",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for easier consumption. Step 5: Use computation_calculator to perform any necessary calculations on the transformed data, ensuring accurate results.",
      "inputs": {
        "data": {
          "values": [
            93,
            94,
            2,
            66,
            55
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "computation_calculator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:32.541836",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a3bd7007",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria, such as removing any entries that do not meet certain conditions. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            66,
            50,
            80,
            26,
            46
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:34.679929",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_818c6f10",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against the defined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 4: Use data_processing_transformer to transform the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            15,
            25,
            82,
            65,
            19
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:36.648482",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_6df15180",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specific criteria. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format.",
      "inputs": {
        "data": {
          "values": [
            92,
            85,
            56,
            84,
            28
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:16.416512",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a0096186",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse the raw data file (e.g., CSV) into a structured format. Step 2: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria (e.g., filtering out invalid entries). Step 4: Use the data_processing_transformer tool to convert the filtered data from its current format (e.g., JSON) to a different format (e.g., XML) if required.",
      "inputs": {
        "data": {
          "values": [
            25,
            6,
            50,
            30,
            33
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:18.693080",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_fbddb220",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specific criteria (e.g., removing entries that do not meet certain conditions). Step 4: Use data_processing_transformer to transform the filtered data from JSON format to XML format for further processing.",
      "inputs": {
        "data": {
          "values": [
            70,
            36,
            35,
            42,
            13
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:20.691111",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1fcb3bec",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 4: Use the data_processing_transformer tool to transform the filtered data from JSON format to XML format for further processing.",
      "inputs": {
        "data": {
          "values": [
            83,
            46,
            35,
            22,
            75
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 0.9968337634760991
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:22.673277",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e4e6f6e7",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 3: Use the data_processing_filter tool to selectively filter the valid data based on specific criteria you define. Step 4: Use the data_processing_transformer tool to convert the filtered data from JSON format to XML format, if needed.",
      "inputs": {
        "data": {
          "values": [
            36,
            62,
            99,
            28,
            89
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "low"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:25.407657",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_b262425b",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the parsed data against the defined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 4: Use the data_processing_transformer to transform the filtered data from JSON format to XML format. Step 5: Use the computation_calculator to perform any necessary calculations on the transformed data.",
      "inputs": {
        "data": {
          "values": [
            60,
            98,
            36,
            40,
            97
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "computation_calculator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:28.271609",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_ce454873",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw CSV data into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria, such as only including records that meet certain conditions. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            57,
            99,
            13,
            50,
            70
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 5.760744943454732
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:30.318424",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_87d8a6ed",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the valid data based on specified criteria. Step 4: Use the data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            47,
            3,
            28,
            34,
            77
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:32.626808",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_291a39ac",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specific criteria, such as removing entries that do not meet certain conditions. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            73,
            93,
            92,
            98,
            14
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 1.7245709698118181
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:34.942624",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_aa532344",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw CSV data into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria, such as removing any entries that do not meet specific conditions. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format, ensuring it is in the desired output format.",
      "inputs": {
        "data": {
          "values": [
            28,
            75,
            58,
            61,
            81
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:37.585791",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_09b7d7a5",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria, retaining only the relevant entries. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for reporting purposes.",
      "inputs": {
        "data": {
          "values": [
            80,
            4,
            4,
            67,
            12
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:16.868656",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_acec97b1",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness. Step 3: Use data_processing_filter to filter the validated data based on specified criteria, such as removing any entries that do not meet certain conditions. Step 4: Use data_processing_transformer to transform the filtered data from JSON format into XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            82,
            90,
            34,
            55,
            88
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 9.780042762240882
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:18.911581",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_38824857",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse the raw data from a CSV file into a structured format. Step 2: Next, use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: If the data is valid, use the data_processing_transformer tool to convert the validated data from JSON format to XML format. Step 4: Finally, use the data_processing_filter tool to selectively filter the transformed XML data based on specified criteria, such as retaining only relevant records.",
      "inputs": {
        "data": {
          "values": [
            34,
            14,
            63,
            84,
            78
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 4.063181953349553
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:21.140972",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_39b10a3b",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw CSV data into a structured format. Step 2: Use data_processing_validator to validate the parsed data against the defined schema to ensure its correctness. Step 3: Use data_processing_filter to filter the valid data based on specified criteria (e.g., removing entries with null values). Step 4: Use data_processing_transformer to transform the filtered data from JSON format to XML format for further usage.",
      "inputs": {
        "data": {
          "values": [
            17,
            83,
            26,
            71,
            28
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:23.132124",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_676b7c7c",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria (e.g., only include records where a certain field meets a condition). Step 4: Use the data_processing_transformer tool to transform the filtered data from JSON format to XML format to prepare it for further use.",
      "inputs": {
        "data": {
          "values": [
            86,
            87,
            36,
            38,
            93
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 6.837559296268072,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:25.762765",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_ee4048b3",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria, such as removing entries with missing values. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            21,
            28,
            21,
            21,
            91
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:27.885719",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_45bf0111",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria to reduce the dataset size. Step 4: Use data_processing_transformer to transform the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            16,
            56,
            16,
            25,
            86
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:30.520099",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_cd1c1a44",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness. Step 3: Use data_processing_filter to selectively filter the valid data based on specified criteria (e.g., filtering out entries with missing values). Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            41,
            55,
            12,
            42,
            5
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 9.541263991926618
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:33.070748",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5a10bb5c",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specific criteria. Step 4: Use the data_processing_transformer tool to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            23,
            49,
            58,
            1,
            94
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 0.43945656406044664,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:35.492164",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8cf54818",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse the raw data from a CSV file into a structured format. This will organize the data for further processing. Step 2: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the valid data based on specified criteria, reducing the dataset to only the relevant information. Step 4: Finally, use the data_processing_transformer tool to transform the filtered data from JSON format to XML format for easier integration with other systems.",
      "inputs": {
        "data": {
          "values": [
            90,
            40,
            57,
            71,
            41
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 3.03188950358154
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:37.964808",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c0fd15b2",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the valid data based on specified criteria to reduce the dataset. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further processing.",
      "inputs": {
        "data": {
          "values": [
            19,
            96,
            45,
            65,
            17
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:16.438351",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_2eb08cca",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            60,
            17,
            75,
            100,
            63
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:18.740824",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_69f1cad6",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria to reduce the dataset. Step 4: Use the data_processing_transformer tool to transform the filtered data from JSON format to XML format, preparing it for further use.",
      "inputs": {
        "data": {
          "values": [
            3,
            43,
            48,
            81,
            97
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:20.996096",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a143ec57",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria to reduce the amount of data processed. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format, applying the necessary transformations.",
      "inputs": {
        "data": {
          "values": [
            31,
            36,
            92,
            33,
            91
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:23.100521",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_b5c8f461",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness. Step 3: Use the data_processing_filter to filter the validated data based on specified criteria, such as removing any entries that do not meet certain thresholds. Step 4: Use the data_processing_transformer to convert the filtered data from JSON format to XML format for further processing or storage.",
      "inputs": {
        "data": {
          "values": [
            80,
            6,
            22,
            16,
            76
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 7.6605215427755
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:25.677377",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0dc5d40d",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for better compatibility. Step 5: Use computation_calculator to perform any necessary calculations on the transformed data, if applicable.",
      "inputs": {
        "data": {
          "values": [
            37,
            73,
            92,
            16,
            24
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "computation_calculator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 3.022250321332213,
        "priority": "low"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:28.165264",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_335ad0ea",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness. Step 3: Use the data_processing_filter to selectively filter the valid data based on specific criteria, such as removing entries with null values. Step 4: Use the data_processing_transformer to transform the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            22,
            4,
            82,
            9,
            8
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 7.151062299855969
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:30.867879",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_b72714c1",
      "task_type": "simple_task",
      "description": "Step 1: Use the 'data_processing_parser' tool to parse raw data from a CSV file into a structured format. This will extract the data and organize it for further processing. Step 2: Use the 'data_processing_validator' tool to validate the structured data against the predefined schema. This step ensures that the data meets the required standards. Step 3: Use the 'data_processing_filter' tool to filter the validated data based on specific criteria, allowing you to focus on the relevant information. Step 4: Finally, use the 'data_processing_transformer' tool to transform the filtered data from JSON format to XML format for compatibility with other systems.",
      "inputs": {
        "data": {
          "values": [
            60,
            2,
            34,
            71,
            86
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:33.825269",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_bff0ea58",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 3: Use data_processing_filter to filter the validated data based on specific criteria to only include relevant entries. Step 4: Use data_processing_transformer to transform the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            27,
            43,
            97,
            47,
            27
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:35.983997",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8b5c423c",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria, such as removing entries that do not meet certain conditions. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for easier integration with other systems.",
      "inputs": {
        "data": {
          "values": [
            57,
            8,
            27,
            75,
            87
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 8.018003076951045,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:38.138809",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d99de464",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 3: Use the data_processing_filter tool to filter the validated data based on specific criteria, keeping only the relevant records. Step 4: Use the data_processing_transformer tool to convert the filtered data from JSON format to XML format for further processing.",
      "inputs": {
        "data": {
          "values": [
            97,
            40,
            32,
            7,
            96
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 0.5354277190689986
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:16.446223",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_ecb395d2",
      "task_type": "simple_task",
      "description": "Step 1: Use the 'data_processing_parser' tool to parse raw data from a CSV file into a structured format. Step 2: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the 'data_processing_filter' tool to selectively filter the validated data based on specified criteria. Step 4: Use the 'data_processing_transformer' tool to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            66,
            11,
            14,
            54,
            72
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:18.967823",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9130e218",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to apply filtering options on the validated data to select only the relevant records. Step 4: Use the data_processing_transformer tool to convert the filtered data from JSON format to XML format for better compatibility with other systems.",
      "inputs": {
        "data": {
          "values": [
            22,
            9,
            68,
            89,
            2
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "low"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:21.129336",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0e1ab79c",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria, narrowing down the dataset to only the relevant entries. Step 4: Use the data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            50,
            60,
            24,
            28,
            52
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:23.879600",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9c1ab582",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, such as removing entries that do not meet certain conditions. Step 4: Use the data_processing_transformer tool to convert the filtered data from JSON format to XML format for easier integration with other systems.",
      "inputs": {
        "data": {
          "values": [
            44,
            93,
            61,
            58,
            86
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 1.256110388439631,
        "priority": "low"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:26.074339",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e3cc2b50",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the valid data based on specified criteria to reduce the dataset size. Step 4: Use the data_processing_transformer to transform the filtered data from JSON format to XML format.",
      "inputs": {
        "data": {
          "values": [
            100,
            45,
            94,
            15,
            25
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 5.342747518086064
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:28.275069",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_08da732b",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the valid data based on specified criteria. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            76,
            30,
            43,
            27,
            84
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 2.365281140559023,
        "priority": "low"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:30.336563",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_977b67a8",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to filter the validated data based on specified criteria, keeping only the relevant entries. Step 4: Use the data_processing_transformer tool to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            26,
            37,
            13,
            49,
            16
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:32.780552",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_7e79f76f",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse the raw data from a CSV file into a structured format. This will organize the data into a more usable form. Step 2: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. This step will check if the data meets the schema requirements. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, such as removing any entries that do not meet certain conditions. Step 4: Use the data_processing_transformer tool to transform the filtered data into a different format, for example, converting it from JSON to XML, making it suitable for further processing or analysis.",
      "inputs": {
        "data": {
          "values": [
            80,
            89,
            21,
            87,
            37
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:35.823155",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c5847246",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specific criteria. Step 4: Use the data_processing_transformer tool to transform the filtered data from JSON format to XML format.",
      "inputs": {
        "data": {
          "values": [
            83,
            75,
            20,
            47,
            47
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 7.000294987431285,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:38.171295",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4c026ee7",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 4: Finally, use the data_processing_transformer tool to convert the filtered data from JSON format to XML format for easier sharing.",
      "inputs": {
        "data": {
          "values": [
            85,
            38,
            21,
            12,
            49
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:17.130182",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_3c169dfa",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. This will help to organize the data for further processing. Step 2: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure that it meets the necessary requirements. This step checks for data integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, allowing you to focus on the relevant information. Step 4: Use the data_processing_transformer tool to convert the filtered data from JSON format to XML format for easier sharing and compatibility with other systems.",
      "inputs": {
        "data": {
          "values": [
            82,
            91,
            73,
            29,
            34
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 2.0758625285399264
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:19.805388",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a7d1c288",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria to reduce the dataset size. Step 4: Use the data_processing_transformer tool to transform the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            3,
            55,
            60,
            97,
            56
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:21.808530",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9806fb46",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 4: Use the data_processing_transformer tool to convert the filtered data from JSON format to XML format for further processing.",
      "inputs": {
        "data": {
          "values": [
            92,
            56,
            82,
            28,
            89
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 7.35799032448793
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:24.649782",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_45f6e6d2",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria to narrow down the dataset. Step 4: Use data_processing_transformer to transform the filtered data from JSON format to XML format for easier integration with other systems.",
      "inputs": {
        "data": {
          "values": [
            27,
            46,
            46,
            23,
            81
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "low"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:27.218160",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d880bb61",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to read raw data from a CSV file and convert it into a structured format. Step 2: Use data_processing_validator to check the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria to reduce the dataset. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            13,
            17,
            75,
            52,
            97
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 6.800139951890656,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:29.416377",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_93f95072",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria, such as removing entries with null values. Step 4: Use the data_processing_transformer to convert the filtered data from JSON format to XML format for further processing.",
      "inputs": {
        "data": {
          "values": [
            94,
            8,
            48,
            28,
            29
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:31.834000",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_92eb09ca",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria, such as removing duplicates or specific values. Step 4: Finally, use the data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            34,
            42,
            63,
            27,
            45
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:33.997282",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a04f4137",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria to reduce the amount of data processed.",
      "inputs": {
        "data": {
          "values": [
            67,
            23,
            68,
            58,
            4
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:35.822741",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a05bec0d",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria, such as removing entries with null values. Step 4: Use the data_processing_transformer to convert the filtered data from JSON format to XML format for further processing.",
      "inputs": {
        "data": {
          "values": [
            66,
            77,
            94,
            57,
            27
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 7.65907639654969
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:38.261057",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a49ead04",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 4: Use the data_processing_transformer tool to convert the filtered data from JSON format to XML format.",
      "inputs": {
        "data": {
          "values": [
            34,
            65,
            89,
            96,
            32
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:17.030840",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a50b8e7a",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 4: Use the data_processing_transformer tool to transform the filtered data from JSON format to XML format.",
      "inputs": {
        "data": {
          "values": [
            29,
            34,
            72,
            2,
            41
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:19.037055",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_cdc78a69",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 3: Use data_processing_filter to selectively filter the validated data based on specific criteria, such as removing entries that do not meet certain conditions. Step 4: Use data_processing_transformer to transform the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            40,
            91,
            65,
            34,
            18
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:21.733826",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f8b4501b",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria, such as removing records that do not meet certain conditions. Step 4: Use the data_processing_transformer to transform the filtered data from JSON format to XML format for easier integration with other systems.",
      "inputs": {
        "data": {
          "values": [
            90,
            18,
            21,
            94,
            19
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:24.266973",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_bc0af147",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria, such as removing entries with null values. Step 4: Use data_processing_transformer to transform the filtered data from JSON format to XML format for further processing.",
      "inputs": {
        "data": {
          "values": [
            72,
            96,
            31,
            17,
            41
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 0.9111876483141159
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:26.247658",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9b3f4065",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria, such as removing entries that do not meet certain conditions. Step 4: Use the data_processing_transformer to transform the filtered data into a different format, such as converting it from JSON to XML. Step 5: Finally, use the computation_calculator to perform any necessary calculations on the transformed data, ensuring accurate results.",
      "inputs": {
        "data": {
          "values": [
            42,
            19,
            85,
            34,
            81
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "computation_calculator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:29.064472",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9193c08e",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria, such as only including entries that meet certain conditions. Step 4: Use the data_processing_transformer to convert the filtered data from JSON format to XML format for further processing.",
      "inputs": {
        "data": {
          "values": [
            10,
            69,
            94,
            79,
            4
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:31.376505",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4e05173b",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse the raw data from a CSV file into a structured format. This will allow us to work with the data more easily. Step 2: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria, such as excluding certain rows or columns. Step 4: Use the data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            83,
            48,
            74,
            66,
            94
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:33.991425",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_888c57c2",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria, such as removing entries that do not meet a certain condition. Step 4: Use the data_processing_transformer to transform the filtered data from JSON format to XML format for further processing or storage.",
      "inputs": {
        "data": {
          "values": [
            75,
            22,
            73,
            75,
            68
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 9.628729108556199
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:36.153357",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_bdab9231",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter out any records from the validated data that do not meet specific criteria. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            84,
            94,
            1,
            4,
            66
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:38.449749",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_46ad4d81",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria, such as removing entries that do not meet certain conditions. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further processing or output.",
      "inputs": {
        "data": {
          "values": [
            30,
            46,
            22,
            68,
            1
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:17.186715",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1ca89fde",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 3: Use data_processing_filter to selectively filter the valid data based on specific criteria (e.g., only keep records where a certain column value meets a condition). Step 4: Use data_processing_transformer to transform the filtered data from JSON format to XML format for further processing or output.",
      "inputs": {
        "data": {
          "values": [
            72,
            80,
            86,
            11,
            19
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:19.515563",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_7a490699",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria, such as removing entries that do not meet certain conditions. Step 4: Finally, use data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            45,
            21,
            54,
            58,
            46
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "low"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:21.614274",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_6d974b8c",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant entries. Step 4: Use data_processing_transformer to transform the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            40,
            44,
            2,
            45,
            51
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:24.052124",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4a66adf4",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, such as only including entries with a certain attribute. Step 4: Use the data_processing_transformer tool to convert the filtered data from JSON format to XML format for further processing or storage.",
      "inputs": {
        "data": {
          "values": [
            29,
            36,
            66,
            5,
            92
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:26.709346",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_78111fe6",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the valid data based on specified criteria. Step 4: Use the data_processing_transformer tool to transform the filtered data into JSON format for further use.",
      "inputs": {
        "data": {
          "values": [
            87,
            76,
            7,
            43,
            9
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "low"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:28.870525",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9e65f708",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria to reduce the dataset size. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format.",
      "inputs": {
        "data": {
          "values": [
            85,
            58,
            63,
            76,
            64
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "low"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:31.214471",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0d9ffd81",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria, such as removing entries that do not meet certain conditions. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for easier sharing. Step 5: Use computation_calculator to perform any necessary calculations on the transformed data, if required.",
      "inputs": {
        "data": {
          "values": [
            96,
            68,
            1,
            10,
            44
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "computation_calculator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:34.077100",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_51a8091e",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the valid data based on specified criteria, reducing the dataset to only relevant entries. Step 4: Use the data_processing_transformer tool to convert the filtered data from JSON format to XML format, preparing it for further use.",
      "inputs": {
        "data": {
          "values": [
            8,
            67,
            83,
            71,
            79
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:36.146274",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_95fe0a6c",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria (e.g., removing entries with missing values). Step 4: Use data_processing_transformer to transform the filtered data from JSON format to XML format for further processing or storage.",
      "inputs": {
        "data": {
          "values": [
            78,
            56,
            35,
            62,
            80
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 2.4279095763642853
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:38.505435",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_bee54b56",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, ensuring that only relevant data is retained. Step 4: Use the data_processing_transformer tool to transform the filtered data from JSON format to XML format for further processing.",
      "inputs": {
        "data": {
          "values": [
            88,
            83,
            7,
            3,
            70
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:17.056113",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0f650eb9",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria to reduce the dataset to only the relevant entries. Step 4: Use data_processing_transformer to transform the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            31,
            69,
            82,
            38,
            37
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:19.334058",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_7e56304a",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria, such as removing any entries that do not meet the criteria. Step 4: Use the data_processing_transformer to convert the filtered data from JSON format to XML format for further processing.",
      "inputs": {
        "data": {
          "values": [
            100,
            85,
            45,
            80,
            92
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:21.754003",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_357471eb",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 4: Use the data_processing_transformer to convert the filtered data from JSON format to XML format for further usage.",
      "inputs": {
        "data": {
          "values": [
            42,
            61,
            15,
            44,
            66
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 3.035453468484536
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:23.975010",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a6d744e3",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria, such as removing entries with missing values. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for output. Step 5: Use computation_calculator to perform basic statistical calculations (like average) on a specific column of the filtered data.",
      "inputs": {
        "data": {
          "values": [
            32,
            86,
            91,
            48,
            44
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "computation_calculator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:26.995752",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4af10af0",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 4: Use data_processing_transformer to transform the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            82,
            28,
            35,
            23,
            42
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:29.028895",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_3d9f6642",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse raw data from a CSV file into a structured JSON format. Step 2: Use the data_processing_validator to validate the parsed JSON data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to filter the validated data based on specified criteria, such as removing entries that do not meet a certain threshold. Step 4: Use the data_processing_transformer to convert the filtered JSON data into XML format for further use. Step 5: Use the computation_calculator to perform any necessary calculations on the transformed XML data, such as summing up certain values.",
      "inputs": {
        "data": {
          "values": [
            35,
            80,
            59,
            25,
            9
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "computation_calculator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:31.901212",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f3b39b93",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a defined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format.",
      "inputs": {
        "data": {
          "values": [
            21,
            25,
            70,
            80,
            33
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 2.4393791621751992
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:34.286194",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_7d74a0a1",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the valid data based on specified criteria, reducing the dataset to only what is needed. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further processing or storage.",
      "inputs": {
        "data": {
          "values": [
            67,
            80,
            82,
            8,
            66
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 3.187687948190149
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:36.384559",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f8c3f200",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 4: Use the data_processing_transformer tool to transform the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            8,
            17,
            80,
            85,
            82
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 8.535759890328327
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:38.902642",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4022a79d",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria, such as removing any entries that do not meet certain conditions. Step 4: Use the data_processing_transformer to convert the filtered data from JSON format to XML format for easier integration with other systems.",
      "inputs": {
        "data": {
          "values": [
            31,
            10,
            68,
            60,
            18
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 3.214957664433323
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:18.318368",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c583e855",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, narrowing down the dataset to only the relevant entries. Step 4: Use the data_processing_transformer tool to convert the filtered data from JSON format to XML format for further processing or storage.",
      "inputs": {
        "data": {
          "values": [
            22,
            41,
            16,
            66,
            14
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:20.504883",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_bdf2a893",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the valid data based on specified criteria (e.g., removing entries that do not meet a certain condition). Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further processing.",
      "inputs": {
        "data": {
          "values": [
            93,
            4,
            14,
            93,
            55
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:22.594130",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_388e1df9",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 4: Use the data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            32,
            14,
            43,
            50,
            95
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 5.966461520692924
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:25.266994",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_cd73c9f3",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: If the data is valid, use the data_processing_filter tool to selectively filter the data based on specified criteria. Step 4: Finally, use the data_processing_transformer tool to convert the filtered data from JSON format to XML format.",
      "inputs": {
        "data": {
          "values": [
            18,
            87,
            66,
            64,
            64
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:27.771516",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9892e6b7",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria, keeping only the relevant entries. Step 4: Use data_processing_transformer to transform the filtered data from JSON format to XML format for further processing.",
      "inputs": {
        "data": {
          "values": [
            27,
            76,
            6,
            19,
            85
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:29.902750",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_fa7d6d3a",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness. Step 3: Use data_processing_filter to filter the validated data based on specific criteria, such as removing entries that do not meet certain conditions. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for better compatibility with other systems.",
      "inputs": {
        "data": {
          "values": [
            32,
            61,
            60,
            47,
            41
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:32.540859",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_b604732d",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to apply filtering options to the validated data, selecting only the relevant records based on specified criteria. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for easier integration with other systems.",
      "inputs": {
        "data": {
          "values": [
            47,
            90,
            91,
            70,
            22
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 3.522320560263556
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:34.584533",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4240ed10",
      "task_type": "simple_task",
      "description": "Step 1: Use the 'data_processing_parser' to parse the raw data from a CSV file into a structured format. Step 2: Use the 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the 'data_processing_filter' to selectively filter the validated data based on specified criteria. Step 4: Use the 'data_processing_transformer' to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            23,
            11,
            20,
            89,
            30
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:36.788714",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8dc31782",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 4: Use the data_processing_transformer tool to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            96,
            74,
            22,
            68,
            2
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 1.3849136509704367
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:39.281907",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_b7e8cdd4",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve raw data from a specified API endpoint. Step 2: Use data_processing_parser to parse the retrieved raw data into a structured format for easier handling. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure that it meets the required standards. Step 4: Use data_processing_transformer to convert the validated data into the desired output format, such as JSON to XML. Step 5: Use network_poster to send the transformed data to a specified destination API endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:16.426242",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c65a39ab",
      "task_type": "api_integration",
      "description": "Step 1: Use the 'network_fetcher' to retrieve raw data from a specified API endpoint. Step 2: Use the 'data_processing_parser' to parse the retrieved raw data into a structured format. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_transformer' to transform the validated data into a desired output format. Step 5: Use the 'network_poster' to send the transformed data to a specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "high"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:18.477813",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5b4acfbc",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved into a structured format (e.g., JSON). Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to transform the validated data into a desired format (e.g., from JSON to XML). Step 5: Use network_poster to send the transformed data to the specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:21.321857",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0540f93b",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve raw data from a specified API endpoint. Step 2: Use data_processing_parser to parse the retrieved raw data into a structured format for easier handling. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format (e.g., converting from JSON to XML). Step 5: Use network_poster to send the transformed data to the specified destination API endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:24.230288",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_7ad4f7d0",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data obtained from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against the predefined schema to ensure its correctness and integrity. Step 4: If the data is valid, use network_poster to send the validated data to the specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:26.713685",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_2e921886",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve raw data from a specified API endpoint. Step 2: Use data_processing_parser to parse the retrieved raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired format (e.g., JSON to XML). Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:29.386055",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_88a44d1d",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:31.977059",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_70d61b51",
      "task_type": "api_integration",
      "description": "Step 1: Use the 'network_fetcher' to retrieve data from a specified API endpoint. Step 2: Use the 'data_processing_parser' to parse the raw data obtained from the API into a structured format. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_transformer' to transform the validated data into a different format as required (e.g., from JSON to XML). Step 5: Use the 'network_poster' to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:34.681718",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_730d8adc",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a different format as required. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "high"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:37.343151",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_2acc70c2",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve raw data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to the specified destination API endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:40.128555",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_3d6d5532",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse raw data from a CSV file into a structured format. This will help us organize the data for further processing. Step 2: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: If the data is valid, use the data_processing_filter to selectively filter the data based on specified criteria, such as removing any entries that do not meet certain conditions. Step 4: Finally, use the data_processing_transformer to convert the filtered data from JSON format to XML format, preparing it for output or further use.",
      "inputs": {
        "data": {
          "values": [
            46,
            20,
            14,
            96,
            10
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:17.516870",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_83012ded",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant information. Step 4: Use the data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            65,
            58,
            74,
            53,
            69
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:19.691597",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4f88927f",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to filter the valid data based on specified criteria, reducing the amount of data processed. Step 4: Use the data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            14,
            11,
            61,
            47,
            96
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:25.763769",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_2e62d9b1",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the valid data based on specified criteria, such as removing entries without required fields. Step 4: Finally, use the data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            87,
            23,
            5,
            23,
            29
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:28.322573",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a81f2088",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria to reduce the dataset. Step 4: Use the data_processing_transformer tool to transform the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            59,
            60,
            11,
            37,
            39
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 5.479692108299642
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:30.450800",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_67363a27",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness. Step 3: Use data_processing_filter to selectively filter the validated data based on specific criteria, such as removing entries that do not meet certain conditions. Step 4: Use data_processing_transformer to transform the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            46,
            79,
            56,
            90,
            13
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "low"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:32.351622",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e75c5856",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the valid data based on specified criteria. Step 4: Use the data_processing_transformer tool to transform the filtered data from JSON format to XML format.",
      "inputs": {
        "data": {
          "values": [
            99,
            54,
            74,
            81,
            22
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:34.254871",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f8a028c2",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 4: Use the data_processing_transformer tool to convert the filtered data from JSON format to XML format for further processing.",
      "inputs": {
        "data": {
          "values": [
            10,
            83,
            24,
            18,
            84
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 1.4016173766390885
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:36.406656",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_757774e2",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria, such as removing entries with missing values. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            3,
            20,
            59,
            66,
            34
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:38.295100",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1fafc03c",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, such as removing entries that do not meet certain conditions. Step 4: Use the data_processing_transformer tool to transform the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            52,
            76,
            46,
            60,
            7
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:40.922728",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_2d6015c6",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 4: Use the data_processing_transformer tool to transform the filtered data from JSON format to XML format for output.",
      "inputs": {
        "data": {
          "values": [
            59,
            44,
            98,
            68,
            85
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:19.370754",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1dfbe2f1",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria, such as removing entries that do not meet certain conditions. Step 4: Use the data_processing_transformer to convert the filtered data from JSON format to XML format for further processing or storage.",
      "inputs": {
        "data": {
          "values": [
            8,
            67,
            69,
            85,
            62
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:21.629428",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0708669c",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 4: Finally, use the data_processing_transformer to convert the filtered data from JSON format to XML format.",
      "inputs": {
        "data": {
          "values": [
            16,
            67,
            93,
            64,
            69
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:24.400628",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c3d3556c",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the valid data based on specific criteria, such as removing entries that do not meet certain conditions. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            74,
            15,
            3,
            55,
            4
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:26.998986",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_298c1355",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw CSV data into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness. Step 3: Use data_processing_filter to filter the validated data based on specified criteria, such as removing any entries that do not meet certain conditions. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for easier integration with other systems.",
      "inputs": {
        "data": {
          "values": [
            16,
            7,
            83,
            39,
            38
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:29.238854",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e79d88e4",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness. Step 3: Use the data_processing_filter to filter the validated data based on specified criteria, such as removing entries that do not meet certain conditions. Step 4: Use the data_processing_transformer to transform the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            59,
            68,
            11,
            89,
            37
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:31.763270",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c630e7ab",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria to focus on relevant information. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            19,
            64,
            69,
            11,
            24
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:34.225684",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4016f847",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 4: Use the data_processing_transformer to convert the filtered data into a JSON format for further use.",
      "inputs": {
        "data": {
          "values": [
            36,
            67,
            43,
            54,
            7
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:36.595000",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_cc26e975",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the parsed data against a defined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria, such as removing entries that do not meet certain conditions. Step 4: Use the data_processing_transformer to convert the filtered data from JSON format to XML format for further processing or storage.",
      "inputs": {
        "data": {
          "values": [
            93,
            85,
            51,
            50,
            25
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:38.972940",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0b94f32a",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, such as removing entries that do not meet certain conditions. Step 4: Use the data_processing_transformer tool to transform the filtered data from JSON format into XML format for further processing.",
      "inputs": {
        "data": {
          "values": [
            96,
            90,
            63,
            17,
            61
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:41.449364",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5c8daf51",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a specified file (e.g., CSV or JSON). Step 2: Use data_processing_parser to parse the raw data from the file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a defined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the valid data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.4932489377634982,
            0.4200463040784138,
            0.39596340106304995,
            0.24648121215838636,
            0.5177842758263692
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:16.380376",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_81cee45e",
      "task_type": "basic_task",
      "description": "Step 1: Use `file_operations_reader` to read a CSV file containing raw data. Step 2: Use `data_processing_parser` to parse the raw data from the CSV file into a structured format. Step 3: Use `data_processing_validator` to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use `data_processing_filter` to selectively filter the validated data based on specified criteria (e.g., specific columns or values). Step 5: Use `data_processing_transformer` to transform the filtered data into a different format, such as converting it from JSON to XML.",
      "inputs": {
        "input_data": {
          "data": [
            0.5048562096699103,
            0.5977049397740081,
            0.651147405168291,
            0.6760941328903103,
            0.1468415668212949
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:18.855127",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_6b0652f0",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read the data from a CSV file. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to filter the validated data based on specified criteria. Step 5: Use the data_processing_transformer tool to transform the filtered data into a different format, such as JSON.",
      "inputs": {
        "input_data": {
          "data": [
            0.37844809871659824,
            0.5594897192958641,
            0.4054095144476536,
            0.24935684964726557,
            0.7702498029404076
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:22.445400",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e2510a70",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read the data from a specified file (e.g., a CSV file). Step 2: Use the data_processing_parser tool to parse the raw data from the file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.14037692927826362,
            0.23474917328361178,
            0.9184625894776717,
            0.8550362115584729,
            0.06449430617833507
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 4.916989286627787
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:25.005894",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a3256b79",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.26319333850582316,
            0.08865388189247025,
            0.2113359016186651,
            0.3234709776542458,
            0.32012562707224035
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 5.7142966120195435
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:28.067373",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8eebadd8",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read a CSV file containing raw data. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_transformer to transform the filtered data into a different format, such as JSON.",
      "inputs": {
        "input_data": {
          "data": [
            0.6991609047612584,
            0.5824709981546087,
            0.9452350699418139,
            0.21429232293994427,
            0.9959497469043777
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:30.433623",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_6cbde800",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read a CSV file containing raw data. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria to obtain only the relevant information.",
      "inputs": {
        "input_data": {
          "data": [
            0.08015045185322434,
            0.032435226985952914,
            0.9499171199902998,
            0.849440822291404,
            0.3228451221234371
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 3.7147045075475034
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:33.778721",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9bc9997c",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a specified file (CSV, JSON, or XML format). Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.3386222636662721,
            0.31030179870714913,
            0.18063713754768174,
            0.9966189773893407,
            0.32164694431154206
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:35.683567",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_3333142e",
      "task_type": "basic_task",
      "description": "Step 1: Use the 'file_operations_reader' to read data from a file. This tool will help you retrieve the data you need from formats like CSV or JSON. Step 2: Use the 'data_processing_parser' to parse the raw data you have read into a structured format. This will organize the data for easier handling. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema. This ensures that the data is correct and meets the required standards. Step 4: Use the 'data_processing_filter' to selectively filter the valid data based on specific criteria you define. This will help you narrow down the data to what you need.",
      "inputs": {
        "input_data": {
          "data": [
            0.5407389935974358,
            0.4461398892678775,
            0.5416969414702053,
            0.3700022028884271,
            0.9982413004767723
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "low"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:39.911237",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c300c979",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader to read data from a CSV file. Step 2: Use the data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.5998930816508754,
            0.14825648491411403,
            0.12866607061121527,
            0.3013360195968592,
            0.4536267929985741
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:41.724831",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_bf06a79c",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read the data from a specified file (e.g., CSV, JSON, or XML). Step 2: Use the data_processing_parser tool to parse the raw data into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Optionally, use the data_processing_transformer tool to transform the filtered data into a different format if needed.",
      "inputs": {
        "input_data": {
          "data": [
            0.37073753258084585,
            0.929667543936176,
            0.3293589991534702,
            0.9866618085586559,
            0.4884508200081703
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 3.195985605430784
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:17.124347",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_3f26e976",
      "task_type": "basic_task",
      "description": "Step 1: Use the 'file_operations_reader' tool to read data from a CSV file containing raw data. Step 2: Use the 'data_processing_parser' tool to parse the raw data from the CSV file into a structured format. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specified criteria. Step 5: Use the 'data_processing_transformer' tool to transform the filtered data into a different format, such as JSON.",
      "inputs": {
        "input_data": {
          "data": [
            0.467679434608833,
            0.6435966629544198,
            0.5564270322499504,
            0.4034150139590408,
            0.973236488726347
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 6.075456511343888
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:20.673806",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_07a7c8c0",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specific criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.5415884425427777,
            0.8002705461447208,
            0.04003735726287094,
            0.45507055457517864,
            0.3907517334575561
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:23.707896",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_87cbfb84",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a CSV file. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema for correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.7340691081564175,
            0.7691894518960266,
            0.22305354929173726,
            0.42295948455919774,
            0.5880422010592418
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:25.627493",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9f0e7943",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read data from a specified CSV file. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_transformer tool to transform the filtered data into a different format, such as converting it from JSON to XML.",
      "inputs": {
        "input_data": {
          "data": [
            0.2773353001753812,
            0.6157237760672053,
            0.9168413600775205,
            0.5698583240083452,
            0.503515927644187
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 2.2071724249861644
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:30.115483",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e3a2e29a",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure it meets the required standards.",
      "inputs": {
        "input_data": {
          "data": [
            0.6160760690569937,
            0.3038703135212608,
            0.3789402024753603,
            0.0474837447049552,
            0.21278977093838003
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:32.982751",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_402a84e9",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader to read data from a specified file (e.g., a CSV file). Step 2: Use the data_processing_parser to parse the raw data from the file into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.40819487286384515,
            0.7595448653029188,
            0.16074133952677427,
            0.49340452578601823,
            0.9633793245549314
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "low"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:34.557739",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_240ea4f4",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read the data from a CSV file. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.7339306006558178,
            0.3337678488654142,
            0.2894459515584342,
            0.9744900095838401,
            0.39822758335880226
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 2.1200903360904486,
        "priority": "low"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:36.452069",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9ffbad3c",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a specified file (e.g., CSV). Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_transformer to transform the filtered data into a different format if needed.",
      "inputs": {
        "input_data": {
          "data": [
            0.5005622489867013,
            0.7862189637148189,
            0.22067253201511772,
            0.8370418033030124,
            0.46282115231708065
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:39.199006",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8f1da562",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read a CSV file containing raw data. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specific criteria. Step 5: Use the data_processing_transformer tool to transform the filtered data into a JSON format for easy consumption.",
      "inputs": {
        "input_data": {
          "data": [
            0.8481841744413684,
            0.4630457388827346,
            0.05094147354151035,
            0.7894215651408106,
            0.4023442996840363
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 2.1030816060961106
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:41.815649",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c1ee6371",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to filter the validated data based on specified criteria to retain only the relevant entries. Step 4: Use the data_processing_transformer tool to transform the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            12,
            18,
            64,
            31,
            79
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:16.535552",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4cf08b7e",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 4: Use the data_processing_transformer to convert the filtered data from JSON format to XML format.",
      "inputs": {
        "data": {
          "values": [
            2,
            88,
            93,
            54,
            15
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 9.296531208794493,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:19.115096",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5a56d45b",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 4: Use data_processing_transformer to transform the filtered data from JSON format to XML format.",
      "inputs": {
        "data": {
          "values": [
            50,
            48,
            52,
            85,
            54
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:23.718118",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_873bd5c8",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure it meets the required standards. Step 3: Use data_processing_filter to filter the validated data based on specific criteria to reduce the dataset to only the relevant entries. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            57,
            23,
            31,
            69,
            91
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:26.716285",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1c354d6f",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to filter the validated data based on specific criteria to reduce the dataset. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for easier sharing.",
      "inputs": {
        "data": {
          "values": [
            69,
            64,
            41,
            73,
            47
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:29.112986",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8ab4de11",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. This will help in organizing the data for further processing. Step 2: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. This will confirm that the data meets the required standards. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria, allowing us to focus on the relevant subset of data. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format, ensuring it can be utilized in different systems or applications.",
      "inputs": {
        "data": {
          "values": [
            72,
            58,
            17,
            12,
            44
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:31.897107",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_dac58205",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 4: Use the data_processing_transformer tool to transform the filtered data from JSON format to XML format.",
      "inputs": {
        "data": {
          "values": [
            30,
            83,
            80,
            88,
            24
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "low"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:34.414556",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5cd4029d",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse the raw data from a CSV file into a structured format. This will allow us to work with the data more easily. Step 2: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. This step ensures the data meets the required standards. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria, allowing us to focus on the relevant information. Step 4: Finally, use the data_processing_transformer to transform the filtered data into JSON format for easier integration with other systems.",
      "inputs": {
        "data": {
          "values": [
            41,
            16,
            35,
            96,
            82
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:37.197276",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9383acea",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to filter the validated data based on specified criteria to reduce the dataset size. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further processing.",
      "inputs": {
        "data": {
          "values": [
            42,
            6,
            79,
            17,
            100
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:39.621058",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4edd4d3e",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse the raw data from the provided CSV file into a structured format. Step 2: Use the data_processing_validator to validate the parsed data against the defined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to filter the valid data based on specified criteria to reduce the dataset to only the relevant information. Step 4: Use the data_processing_transformer to transform the filtered data from JSON format to XML format for further use. Step 5: Use the computation_calculator to perform any necessary calculations on the transformed data, if required.",
      "inputs": {
        "data": {
          "values": [
            19,
            11,
            40,
            74,
            60
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "computation_calculator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:42.270196",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5ff23e9b",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria, such as removing entries that do not meet certain conditions. Step 4: Use data_processing_transformer to transform the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            37,
            67,
            19,
            57,
            39
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:16.899141",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_eb02e85f",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant entries. Step 4: Use the data_processing_transformer tool to convert the filtered data from JSON format to XML format, ensuring it is ready for further use.",
      "inputs": {
        "data": {
          "values": [
            65,
            78,
            49,
            82,
            100
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:19.114163",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_cfca24c0",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, such as removing entries that do not meet certain conditions. Step 4: Use the data_processing_transformer tool to convert the filtered data from JSON format to XML format for further processing or storage.",
      "inputs": {
        "data": {
          "values": [
            89,
            83,
            15,
            56,
            90
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:23.794884",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_bfba0173",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria, such as removing any entries that do not meet certain conditions. Step 4: Use the data_processing_transformer to convert the filtered data from JSON format to XML format for easier sharing and integration. Step 5: Finally, use the computation_calculator to perform any necessary calculations on the transformed data, such as summing up specific numerical fields.",
      "inputs": {
        "data": {
          "values": [
            43,
            13,
            10,
            18,
            53
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "computation_calculator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:26.385212",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8ef9741a",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria, such as removing any entries that do not meet the criteria. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further usage.",
      "inputs": {
        "data": {
          "values": [
            49,
            16,
            20,
            18,
            17
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:28.946158",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_df96626c",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria to extract relevant information. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further processing.",
      "inputs": {
        "data": {
          "values": [
            61,
            5,
            85,
            6,
            7
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:31.238084",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f501aeb6",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria, removing any unwanted entries. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            19,
            68,
            25,
            43,
            55
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 2.762869186406496,
        "priority": "low"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:33.596479",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4ba01d4b",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria, such as removing entries that do not meet certain conditions. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further processing or storage.",
      "inputs": {
        "data": {
          "values": [
            31,
            62,
            88,
            98,
            31
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 4.27184503958516
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:36.034170",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c108ad12",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to apply filtering options to the validated data, selecting only the relevant entries based on specified criteria. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further processing or storage.",
      "inputs": {
        "data": {
          "values": [
            5,
            11,
            29,
            78,
            31
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 4.783216131884802,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:40.339582",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5a345abc",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 4: Use the data_processing_transformer tool to transform the filtered data from JSON format to XML format for further processing.",
      "inputs": {
        "data": {
          "values": [
            53,
            4,
            31,
            27,
            43
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 7.425247934029543
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:42.704590",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_b349daa1",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a different format as required. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "medium"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:16.917477",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_982aa558",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a different format as required (e.g., from JSON to XML). Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:19.538272",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f27083a4",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:21.844692",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0f7e9b9b",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher to retrieve data from the specified API endpoint. Step 2: Use the data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: If the data is valid, use the network_poster to send the validated data to the specified destination. Step 5: If the data is not valid, log the errors for further analysis.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 2.6216979038209676
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:24.575521",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_92735f41",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher to retrieve raw data from a specified API endpoint. Step 2: Use the data_processing_parser to parse the raw data into a structured format for easier handling. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_transformer to transform the validated data into the desired output format, such as converting JSON to XML. Step 5: Use the network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:28.668482",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1e7ba814",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the parsed data against the defined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to the specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:33.386220",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c9894851",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data obtained from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 8.666519209509866
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:35.694190",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9f207827",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data received from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a defined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format (e.g., JSON to XML). Step 5: Use network_poster to send the transformed data to a specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:37.924244",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_3aefa09c",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a different format as required. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 5.601002490359336
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:41.122203",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1c0f8efc",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: If the data is valid, use network_poster to send the validated data to a specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:43.121266",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a3fe94f0",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use network_poster to send the validated data to the specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:16.680782",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_dfdc7311",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the retrieved raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to convert the validated data into a different format as required. Step 5: Use network_poster to send the transformed data to the specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 3.6409837903666893,
        "priority": "medium"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:18.865834",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5f1073d3",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data received from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "high"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:21.396330",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_94acab69",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the retrieved raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a defined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:26.231888",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e12efa6d",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher to retrieve data from a specified API endpoint. Step 2: Use the data_processing_parser to parse the raw data retrieved into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the network_poster to send the validated data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 2.817936575833509,
        "priority": "high"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:30.082828",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4dc0168f",
      "task_type": "api_integration",
      "description": "Step 1: Use the `network_fetcher` to retrieve data from the specified API endpoint. Step 2: Use the `data_processing_parser` to parse the raw data retrieved from the API into a structured format. Step 3: Use the `data_processing_validator` to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: If the data is valid, use the `network_poster` to send the validated data to the specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:32.242366",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_ad3d2b57",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the retrieved raw data into a structured format for easier manipulation. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format (e.g., from JSON to XML). Step 5: Use network_poster to send the transformed data to a specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:34.993086",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_523db1d3",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from an external API that provides user information in JSON format. Step 2: Use data_processing_parser to parse the retrieved JSON data into a structured format that includes user details such as name, email, and address. Step 3: Use data_processing_validator to validate the structured user data against a predefined schema to ensure all required fields are present and correctly formatted. Step 4: Use data_processing_transformer to convert the validated user data from JSON format to XML format for compatibility with another system. Step 5: Use network_poster to send the transformed XML data to a specified endpoint for further processing.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:37.388191",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9b4ec9e3",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to the specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:40.565580",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_08fb1201",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a different format as required. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 9.437181292999039
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:43.279290",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_610d848d",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve raw data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data into a structured format such as JSON. Step 3: Use data_processing_validator to validate the structured data against a defined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to convert the validated data from JSON format to XML format. Step 5: Use network_poster to send the transformed XML data to a specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 7.007262239224564
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:17.231339",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5e84acf8",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the retrieved raw data into a structured format. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a desired output format. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:19.313733",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_33d7cfd1",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve raw data from a specified API endpoint. Step 2: Use data_processing_parser to parse the retrieved raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to a specified destination endpoint over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:21.599503",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a51b7bf9",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved into a structured format. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to transform the validated data into the required output format. Step 5: Use network_poster to send the transformed data to a specified destination API endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:26.700865",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_59ddbac4",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve raw data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to convert the validated data into a desired output format. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:29.285858",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_15cf3d60",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher to retrieve data from a specified API endpoint. Step 2: Use the data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_transformer to transform the validated data into a desired format (e.g., from JSON to XML). Step 5: Use the network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 4.025915893796629
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:32.334380",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_23093681",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data obtained from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to the specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 1.947917786882706
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:35.003169",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e4dae57b",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data received from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into another format, if necessary. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:37.284962",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_7f82bb2f",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: If the data is valid, use network_poster to send the validated data to the designated destination endpoint. Step 5: In case of any issues during validation, use file_operations_reader to read an error log file to identify the nature of the problem.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "network_poster",
        "file_operations_reader"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:40.236460",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5b2b8e5a",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data obtained from the API into a structured format. Step 3: Use data_processing_validator to check the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to the specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:43.289513",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_3f1b3bd4",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read the data from the input file (e.g., a CSV file). Step 2: Use the data_processing_parser tool to parse the raw data into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_transformer tool to transform the filtered data into a desired output format (e.g., convert it from JSON to XML).",
      "inputs": {
        "input_data": {
          "data": [
            0.4297046077909661,
            0.8073674961919887,
            0.6635822201078364,
            0.785570350702751,
            0.11922788084483049
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:17.034370",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8ec84130",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read the data from a specified file, such as a CSV or JSON file. Step 2: Use the data_processing_parser tool to parse the raw data obtained from the file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a defined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.2139265469647662,
            0.6102854753876648,
            0.17019826458938558,
            0.8124899644280584,
            0.5685038328454438
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 6.699672698089597
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:19.647025",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_b1058274",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a CSV file. Step 2: Use data_processing_parser to parse the raw CSV data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema. Step 4: Use data_processing_filter to filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.9575509852065589,
            0.14742975167732508,
            0.38750772312897375,
            0.22286512693486538,
            0.46430249125339884
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:24.183825",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_74bb4bd4",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader to read the data from a specified file (e.g., a CSV or JSON file). Step 2: Use the data_processing_parser to parse the raw data obtained from the file into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.39181138943845983,
            0.3552590201289181,
            0.7823274710548727,
            0.04312894975452197,
            0.8525978448239252
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 1.8085768060540302,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:26.052503",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0bf392ae",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader to read the data from a source file (e.g., a CSV file). Step 2: Use the data_processing_parser to parse the raw data from the file into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_transformer to transform the filtered data into a different format (e.g., from JSON to XML) for final output.",
      "inputs": {
        "input_data": {
          "data": [
            0.28598817124679643,
            0.09266408844489049,
            0.8168904544547992,
            0.4434230868325706,
            0.2524224937851891
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:29.224538",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_12c5f5df",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a CSV file. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_transformer to convert the filtered data into a different format, such as JSON.",
      "inputs": {
        "input_data": {
          "data": [
            0.2293420108722355,
            0.6541601618388152,
            0.6351602974311341,
            0.8122015583058035,
            0.5238067026829804
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:32.801396",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_85c9edd3",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a file (e.g., CSV, JSON, or XML format). Step 2: Use data_processing_parser to parse the raw data into a structured format for easier manipulation. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_transformer to transform the filtered data into a different format if needed (e.g., converting from JSON to XML).",
      "inputs": {
        "input_data": {
          "data": [
            0.10804509195239909,
            0.8305359262159477,
            0.19985511230390196,
            0.07167819069538284,
            0.8001152303212663
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "low"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:36.848001",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_bd447652",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read a CSV file containing raw data. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema, ensuring its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria to reduce the dataset. Step 5: Use data_processing_transformer to transform the filtered data into a different format, such as converting it from JSON to XML.",
      "inputs": {
        "input_data": {
          "data": [
            0.8327383203450048,
            0.43487265047118806,
            0.6754576934407275,
            0.6303908920631854,
            0.07225896550773503
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:39.925650",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_ffb1a340",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a specified file (e.g., CSV, JSON). Step 2: Use data_processing_parser to parse the raw data into a structured format for easier manipulation. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure it meets the required standards.",
      "inputs": {
        "input_data": {
          "data": [
            0.4128183184884937,
            0.2648843579648511,
            0.9111925109601617,
            0.18938775024869348,
            0.6751202721082468
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "low"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:41.648087",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_74feeaaa",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file. Step 2: Use data_processing_parser to parse the raw CSV data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.3093292585660634,
            0.23380193127311055,
            0.33002546552627776,
            0.824873467780376,
            0.0012768305969598215
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:43.827340",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e1b689f5",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved into a structured format. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a different format as required by the target system. Step 5: Use network_poster to send the transformed data to the specified destination API endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:16.402458",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_38fdff55",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from an external API endpoint. Step 2: Use data_processing_parser to parse the raw data obtained from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure it meets the required standards. Step 4: Use data_processing_transformer to convert the validated data into a desired output format (e.g., from JSON to XML). Step 5: Use network_poster to send the transformed data to a specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 5.20027724295966,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:19.468552",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_46e93caa",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved into a structured format. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a different format if necessary. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:21.816108",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_49a32550",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to convert the validated data into the desired output format (e.g., JSON to XML). Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:26.168074",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_93b21197",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw JSON data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a different format (e.g., from JSON to XML). Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:30.530075",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_3915a195",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to convert the validated data into a different format (e.g., from JSON to XML) if needed. Step 5: Finally, use network_poster to send the transformed data to a specified destination API endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:32.889711",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_87a40eb1",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a different format, if necessary. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:35.253383",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_98a68757",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve raw data from a specified API endpoint. Step 2: Use data_processing_parser to parse the retrieved raw data into a structured format (e.g., JSON). Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a different format (e.g., from JSON to XML). Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:37.396569",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f2f6f603",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the retrieved raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 2.308529519117622
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:39.722320",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f0db4d1f",
      "task_type": "api_integration",
      "description": "Step 1: Use the 'network_fetcher' tool to retrieve data from a specified API endpoint. This will involve specifying the URL of the API and any required parameters to get the necessary data. Step 2: Use the 'data_processing_parser' tool to parse the raw data retrieved from the API into a structured format, such as converting the JSON response into a more usable structure. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_transformer' tool to transform the validated data into the desired output format, such as converting it from JSON to XML if needed. Step 5: Finally, use the 'network_poster' tool to send the transformed data to a specified destination, such as another API endpoint or a database.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:43.935628",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_3ecef24c",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.058077224677456574,
            0.3057061897848231,
            0.34612742626775406,
            0.18777367107887422,
            0.6357394127059495
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:16.934566",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_978b4595",
      "task_type": "basic_task",
      "description": "Step 1: Use the 'file_operations_reader' to read data from a specified file (e.g., a CSV file). Step 2: Use the 'data_processing_parser' to parse the raw data retrieved from the file into a structured format. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' to selectively filter the validated data based on specified criteria. Step 5: Optionally, use the 'data_processing_transformer' to convert the filtered data into a different format if needed.",
      "inputs": {
        "input_data": {
          "data": [
            0.8773833659224335,
            0.57477834536644,
            0.8870784793234563,
            0.6589033274432086,
            0.49392998067202454
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:20.351240",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_aec3a553",
      "task_type": "basic_task",
      "description": "Step 1: Use 'file_operations_reader' to read the data from a CSV file containing customer information. Step 2: Use 'data_processing_parser' to parse the raw data from the CSV file into a structured format. Step 3: Use 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use 'data_processing_filter' to selectively filter the validated data based on specific criteria, such as customers from a particular city. Step 5: Use 'data_processing_transformer' to transform the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "input_data": {
          "data": [
            0.7301380935628703,
            0.44821026203642256,
            0.062182855893007094,
            0.44728046848536274,
            0.5730360153696556
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:23.159471",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1f6eb94c",
      "task_type": "basic_task",
      "description": "Step 1: Use the 'file_operations_reader' tool to read the data from a specified file (e.g., a CSV file). Step 2: Use the 'data_processing_parser' tool to parse the raw data retrieved from the file into a structured format. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specified criteria. Step 5: Optionally, use the 'data_processing_transformer' tool to convert the filtered data into a different format if needed.",
      "inputs": {
        "input_data": {
          "data": [
            0.12580244207593738,
            0.5004494772931046,
            0.23734176943848306,
            0.6197289018337332,
            0.2825775357439734
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 4.932907464227617
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:26.724146",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1e98732e",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read a CSV file containing raw data. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.24518453622091563,
            0.6290063633355671,
            0.6442145600160241,
            0.2898531101624189,
            0.07302752401677981
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:32.769298",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1d262bf9",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a specified CSV file. Step 2: Use data_processing_parser to parse the raw data retrieved from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a defined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.258899886427818,
            0.8902507552280992,
            0.5044665922648808,
            0.8429237132456672,
            0.5829514841836811
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:34.476744",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d8b01323",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a CSV file containing raw data. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_transformer to transform the filtered data into a different format, such as JSON.",
      "inputs": {
        "input_data": {
          "data": [
            0.8150690554711797,
            0.5888069086178279,
            0.4326070873029312,
            0.19321026790006202,
            0.36455010639952046
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 0.4517555706896824
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:36.902625",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_46bb2145",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a CSV file. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.1467841175671699,
            0.5247324686855722,
            0.4586449024498923,
            0.22687344867339077,
            0.8925900355096281
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 9.467077561493248
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:39.315830",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_3bc2cdf9",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file containing raw information. Step 2: Use data_processing_parser to parse the raw data into a structured format, making it easier to work with. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.7936802993894461,
            0.8588830141123798,
            0.8532164630164932,
            0.6561808213009798,
            0.22414221369218812
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:41.444795",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_468f4737",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader to read data from a CSV file. Step 2: Use the data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_transformer to transform the filtered data into a different format, such as JSON.",
      "inputs": {
        "input_data": {
          "data": [
            0.742784306725625,
            0.6943308596698213,
            0.1211804703679702,
            0.19465277677849235,
            0.9206959351041814
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:44.141541",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_35ab9de9",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.9512865066053566,
            0.9095339369650042,
            0.8220073627276387,
            0.39428152625101576,
            0.1495915522169705
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:17.400050",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_49f26483",
      "task_type": "basic_task",
      "description": "Step 1: Use 'file_operations_reader' to read data from a CSV file. Step 2: Use 'data_processing_parser' to parse the raw data from the CSV file into a structured format. Step 3: Use 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use 'data_processing_filter' to selectively filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.3575024592211199,
            0.783676883799775,
            0.6969443372543525,
            0.16684725316732651,
            0.5222686098174723
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "low"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:20.525072",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1129d9c0",
      "task_type": "basic_task",
      "description": "Step 1: Use the 'file_operations_reader' to read data from a source file in CSV format. Step 2: Use the 'data_processing_parser' to parse the raw CSV data into a structured format. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the 'data_processing_filter' to selectively filter the validated data based on specified criteria. Step 5: Use the 'data_processing_transformer' to transform the filtered data into a JSON format.",
      "inputs": {
        "input_data": {
          "data": [
            0.3695888452280286,
            0.8185586504515479,
            0.7233420952835083,
            0.5282614516845576,
            0.6092072392045255
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:26.979080",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0151186b",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.8904952942750144,
            0.11112742229861117,
            0.7549721048052767,
            0.008091691100500897,
            0.41335131260016034
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:28.845446",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_76c880ad",
      "task_type": "basic_task",
      "description": "Step 1: Use the 'file_operations_reader' tool to read data from a CSV file containing raw data. Step 2: Use the 'data_processing_parser' tool to parse the raw data extracted from the CSV file into a structured format. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.007671943291759242,
            0.2024184599452401,
            0.21919819958767373,
            0.10768143166163047,
            0.697993783879443
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:30.695451",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_aaba2b7e",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read a CSV file containing raw data. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.2551017991440989,
            0.26648549992306114,
            0.020167509233574132,
            0.44983164311555857,
            0.2844302916438449
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:33.487350",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_567d8707",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read a CSV file containing raw data. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.9451741310598533,
            0.764572082442248,
            0.23248297469370338,
            0.9780579014841361,
            0.1980750199750717
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:35.869183",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c1833caa",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file. This will retrieve the raw data into a readable format. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format, making it easier to work with. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure it meets the necessary requirements. Step 4: Use data_processing_filter to selectively filter the validated data based on specific criteria, allowing you to focus on the most relevant information.",
      "inputs": {
        "input_data": {
          "data": [
            0.1605846003696294,
            0.1498409026575135,
            0.23532623490167182,
            0.46732784668821825,
            0.5936780423145321
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:40.079444",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c81a33f7",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a specified file. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.3231427356984051,
            0.1390492728409608,
            0.16540601542057642,
            0.017893171490012794,
            0.12756521005566424
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:41.824267",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5c4a1c8e",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.04062013970134992,
            0.1261847652855994,
            0.9023648697789022,
            0.3406666337605532,
            0.14542271320672817
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:44.241206",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_ab17a725",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 4: Use the data_processing_transformer to convert the filtered data from JSON format to XML format.",
      "inputs": {
        "data": {
          "values": [
            13,
            1,
            18,
            49,
            48
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 5.990047430949226
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:17.291842",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_fd7cf529",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, such as removing entries that do not meet certain conditions. Step 4: Use the data_processing_transformer tool to convert the filtered data from JSON format to XML format for further processing.",
      "inputs": {
        "data": {
          "values": [
            83,
            36,
            95,
            85,
            8
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:19.554495",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_82a50f4d",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 3: Use the data_processing_filter to filter the validated data based on specific criteria to reduce the dataset size. Step 4: Use the data_processing_transformer to convert the filtered data from JSON format to XML format for further usage.",
      "inputs": {
        "data": {
          "values": [
            34,
            76,
            9,
            10,
            14
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:24.775980",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_7252bdeb",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse the raw data from a CSV file into a structured format. This will help in organizing the data for further processing. Step 2: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. This ensures the data meets the required standards. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specific criteria that you define. This step helps in narrowing down the data to only what is necessary. Step 4: Use the data_processing_transformer tool to convert the filtered data from JSON format to XML format if needed for compatibility with other systems.",
      "inputs": {
        "data": {
          "values": [
            27,
            97,
            76,
            43,
            70
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:28.042044",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_2cfec4b5",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to filter the validated data based on specified criteria to reduce the dataset size. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further processing.",
      "inputs": {
        "data": {
          "values": [
            98,
            83,
            55,
            35,
            48
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:32.865370",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_96f14f3a",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria, such as removing entries that do not meet certain conditions. Step 4: Use the data_processing_transformer to convert the filtered data from JSON format to XML format for further processing.",
      "inputs": {
        "data": {
          "values": [
            75,
            57,
            60,
            21,
            38
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:35.028345",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_533b7349",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. This will help in extracting the necessary data fields. Step 2: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, such as removing any entries that do not meet certain conditions. Step 4: Use the data_processing_transformer tool to convert the filtered data from JSON format to XML format, making it suitable for the intended application.",
      "inputs": {
        "data": {
          "values": [
            26,
            71,
            98,
            25,
            9
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:37.420680",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5708e5e4",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness. Step 3: Use the data_processing_filter tool to selectively filter the valid data based on specified criteria. Step 4: Use the data_processing_transformer tool to convert the filtered data from JSON format to XML format.",
      "inputs": {
        "data": {
          "values": [
            76,
            50,
            68,
            4,
            18
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:39.879461",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_af10aa72",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, such as only including rows where a certain column meets a condition. Step 4: Use the data_processing_transformer tool to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            51,
            18,
            74,
            18,
            34
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "low"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:42.208260",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f9933e15",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria, such as removing entries that do not meet certain conditions. Step 4: Use data_processing_transformer to transform the filtered data from JSON format into XML format for further use or storage.",
      "inputs": {
        "data": {
          "values": [
            40,
            90,
            82,
            46,
            91
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:44.245379",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0190f109",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired format (e.g., JSON to XML). Step 5: Use network_poster to send the transformed data to the specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:17.535473",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_6d9a8118",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved in Step 1 into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: If the data is valid, use network_poster to send the validated data to a specified destination API endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 2.563927166567282
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:19.185028",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_93f88042",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the retrieved raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a desired output format. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:22.364677",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5f028085",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the fetched raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against the predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to the specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:24.994246",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_b5c148d6",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher tool to retrieve data from the specified API endpoint. Step 2: Use the data_processing_parser tool to parse the raw data retrieved from the API into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_transformer tool to transform the validated data into the desired format (e.g., from JSON to XML). Step 5: Use the network_poster tool to send the transformed data to the specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:29.221648",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_10740401",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher tool to retrieve data from a specified API endpoint. Step 2: Use the data_processing_parser tool to parse the raw data retrieved from the API into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_transformer tool to transform the validated data into a different format if necessary. Step 5: Use the network_poster tool to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "medium"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:31.776300",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f5e5a3c5",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired format, such as converting from JSON to XML. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:34.277244",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_207e8714",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve raw data from a specified API endpoint. Step 2: Use data_processing_parser to parse the retrieved raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format (e.g., converting JSON to XML). Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 7.812338741708371
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:36.349605",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a437453d",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to transform the validated data into a different format, such as converting JSON data to XML. Step 5: Use network_poster to send the transformed data to the specified destination endpoint over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:41.644560",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1946454c",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the retrieved raw data into a structured format. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to a specified destination API endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:44.326759",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_189c1b21",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria to extract only the relevant information. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            40,
            56,
            57,
            35,
            79
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:17.212281",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_12d02a90",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse the raw data file (e.g., CSV) into a structured format. Step 2: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the valid data based on specified criteria, keeping only the relevant entries. Step 4: Use the data_processing_transformer tool to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            61,
            89,
            11,
            49,
            20
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:19.436745",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_93e1926d",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, such as removing any entries that do not meet certain conditions. Step 4: Use the data_processing_transformer tool to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            80,
            16,
            86,
            38,
            47
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:25.673049",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f2c3c75a",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria, such as removing entries that do not meet certain conditions. Step 4: Use data_processing_transformer to transform the filtered data from JSON format to XML format for further processing or storage.",
      "inputs": {
        "data": {
          "values": [
            92,
            82,
            30,
            66,
            60
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "low"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:28.083774",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1c3fba15",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against the defined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant entries. Step 4: Use data_processing_transformer to transform the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            85,
            90,
            67,
            94,
            63
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "low"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:30.203161",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_df622db7",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the data based on specified criteria, removing any unnecessary entries. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for compatibility with another system.",
      "inputs": {
        "data": {
          "values": [
            33,
            34,
            25,
            14,
            64
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:33.043623",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_096c412b",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria, such as removing entries that do not meet certain conditions. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            98,
            34,
            97,
            51,
            59
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:35.808096",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_939cc74e",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV source into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the valid data based on specified criteria. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            58,
            88,
            16,
            23,
            13
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:38.081035",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_3acc6985",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse the raw CSV data into a structured format. Step 2: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 4: Use the data_processing_transformer tool to convert the filtered data from JSON format to XML format for final output.",
      "inputs": {
        "data": {
          "values": [
            74,
            99,
            93,
            83,
            46
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:42.502982",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9620d053",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw CSV data into a structured format. Step 2: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            30,
            35,
            6,
            53,
            87
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 7.506626526767632
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:44.392797",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c08d6119",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "high"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:16.536175",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f579ae73",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the retrieved raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to the specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:18.857722",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f26d36b7",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint that provides the necessary information. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format, such as JSON. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to convert the validated data into a different format, if required, such as transforming JSON to XML. Step 5: Finally, use network_poster to send the transformed data to a specified destination endpoint for further processing or storage.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:22.373861",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5d8b797c",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to the specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:28.192116",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_aa567dc2",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to convert the validated data into the required output format (e.g., JSON to XML). Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:31.092199",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9c4fc50d",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:34.447937",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8215e750",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve raw data from a specified API endpoint. Step 2: Use data_processing_parser to parse the retrieved raw data into a structured format suitable for further processing. Step 3: Use data_processing_validator to validate the structured data against a defined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format (e.g., from JSON to XML). Step 5: Use network_poster to send the transformed data to the specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 8.363701723623473
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:36.857617",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_3d3e533e",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve raw data from a specified API endpoint. Step 2: Use data_processing_parser to parse the retrieved raw data into a structured format suitable for further processing. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format, such as converting JSON to XML. Step 5: Use network_poster to send the transformed data to a specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:39.532803",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1920d9a5",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher to retrieve raw data from a specified API endpoint. Step 2: Use the data_processing_parser to parse the raw data into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the network_poster to send the validated data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 3.362638852498067,
        "priority": "medium"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:41.582629",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4e192d7e",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the retrieved raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to convert the validated data into a different format as required. Step 5: Use network_poster to send the transformed data to the specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:44.451645",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f385aac8",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria, such as removing any entries that do not meet certain conditions. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for easier sharing. Step 5: Lastly, use computation_calculator to perform any necessary calculations on the final dataset, such as calculating averages or totals.",
      "inputs": {
        "data": {
          "values": [
            26,
            64,
            44,
            1,
            84
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "computation_calculator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 1.4397901737477647
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:17.027830",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_2a879241",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse the raw data from a CSV file into a structured format. Step 2: After parsing, use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: If the data is valid, use the data_processing_filter tool to selectively filter the data based on specific criteria to reduce the dataset size. Step 4: Finally, use the data_processing_transformer tool to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            94,
            26,
            88,
            9,
            48
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:19.442617",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1644f692",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, such as removing entries that do not meet certain conditions. Step 4: Use the data_processing_transformer tool to transform the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            62,
            4,
            32,
            22,
            73
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:25.086654",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_b0a7152d",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to apply filtering options to the validated data, selecting only the relevant entries based on specified criteria. Step 4: Use data_processing_transformer to transform the filtered data into a different format, such as converting it from JSON to XML.",
      "inputs": {
        "data": {
          "values": [
            99,
            54,
            66,
            16,
            14
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 5.807569310797796
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:27.621867",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_b6e21778",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse the raw data from a CSV file and convert it into a structured format. Step 2: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: If the data is valid, use the data_processing_transformer tool to convert the structured data from JSON format to XML format. Step 4: Finally, use the data_processing_filter tool to selectively filter the transformed XML data based on specified criteria, such as only including records that meet certain conditions.",
      "inputs": {
        "data": {
          "values": [
            40,
            12,
            73,
            9,
            38
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:30.215433",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1df39772",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data file (e.g., CSV) into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria to reduce the amount of processed data. Step 4: Use data_processing_transformer to convert the filtered data from one format (e.g., JSON) to another format (e.g., XML) as required.",
      "inputs": {
        "data": {
          "values": [
            11,
            36,
            52,
            29,
            69
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 9.12592981710634
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:32.828731",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_cd76bf18",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to filter the validated data based on specified criteria to obtain only the relevant records. Step 4: Use data_processing_transformer to transform the filtered data from JSON format to XML format.",
      "inputs": {
        "data": {
          "values": [
            81,
            46,
            58,
            21,
            18
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:34.819299",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a09802e7",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the valid data based on specified criteria, such as removing entries with null values. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            9,
            89,
            5,
            50,
            46
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 9.690182048647062,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:37.424387",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_93d11cd2",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria, such as removing entries with missing values. Step 4: Use the data_processing_transformer to convert the filtered data from JSON format to XML format for further usage. Step 5: Use the computation_calculator to perform any necessary calculations on the transformed data, such as summing specific fields.",
      "inputs": {
        "data": {
          "values": [
            25,
            63,
            14,
            31,
            15
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "computation_calculator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:42.374715",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_44c875db",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser tool to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 4: Use the data_processing_transformer tool to transform the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            28,
            38,
            52,
            91,
            1
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:44.549907",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_760bcaa8",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a chosen file (e.g., a CSV file). Step 2: Use data_processing_parser to parse the raw data into a structured format, making it easier to work with. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to filter the validated data based on specified criteria (e.g., only include specific entries). Step 5: Use data_processing_transformer to transform the filtered data into a different format if needed (e.g., from JSON to XML).",
      "inputs": {
        "input_data": {
          "data": [
            0.18183261965489472,
            0.15615408426727384,
            0.9352156468250181,
            0.5402702332664246,
            0.24301321720418267
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:17.998245",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_3093e5a6",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file. Step 2: Use data_processing_parser to parse the raw data from the CSV into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.7298679290379186,
            0.04913466513253506,
            0.552162372942757,
            0.2357843987138103,
            0.23807456769975432
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:20.127355",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e9fe61cb",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read a CSV file containing raw data. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.748498210690107,
            0.26222708140578466,
            0.7059904039354995,
            0.29408743885565936,
            0.2583848109487944
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:23.645538",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4eb45ea7",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a specified file (e.g., CSV, JSON). Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a defined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.36336311182488146,
            0.742766788761385,
            0.367597297030372,
            0.2854403212691722,
            0.8159261131643615
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:25.661452",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0f963022",
      "task_type": "basic_task",
      "description": "Step 1: Use the 'file_operations_reader' tool to read the input data file (in CSV format). Step 2: Use the 'data_processing_parser' tool to parse the raw data from the file into a structured format. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.975268195121346,
            0.25859480932204393,
            0.3583698318903257,
            0.7816142704427755,
            0.18461128151222816
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:31.066950",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_604b361a",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the valid data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.07938431241488297,
            0.6192872964060022,
            0.253443635728579,
            0.18169484447736584,
            0.6585245688617221
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:33.266153",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5b9c0c77",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to filter the validated data based on specific criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.11762095439854547,
            0.7916371299741715,
            0.8120534423662397,
            0.8483547084392902,
            0.07153461038128683
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:35.379479",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_df2b9485",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema. Step 4: Use data_processing_filter to filter the data based on specific criteria. Step 5: Optionally, use data_processing_transformer to convert the filtered data into a different format if needed.",
      "inputs": {
        "input_data": {
          "data": [
            0.05020702057773396,
            0.6442623726447321,
            0.11964406200695843,
            0.22017936665400328,
            0.6784165062726183
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 6.495777458108814
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:39.077543",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_fd2ebc36",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a CSV file containing raw data. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_transformer to transform the filtered data into a different format, such as JSON.",
      "inputs": {
        "input_data": {
          "data": [
            0.7296814604674063,
            0.5799456923117264,
            0.3194701387426391,
            0.458433868972949,
            0.5918866325712946
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 6.787508785178781
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:41.761653",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5015fb8b",
      "task_type": "basic_task",
      "description": "Step 1: Use the 'file_operations_reader' tool to read the data from a CSV file. Step 2: Use the 'data_processing_parser' tool to parse the raw data from the CSV file into a structured format. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: If the data is valid, use the 'data_processing_filter' tool to selectively filter the data based on specified criteria. Step 5: Finally, use the 'data_processing_transformer' tool to convert the filtered data into a different format, such as JSON.",
      "inputs": {
        "input_data": {
          "data": [
            0.5390589853880694,
            0.7972851581471991,
            0.5620417174475415,
            0.8490947905700272,
            0.8543554013456672
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:45.137684",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_28e464a5",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse the raw data from the CSV file into a structured format. Step 2: Use the data_processing_validator to validate the parsed data against the predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the valid data based on specified criteria to reduce the dataset. Step 4: Use the data_processing_transformer to convert the filtered data from JSON format to XML format for further processing.",
      "inputs": {
        "data": {
          "values": [
            35,
            77,
            65,
            77,
            67
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:17.633541",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_16ab69fa",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the structured data against a defined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria to reduce the dataset to only the relevant entries. Step 4: Use the data_processing_transformer to transform the filtered data from JSON format to XML format for further processing.",
      "inputs": {
        "data": {
          "values": [
            41,
            69,
            100,
            43,
            41
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:19.773356",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_99041f87",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the valid data based on specified criteria, such as only including entries that meet certain conditions. Step 4: Use the data_processing_transformer to convert the filtered data from JSON format to XML format. Step 5: Use the computation_calculator to perform any necessary calculations on the transformed data, such as summing up specific numerical fields.",
      "inputs": {
        "data": {
          "values": [
            95,
            18,
            11,
            44,
            51
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "computation_calculator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:29.830846",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f8579910",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria, such as removing any entries that do not meet certain requirements. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            74,
            87,
            52,
            27,
            31
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:32.544998",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5e188382",
      "task_type": "simple_task",
      "description": "Step 1: Use the data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use the data_processing_filter to selectively filter the validated data based on specified criteria, such as removing entries that do not meet certain conditions. Step 4: Use the data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            35,
            85,
            58,
            42,
            19
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:34.692909",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c513c27c",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 4: Use data_processing_transformer to transform the filtered data from JSON format to XML format for easier sharing.",
      "inputs": {
        "data": {
          "values": [
            64,
            40,
            79,
            38,
            36
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:36.815681",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_3e4160bd",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to apply specific filtering criteria to the validated data to select only the relevant entries. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            5,
            7,
            75,
            51,
            65
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 8.74431569065423
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:39.155937",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_feeee1a8",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format.",
      "inputs": {
        "data": {
          "values": [
            1,
            21,
            55,
            69,
            98
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 7.855464216345402
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:41.483599",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_ead49530",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to filter the validated data based on specific criteria to retain only relevant entries. Step 4: Use data_processing_transformer to convert the filtered data from JSON format to XML format for further use.",
      "inputs": {
        "data": {
          "values": [
            30,
            84,
            82,
            78,
            85
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 2.7002758442631603
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:43.633403",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e958c179",
      "task_type": "simple_task",
      "description": "Step 1: Use data_processing_parser to parse the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 4: Use data_processing_transformer to transform the filtered data into a different format, such as converting it from JSON to XML.",
      "inputs": {
        "data": {
          "values": [
            8,
            88,
            83,
            64,
            20
          ]
        },
        "config": {
          "mode": "standard",
          "options": {}
        }
      },
      "expected_outputs": {
        "final_output": {
          "success": true,
          "data": {}
        }
      },
      "required_tools": [
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "low"
      },
      "complexity": "easy",
      "metadata": {
        "template": "simple_task",
        "generated_at": "2025-07-09T00:25:45.616017",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_cbb8f12b",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher to retrieve raw data from a specified API endpoint. Step 2: Use the data_processing_parser to parse the retrieved raw data into a structured format (e.g., JSON). Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_transformer to convert the validated data into a different format (e.g., from JSON to XML). Step 5: Use the network_poster to send the transformed data to a specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 5.907838302413631
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:17.033470",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1a44aee3",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a defined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a different format as required. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 3.93349505905783
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:19.302307",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_22519128",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the parsed data against a defined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format (e.g., JSON to XML). Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:23.978365",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_6b59ec3a",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher tool to retrieve raw data from a specified API endpoint. Step 2: Use the data_processing_parser tool to parse the raw data retrieved and convert it into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the network_poster tool to send the validated data to a specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:26.376490",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c8b88cd3",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to convert the validated structured data into the desired output format. Step 5: Use network_poster to send the transformed data to the specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:28.851704",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e938ab6a",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher tool to retrieve data from a specified API endpoint. This will allow us to gather the raw data needed for further processing. Step 2: Use the data_processing_parser tool to parse the raw data retrieved in Step 1 into a structured format, making it easier to work with the data. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_transformer tool to transform the validated data into a different format as required by the next system. Step 5: Use the network_poster tool to send the transformed data to a specified destination API endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:32.148946",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d8b7cf4d",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher to retrieve data from a specified API endpoint. Step 2: Use the data_processing_parser to parse the raw data retrieved into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_transformer to convert the validated data into a different format if necessary. Step 5: Use the network_poster to send the transformed data to a specified destination API endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:34.732197",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_31ae09af",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the required format for further use. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:36.953248",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e65e855a",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from an API endpoint that provides the necessary information. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format for easier processing. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired format (e.g., from JSON to XML). Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:40.310275",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1b038909",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data obtained from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to convert the validated data into a different format, if necessary (e.g., from JSON to XML). Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:45.680534",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_bc2d8484",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a different format as required (e.g., from JSON to XML). Step 5: Use network_poster to send the transformed data to the specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:17.253931",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0727d207",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against the predefined schema to ensure its correctness and integrity. Step 4: If the data is valid, use network_poster to send the validated data to the specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:18.986273",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e02fc740",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to transform the validated data into a different format if necessary. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:22.063041",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_ad0ab680",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to transform the validated data into a different format (e.g., from JSON to XML). Step 5: Use network_poster to send the transformed data to a specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 4.734691069513659
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:25.021298",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_bad3bbdf",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to convert the validated data into a different format as required (e.g., from JSON to XML). Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:27.835063",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_db960d62",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint that provides the required information. Step 2: Use data_processing_parser to parse the raw data retrieved into a structured format (e.g., converting JSON to a usable format). Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: If the data is valid, use network_poster to send the validated data to a specified destination for further processing or storage.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 7.713538060792264
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:31.832804",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_04d0b8c7",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a defined schema to ensure its correctness. Step 4: Use data_processing_transformer to transform the validated data into the desired output format (e.g., from JSON to XML). Step 5: Use network_poster to send the transformed data to the specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:34.361852",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c09f0506",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the parsed data against a defined schema to ensure its correctness. Step 4: Use data_processing_transformer to transform the validated data into a different format if necessary. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:36.503573",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1707a749",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the required output format. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 5.948544270955735
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:38.847285",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0a40b01a",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format (e.g., from JSON to XML). Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:45.694159",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_3ba33542",
      "task_type": "basic_task",
      "description": "Step 1: Use the 'file_operations_reader' tool to read data from a CSV file. This will retrieve the raw data needed for processing. Step 2: Use the 'data_processing_parser' tool to parse the raw data from the CSV into a structured format. This will make it easier to work with the data. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specified criteria, allowing you to focus on relevant information. Step 5: Use the 'data_processing_transformer' tool to transform the filtered data into a different format, if necessary, such as converting it from JSON to XML.",
      "inputs": {
        "input_data": {
          "data": [
            0.6865167413524069,
            0.7892104004273672,
            0.28585649467321683,
            0.7677645391428267,
            0.2120006220154933
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 9.577845647273383
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:21.977258",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_cff6bc2e",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.3823797802415495,
            0.8279533196389274,
            0.840507104110996,
            0.49361139668638676,
            0.8096797684252255
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:23.712144",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a97cd0c6",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a CSV file. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.368626327676001,
            0.8812259622041361,
            0.578842388609342,
            0.7137045146572579,
            0.21641166472483908
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:25.791483",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c500b35b",
      "task_type": "basic_task",
      "description": "Step 1: Use the 'file_operations_reader' tool to read a CSV file containing raw data. Step 2: Use the 'data_processing_parser' tool to parse the data from the CSV file into a structured format. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the 'data_processing_filter' tool to filter the validated data based on specified criteria, keeping only the relevant entries. Step 5: Use the 'data_processing_transformer' tool to convert the filtered data into a JSON format for easier access and usage.",
      "inputs": {
        "input_data": {
          "data": [
            0.6641027102972626,
            0.9994620525568481,
            0.7805681919021827,
            0.8142845389688876,
            0.29220114795511487
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 5.88252719737809,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:28.996843",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_fbdcda53",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from the specified file (e.g., a CSV file). Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_transformer to transform the filtered data into a different format (e.g., from JSON to XML).",
      "inputs": {
        "input_data": {
          "data": [
            0.1373101035883364,
            0.5423426890497701,
            0.8216286117740257,
            0.32947048693567604,
            0.6461336177918323
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:33.681655",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0a32c6c6",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.9273967939548426,
            0.8263479713032956,
            0.518664147565953,
            0.7845093317572727,
            0.7112709158951706
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 5.092037431906385
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:35.717697",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_316daf7d",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_transformer to convert the filtered data into a different format, such as JSON.",
      "inputs": {
        "input_data": {
          "data": [
            0.6603882608925589,
            0.23971573617899966,
            0.7831854328272361,
            0.5638784032696668,
            0.6046400344085434
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:39.800683",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_6852a50e",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read the data from a file in CSV format. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a defined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to selectively filter the valid data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.43059028723464754,
            0.6377807527804567,
            0.4985257228092602,
            0.3219485888771785,
            0.8001633933883291
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 4.286708788969636
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:41.788012",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_18fad08a",
      "task_type": "basic_task",
      "description": "Step 1: Use the 'file_operations_reader' tool to read data from a specified CSV file. Step 2: Use the 'data_processing_parser' tool to parse the raw data extracted from the file into a structured format. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.296370573633278,
            0.18248222935131808,
            0.19861966164159162,
            0.5888796137947593,
            0.3864633330252917
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:44.350256",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_17791361",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.02495254627390242,
            0.21386742247323987,
            0.5327177005132352,
            0.6195597605069253,
            0.2052369872448625
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:45.938226",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_7040b718",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: If the data is valid, use network_poster to send the validated data to the specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:17.224644",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_11b025e0",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve raw data from the specified API endpoint. Step 2: Use data_processing_parser to parse the retrieved raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to convert the validated data from JSON format to XML format. Step 5: Use network_poster to send the transformed XML data to the specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:19.330656",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0f73f1a2",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema ensuring its correctness. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to a specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:23.259815",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_65c147ea",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a different format if necessary. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:30.408925",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4ddb65c2",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to transform the validated data from its current format to a desired format (e.g., from JSON to XML). Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:32.809053",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5f03976a",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data received from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a different format as required. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:34.844142",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_b4aec68e",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to transform the validated data into the desired format for further use. Step 5: Use network_poster to send the transformed data to the specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "medium"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:37.349133",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_05909d5a",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the valid data into a desired output format, such as converting JSON to XML. Step 5: Finally, use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "medium"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:39.987999",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_ebcb07a0",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use network_poster to send the validated data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:43.783587",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_eeaa595f",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher to retrieve raw data from a specified API endpoint. Step 2: Use the data_processing_parser to parse the raw data into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: If the data is valid, use the network_poster to send the validated data to a specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 9.735363640026849,
        "priority": "high"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:46.159615",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_87f94189",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read the raw data from the input file in CSV format. Step 2: Use data_processing_parser to parse the read data into a structured format, extracting the necessary fields. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to apply filtering options on the validated data to select only the relevant records. Step 5: Use data_processing_aggregator to aggregate the filtered data based on specified criteria, producing summarized results.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.4949732797176971
            },
            {
              "id": 1,
              "value": 0.4436045129621642
            },
            {
              "id": 2,
              "value": 0.8153299613285324
            },
            {
              "id": 3,
              "value": 0.19962227059970394
            },
            {
              "id": 4,
              "value": 0.3177296484702
            },
            {
              "id": 5,
              "value": 0.43346540210672146
            },
            {
              "id": 6,
              "value": 0.6545701623721953
            },
            {
              "id": 7,
              "value": 0.35786729442987775
            },
            {
              "id": 8,
              "value": 0.3079967594942139
            },
            {
              "id": 9,
              "value": 0.8930429552112045
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 2.3657987623747596
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:19.154293",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_804f2d76",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read and retrieve raw data from a specified file (e.g., CSV, JSON) into the pipeline. Step 2: Use data_processing_parser to parse the raw data into a structured format, extracting relevant fields and organizing the data. Step 3: Use data_processing_validator to validate the structured data against a predefined schema, ensuring its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria, reducing the dataset to only the required entries. Step 5: Use data_processing_aggregator to aggregate the filtered data, summarizing key metrics or insights as needed for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.1988406106489442
            },
            {
              "id": 1,
              "value": 0.36797669863540716
            },
            {
              "id": 2,
              "value": 0.10396249118953149
            },
            {
              "id": 3,
              "value": 0.13427153069747455
            },
            {
              "id": 4,
              "value": 0.9807567527183758
            },
            {
              "id": 5,
              "value": 0.4080206869928579
            },
            {
              "id": 6,
              "value": 0.759992752389306
            },
            {
              "id": 7,
              "value": 0.023440125229191366
            },
            {
              "id": 8,
              "value": 0.4972441118698524
            },
            {
              "id": 9,
              "value": 0.17915280127400957
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:22.100427",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_52e2651b",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from a specified file format (e.g., CSV or JSON). Step 2: Use the data_processing_parser to parse the retrieved raw data into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Finally, use the data_processing_aggregator to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.14017507474061974
            },
            {
              "id": 1,
              "value": 0.16751266723469693
            },
            {
              "id": 2,
              "value": 0.2844034782913716
            },
            {
              "id": 3,
              "value": 0.08833005657318249
            },
            {
              "id": 4,
              "value": 0.3961359802917632
            },
            {
              "id": 5,
              "value": 0.7939434942070744
            },
            {
              "id": 6,
              "value": 0.5858543723411236
            },
            {
              "id": 7,
              "value": 0.9440084668133599
            },
            {
              "id": 8,
              "value": 0.03227291742299965
            },
            {
              "id": 9,
              "value": 0.8663220248234071
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:25.752994",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_227b7c75",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data file in CSV format. Step 2: Use the data_processing_parser to parse the retrieved data into a structured format. Step 3: Use the data_processing_validator to validate the structured data against the predefined schema to ensure its correctness. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_aggregator to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.27308352341951525
            },
            {
              "id": 1,
              "value": 0.42443713105293424
            },
            {
              "id": 2,
              "value": 0.32487815062475744
            },
            {
              "id": 3,
              "value": 0.10971199667112252
            },
            {
              "id": 4,
              "value": 0.3756739875840769
            },
            {
              "id": 5,
              "value": 0.4357522532752641
            },
            {
              "id": 6,
              "value": 0.028784410792238813
            },
            {
              "id": 7,
              "value": 0.3494810436733833
            },
            {
              "id": 8,
              "value": 0.8653642766648393
            },
            {
              "id": 9,
              "value": 0.33996051090900403
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 6.431598938618099
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:28.820027",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_acbbcd7a",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read raw data from a CSV file. Step 2: Use the data_processing_parser to parse the raw data into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_aggregator to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.7549425557887928
            },
            {
              "id": 1,
              "value": 0.14070368275167044
            },
            {
              "id": 2,
              "value": 0.21685417828885412
            },
            {
              "id": 3,
              "value": 0.21966422304140754
            },
            {
              "id": 4,
              "value": 0.8425093110296862
            },
            {
              "id": 5,
              "value": 0.5407151318599885
            },
            {
              "id": 6,
              "value": 0.06979576106492735
            },
            {
              "id": 7,
              "value": 0.8003159430742345
            },
            {
              "id": 8,
              "value": 0.818004824729859
            },
            {
              "id": 9,
              "value": 0.5630615384790044
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 6.768639524702585
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:31.098206",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_88a342a7",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from the input file (in CSV format). Step 2: Next, apply the data_processing_parser tool to parse the raw data retrieved from the file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: After validation, employ the data_processing_filter tool to selectively filter the structured data based on specified criteria, reducing the dataset to only the relevant information. Step 5: Finally, use the data_processing_aggregator tool to aggregate the filtered data for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.44310916808157463
            },
            {
              "id": 1,
              "value": 0.8315946136561815
            },
            {
              "id": 2,
              "value": 0.870902125357633
            },
            {
              "id": 3,
              "value": 0.4164366511835146
            },
            {
              "id": 4,
              "value": 0.9587517922385522
            },
            {
              "id": 5,
              "value": 0.41619034360230933
            },
            {
              "id": 6,
              "value": 0.6536701868043049
            },
            {
              "id": 7,
              "value": 0.5505020877254975
            },
            {
              "id": 8,
              "value": 0.7960199686517261
            },
            {
              "id": 9,
              "value": 0.03427841746435567
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:33.898790",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a5287b00",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a source file (e.g., CSV or JSON). This step will retrieve the raw data for further processing. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format. This will ensure that the data is well-organized for analysis. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema. This step will ensure that the data meets the required standards for accuracy and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. This will reduce the dataset to only the necessary information. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data into meaningful statistics or summaries. This final step will prepare the data for reporting or further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.305025226356031
            },
            {
              "id": 1,
              "value": 0.900640181903899
            },
            {
              "id": 2,
              "value": 0.8711791528892676
            },
            {
              "id": 3,
              "value": 0.6275572180438418
            },
            {
              "id": 4,
              "value": 0.9807061641159915
            },
            {
              "id": 5,
              "value": 0.18951295518961842
            },
            {
              "id": 6,
              "value": 0.7432869751193262
            },
            {
              "id": 7,
              "value": 0.9744307316846023
            },
            {
              "id": 8,
              "value": 0.20368001824751658
            },
            {
              "id": 9,
              "value": 0.11852924886739957
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:37.391243",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_790c6d99",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read and retrieve raw data from a specified source file (e.g., CSV or JSON). Step 2: Use the data_processing_parser to parse the retrieved raw data into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the valid data based on specified criteria to reduce the dataset size. Step 5: Use the data_processing_aggregator to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.9122321781283573
            },
            {
              "id": 1,
              "value": 0.9409665021334208
            },
            {
              "id": 2,
              "value": 0.6832916400328295
            },
            {
              "id": 3,
              "value": 0.3589134624741953
            },
            {
              "id": 4,
              "value": 0.962909345126363
            },
            {
              "id": 5,
              "value": 0.467926352264761
            },
            {
              "id": 6,
              "value": 0.4671488540964899
            },
            {
              "id": 7,
              "value": 0.33156599230719974
            },
            {
              "id": 8,
              "value": 0.04829385314922818
            },
            {
              "id": 9,
              "value": 0.3741771660382698
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 9.446851709136936
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:39.860355",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_b60b7c48",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read the raw data file in CSV format. Step 2: Use data_processing_parser to parse the CSV data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specific criteria, reducing the dataset to relevant entries. Step 5: Use data_processing_aggregator to aggregate the filtered data, summarizing it for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.2233432953448231
            },
            {
              "id": 1,
              "value": 0.7290785441469931
            },
            {
              "id": 2,
              "value": 0.11783304842268094
            },
            {
              "id": 3,
              "value": 0.5990005435226488
            },
            {
              "id": 4,
              "value": 0.3481299447292937
            },
            {
              "id": 5,
              "value": 0.6856401161168755
            },
            {
              "id": 6,
              "value": 0.5072214016445146
            },
            {
              "id": 7,
              "value": 0.8673977036027212
            },
            {
              "id": 8,
              "value": 0.5323632220297839
            },
            {
              "id": 9,
              "value": 0.4584422873067555
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:42.566602",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a6cc2723",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read raw data from a CSV file. Step 2: Use the data_processing_parser to parse the raw data into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_aggregator to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.1105343020808115
            },
            {
              "id": 1,
              "value": 0.8570135406094076
            },
            {
              "id": 2,
              "value": 0.48528339678125176
            },
            {
              "id": 3,
              "value": 0.2624722849132325
            },
            {
              "id": 4,
              "value": 0.6739243063009728
            },
            {
              "id": 5,
              "value": 0.754775203610847
            },
            {
              "id": 6,
              "value": 0.9211125896264657
            },
            {
              "id": 7,
              "value": 0.9820518319851018
            },
            {
              "id": 8,
              "value": 0.7248621123971133
            },
            {
              "id": 9,
              "value": 0.09083995341032158
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:46.513473",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9d640fbf",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to transform the validated data into a different format if necessary. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:16.791421",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_213277bc",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher to retrieve data from a specified API endpoint. Step 2: Use the data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_transformer to convert the validated data into a different format as required (e.g., from JSON to XML). Step 5: Use the network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 6.801508406792086
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:19.169972",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_127d650e",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a different format as required. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:22.541765",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_42441525",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data obtained from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a required output format, such as converting it from JSON to XML. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:28.648781",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e194fa7e",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve raw data from the specified API endpoint. Step 2: Use data_processing_parser to parse the retrieved raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a defined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to the specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 3.5621743991345753
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:30.830791",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_cc1345a8",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a different format as required (e.g., JSON to XML). Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:33.298569",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a25cbd75",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher to retrieve raw data from a specified API endpoint. Step 2: Use the data_processing_parser to parse the retrieved raw data into a structured format (e.g., converting JSON data into a structured object). Step 3: Use the data_processing_validator to validate the structured data against a defined schema to ensure its correctness and integrity. Step 4: If the data is valid, use the network_poster to send the validated data to a specified destination over the network. Step 5: If needed, use the data_processing_transformer to transform the data into another format (e.g., from JSON to XML) before sending it.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "network_poster",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:36.455250",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_851b4a87",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved into a structured format. Step 3: Use data_processing_validator to validate the structured data against a defined schema to ensure its correctness. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to the specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:38.736687",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c2069e06",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to a specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:44.311400",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0460dd5d",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use network_poster to send the validated data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:46.678437",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_36439809",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher to retrieve data from a specified API endpoint. This data will be in a raw format. Step 2: Use the data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_transformer to transform the validated data into a required output format (for example, converting JSON to XML). Step 5: Finally, use the network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "medium"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:17.431548",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_37f26848",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to transform the validated data into the required output format. Step 5: Use network_poster to send the transformed data to the designated destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 9.179195826397644
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:19.802342",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_71b9ee48",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data received from the API into a structured format. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a different format as required. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:22.490850",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_fb82d515",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher tool to retrieve data from a specified API endpoint. This will provide the raw data needed for further processing. Step 2: Use the data_processing_parser tool to parse the raw data retrieved from the API into a structured format, such as JSON. This will allow for easier manipulation of the data. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. This step ensures that the data meets the necessary requirements. Step 4: Use the data_processing_transformer tool to transform the validated data into a different format, if required (e.g., from JSON to XML). This step prepares the data for the next stage of processing or storage. Step 5: Use the network_poster tool to send the transformed data to a specified destination over the network, completing the integration process.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:27.282991",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e10ab3ca",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:30.797315",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_59abb9cc",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve raw data from a specified URL. Step 2: Use data_processing_parser to parse the retrieved raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the valid data into another format as required (e.g., from JSON to XML). Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 2.350595099479047,
        "priority": "high"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:35.469324",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_83122d16",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to transform the validated data into a different format as required for the next step. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:38.082783",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a5f386f3",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher to retrieve data from the specified API endpoint. Step 2: Use the data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_transformer to transform the validated data into the required output format. Step 5: Use the network_poster to send the transformed data to the specified destination API endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 2.554420997579341
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:41.129296",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_be198a85",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher tool to retrieve raw data from a specified API endpoint. Step 2: Use the data_processing_parser tool to parse the retrieved raw data into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_transformer tool to convert the validated data into the desired output format (e.g., JSON to XML). Step 5: Use the network_poster tool to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:43.826301",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4f86f851",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format (e.g., convert JSON to XML). Step 5: Use network_poster to send the transformed data to the specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:46.705407",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9bdd4262",
      "task_type": "data_pipeline",
      "description": "Step 1: Use [file_operations_reader] to read raw data from a specified file in CSV format. Step 2: Use [data_processing_parser] to parse the raw CSV data into a structured format, extracting relevant fields. Step 3: Use [data_processing_validator] to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use [data_processing_filter] to selectively filter the validated data based on specified criteria, reducing the dataset to only the necessary entries. Step 5: Use [data_processing_aggregator] to aggregate the filtered data for further analysis, summarizing key metrics.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.9866379071316549
            },
            {
              "id": 1,
              "value": 0.1978917780767212
            },
            {
              "id": 2,
              "value": 0.10544041229652634
            },
            {
              "id": 3,
              "value": 0.7517743050207969
            },
            {
              "id": 4,
              "value": 0.4804159786039751
            },
            {
              "id": 5,
              "value": 0.9967482768978234
            },
            {
              "id": 6,
              "value": 0.8596341419104796
            },
            {
              "id": 7,
              "value": 0.7464588464925082
            },
            {
              "id": 8,
              "value": 0.9749437783772216
            },
            {
              "id": 9,
              "value": 0.6413276066015239
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 4.756935607650745
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:19.800041",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_37b33ba1",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Finally, use the data_processing_aggregator tool to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.742705872776179
            },
            {
              "id": 1,
              "value": 0.27037428121044094
            },
            {
              "id": 2,
              "value": 0.3392381118934946
            },
            {
              "id": 3,
              "value": 0.05237256413155911
            },
            {
              "id": 4,
              "value": 0.829525015368971
            },
            {
              "id": 5,
              "value": 0.4667418895877339
            },
            {
              "id": 6,
              "value": 0.22612981859600956
            },
            {
              "id": 7,
              "value": 0.828377936594926
            },
            {
              "id": 8,
              "value": 0.5915264242063996
            },
            {
              "id": 9,
              "value": 0.42394729481990856
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:23.109381",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_6cc09197",
      "task_type": "data_pipeline",
      "description": "Step 1: Use [file_operations_reader] to read the raw data from a specified file format (e.g., CSV or JSON). Step 2: Use [data_processing_parser] to parse the raw data into a structured format, making it easier to work with. Step 3: Use [data_processing_validator] to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use [data_processing_filter] to apply specific filtering criteria to the validated data, reducing the dataset to only relevant entries. Step 5: Use [data_processing_aggregator] to aggregate the filtered data based on specified metrics or dimensions for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.6449290913422931
            },
            {
              "id": 1,
              "value": 0.42522957003630757
            },
            {
              "id": 2,
              "value": 0.6378395454881441
            },
            {
              "id": 3,
              "value": 0.7593209091736204
            },
            {
              "id": 4,
              "value": 0.6141027231640921
            },
            {
              "id": 5,
              "value": 0.8107258172239604
            },
            {
              "id": 6,
              "value": 0.5956564705042598
            },
            {
              "id": 7,
              "value": 0.2991154531468313
            },
            {
              "id": 8,
              "value": 0.13652035775594518
            },
            {
              "id": 9,
              "value": 0.2305381831265353
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:26.195229",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1c7bf4b2",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read the raw data from a CSV file. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to filter the validated data based on specific criteria, reducing the dataset size. Step 5: Use data_processing_aggregator to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.8046797130223876
            },
            {
              "id": 1,
              "value": 0.8997355001360618
            },
            {
              "id": 2,
              "value": 0.9268167133429409
            },
            {
              "id": 3,
              "value": 0.38856306335698254
            },
            {
              "id": 4,
              "value": 0.6919643561292342
            },
            {
              "id": 5,
              "value": 0.40483440076419897
            },
            {
              "id": 6,
              "value": 0.7790152537196766
            },
            {
              "id": 7,
              "value": 0.814349175891635
            },
            {
              "id": 8,
              "value": 0.6371196285250288
            },
            {
              "id": 9,
              "value": 0.20490531668389844
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:28.331578",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_026de4ae",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read raw data from a specified input file in CSV format. Step 2: Use the 'data_processing_parser' tool to parse the raw CSV data into a structured format. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specified criteria. Step 5: Use the 'data_processing_aggregator' tool to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.5691247870105645
            },
            {
              "id": 1,
              "value": 0.04579988070110241
            },
            {
              "id": 2,
              "value": 0.25939191829456276
            },
            {
              "id": 3,
              "value": 0.8638660919118906
            },
            {
              "id": 4,
              "value": 0.18076930833018678
            },
            {
              "id": 5,
              "value": 0.833832024310821
            },
            {
              "id": 6,
              "value": 0.9286284553444755
            },
            {
              "id": 7,
              "value": 0.30913004017687584
            },
            {
              "id": 8,
              "value": 0.8101793212929486
            },
            {
              "id": 9,
              "value": 0.4951850364753414
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 5.8482301328591095
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:31.318115",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4a0be705",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from a CSV file. Step 2: Use the data_processing_parser to parse the raw data into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_aggregator to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.7240499447418727
            },
            {
              "id": 1,
              "value": 0.6015692473105932
            },
            {
              "id": 2,
              "value": 0.34406873156966833
            },
            {
              "id": 3,
              "value": 0.392781962426344
            },
            {
              "id": 4,
              "value": 0.6908974785160027
            },
            {
              "id": 5,
              "value": 0.5496040407949331
            },
            {
              "id": 6,
              "value": 0.08109434819894823
            },
            {
              "id": 7,
              "value": 0.35707880916254886
            },
            {
              "id": 8,
              "value": 0.03148110145655114
            },
            {
              "id": 9,
              "value": 0.524903373031319
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:34.069050",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_82ab1a7f",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' to read the raw data from a specified CSV file. Step 2: Use the 'data_processing_parser' to parse the raw data into a structured format, extracting relevant fields. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the 'data_processing_filter' to filter the validated data based on specific criteria, retaining only the necessary records. Step 5: Use the 'data_processing_aggregator' to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.8328313989230832
            },
            {
              "id": 1,
              "value": 0.6255211312686928
            },
            {
              "id": 2,
              "value": 0.7514210692018455
            },
            {
              "id": 3,
              "value": 0.6472327994631422
            },
            {
              "id": 4,
              "value": 0.7869813003618515
            },
            {
              "id": 5,
              "value": 0.5053121085273444
            },
            {
              "id": 6,
              "value": 0.7055463508065055
            },
            {
              "id": 7,
              "value": 0.8587084154444642
            },
            {
              "id": 8,
              "value": 0.4992892316126196
            },
            {
              "id": 9,
              "value": 0.989769193338001
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 2.011019852245721
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:38.084791",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c3295885",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read raw data from a specified file (e.g., CSV, JSON, or XML format). Step 2: Use data_processing_parser to parse the raw data into a structured format for easier manipulation. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema, ensuring its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant information. Step 5: Use data_processing_aggregator to aggregate the filtered data, summarizing key metrics or insights derived from it.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.2180339792756789
            },
            {
              "id": 1,
              "value": 0.2400131715383258
            },
            {
              "id": 2,
              "value": 0.3727843564838267
            },
            {
              "id": 3,
              "value": 0.05684850963866794
            },
            {
              "id": 4,
              "value": 0.5609466757919811
            },
            {
              "id": 5,
              "value": 0.14346101744757966
            },
            {
              "id": 6,
              "value": 0.2089307851933041
            },
            {
              "id": 7,
              "value": 0.3885902461876023
            },
            {
              "id": 8,
              "value": 0.5915257264178743
            },
            {
              "id": 9,
              "value": 0.4585075783618804
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "high"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:40.946998",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_ba80fb1d",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read the raw data from the source file in CSV format. Step 2: Use data_processing_parser to parse the raw data into a structured format such as JSON. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the data based on specific criteria to reduce the dataset size. Step 5: Use data_processing_aggregator to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.23278319313900342
            },
            {
              "id": 1,
              "value": 0.1730275638762082
            },
            {
              "id": 2,
              "value": 0.9303232935514937
            },
            {
              "id": 3,
              "value": 0.400661423178006
            },
            {
              "id": 4,
              "value": 0.6263027240681825
            },
            {
              "id": 5,
              "value": 0.8648616647422214
            },
            {
              "id": 6,
              "value": 0.6903251336817238
            },
            {
              "id": 7,
              "value": 0.5612624734028118
            },
            {
              "id": 8,
              "value": 0.39548514082034125
            },
            {
              "id": 9,
              "value": 0.606370933694531
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:43.685639",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_eee6e661",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read the raw data from the input files in CSV format. Step 2: Use data_processing_parser to parse the raw CSV data into a structured format, extracting relevant fields. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter out any invalid or unnecessary records based on specified criteria. Step 5: Use data_processing_aggregator to aggregate the filtered data for further analysis. Step 6: Use computation_analyzer to analyze the aggregated data, generating statistical insights and trends.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.5280828133786967
            },
            {
              "id": 1,
              "value": 0.6014724909480843
            },
            {
              "id": 2,
              "value": 0.630594805034539
            },
            {
              "id": 3,
              "value": 0.4797319870647162
            },
            {
              "id": 4,
              "value": 0.2785624651746159
            },
            {
              "id": 5,
              "value": 0.8471183410491524
            },
            {
              "id": 6,
              "value": 0.22148431077127528
            },
            {
              "id": 7,
              "value": 0.7771168024843753
            },
            {
              "id": 8,
              "value": 0.011804914486506446
            },
            {
              "id": 9,
              "value": 0.5010364885032691
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "computation_analyzer"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "high"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:46.745170",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c594cf03",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data files (in CSV format) from the specified directory. Step 2: Use the data_processing_parser to parse the raw data into a structured format, extracting relevant information. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specific criteria. Step 5: Finally, use the data_processing_aggregator to aggregate the filtered data for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.16781047606332122
            },
            {
              "id": 1,
              "value": 0.8738106468530396
            },
            {
              "id": 2,
              "value": 0.09535659517074713
            },
            {
              "id": 3,
              "value": 0.18724857265830386
            },
            {
              "id": 4,
              "value": 0.4622992106646392
            },
            {
              "id": 5,
              "value": 0.9007335822996153
            },
            {
              "id": 6,
              "value": 0.29225725834619143
            },
            {
              "id": 7,
              "value": 0.12827950481088735
            },
            {
              "id": 8,
              "value": 0.32887700128702946
            },
            {
              "id": 9,
              "value": 0.6943603438216029
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 3.8589219477390726,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:20.067009",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5e07cb14",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data file in CSV format. Step 2: Use the data_processing_parser to parse the raw data into a structured format. Step 3: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter to apply specific filtering criteria to the validated data, reducing it to only the relevant entries. Step 5: Use the data_processing_aggregator to aggregate the filtered data to produce summary statistics or insights.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.19838626435995066
            },
            {
              "id": 1,
              "value": 0.9286027481872606
            },
            {
              "id": 2,
              "value": 0.30014208631581984
            },
            {
              "id": 3,
              "value": 0.3857070543547685
            },
            {
              "id": 4,
              "value": 0.512135952937231
            },
            {
              "id": 5,
              "value": 0.5722174099994345
            },
            {
              "id": 6,
              "value": 0.5399327104768951
            },
            {
              "id": 7,
              "value": 0.7768265427715926
            },
            {
              "id": 8,
              "value": 0.06471633742646532
            },
            {
              "id": 9,
              "value": 0.7187532257339317
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:23.273165",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_fcbe009a",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read raw data from a CSV file. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.40797394357965155
            },
            {
              "id": 1,
              "value": 0.5581819837330113
            },
            {
              "id": 2,
              "value": 0.3762116750069824
            },
            {
              "id": 3,
              "value": 0.9325416755386271
            },
            {
              "id": 4,
              "value": 0.509595609348352
            },
            {
              "id": 5,
              "value": 0.3356574813130869
            },
            {
              "id": 6,
              "value": 0.5435283354918957
            },
            {
              "id": 7,
              "value": 0.9028017049402526
            },
            {
              "id": 8,
              "value": 0.7451998781925866
            },
            {
              "id": 9,
              "value": 0.5899617187902103
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:26.707249",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_cd9ad37c",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read raw data from a specified file format (e.g., CSV or JSON). Step 2: Use the data_processing_parser to parse the raw data into a structured format, extracting relevant fields. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter to filter the validated data based on specific criteria (e.g., removing entries that do not meet certain conditions). Step 5: Use the data_processing_aggregator to aggregate the filtered data for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.002364853248044474
            },
            {
              "id": 1,
              "value": 0.1529703125166444
            },
            {
              "id": 2,
              "value": 0.3433585923152781
            },
            {
              "id": 3,
              "value": 0.8559580963397027
            },
            {
              "id": 4,
              "value": 0.4987050998094237
            },
            {
              "id": 5,
              "value": 0.8041674171217289
            },
            {
              "id": 6,
              "value": 0.13095972232035002
            },
            {
              "id": 7,
              "value": 0.4823843128116214
            },
            {
              "id": 8,
              "value": 0.27952062921539667
            },
            {
              "id": 9,
              "value": 0.5418243493657369
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:29.785005",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_bac1a2ed",
      "task_type": "data_pipeline",
      "description": "Step 1: Use [file_operations_reader] to read the raw data from a specified input file in CSV format. Step 2: Use [data_processing_parser] to parse the retrieved data into a structured format, making it easier to work with. Step 3: Use [data_processing_validator] to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use [data_processing_filter] to selectively filter the validated data based on specified criteria to reduce the dataset to only the relevant information. Step 5: Use [data_processing_aggregator] to aggregate the filtered data for summarization or further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.3564291178860869
            },
            {
              "id": 1,
              "value": 0.45292691132014196
            },
            {
              "id": 2,
              "value": 0.2217253749687419
            },
            {
              "id": 3,
              "value": 0.9246615146852794
            },
            {
              "id": 4,
              "value": 0.3324277200800445
            },
            {
              "id": 5,
              "value": 0.22146217120827583
            },
            {
              "id": 6,
              "value": 0.7975390772696525
            },
            {
              "id": 7,
              "value": 0.03600774705570653
            },
            {
              "id": 8,
              "value": 0.6745826896312619
            },
            {
              "id": 9,
              "value": 0.8509779918152645
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:32.444057",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e9989458",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data file in CSV format. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to filter the validated data based on specified criteria to reduce the dataset size. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.971972818440435
            },
            {
              "id": 1,
              "value": 0.6672192961202675
            },
            {
              "id": 2,
              "value": 0.9530800940652376
            },
            {
              "id": 3,
              "value": 0.22866394771714427
            },
            {
              "id": 4,
              "value": 0.3831873317557004
            },
            {
              "id": 5,
              "value": 0.5899376742477012
            },
            {
              "id": 6,
              "value": 0.40075817397448665
            },
            {
              "id": 7,
              "value": 0.7229266442611663
            },
            {
              "id": 8,
              "value": 0.22037609248921486
            },
            {
              "id": 9,
              "value": 0.002172383104265485
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "medium"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:34.596684",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_50e51c1c",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from a CSV file. Step 2: Use the data_processing_parser to parse the raw data into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_aggregator to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.8476142445242266
            },
            {
              "id": 1,
              "value": 0.9092838643787621
            },
            {
              "id": 2,
              "value": 0.10462013960078209
            },
            {
              "id": 3,
              "value": 0.07480806610019908
            },
            {
              "id": 4,
              "value": 0.7864494555534738
            },
            {
              "id": 5,
              "value": 0.341389817741518
            },
            {
              "id": 6,
              "value": 0.056898983674142256
            },
            {
              "id": 7,
              "value": 0.0268190454811309
            },
            {
              "id": 8,
              "value": 0.7260896208146148
            },
            {
              "id": 9,
              "value": 0.32379130504840714
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 0.7896569196772636
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:37.530374",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_6154406f",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read the raw data from a CSV file. This tool will retrieve the data and ensure that it is correctly loaded into memory. Step 2: Use the 'data_processing_parser' tool to parse the raw data retrieved from the file into a structured format. This step will help organize the data for further processing. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness and integrity. This step will confirm that the data meets the necessary requirements. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant entries. Step 5: Finally, use the 'data_processing_aggregator' tool to aggregate the filtered data, summarizing the key information for analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.995632905399381
            },
            {
              "id": 1,
              "value": 0.12872880607025305
            },
            {
              "id": 2,
              "value": 0.9332333834663611
            },
            {
              "id": 3,
              "value": 0.7394387268651677
            },
            {
              "id": 4,
              "value": 0.9216930375444801
            },
            {
              "id": 5,
              "value": 0.9255066583901599
            },
            {
              "id": 6,
              "value": 0.1625637905398417
            },
            {
              "id": 7,
              "value": 0.6413008367361004
            },
            {
              "id": 8,
              "value": 0.23620763874683648
            },
            {
              "id": 9,
              "value": 0.6845855854248347
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:41.099931",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_ae45a53c",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' to read raw data from a specified CSV file. Step 2: Use the 'data_processing_parser' to parse the retrieved raw data into a structured format. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' to selectively filter the validated data based on specific criteria. Step 5: Use the 'data_processing_aggregator' to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.6144346983443593
            },
            {
              "id": 1,
              "value": 0.42020761180686383
            },
            {
              "id": 2,
              "value": 0.2780421738027614
            },
            {
              "id": 3,
              "value": 0.9689001988076235
            },
            {
              "id": 4,
              "value": 0.057560444650238685
            },
            {
              "id": 5,
              "value": 0.18649034814066823
            },
            {
              "id": 6,
              "value": 0.6169855189476405
            },
            {
              "id": 7,
              "value": 0.5292960483651244
            },
            {
              "id": 8,
              "value": 0.3073532998388596
            },
            {
              "id": 9,
              "value": 0.033101712238253644
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 4.465847620120067
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:44.251914",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_90301ccf",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file containing raw data. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to filter the validated data based on specified criteria to reduce the dataset size. Step 5: Use data_processing_aggregator to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.3712139220313677
            },
            {
              "id": 1,
              "value": 0.5678782569470046
            },
            {
              "id": 2,
              "value": 0.7621934377957195
            },
            {
              "id": 3,
              "value": 0.8778047612566326
            },
            {
              "id": 4,
              "value": 0.35846718877595984
            },
            {
              "id": 5,
              "value": 0.7820599572440585
            },
            {
              "id": 6,
              "value": 0.9567178743554802
            },
            {
              "id": 7,
              "value": 0.5404860065927451
            },
            {
              "id": 8,
              "value": 0.9995459716453348
            },
            {
              "id": 9,
              "value": 0.8935366244012265
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:46.773685",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c52714e6",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read the data from a CSV file. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.19649085264964028,
            0.02175090598703755,
            0.9764521005503362,
            0.3609271333565468,
            0.8672257049582278
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 2.3680091104992513,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:17.709300",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f9843818",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a CSV file. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.8327241490074923,
            0.3733103560840929,
            0.023197296685745772,
            0.19715463050274995,
            0.6094210857892878
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 1.1389172066888482
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:20.349360",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1d41d22c",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read a data file in CSV format. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to filter the validated data based on specified criteria. Step 5: Use data_processing_transformer to transform the filtered data from JSON format to XML format.",
      "inputs": {
        "input_data": {
          "data": [
            0.022912513181187033,
            0.8985461949786975,
            0.8076048608948413,
            0.49439425489291355,
            0.754466062590834
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:22.497092",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1958eff5",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a specified file (e.g., a CSV file). Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_transformer to transform the filtered data into another format (e.g., from JSON to XML).",
      "inputs": {
        "input_data": {
          "data": [
            0.6098680005373751,
            0.9263006931119954,
            0.8713590766413177,
            0.1850415421147228,
            0.06109201676739329
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:24.716477",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_eeaa1519",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader to read data from a specified file (e.g., CSV, JSON, or XML). Step 2: Use the data_processing_parser to parse the raw data into a structured format that is easier to work with. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria to focus on the relevant information.",
      "inputs": {
        "input_data": {
          "data": [
            0.7495315127483677,
            0.6937682573834558,
            0.8246293496604347,
            0.4226349875986425,
            0.9795540014212445
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:32.505263",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c4368d0c",
      "task_type": "basic_task",
      "description": "Step 1: Use the 'file_operations_reader' to read data from a specified CSV file. Step 2: Use the 'data_processing_parser' to parse the raw data from the CSV file into a structured format. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.12435649867615306,
            0.06522642389812672,
            0.3942293539766988,
            0.10655372497115789,
            0.2912996994644341
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 8.353442442199325
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:34.361474",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f44a84dc",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a specified file (e.g., a CSV file). Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria (e.g., removing entries that don't meet certain conditions). Step 5: Use data_processing_transformer to convert the filtered data into a different format (e.g., from JSON to XML) for final output.",
      "inputs": {
        "input_data": {
          "data": [
            0.638721601684423,
            0.9324536850327532,
            0.9714968537296346,
            0.9631855986701379,
            0.670786634983311
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:38.345320",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8e04258e",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a file in CSV format. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.32578427280923516,
            0.5829032752162873,
            0.4684528362959185,
            0.4148460652123588,
            0.31682201778070773
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:40.459795",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_544ebe22",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data file (CSV, JSON, or XML) containing the raw data. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.9910470846518696,
            0.7249200965578997,
            0.14259909766781764,
            0.7722124720865197,
            0.2575079210675575
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 6.073060645835072
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:42.567133",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5a932082",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read the data from a CSV file. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to filter the validated data based on specified criteria, retaining only the relevant entries. Step 5: (Optional) Use the data_processing_transformer tool to convert the filtered data into a different format, such as JSON, if needed.",
      "inputs": {
        "input_data": {
          "data": [
            0.9237235794456379,
            0.8871946749672569,
            0.17217164054698053,
            0.6169259007717407,
            0.39546858658059425
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:46.903264",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_577166f7",
      "task_type": "basic_task",
      "description": "Step 1: Use the 'file_operations_reader' to read the data from a specified file (e.g., a CSV or JSON file). Step 2: Use the 'data_processing_parser' to parse the raw data into a structured format. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.27806323538963285,
            0.7914733745926049,
            0.5981892582310123,
            0.7707853314565855,
            0.004033978784912073
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 4.7088525042245095
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:16.740705",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a1139f27",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read the input data file. This will retrieve the raw data from the specified file format (e.g., CSV, JSON). Step 2: Use the data_processing_parser tool to parse the raw data into a structured format. This will help organize the data for further analysis. Step 3: Use the data_processing_filter tool to selectively filter the structured data based on specified criteria. This will allow us to focus on the relevant subset of data. Step 4: Use the data_processing_validator tool to validate the filtered data against a predefined schema. This ensures that the data meets the required standards of correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.08110672694983723,
            0.34475653778994,
            0.7387756084028373,
            0.8143460642444803,
            0.9288681845175167
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_filter",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:20.239046",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_12137248",
      "task_type": "basic_task",
      "description": "Step 1: Use the 'file_operations_reader' tool to read a CSV file containing raw data. Step 2: Use the 'data_processing_parser' tool to parse the raw data from the CSV file into a structured format. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the 'data_processing_filter' tool to filter the validated data based on specified criteria, keeping only the relevant entries. Step 5: Use the 'data_processing_transformer' tool to convert the filtered data from its current format to a JSON format for easier usage.",
      "inputs": {
        "input_data": {
          "data": [
            0.896057889171891,
            0.47049105500997157,
            0.600325598501317,
            0.5960933420886468,
            0.8368762306842747
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 9.931965962747315,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:22.883405",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_51b6efc7",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a specified file (e.g., CSV, JSON, or XML). Step 2: Use data_processing_parser to parse the raw data retrieved from the file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: (Optional) Use data_processing_transformer to convert the filtered data into a different format if needed.",
      "inputs": {
        "input_data": {
          "data": [
            0.7552940580536533,
            0.023250142147877573,
            0.17774902612780452,
            0.6567888281635216,
            0.727452451512333
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:27.938251",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c45b51d6",
      "task_type": "basic_task",
      "description": "Step 1: Use the 'file_operations_reader' tool to read a CSV file containing raw data. Step 2: Use the 'data_processing_parser' tool to parse the raw data from the CSV file into a structured format. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the 'data_processing_filter' tool to filter the validated data based on specified criteria to extract relevant information.",
      "inputs": {
        "input_data": {
          "data": [
            0.4407272876626822,
            0.33183304747282805,
            0.6816857804998913,
            0.9409659688767472,
            0.48927350559594296
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:31.734395",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9e77b133",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader to read the raw data file (in CSV format) and retrieve its contents. Step 2: Use the data_processing_parser to parse the retrieved raw data into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.7395422332824374,
            0.008291080828310426,
            0.25206977491842164,
            0.48021319250521677,
            0.4684331062033329
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:34.457353",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_25fd0255",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the input data file (in CSV format) to retrieve the raw data. Step 2: Use data_processing_parser to parse the raw data retrieved from the file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.6077970122802221,
            0.3451869556437063,
            0.1564470153991152,
            0.7364838736317796,
            0.9451423707618819
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 3.7238007088404856
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:39.476915",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_60158452",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file. Step 2: Use data_processing_parser to parse the raw CSV data into a structured format. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_transformer to transform the filtered data into a different format, such as JSON.",
      "inputs": {
        "input_data": {
          "data": [
            0.910356925260274,
            0.523131439582939,
            0.23288078536090961,
            0.8640567801688471,
            0.17921365302963577
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:42.113039",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_93eb54c1",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a specified file (e.g., CSV). Step 2: Use data_processing_parser to parse the raw data retrieved from the file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a defined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_transformer to transform the filtered data into a different format if needed.",
      "inputs": {
        "input_data": {
          "data": [
            0.14506660399458082,
            0.4804506511098998,
            0.6419135711262215,
            0.8471290787096253,
            0.27486664632676217
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 1.5415611944882688
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:44.833985",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_266e99c0",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a CSV file. Step 2: Use data_processing_parser to parse the raw CSV data into a structured format. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_transformer to transform the filtered data into a JSON format.",
      "inputs": {
        "input_data": {
          "data": [
            0.5155399048143429,
            0.49786096430667204,
            0.987806531689771,
            0.7687846569640866,
            0.5168235184726518
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:46.940421",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_37467612",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the raw data from a CSV file. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.7537257035370071,
            0.6486608073422763,
            0.1609135002676383,
            0.10103026286557637,
            0.8102762780303804
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 4.906263334284403
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:19.406416",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_326245e0",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a specified file in CSV format. Step 2: Use data_processing_parser to parse the raw CSV data into a structured format for easier manipulation. Step 3: Use data_processing_filter to filter the parsed data based on specific criteria, such as selecting rows where a certain column value meets a condition. Step 4: Use data_processing_validator to validate the filtered data against a predefined schema to ensure it meets the required standards.",
      "inputs": {
        "input_data": {
          "data": [
            0.22313207134280055,
            0.10702291074449444,
            0.8048472494636748,
            0.9454974154095582,
            0.025736487214588544
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_filter",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 9.390235569411214
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:22.153821",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_2b6601ab",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file containing raw data. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria, keeping only the relevant entries.",
      "inputs": {
        "input_data": {
          "data": [
            0.778133688454017,
            0.3527736564884598,
            0.18925791829946048,
            0.976434455414998,
            0.6222280015695059
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 5.9418972239648244
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:25.109365",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_ff29eedd",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a specified file in CSV format. Step 2: Use data_processing_parser to parse the raw CSV data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specific criteria. Step 5: Use data_processing_transformer to convert the filtered data into a different format, such as JSON.",
      "inputs": {
        "input_data": {
          "data": [
            0.036301438520241924,
            0.9470228022495067,
            0.36417403354004463,
            0.3000122942991241,
            0.33164116617156947
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 8.605202713906193
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:28.866322",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1e4bff0d",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a CSV file. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the valid data based on specified criteria. Step 5: Use data_processing_transformer to transform the filtered data into a different format, such as JSON.",
      "inputs": {
        "input_data": {
          "data": [
            0.4312785384753469,
            0.25786953891521336,
            0.9689943693761757,
            0.9170939707154773,
            0.8485932686195031
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:36.005976",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_2e56314b",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_transformer to transform the filtered data into a different format, such as JSON.",
      "inputs": {
        "input_data": {
          "data": [
            0.6980242653154506,
            0.3658557901032755,
            0.6693018004521352,
            0.7771837196222363,
            0.6388101798380551
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:38.443764",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d7ff1a51",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data file containing raw data. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.9078495380946243,
            0.807047297597475,
            0.9467090620586776,
            0.24114176684766409,
            0.04085348258022348
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:40.234626",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_7391a4a1",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema, ensuring its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.8475867900210166,
            0.06647281235575364,
            0.4349720669668382,
            0.7686950333298194,
            0.0993299372372376
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:42.602351",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d52c9daf",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a specified CSV file. Step 2: Use data_processing_parser to parse the raw data extracted from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_transformer to convert the filtered data into a different format, such as JSON.",
      "inputs": {
        "input_data": {
          "data": [
            0.8342039651220349,
            0.8927792207774756,
            0.38466842669611967,
            0.5434354552431653,
            0.6162377205619046
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:45.334777",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_7fbe5d5e",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader to read data from a CSV file. Step 2: Use the data_processing_parser to parse the raw data from the CSV into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.03242991257490213,
            0.006317193515482433,
            0.3607567078517767,
            0.6527724187059719,
            0.031532340950739646
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "low"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:46.974973",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f03da290",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a specified input file (e.g., CSV or JSON format). Step 2: Use the data_processing_parser tool to parse the raw data into a structured format, extracting the necessary fields. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to filter the validated data based on specific criteria, reducing the dataset to only include relevant records. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data, summarizing key metrics as required.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.48466158614070254
            },
            {
              "id": 1,
              "value": 0.4622523738112645
            },
            {
              "id": 2,
              "value": 0.9818518448939096
            },
            {
              "id": 3,
              "value": 0.9131343956779848
            },
            {
              "id": 4,
              "value": 0.6487541851907259
            },
            {
              "id": 5,
              "value": 0.9779114447998636
            },
            {
              "id": 6,
              "value": 0.9660370270083416
            },
            {
              "id": 7,
              "value": 0.3109367202089035
            },
            {
              "id": 8,
              "value": 0.3840842141725693
            },
            {
              "id": 9,
              "value": 0.3062854977937062
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:18.332921",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_b8b6bef4",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read raw data from a CSV file. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.24940011365694337
            },
            {
              "id": 1,
              "value": 0.5963426770791633
            },
            {
              "id": 2,
              "value": 0.6029550498969355
            },
            {
              "id": 3,
              "value": 0.31914271372092684
            },
            {
              "id": 4,
              "value": 0.07677364278343746
            },
            {
              "id": 5,
              "value": 0.47295989970600927
            },
            {
              "id": 6,
              "value": 0.39222649904431595
            },
            {
              "id": 7,
              "value": 0.6590542336777552
            },
            {
              "id": 8,
              "value": 0.25404926010749895
            },
            {
              "id": 9,
              "value": 0.8075928645384666
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 9.24735262806714
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:21.453741",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_2278bc20",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data file (in CSV format) containing the data. Step 2: Use the data_processing_parser to parse the raw data into a structured format, making it easier to work with. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria (e.g., removing entries that do not meet certain thresholds). Step 5: Use the data_processing_aggregator to aggregate the filtered data, summarizing key metrics (e.g., averages, counts) for analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.16785320932802483
            },
            {
              "id": 1,
              "value": 0.23671907048743168
            },
            {
              "id": 2,
              "value": 0.6915660765467626
            },
            {
              "id": 3,
              "value": 0.6465497758972435
            },
            {
              "id": 4,
              "value": 0.9048290737828771
            },
            {
              "id": 5,
              "value": 0.865104973902719
            },
            {
              "id": 6,
              "value": 0.9101522372279583
            },
            {
              "id": 7,
              "value": 0.6259352720505643
            },
            {
              "id": 8,
              "value": 0.22842782829277408
            },
            {
              "id": 9,
              "value": 0.5681238591632751
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:25.410280",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_2f90bdf8",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a specified input file in CSV format. Step 2: Use the data_processing_parser tool to parse the raw CSV data into a structured format, extracting relevant fields. Step 3: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant records. Step 5: Finally, use the data_processing_aggregator tool to aggregate the filtered data, summarizing it for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.315744564851673
            },
            {
              "id": 1,
              "value": 0.026317353614022876
            },
            {
              "id": 2,
              "value": 0.8296632423057795
            },
            {
              "id": 3,
              "value": 0.42972424344709115
            },
            {
              "id": 4,
              "value": 0.3327061165217313
            },
            {
              "id": 5,
              "value": 0.12679606708777236
            },
            {
              "id": 6,
              "value": 0.8406204777426755
            },
            {
              "id": 7,
              "value": 0.848806671661594
            },
            {
              "id": 8,
              "value": 0.3445148103878063
            },
            {
              "id": 9,
              "value": 0.7834146119636801
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:28.944783",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_33451d15",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read and retrieve raw data from a specified file (e.g., CSV, JSON, or XML format). Step 2: Use the 'data_processing_parser' tool to parse the raw data into a structured format for easier processing. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specified criteria to reduce the dataset. Step 5: Finally, use the 'data_processing_aggregator' tool to aggregate the filtered data for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.8348304854364368
            },
            {
              "id": 1,
              "value": 0.981133046597135
            },
            {
              "id": 2,
              "value": 0.8858269547315509
            },
            {
              "id": 3,
              "value": 0.8590389478611756
            },
            {
              "id": 4,
              "value": 0.2952488787691959
            },
            {
              "id": 5,
              "value": 0.30291952730639826
            },
            {
              "id": 6,
              "value": 0.7469163934413684
            },
            {
              "id": 7,
              "value": 0.28591551656021574
            },
            {
              "id": 8,
              "value": 0.691027145825297
            },
            {
              "id": 9,
              "value": 0.8351970584569148
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:31.903021",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_308d79c3",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a specified source file in CSV format. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format, extracting relevant fields. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, reducing the size of the dataset. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.9466967868450811
            },
            {
              "id": 1,
              "value": 0.39998497386736065
            },
            {
              "id": 2,
              "value": 0.7713523514378143
            },
            {
              "id": 3,
              "value": 0.3790406077653279
            },
            {
              "id": 4,
              "value": 0.6798136860949267
            },
            {
              "id": 5,
              "value": 0.666323498712316
            },
            {
              "id": 6,
              "value": 0.41672013927346996
            },
            {
              "id": 7,
              "value": 0.1461862375398295
            },
            {
              "id": 8,
              "value": 0.12840303019044452
            },
            {
              "id": 9,
              "value": 0.9068732220760366
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:34.308283",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_47916c8b",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data files in CSV format. This will retrieve the data needed for processing. Step 2: Use the data_processing_parser tool to parse the retrieved raw data into a structured format, making it easier to work with. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema, ensuring that it is correct and meets the necessary requirements. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, reducing the dataset to only the necessary records. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.3706847747726577
            },
            {
              "id": 1,
              "value": 0.39207058859879496
            },
            {
              "id": 2,
              "value": 0.6245739492023356
            },
            {
              "id": 3,
              "value": 0.6255783518837571
            },
            {
              "id": 4,
              "value": 0.7769392136654548
            },
            {
              "id": 5,
              "value": 0.34752900478116056
            },
            {
              "id": 6,
              "value": 0.8298546526853643
            },
            {
              "id": 7,
              "value": 0.06758955664442146
            },
            {
              "id": 8,
              "value": 0.16257976175671507
            },
            {
              "id": 9,
              "value": 0.6071810713009954
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:37.547430",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_51d7a0dd",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from a specified file in CSV format. Step 2: Use the data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria to reduce the dataset size. Step 5: Use the data_processing_aggregator to aggregate the filtered data to summarize the key metrics.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.13802284688194333
            },
            {
              "id": 1,
              "value": 0.4033573158196564
            },
            {
              "id": 2,
              "value": 0.305996751926338
            },
            {
              "id": 3,
              "value": 0.33793310840141333
            },
            {
              "id": 4,
              "value": 0.22788307981863176
            },
            {
              "id": 5,
              "value": 0.765811825201696
            },
            {
              "id": 6,
              "value": 0.8123040357378117
            },
            {
              "id": 7,
              "value": 0.8709101121530569
            },
            {
              "id": 8,
              "value": 0.6648620746473387
            },
            {
              "id": 9,
              "value": 0.6561978644147124
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 6.5643453149766975
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:40.881812",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_cc4e8a1b",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file containing raw input data. Step 2: Use the data_processing_parser tool to parse the raw CSV data into a structured format. Step 3: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specific criteria. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.5076305789762383
            },
            {
              "id": 1,
              "value": 0.5093481298473168
            },
            {
              "id": 2,
              "value": 0.43154334232469704
            },
            {
              "id": 3,
              "value": 0.6972305122906676
            },
            {
              "id": 4,
              "value": 0.5342489416735245
            },
            {
              "id": 5,
              "value": 0.3018397559809628
            },
            {
              "id": 6,
              "value": 0.9591314869167817
            },
            {
              "id": 7,
              "value": 0.2757830196023757
            },
            {
              "id": 8,
              "value": 0.014167647192842359
            },
            {
              "id": 9,
              "value": 0.13833374196618498
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:43.643792",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_585524d0",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read the raw data from a specified data file (e.g., CSV, JSON). This will retrieve the data for further processing. Step 2: Use the 'data_processing_parser' tool to parse the raw data into a structured format. This ensures the data is organized and ready for analysis. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema. This step ensures that the data meets the required integrity and correctness standards. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specified criteria, reducing the dataset to only relevant information. Step 5: Finally, use the 'data_processing_aggregator' tool to aggregate the filtered data, summarizing the results for easier analysis and reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.935760253890114
            },
            {
              "id": 1,
              "value": 0.6419176555672857
            },
            {
              "id": 2,
              "value": 0.5724772641785385
            },
            {
              "id": 3,
              "value": 0.2853277063458348
            },
            {
              "id": 4,
              "value": 0.19243247640806316
            },
            {
              "id": 5,
              "value": 0.39275157361786583
            },
            {
              "id": 6,
              "value": 0.7652542506941801
            },
            {
              "id": 7,
              "value": 0.3533655836060837
            },
            {
              "id": 8,
              "value": 0.9308657489312043
            },
            {
              "id": 9,
              "value": 0.14229559635461264
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:47.260379",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_407bf1f2",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read the data from the input file (e.g., a CSV file). Step 2: Use the data_processing_parser tool to parse the raw data from the file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the valid data based on specified criteria. Step 5: Use the data_processing_transformer tool to transform the filtered data into a different format (e.g., convert it from JSON to XML).",
      "inputs": {
        "input_data": {
          "data": [
            0.5071573240280632,
            0.13606650315528546,
            0.31105009226952085,
            0.04677502968005698,
            0.20557696163953743
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "low"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:17.132852",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_ddfe4a0f",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read the data from a specified file (e.g., a CSV file). Step 2: Use the data_processing_parser tool to parse the raw data into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria (e.g., filtering out irrelevant entries). Step 5: Use the data_processing_transformer tool to transform the filtered data into a different format if necessary (e.g., converting from JSON to XML).",
      "inputs": {
        "input_data": {
          "data": [
            0.14522103755253446,
            0.6651129135365391,
            0.26759403582403296,
            0.9373668818579456,
            0.26881612924452736
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 5.622636632775849
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:21.083296",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_fc4fc184",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read a data file in CSV format. Step 2: Use the data_processing_parser tool to parse the raw CSV data into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specific criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.2942719712538491,
            0.2986986491800667,
            0.4688750977061178,
            0.6571964200960638,
            0.1117768935251735
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 0.12860953450509627
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:24.272063",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_280b94c7",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data file containing raw data. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a defined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_transformer to transform the filtered data into a different format as required.",
      "inputs": {
        "input_data": {
          "data": [
            0.04924006300029904,
            0.5915880434965894,
            0.7095267609818751,
            0.8152136363525975,
            0.6963096030005443
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 6.402648881031751
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:26.721130",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_baaff2e7",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.626393200626576,
            0.24379872825687043,
            0.617720393717061,
            0.9820221633658832,
            0.3657058122159822
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:29.356751",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_fcc50476",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read data from a specified file (e.g., a CSV file). Step 2: Use the data_processing_parser tool to parse the raw data from the file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the valid data based on specified criteria, if necessary. Step 5: (Optional) Use the data_processing_transformer tool to transform the filtered data into a different format (e.g., from JSON to XML) if required.",
      "inputs": {
        "input_data": {
          "data": [
            0.14921456153464185,
            0.6699393096879835,
            0.8120574559326488,
            0.5776565225109539,
            0.576900466822122
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 6.755918252457315
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:34.414144",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5af0f1b5",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read a CSV file containing raw data. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to apply filtering options to the validated data, selecting only the relevant entries. Step 5: Use data_processing_transformer to convert the filtered data into a JSON format for further usage.",
      "inputs": {
        "input_data": {
          "data": [
            0.7526907320776133,
            0.4308131897204832,
            0.46826954788242814,
            0.057778372419528545,
            0.03965675416159886
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:37.380275",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8a95efb9",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a CSV file. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the valid data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.6038327392336739,
            0.20739487482459007,
            0.47096679139708664,
            0.7591553742850358,
            0.5867754197630987
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:40.891772",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8828d1cd",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read the data from a CSV file. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.871482052406909,
            0.456314417186125,
            0.24174366129027058,
            0.9649043168260768,
            0.059668432347297795
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 3.7217911255923095
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:43.688786",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5cab4440",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read data from a specified input file (e.g., a CSV file). Step 2: Use the data_processing_parser tool to parse the raw data extracted from the file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to selectively filter the valid data based on specified criteria. Step 5: Finally, use the data_processing_transformer tool to transform the filtered data into a different format (e.g., converting from JSON to XML).",
      "inputs": {
        "input_data": {
          "data": [
            0.49383272864230476,
            0.2392760588838263,
            0.3425543719151929,
            0.2231856331970734,
            0.8163209844850774
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:47.384805",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_2952793c",
      "task_type": "basic_task",
      "description": "Step 1: Use the 'file_operations_reader' tool to read data from a CSV file containing raw data. Step 2: Use the 'data_processing_parser' tool to parse the raw data from the CSV file into a structured format. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.13332442469557282,
            0.8543401866699845,
            0.9860271726675807,
            0.48558419885625415,
            0.42599295024907224
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:16.108032",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5d5845f1",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader to read a CSV file containing raw data. Step 2: Use the data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria, such as removing entries that do not meet certain conditions. Step 5: Finally, use the data_processing_transformer to convert the filtered data into a different format, such as JSON.",
      "inputs": {
        "input_data": {
          "data": [
            0.06889027184460927,
            0.023798727151008614,
            0.7962219108364821,
            0.2874382798718458,
            0.30695381387941945
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 4.795072975465394
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:18.585728",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_39663eb4",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a specified CSV file. Step 2: Use data_processing_parser to parse the raw data retrieved from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.5160446499551041,
            0.370538152868054,
            0.0038225033622668603,
            0.912107658078413,
            0.34046182611758313
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:22.042951",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_977f0cc5",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a specified input file (e.g., CSV, JSON). Step 2: Use data_processing_parser to parse the raw data retrieved from the file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_transformer to transform the filtered data into the desired output format (e.g., converting from JSON to XML).",
      "inputs": {
        "input_data": {
          "data": [
            0.9948636151436541,
            0.06679355675892851,
            0.43984130513073016,
            0.4087075211943534,
            0.839271019177917
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:25.453760",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9694459a",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.003832404689686708,
            0.5330870163611116,
            0.8001204427083796,
            0.8544375214749583,
            0.13799364695638272
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 3.390884814822749
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:28.368180",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_48b3c643",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read the data from a specified input file (e.g., a CSV file). Step 2: Use the data_processing_parser tool to parse the raw data into a structured format (e.g., converting it into a JSON format). Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria (e.g., extracting records that meet certain conditions). Step 5: Use the data_processing_transformer tool to transform the filtered data into a different format (e.g., converting it from JSON to XML).",
      "inputs": {
        "input_data": {
          "data": [
            0.12715309299366895,
            0.4843960326439196,
            0.6132611896410959,
            0.7333422603018017,
            0.6219919489741761
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:33.412018",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a08b4f86",
      "task_type": "basic_task",
      "description": "Step 1: Use the 'file_operations_reader' tool to read a CSV file containing raw data. Step 2: Use the 'data_processing_parser' tool to parse the raw data from the CSV file into a structured format. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specified criteria. Step 5: Finally, use the 'data_processing_transformer' tool to transform the filtered data into a different format, such as JSON.",
      "inputs": {
        "input_data": {
          "data": [
            0.02486293414363905,
            0.6347728558698564,
            0.8492100191389184,
            0.37892804040876116,
            0.652531792492266
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:38.690630",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_80906333",
      "task_type": "basic_task",
      "description": "Step 1: Use 'file_operations_reader' to read data from a specified input file (e.g., a CSV or JSON file). Step 2: Use 'data_processing_parser' to parse the raw data into a structured format. Step 3: Use 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use 'data_processing_filter' to selectively filter the validated data based on specified criteria. Step 5: Optionally, use 'data_processing_transformer' to transform the filtered data into a different format if needed.",
      "inputs": {
        "input_data": {
          "data": [
            0.6850130207089113,
            0.535797766611894,
            0.8411914874307085,
            0.8450523597406013,
            0.8251937321343202
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:42.382258",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_7074668f",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader to read data from a specified file. This will retrieve the raw data needed for processing. Step 2: Use the data_processing_parser to parse the raw data into a structured format, making it easier to work with. Step 3: Use the data_processing_validator to check the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter to selectively filter the structured data based on specific criteria, narrowing down the results. Step 5: Use the data_processing_transformer to convert the filtered data into a desired output format, ensuring it meets the required specifications.",
      "inputs": {
        "input_data": {
          "data": [
            0.2791104345390587,
            0.18777235411391813,
            0.8566431802061637,
            0.8970187043010704,
            0.4112182302929823
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 5.4563643338838705
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:45.627135",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8682d5b4",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file. Step 2: Use data_processing_parser to parse the raw CSV data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_transformer to transform the filtered data into a different format, such as JSON.",
      "inputs": {
        "input_data": {
          "data": [
            0.8770483121703271,
            0.9024703890623981,
            0.6769952475109873,
            0.0757905323093544,
            0.6811378239691332
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:47.795314",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_b266c364",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve raw data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data into a structured format for easier manipulation. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to convert the validated data into the desired output format (e.g., JSON to XML). Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 7.5318064479116575
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:17.517163",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_bbbc2e12",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher tool to retrieve raw data from a specified API endpoint. Step 2: Use the data_processing_parser tool to parse the retrieved raw data into a structured format (e.g., JSON). Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_transformer tool to transform the validated data into a different format, if necessary (e.g., from JSON to XML). Step 5: Use the network_poster tool to send the transformed data to a specified destination endpoint over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "high"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:20.567273",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9f2b00e7",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to the specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 0.8505231383284553
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:27.648273",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0bc8582f",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to transform the validated data into a different format as required. Step 5: Use network_poster to send the transformed data to the specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 5.813886473023515,
        "priority": "medium"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:30.493409",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8a630a64",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data from JSON format to XML format (or another specified format). Step 5: Use network_poster to send the transformed data to a specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 1.9613094583403186,
        "priority": "high"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:33.114544",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1a90b345",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher tool to retrieve data from the specified API endpoint. Step 2: Use the data_processing_parser tool to parse the raw data retrieved from the API into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_transformer tool to transform the validated data into a different format if necessary (e.g., JSON to XML). Step 5: Use the network_poster tool to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:35.779390",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_239bccb5",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve raw data from a specified API endpoint. Step 2: Use data_processing_parser to parse the retrieved raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a different format as required. Step 5: Use network_poster to send the transformed data to a specified destination API endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:40.135802",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_68a4244c",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher to retrieve raw data from a specified API endpoint. Step 2: Use the data_processing_parser to parse the retrieved raw data into a structured format, such as JSON. Step 3: Use the data_processing_validator to check the structured data against a predefined schema to ensure its correctness and integrity. Step 4: If the data is valid, use the network_poster to send the validated data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 4.04246370161997
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:42.423706",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a5937129",
      "task_type": "api_integration",
      "description": "Step 1: Use the 'network_fetcher' to retrieve raw data from a specified API endpoint. Step 2: Use the 'data_processing_parser' to parse the raw data retrieved into a structured format. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the 'data_processing_transformer' to transform the validated data into a different format, if necessary. Step 5: Finally, use the 'network_poster' to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:45.564759",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_32c2475f",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_validator to validate the retrieved data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_parser to parse the validated data into a structured format for easier processing. Step 4: Use network_poster to send the structured data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_validator",
        "data_processing_parser",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "medium"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:47.823494",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_24b3224e",
      "task_type": "basic_task",
      "description": "Step 1: Use the 'file_operations_reader' tool to read the data from a specified file (e.g., a CSV file). Step 2: Use the 'data_processing_parser' tool to parse the raw data read from the file into a structured format. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specified criteria. Step 5: Use the 'data_processing_transformer' tool to transform the filtered data into a different format if needed (e.g., converting JSON to XML).",
      "inputs": {
        "input_data": {
          "data": [
            0.8554818590820215,
            0.21749109280511902,
            0.8816846945393558,
            0.02729492403992073,
            0.44130580237226713
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 1.3847829300921841
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:17.567925",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e0a3f1ca",
      "task_type": "basic_task",
      "description": "Step 1: Use the 'file_operations_reader' to read a CSV file containing raw data. Step 2: Use the 'data_processing_parser' to parse the raw data from the CSV file into a structured format. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' to selectively filter the validated data based on specified criteria. Step 5: Use the 'data_processing_transformer' to convert the filtered data from JSON format to XML format for final output.",
      "inputs": {
        "input_data": {
          "data": [
            0.5875028093677318,
            0.8670783051609583,
            0.30548049810490663,
            0.9623690254737869,
            0.9814737988718336
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:23.057905",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_18e119b2",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a CSV file. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.5915325408643403,
            0.5571794471037121,
            0.10754909978184224,
            0.006174908634275056,
            0.20546239996447835
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 0.7440513942095004
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:26.703467",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_11286d0d",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read the data from a specified input file (e.g., CSV, JSON). Step 2: Use the data_processing_parser tool to parse the raw data read from the file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a defined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.3032620864733142,
            0.254701218219075,
            0.35895369774133,
            0.13035992554544207,
            0.9917310410696829
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:29.075357",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d03ae071",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read the data from a specified file (e.g., CSV, JSON, or XML). Step 2: Use the data_processing_parser tool to parse the raw data retrieved in Step 1 into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data from Step 2 against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_transformer tool to transform the filtered data into a desired output format (e.g., converting JSON to XML).",
      "inputs": {
        "input_data": {
          "data": [
            0.8484201345768874,
            0.8520778207421056,
            0.2052977872364007,
            0.7151603204348956,
            0.23722697040911067
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:33.289675",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_98080489",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the raw data file in CSV format. Step 2: Use data_processing_parser to parse the raw CSV data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.8438226545994701,
            0.04111260896409752,
            0.5133008688233314,
            0.763187698423528,
            0.6620626365939153
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:36.825459",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e20e1714",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file named 'data.csv'. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_transformer to transform the filtered data into a JSON format for further use.",
      "inputs": {
        "input_data": {
          "data": [
            0.9068353622819157,
            0.6974799001176134,
            0.31841044463627566,
            0.9990435415339821,
            0.463391581002777
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:39.476554",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f9c6c045",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a specified file (e.g., a CSV file). Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_transformer to convert the filtered data into a different format (e.g., from JSON to XML).",
      "inputs": {
        "input_data": {
          "data": [
            0.27388621651921685,
            0.8350002286453362,
            0.7481839638102271,
            0.04777611322493369,
            0.372674747442578
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:42.354996",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_bbebf27d",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read a CSV file containing raw data. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_transformer tool to convert the filtered data into a different format, such as JSON.",
      "inputs": {
        "input_data": {
          "data": [
            0.5054257756258335,
            0.42992605678991125,
            0.044174603508096544,
            0.4176273997468034,
            0.9235074451013242
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:44.988758",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_35aebbe0",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a specified input file in CSV format. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to filter the validated data based on specified criteria to narrow down the results. Step 5: Use data_processing_transformer to convert the filtered data from CSV format to JSON format for easier consumption.",
      "inputs": {
        "input_data": {
          "data": [
            0.2260812935550548,
            0.08328961592543804,
            0.06841857670820373,
            0.3965969312408444,
            0.6131567048240356
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:47.961054",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0a636fdd",
      "task_type": "basic_task",
      "description": "Step 1: Use the 'file_operations_reader' tool to read data from a CSV file containing raw data. Step 2: Use the 'data_processing_parser' tool to parse the raw data from the CSV file into a structured format. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.8422648477016436,
            0.11046002846521696,
            0.08560621321920936,
            0.8505487212703938,
            0.7470116476728813
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 5.974067452729338
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:17.542886",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_2a84e63a",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read a CSV file containing raw data. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_transformer tool to transform the filtered data into JSON format for final output.",
      "inputs": {
        "input_data": {
          "data": [
            0.07097492555555218,
            0.6113135112959154,
            0.676393234481858,
            0.871820734673611,
            0.7568316302694225
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:25.323770",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0a0c1864",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to selectively filter the valid data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.5017371888760456,
            0.8929891909882883,
            0.7644271973411654,
            0.7931789484009956,
            0.6166502017468087
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 0.8053656971347747
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:28.620458",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_7405f269",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file containing raw data. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria, such as removing entries that do not meet certain conditions. Step 5: Optionally, use data_processing_transformer to convert the filtered data into a different format, such as JSON, for further use.",
      "inputs": {
        "input_data": {
          "data": [
            0.816103712026862,
            0.1918672026067899,
            0.04299585236042758,
            0.5255534452118072,
            0.08173370062290586
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:31.264724",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5d78fa1c",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file containing raw data. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.26878101158250367,
            0.9314368900663068,
            0.6741058561526815,
            0.4245229768438328,
            0.9132012037443376
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 7.679872898987696
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:34.047137",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_2083651b",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a CSV file. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.42249017909107844,
            0.5383365060982647,
            0.24709093261020554,
            0.6135260327208745,
            0.8041583476024882
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:36.867933",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0fa245e2",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader to read the data from a CSV file. Step 2: Use the data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.42462644371085656,
            0.6210402003248503,
            0.2528349187703307,
            0.21555808867754733,
            0.7759333011885968
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 9.703323712504183,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:39.941011",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d31ab87a",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.6166808152321238,
            0.39843113454284496,
            0.9383479178690596,
            0.8817376714206995,
            0.4520023313674534
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:42.434124",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4483eb8e",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file. Step 2: Use the data_processing_parser tool to parse the raw CSV data into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_transformer tool to transform the filtered data into a JSON format for final output.",
      "inputs": {
        "input_data": {
          "data": [
            0.4506569375757482,
            0.9248782592032161,
            0.8006439333775126,
            0.1790192349996037,
            0.7762719233357045
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:45.051888",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_52949e95",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a file in CSV format. Step 2: Use data_processing_parser to parse the raw CSV data into a structured format, such as a data table. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to apply specific filtering criteria to the validated data, narrowing down the dataset. Step 5: Optionally, use data_processing_transformer to convert the filtered data into another format, such as JSON, if needed.",
      "inputs": {
        "input_data": {
          "data": [
            0.6699690961838329,
            0.13765194819991722,
            0.07361189051157235,
            0.9378872890124292,
            0.35746347154166647
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:47.979072",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8fbc291f",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read the raw data from the input file, which can be in formats like CSV, JSON, or XML. Step 2: Use the 'data_processing_parser' tool to parse the raw data into a structured format, allowing for easier manipulation and analysis. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' tool to apply filtering criteria on the validated data to extract only the relevant information needed for further processing. Step 5: Finally, use the 'data_processing_aggregator' tool to aggregate the filtered data, summarizing it for reporting or analysis purposes.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.423253375791516
            },
            {
              "id": 1,
              "value": 0.45748044399824206
            },
            {
              "id": 2,
              "value": 0.6033427837338305
            },
            {
              "id": 3,
              "value": 0.5133966200333702
            },
            {
              "id": 4,
              "value": 0.4486068778756286
            },
            {
              "id": 5,
              "value": 0.22874916747894136
            },
            {
              "id": 6,
              "value": 0.4983563250440496
            },
            {
              "id": 7,
              "value": 0.050461377771206406
            },
            {
              "id": 8,
              "value": 0.8447583196474903
            },
            {
              "id": 9,
              "value": 0.6821607243545893
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:18.846796",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d4afe033",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from the input files (e.g., CSV or JSON). Step 2: Use the data_processing_parser tool to parse the raw data into a structured format, ensuring that it is ready for further processing. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, reducing the dataset to relevant entries. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data, summarizing key insights and preparing it for analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.43436552454933497
            },
            {
              "id": 1,
              "value": 0.9367696800433984
            },
            {
              "id": 2,
              "value": 0.5973547742389675
            },
            {
              "id": 3,
              "value": 0.7078071956737704
            },
            {
              "id": 4,
              "value": 0.555688859118149
            },
            {
              "id": 5,
              "value": 0.582414924554098
            },
            {
              "id": 6,
              "value": 0.27625520747762766
            },
            {
              "id": 7,
              "value": 0.6191924696657259
            },
            {
              "id": 8,
              "value": 0.27050776582568914
            },
            {
              "id": 9,
              "value": 0.932186773456167
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:21.987543",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_30752c5d",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data file in CSV format. Step 2: Use the data_processing_parser to parse the raw data into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_aggregator to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.36733480846631095
            },
            {
              "id": 1,
              "value": 0.6945558920737831
            },
            {
              "id": 2,
              "value": 0.7308243429408562
            },
            {
              "id": 3,
              "value": 0.9626836883780187
            },
            {
              "id": 4,
              "value": 0.4962338363638563
            },
            {
              "id": 5,
              "value": 0.5304529627070678
            },
            {
              "id": 6,
              "value": 0.14547857252082874
            },
            {
              "id": 7,
              "value": 0.6217074859734147
            },
            {
              "id": 8,
              "value": 0.5640120186635168
            },
            {
              "id": 9,
              "value": 0.6061247443295694
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "high"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:25.812800",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1f0f5a56",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a specified input file (e.g., CSV, JSON). Step 2: Use the data_processing_parser tool to parse the raw data retrieved from the file into a structured format. Step 3: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Finally, use the data_processing_aggregator tool to aggregate the filtered data for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.6477030139842451
            },
            {
              "id": 1,
              "value": 0.5774543315918481
            },
            {
              "id": 2,
              "value": 0.44744475800866657
            },
            {
              "id": 3,
              "value": 0.3294745634766917
            },
            {
              "id": 4,
              "value": 0.1165493510464205
            },
            {
              "id": 5,
              "value": 0.8235731453223866
            },
            {
              "id": 6,
              "value": 0.8485115900223965
            },
            {
              "id": 7,
              "value": 0.9753578229216108
            },
            {
              "id": 8,
              "value": 0.203522282604931
            },
            {
              "id": 9,
              "value": 0.4454436636852662
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 8.660978023005283
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:29.224120",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_aea1433f",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a specified file (e.g., a CSV or JSON file). Step 2: Use the data_processing_parser tool to parse the raw data into a structured format, making it easier to work with. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, such as certain value ranges or categories. Step 5: Finally, use the data_processing_aggregator tool to aggregate the filtered data, providing summarized insights or statistics as needed.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.8537049998920497
            },
            {
              "id": 1,
              "value": 0.6041811735188859
            },
            {
              "id": 2,
              "value": 0.4531349250474075
            },
            {
              "id": 3,
              "value": 0.13069944878136963
            },
            {
              "id": 4,
              "value": 0.8809154232547773
            },
            {
              "id": 5,
              "value": 0.8870680236635314
            },
            {
              "id": 6,
              "value": 0.9217521111359781
            },
            {
              "id": 7,
              "value": 0.39920052963510755
            },
            {
              "id": 8,
              "value": 0.9054933029856476
            },
            {
              "id": 9,
              "value": 0.17405758725160037
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:32.888950",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_29b8c5b1",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read and retrieve data from a CSV file containing raw data. Step 2: Use the data_processing_parser to parse the raw data from the CSV file into a structured format such as JSON. Step 3: Use the data_processing_validator to validate the structured data against a defined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the valid data based on specified criteria. Step 5: Use the data_processing_aggregator to aggregate the filtered data, summarizing it for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.524856307476625
            },
            {
              "id": 1,
              "value": 0.6768799671223196
            },
            {
              "id": 2,
              "value": 0.9902504518673587
            },
            {
              "id": 3,
              "value": 0.36857405262665643
            },
            {
              "id": 4,
              "value": 0.9563425489234587
            },
            {
              "id": 5,
              "value": 0.5819984147724647
            },
            {
              "id": 6,
              "value": 0.9717828067989062
            },
            {
              "id": 7,
              "value": 0.5998993554403926
            },
            {
              "id": 8,
              "value": 0.6534159656068129
            },
            {
              "id": 9,
              "value": 0.3194118703604363
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:35.769999",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1c634dc4",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read raw data from a CSV file. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to apply specific filtering criteria to the validated data, selectively reducing the dataset. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.6354500882277399
            },
            {
              "id": 1,
              "value": 0.8151596665275295
            },
            {
              "id": 2,
              "value": 0.046893114751598186
            },
            {
              "id": 3,
              "value": 0.10664782315107646
            },
            {
              "id": 4,
              "value": 0.5426142375626845
            },
            {
              "id": 5,
              "value": 0.056868855032298216
            },
            {
              "id": 6,
              "value": 0.9912156666200959
            },
            {
              "id": 7,
              "value": 0.17515839596441074
            },
            {
              "id": 8,
              "value": 0.3392346349900126
            },
            {
              "id": 9,
              "value": 0.6563202692589628
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 3.5816798384372746
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:38.833310",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_ee45631a",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read the raw data from a specified input file (e.g., CSV or JSON). Step 2: Use data_processing_parser to parse the raw data into a structured format for easier analysis. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria, reducing the dataset to only relevant entries. Step 5: Use data_processing_aggregator to aggregate the filtered data for reporting or further analysis. Step 6: Finally, use computation_analyzer to analyze the aggregated data and generate statistical insights or trend analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.7445298986585552
            },
            {
              "id": 1,
              "value": 0.8022227802492439
            },
            {
              "id": 2,
              "value": 0.13089756156474208
            },
            {
              "id": 3,
              "value": 0.8850423595928345
            },
            {
              "id": 4,
              "value": 0.0021090804731065393
            },
            {
              "id": 5,
              "value": 0.00035606575192015377
            },
            {
              "id": 6,
              "value": 0.17525614152143587
            },
            {
              "id": 7,
              "value": 0.8882647768114434
            },
            {
              "id": 8,
              "value": 0.9696076460180336
            },
            {
              "id": 9,
              "value": 0.5529415671195386
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "computation_analyzer"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 1.6335844150758858
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:42.030343",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_108fada8",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read the raw data from a CSV file. Step 2: Use data_processing_parser to parse the raw data and structure it into a defined format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_aggregator to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.09513538157811252
            },
            {
              "id": 1,
              "value": 0.6633717305101792
            },
            {
              "id": 2,
              "value": 0.9976874229290251
            },
            {
              "id": 3,
              "value": 0.6090611822230272
            },
            {
              "id": 4,
              "value": 0.7877881973986377
            },
            {
              "id": 5,
              "value": 0.4435654162331091
            },
            {
              "id": 6,
              "value": 0.3634244851167152
            },
            {
              "id": 7,
              "value": 0.7541595730120073
            },
            {
              "id": 8,
              "value": 0.3333221937301488
            },
            {
              "id": 9,
              "value": 0.9090251015621652
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 1.5686399384971677,
        "priority": "high"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:45.918190",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_7f83be4e",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read and retrieve the raw data from a specified file format (e.g., CSV). Step 2: Use the data_processing_parser to parse the raw data into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Finally, use the data_processing_aggregator to aggregate the filtered data for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.07529843837457406
            },
            {
              "id": 1,
              "value": 0.9535172637919429
            },
            {
              "id": 2,
              "value": 0.9647019163891764
            },
            {
              "id": 3,
              "value": 0.9338516577433281
            },
            {
              "id": 4,
              "value": 0.4396302294123027
            },
            {
              "id": 5,
              "value": 0.39640520657565426
            },
            {
              "id": 6,
              "value": 0.9697165709983611
            },
            {
              "id": 7,
              "value": 0.3413938275011448
            },
            {
              "id": 8,
              "value": 0.255170370725604
            },
            {
              "id": 9,
              "value": 0.2205490115358173
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:48.290594",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0909e101",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read raw data from a CSV file. This will retrieve the data necessary for processing. Step 2: Use the 'data_processing_parser' tool to parse the raw data retrieved in the previous step into a structured format, ensuring that it is ready for further processing. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to verify its correctness and integrity. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant entries. Step 5: Finally, use the 'data_processing_aggregator' tool to aggregate the filtered data into meaningful summaries or metrics that can be used for analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.38224758695878314
            },
            {
              "id": 1,
              "value": 0.1269092249202317
            },
            {
              "id": 2,
              "value": 0.60473624482458
            },
            {
              "id": 3,
              "value": 0.7748361584015435
            },
            {
              "id": 4,
              "value": 0.195429541896607
            },
            {
              "id": 5,
              "value": 0.07797245950455878
            },
            {
              "id": 6,
              "value": 0.5354627104422081
            },
            {
              "id": 7,
              "value": 0.9945624458565085
            },
            {
              "id": 8,
              "value": 0.5017456519522602
            },
            {
              "id": 9,
              "value": 0.44710143634397337
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:19.271569",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d9ad0aeb",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a CSV file and retrieve its contents. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format, allowing us to extract relevant information. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant entries. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for further analysis, summarizing key metrics.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.39070650048854927
            },
            {
              "id": 1,
              "value": 0.1786859225998897
            },
            {
              "id": 2,
              "value": 0.594919156797333
            },
            {
              "id": 3,
              "value": 0.6543092369324415
            },
            {
              "id": 4,
              "value": 0.6170690352341132
            },
            {
              "id": 5,
              "value": 0.782849591433992
            },
            {
              "id": 6,
              "value": 0.06223456811954109
            },
            {
              "id": 7,
              "value": 0.3716143227888674
            },
            {
              "id": 8,
              "value": 0.3637862112236453
            },
            {
              "id": 9,
              "value": 0.48710093916313413
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:22.595509",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_ca8e8c53",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' to read the raw data from a specified input file in CSV format. Step 2: Use the 'data_processing_parser' to parse the raw CSV data into a structured format such as JSON. Step 3: Use the 'data_processing_validator' to validate the parsed JSON data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant records. Step 5: Use the 'data_processing_aggregator' to aggregate the filtered data for summary statistics or insights.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.21048838455073682
            },
            {
              "id": 1,
              "value": 0.5107642761081441
            },
            {
              "id": 2,
              "value": 0.31122652854099286
            },
            {
              "id": 3,
              "value": 0.2521820561368976
            },
            {
              "id": 4,
              "value": 0.7173110176980558
            },
            {
              "id": 5,
              "value": 0.2979363332290822
            },
            {
              "id": 6,
              "value": 0.8203675543030851
            },
            {
              "id": 7,
              "value": 0.5884050994334695
            },
            {
              "id": 8,
              "value": 0.25183152907038797
            },
            {
              "id": 9,
              "value": 0.5340563846996459
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "high"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:26.372824",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_3bde7d21",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a specified input file (e.g., data.csv). Step 2: Use the data_processing_parser tool to parse the raw data into a structured format, extracting relevant fields. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure data integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specific criteria (e.g., filtering out records that do not meet certain thresholds). Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for further analysis, summarizing key metrics as needed.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.4859789227493051
            },
            {
              "id": 1,
              "value": 0.2351256103153755
            },
            {
              "id": 2,
              "value": 0.689744072034507
            },
            {
              "id": 3,
              "value": 0.7722043165461745
            },
            {
              "id": 4,
              "value": 0.2878220965465359
            },
            {
              "id": 5,
              "value": 0.6451950380640431
            },
            {
              "id": 6,
              "value": 0.5550069317939995
            },
            {
              "id": 7,
              "value": 0.6472035340433705
            },
            {
              "id": 8,
              "value": 0.9957573487118293
            },
            {
              "id": 9,
              "value": 0.5514820606459854
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "medium"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:30.239469",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c464d282",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read and retrieve raw data from a CSV file. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_aggregator to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.7120234975752756
            },
            {
              "id": 1,
              "value": 0.7870981893508617
            },
            {
              "id": 2,
              "value": 0.0067909198442522856
            },
            {
              "id": 3,
              "value": 0.02221227801252068
            },
            {
              "id": 4,
              "value": 0.0681087747162985
            },
            {
              "id": 5,
              "value": 0.2891481411149057
            },
            {
              "id": 6,
              "value": 0.8039481280322578
            },
            {
              "id": 7,
              "value": 0.7805891303997408
            },
            {
              "id": 8,
              "value": 0.24306265589411036
            },
            {
              "id": 9,
              "value": 0.14239635027513797
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:32.723892",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_35c7dabd",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from a specified CSV file and retrieve its content. Step 2: Use the data_processing_parser to parse the retrieved raw data into a structured format, extracting relevant fields. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema, ensuring that it meets the required standards. Step 4: Use the data_processing_filter to selectively filter the validated data based on specific criteria, such as removing entries that do not meet certain conditions. Step 5: Use the data_processing_aggregator to aggregate the filtered data, summarizing it according to specified metrics or grouping criteria.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.04862762621383632
            },
            {
              "id": 1,
              "value": 0.6560011526911333
            },
            {
              "id": 2,
              "value": 0.5539792125109123
            },
            {
              "id": 3,
              "value": 0.9716415526768885
            },
            {
              "id": 4,
              "value": 0.9458682863880639
            },
            {
              "id": 5,
              "value": 0.6954259191532252
            },
            {
              "id": 6,
              "value": 0.6655280137529086
            },
            {
              "id": 7,
              "value": 0.12120476788974699
            },
            {
              "id": 8,
              "value": 0.5000211860819244
            },
            {
              "id": 9,
              "value": 0.3907308336807088
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:35.478262",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_213d1027",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to filter the validated data based on specified criteria to reduce the dataset to only relevant entries. Step 5: Use the data_processing_aggregator to aggregate the filtered data for summarization and insights.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.38189021635399967
            },
            {
              "id": 1,
              "value": 0.19064731165832793
            },
            {
              "id": 2,
              "value": 0.3158892968080499
            },
            {
              "id": 3,
              "value": 0.929950377088539
            },
            {
              "id": 4,
              "value": 0.6295545863498005
            },
            {
              "id": 5,
              "value": 0.9875120940802039
            },
            {
              "id": 6,
              "value": 0.03724132181593964
            },
            {
              "id": 7,
              "value": 0.8988007959025127
            },
            {
              "id": 8,
              "value": 0.5161228757225857
            },
            {
              "id": 9,
              "value": 0.43387194437797993
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:37.898794",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_33eefd4d",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read raw data from a CSV file. Step 2: Use the data_processing_parser to parse the raw CSV data into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter to filter the validated data based on specified criteria. Step 5: Use the data_processing_aggregator to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.7827651178998931
            },
            {
              "id": 1,
              "value": 0.7347569215858271
            },
            {
              "id": 2,
              "value": 0.6864143271359724
            },
            {
              "id": 3,
              "value": 0.8553019367958615
            },
            {
              "id": 4,
              "value": 0.7116487093536907
            },
            {
              "id": 5,
              "value": 0.9283810126908048
            },
            {
              "id": 6,
              "value": 0.1699686849011378
            },
            {
              "id": 7,
              "value": 0.4273630110889063
            },
            {
              "id": 8,
              "value": 0.10688345644068287
            },
            {
              "id": 9,
              "value": 0.17525817086969353
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 3.075746257901398,
        "priority": "medium"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:42.067782",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_49e1ac52",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data files in CSV format. This will retrieve the data needed for processing. Step 2: Use the data_processing_parser to parse the raw data from the files into a structured format, making it easier to work with. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant information. Step 5: Use the data_processing_aggregator to aggregate the filtered data, providing a summary of the results.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.8876267726175364
            },
            {
              "id": 1,
              "value": 0.30376202753320725
            },
            {
              "id": 2,
              "value": 0.3049964895819429
            },
            {
              "id": 3,
              "value": 0.44683585723646424
            },
            {
              "id": 4,
              "value": 0.40126038442247236
            },
            {
              "id": 5,
              "value": 0.4822113766090179
            },
            {
              "id": 6,
              "value": 0.7069430244239985
            },
            {
              "id": 7,
              "value": 0.7487828795595445
            },
            {
              "id": 8,
              "value": 0.09073403393578572
            },
            {
              "id": 9,
              "value": 0.8004335501414712
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:45.253317",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_ff4dfcd0",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from the specified input file (e.g., CSV or JSON). Step 2: Use the data_processing_parser to parse the raw data into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_aggregator to aggregate the filtered data as needed for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.16749121143278944
            },
            {
              "id": 1,
              "value": 0.5305618340984821
            },
            {
              "id": 2,
              "value": 0.9972784669096282
            },
            {
              "id": 3,
              "value": 0.616261767029612
            },
            {
              "id": 4,
              "value": 0.1222336810234127
            },
            {
              "id": 5,
              "value": 0.6902048657683361
            },
            {
              "id": 6,
              "value": 0.20643302660459606
            },
            {
              "id": 7,
              "value": 0.7217915942388374
            },
            {
              "id": 8,
              "value": 0.06610319354443062
            },
            {
              "id": 9,
              "value": 0.35774218725122775
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:49.233439",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8fef2a95",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a CSV file. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.8455857480527604
            },
            {
              "id": 1,
              "value": 0.8546567045304907
            },
            {
              "id": 2,
              "value": 0.9223659186134432
            },
            {
              "id": 3,
              "value": 0.007067164656337943
            },
            {
              "id": 4,
              "value": 0.6715171664884091
            },
            {
              "id": 5,
              "value": 0.8889979636541405
            },
            {
              "id": 6,
              "value": 0.5239616609211376
            },
            {
              "id": 7,
              "value": 0.6819022527984933
            },
            {
              "id": 8,
              "value": 0.2246690076287795
            },
            {
              "id": 9,
              "value": 0.1641672293872023
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:19.926831",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e9a6424a",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data file in CSV format. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the structured data based on specified criteria, reducing the dataset to only the relevant entries. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data, summarizing key metrics or insights as needed.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.5399776772854445
            },
            {
              "id": 1,
              "value": 0.3773874545388085
            },
            {
              "id": 2,
              "value": 0.2344337442649822
            },
            {
              "id": 3,
              "value": 0.572398326842701
            },
            {
              "id": 4,
              "value": 0.14494761924315858
            },
            {
              "id": 5,
              "value": 0.7169715639348926
            },
            {
              "id": 6,
              "value": 0.12709410136212773
            },
            {
              "id": 7,
              "value": 0.7900236489499609
            },
            {
              "id": 8,
              "value": 0.10123321891468728
            },
            {
              "id": 9,
              "value": 0.5791474347758604
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 3.349000890227221
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:23.685708",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a41b6b5d",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read raw data from a specified file format (e.g., CSV or JSON). Step 2: Use the data_processing_parser tool to parse the raw data into a structured format, making it easier to work with. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria (for example, removing entries that do not meet certain conditions). Step 5: Use the data_processing_aggregator tool to aggregate the filtered data, summarizing it according to specified metrics or dimensions.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.08562918556065047
            },
            {
              "id": 1,
              "value": 0.6917031508110552
            },
            {
              "id": 2,
              "value": 0.7203618429194745
            },
            {
              "id": 3,
              "value": 0.6299581787172539
            },
            {
              "id": 4,
              "value": 0.6026159150216002
            },
            {
              "id": 5,
              "value": 0.6465730081084602
            },
            {
              "id": 6,
              "value": 0.66979038548934
            },
            {
              "id": 7,
              "value": 0.18557773716669423
            },
            {
              "id": 8,
              "value": 0.028394430287270955
            },
            {
              "id": 9,
              "value": 0.8601739806542446
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 6.148264587653616
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:26.920131",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8c8440bc",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read raw data files in CSV format from the specified directory. Step 2: Use the data_processing_parser tool to parse the raw CSV data into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against the predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria to reduce the dataset size. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.6983788713490175
            },
            {
              "id": 1,
              "value": 0.015745054384754598
            },
            {
              "id": 2,
              "value": 0.2612280221266371
            },
            {
              "id": 3,
              "value": 0.9158124161763536
            },
            {
              "id": 4,
              "value": 0.6687845004058272
            },
            {
              "id": 5,
              "value": 0.24653332674676154
            },
            {
              "id": 6,
              "value": 0.35364250357938076
            },
            {
              "id": 7,
              "value": 0.5921333946286896
            },
            {
              "id": 8,
              "value": 0.6315283133238992
            },
            {
              "id": 9,
              "value": 0.458842135156208
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:30.195762",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_23afe920",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser tool to parse the raw data retrieved from the CSV file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to filter the validated data based on specific criteria to retain only the relevant records. Step 5: Finally, use the data_processing_aggregator tool to aggregate the filtered data into a summarized format, suitable for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.8320434744342253
            },
            {
              "id": 1,
              "value": 0.6539132096617268
            },
            {
              "id": 2,
              "value": 0.45733567299714206
            },
            {
              "id": 3,
              "value": 0.7548537446512086
            },
            {
              "id": 4,
              "value": 0.15317716737648612
            },
            {
              "id": 5,
              "value": 0.08427273861642215
            },
            {
              "id": 6,
              "value": 0.024812603680299694
            },
            {
              "id": 7,
              "value": 0.4491120877130238
            },
            {
              "id": 8,
              "value": 0.703311362927839
            },
            {
              "id": 9,
              "value": 0.4560869977013764
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 7.427276773930962
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:33.134168",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_56961fa1",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from a specified file format (e.g., CSV or JSON). Step 2: Use the data_processing_parser to parse the raw data retrieved from the file and convert it into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to apply specific filtering criteria to the validated structured data, reducing the dataset to only the relevant entries. Step 5: Use the data_processing_aggregator to aggregate the filtered data, summarizing key metrics based on the requirements.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.6206757333570744
            },
            {
              "id": 1,
              "value": 0.0193204752206958
            },
            {
              "id": 2,
              "value": 0.2375521657361609
            },
            {
              "id": 3,
              "value": 0.5855984367518755
            },
            {
              "id": 4,
              "value": 0.8241423565019763
            },
            {
              "id": 5,
              "value": 0.5249571605695221
            },
            {
              "id": 6,
              "value": 0.4244009787070018
            },
            {
              "id": 7,
              "value": 0.013014730777580752
            },
            {
              "id": 8,
              "value": 0.9513330402296069
            },
            {
              "id": 9,
              "value": 0.0796026167482694
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 7.510132699520052,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:36.487656",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a0663635",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read the raw data from a CSV file. This will ensure that we retrieve the necessary data for processing. Step 2: Use the 'data_processing_parser' tool to parse the raw data into a structured format, making it easier to work with. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant information. Step 5: Finally, use the 'data_processing_aggregator' tool to aggregate the filtered data for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.978274818926147
            },
            {
              "id": 1,
              "value": 0.08989091275098038
            },
            {
              "id": 2,
              "value": 0.6007483517808133
            },
            {
              "id": 3,
              "value": 0.5109469450844426
            },
            {
              "id": 4,
              "value": 0.4587844549220811
            },
            {
              "id": 5,
              "value": 0.9864816518612
            },
            {
              "id": 6,
              "value": 0.5103307151773775
            },
            {
              "id": 7,
              "value": 0.8522956760260402
            },
            {
              "id": 8,
              "value": 0.011724817520188169
            },
            {
              "id": 9,
              "value": 0.9635186368049776
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:40.243722",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4baa4ac9",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read raw data from a specified input file in CSV format. Step 2: Use the data_processing_parser to parse the read raw data into a structured format (e.g., a list of records). Step 3: Use the data_processing_validator to validate the structured data against a predefined schema, ensuring it meets the integrity requirements. Step 4: Use the data_processing_filter to selectively filter the validated data based on specific criteria (e.g., filtering out records that do not meet certain conditions). Step 5: Use the data_processing_aggregator to aggregate the filtered data based on specific metrics or dimensions, preparing it for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.9262093418671278
            },
            {
              "id": 1,
              "value": 0.4505005227108272
            },
            {
              "id": 2,
              "value": 0.5841791081275101
            },
            {
              "id": 3,
              "value": 0.23432010191945007
            },
            {
              "id": 4,
              "value": 0.5109077872453357
            },
            {
              "id": 5,
              "value": 0.8349597464845087
            },
            {
              "id": 6,
              "value": 0.04524166997178092
            },
            {
              "id": 7,
              "value": 0.21294467993676902
            },
            {
              "id": 8,
              "value": 0.9487625590866702
            },
            {
              "id": 9,
              "value": 0.5058078344511405
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:44.139477",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e55758b5",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a CSV file. This step will retrieve the data into a usable format. Step 2: Use the data_processing_parser tool to parse the retrieved data into a structured format, extracting relevant fields from the raw data. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema, ensuring that the data meets the specified requirements. Step 4: Use the data_processing_filter tool to apply filtering criteria to the validated data, selecting only the records that meet certain conditions. Step 5: Finally, use the data_processing_aggregator tool to aggregate the filtered data, summarizing it for further analysis or storage.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.6349564340639695
            },
            {
              "id": 1,
              "value": 0.6230577235943362
            },
            {
              "id": 2,
              "value": 0.9178859535490314
            },
            {
              "id": 3,
              "value": 0.4035202281068627
            },
            {
              "id": 4,
              "value": 0.7950606590773737
            },
            {
              "id": 5,
              "value": 0.7089487719983024
            },
            {
              "id": 6,
              "value": 0.236638743278883
            },
            {
              "id": 7,
              "value": 0.1112129267679206
            },
            {
              "id": 8,
              "value": 0.9078424809387906
            },
            {
              "id": 9,
              "value": 0.41341957529602147
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 1.9863766661600268,
        "priority": "medium"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:47.107997",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1e42daa9",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read raw data from a CSV file. Step 2: Use the data_processing_parser to parse the retrieved raw data into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_aggregator to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.10409809015027927
            },
            {
              "id": 1,
              "value": 0.03575113133557073
            },
            {
              "id": 2,
              "value": 0.635316702826474
            },
            {
              "id": 3,
              "value": 0.8569596632517145
            },
            {
              "id": 4,
              "value": 0.7534867452876357
            },
            {
              "id": 5,
              "value": 0.8146349327251603
            },
            {
              "id": 6,
              "value": 0.5998028524199313
            },
            {
              "id": 7,
              "value": 0.8030783328644128
            },
            {
              "id": 8,
              "value": 0.9873066108916208
            },
            {
              "id": 9,
              "value": 0.4516799230825962
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 8.006196087825955
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:49.244385",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_05674f48",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to convert the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to the specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:16.417570",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_2ce3edb0",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:18.578087",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_3a8cf1a1",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a different format as required (e.g., from JSON to XML). Step 5: Use network_poster to send the transformed data to the specified destination endpoint over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:21.782417",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a43b3101",
      "task_type": "api_integration",
      "description": "Step 1: Use the `network_fetcher` to retrieve JSON data from the specified API endpoint. Step 2: Use the `data_processing_parser` to parse the raw JSON data into a structured format for easier handling. Step 3: Use the `data_processing_validator` to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the `data_processing_transformer` to transform the validated data into another format, such as XML. Step 5: Use the `network_poster` to send the transformed data to a specified destination API endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:26.993721",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_6e33245e",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. This will allow you to get the raw data needed for further processing. Step 2: Use data_processing_parser to parse the retrieved raw data into a structured format, making it easier to work with. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format (e.g., converting from JSON to XML). Step 5: Use network_poster to send the transformed data to a specified destination API endpoint, completing the integration process.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 7.212921273779116
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:30.869322",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a66eb7e0",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher to retrieve raw data from the specified API endpoint. Step 2: Use the data_processing_parser to parse the raw data into a structured format for easier handling. Step 3: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_transformer to transform the validated data into the desired output format (e.g., from JSON to XML). Step 5: Use the network_poster to send the transformed data to the specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:33.576630",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a9ae763b",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher tool to retrieve data from a specified API endpoint. This data will be in JSON format. Step 2: Use the data_processing_parser tool to parse the retrieved JSON data into a structured format. This will make it easier to work with the data in subsequent steps. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: If the data is valid, use the data_processing_transformer tool to convert the structured data into XML format. Step 5: Finally, use the network_poster tool to send the transformed XML data to a specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:36.454527",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_427c6a05",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a defined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:38.715439",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a1a944c0",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data obtained from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to convert the validated data into the desired output format, such as from JSON to XML. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:45.527571",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_7fff6bad",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a different format if necessary. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:49.247661",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e5c4841e",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to selectively filter the valid data based on specified criteria. Step 5: Finally, use the data_processing_transformer tool to transform the filtered data into a different format (e.g., from JSON to XML).",
      "inputs": {
        "input_data": {
          "data": [
            0.4256619981890528,
            0.5751545423885016,
            0.6485256199810246,
            0.6513357084892616,
            0.4137913320986264
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:17.009174",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4361707f",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.911035574728862,
            0.8288192793375463,
            0.603668212626204,
            0.1448004781630502,
            0.46992014421890393
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 5.079543018631876
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:18.448073",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_30226464",
      "task_type": "basic_task",
      "description": "Step 1: Use the 'file_operations_reader' tool to read the data from a specified file (e.g., a CSV file). Step 2: Use the 'data_processing_parser' tool to parse the raw data read from the file into a structured format. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specified criteria. Step 5: Use the 'data_processing_transformer' tool to transform the filtered data into a different format if needed (e.g., from JSON to XML).",
      "inputs": {
        "input_data": {
          "data": [
            0.105857984776633,
            0.09280986415545567,
            0.9867984082698478,
            0.6208724442916943,
            0.9452162945735574
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:24.324052",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_7e5e2a28",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a specified file (e.g., a CSV file). Step 2: Use data_processing_parser to parse the raw data obtained from the file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria, if needed. Step 5: Use data_processing_transformer to transform the filtered data into a different format (e.g., convert JSON to XML) if required.",
      "inputs": {
        "input_data": {
          "data": [
            0.21187510842761226,
            0.5680813626432898,
            0.576305175755594,
            0.47383097828872145,
            0.5722759184270462
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:33.802039",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_2651f6f9",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from the input file. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.9054996097659906,
            0.9184168460898754,
            0.9269323312210878,
            0.692939967221646,
            0.5659932703276233
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:35.160586",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_3743bf6f",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data file containing raw data. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.682886908668869,
            0.9639345310767884,
            0.39155696869263945,
            0.9136756309524942,
            0.3281725171595765
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:37.229480",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4ea6f870",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader to read the data from a CSV file. Step 2: Use the data_processing_parser to parse the raw data from the CSV into a structured format. Step 3: Use the data_processing_filter to filter the structured data based on specified criteria, such as removing any entries that do not meet the requirements. Step 4: Use the data_processing_validator to validate the filtered data against a predefined schema to ensure its correctness. Step 5: Use the data_processing_transformer to convert the validated data into a different format, such as JSON.",
      "inputs": {
        "input_data": {
          "data": [
            0.1299684051184068,
            0.9458684303681738,
            0.3967452984514632,
            0.5518044601076245,
            0.17403827234383284
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_filter",
        "data_processing_validator",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 5.326446178497241
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:41.215920",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_196756fe",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data file in CSV format. This will retrieve the data needed for further processing. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format, making it easier to work with. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, narrowing down the results to only the relevant entries. Step 5: Finally, use the data_processing_transformer tool to transform the filtered data into a different format, such as JSON, based on your requirements.",
      "inputs": {
        "input_data": {
          "data": [
            0.8449124453826169,
            0.5230383497845502,
            0.33040867119527106,
            0.6682944786192135,
            0.7928585096668441
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "low"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:44.878432",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_fa6ef4de",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the valid data based on specified criteria. Step 5: Use data_processing_transformer to transform the filtered data into a different format, such as JSON.",
      "inputs": {
        "input_data": {
          "data": [
            0.5137471200664969,
            0.4718956641629225,
            0.6512420484432346,
            0.6060168014450392,
            0.09003699364503903
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:47.368352",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_7c181788",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a specified file (e.g., CSV, JSON, or XML). Step 2: Use data_processing_parser to parse the raw data retrieved from the file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a defined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria, if needed.",
      "inputs": {
        "input_data": {
          "data": [
            0.9061978670970832,
            0.9021560910300999,
            0.015042266253657899,
            0.30598183594569206,
            0.1265008291059766
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 7.972762575467252
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:49.709795",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_3318af30",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read the raw data file containing customer information in CSV format. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specific criteria such as active customers only. Step 5: Use data_processing_aggregator to aggregate the filtered data to generate summary reports, such as total active customers by region.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.8365427171552569
            },
            {
              "id": 1,
              "value": 0.9404464516108195
            },
            {
              "id": 2,
              "value": 0.9676595143128752
            },
            {
              "id": 3,
              "value": 0.6280722286788853
            },
            {
              "id": 4,
              "value": 0.7142548838496403
            },
            {
              "id": 5,
              "value": 0.8560965598784248
            },
            {
              "id": 6,
              "value": 0.6013515304575595
            },
            {
              "id": 7,
              "value": 0.8593697207160984
            },
            {
              "id": 8,
              "value": 0.8184481285606371
            },
            {
              "id": 9,
              "value": 0.5991051677762521
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 3.271920841715449
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:19.299115",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5a1175af",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from the input file in CSV format. Step 2: Use the data_processing_parser to parse the raw CSV data into a structured format, making it easier to work with. Step 3: Use the data_processing_validator to validate the parsed data against a predefined schema, ensuring its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria, reducing the dataset to only the necessary information. Step 5: Use the data_processing_aggregator to aggregate the filtered data, summarizing the results for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.4206745739264971
            },
            {
              "id": 1,
              "value": 0.18704535207758433
            },
            {
              "id": 2,
              "value": 0.11081203111676519
            },
            {
              "id": 3,
              "value": 0.5770007835275949
            },
            {
              "id": 4,
              "value": 0.9400935803292491
            },
            {
              "id": 5,
              "value": 0.5594912304870445
            },
            {
              "id": 6,
              "value": 0.12405949061886035
            },
            {
              "id": 7,
              "value": 0.9305452387650239
            },
            {
              "id": 8,
              "value": 0.7942780314958596
            },
            {
              "id": 9,
              "value": 0.20889571201268597
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 6.178448274805836,
        "priority": "high"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:24.042911",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_16005906",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read the raw data files from the specified directory. Step 2: Use data_processing_parser to parse the raw data into a structured format, extracting the relevant fields. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the valid data based on specified criteria, reducing the dataset size. Step 5: Use data_processing_aggregator to aggregate the filtered data, summarizing it for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.8402974751121215
            },
            {
              "id": 1,
              "value": 0.6785875737613293
            },
            {
              "id": 2,
              "value": 0.21234635996406914
            },
            {
              "id": 3,
              "value": 0.9115050398103661
            },
            {
              "id": 4,
              "value": 0.01747425472910502
            },
            {
              "id": 5,
              "value": 0.18395891066736314
            },
            {
              "id": 6,
              "value": 0.9332253621020073
            },
            {
              "id": 7,
              "value": 0.766423178343157
            },
            {
              "id": 8,
              "value": 0.5949637719173606
            },
            {
              "id": 9,
              "value": 0.9858372844450541
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:27.597350",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f57be5d0",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file containing raw data. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_aggregator to aggregate the filtered data into a summary report.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.8689582279738589
            },
            {
              "id": 1,
              "value": 0.614958570981269
            },
            {
              "id": 2,
              "value": 0.5478760984660214
            },
            {
              "id": 3,
              "value": 0.8069265015556996
            },
            {
              "id": 4,
              "value": 0.08852156915200127
            },
            {
              "id": 5,
              "value": 0.3196420590479965
            },
            {
              "id": 6,
              "value": 0.6312469806629762
            },
            {
              "id": 7,
              "value": 0.5815918997219169
            },
            {
              "id": 8,
              "value": 0.793244385183822
            },
            {
              "id": 9,
              "value": 0.8409538033910993
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:29.960981",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_516b13a0",
      "task_type": "data_pipeline",
      "description": "Step 1: Use [file_operations_reader] to read the raw data from a specified file format (e.g., CSV or JSON). Step 2: Use [data_processing_parser] to parse the raw data into a structured format for easier manipulation. Step 3: Use [data_processing_validator] to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use [data_processing_filter] to filter the validated data based on specific criteria, reducing the dataset to only relevant entries. Step 5: Use [data_processing_aggregator] to aggregate the filtered data for summarization or further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.9137112082976556
            },
            {
              "id": 1,
              "value": 0.13344298538409005
            },
            {
              "id": 2,
              "value": 0.6133085205053206
            },
            {
              "id": 3,
              "value": 0.972008343258706
            },
            {
              "id": 4,
              "value": 0.4131255032746848
            },
            {
              "id": 5,
              "value": 0.5179973906183258
            },
            {
              "id": 6,
              "value": 0.6271952619958111
            },
            {
              "id": 7,
              "value": 0.2450333421949591
            },
            {
              "id": 8,
              "value": 0.9175597821438308
            },
            {
              "id": 9,
              "value": 0.6903369099683236
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 1.854451759626087
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:32.919094",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_70a920ca",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data file containing customer information in CSV format. Step 2: Use the data_processing_parser to parse the retrieved raw data into a structured format that can be easily processed. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria, such as customers from a particular region. Step 5: Use the data_processing_aggregator to aggregate the filtered data to obtain summary statistics, such as the total number of customers per region.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.2278394581768699
            },
            {
              "id": 1,
              "value": 0.29090779726147753
            },
            {
              "id": 2,
              "value": 0.9589261846878226
            },
            {
              "id": 3,
              "value": 0.9854625937130481
            },
            {
              "id": 4,
              "value": 0.889606950143523
            },
            {
              "id": 5,
              "value": 0.2864448103997307
            },
            {
              "id": 6,
              "value": 0.243810870061683
            },
            {
              "id": 7,
              "value": 0.11323471554660114
            },
            {
              "id": 8,
              "value": 0.9546470809070826
            },
            {
              "id": 9,
              "value": 0.5817041850528558
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:36.065015",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8033ecdc",
      "task_type": "data_pipeline",
      "description": "Step 1: Use [file_operations_reader] to read raw data from a CSV file. This will retrieve the data needed for processing. Step 2: Use [data_processing_parser] to parse the raw data into a structured format. This will ensure the data is organized for further operations. Step 3: Use [data_processing_validator] to validate the structured data against a predefined schema. This step ensures that the data meets the required standards and is free of errors. Step 4: Use [data_processing_filter] to selectively filter the validated data based on specific criteria. This will help in narrowing down the dataset to only the relevant information. Step 5: Use [data_processing_aggregator] to aggregate the filtered data, summarizing key insights that can be used for analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.04404093944957532
            },
            {
              "id": 1,
              "value": 0.4575600327289745
            },
            {
              "id": 2,
              "value": 0.9048380132856793
            },
            {
              "id": 3,
              "value": 0.8273373929558673
            },
            {
              "id": 4,
              "value": 0.42556535776917914
            },
            {
              "id": 5,
              "value": 0.6018049360492798
            },
            {
              "id": 6,
              "value": 0.4061808780333688
            },
            {
              "id": 7,
              "value": 0.6086838288719362
            },
            {
              "id": 8,
              "value": 0.8176046835990833
            },
            {
              "id": 9,
              "value": 0.6892211244201807
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 7.051790115335379
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:39.374999",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_75bd4902",
      "task_type": "data_pipeline",
      "description": "Step 1: Use [file_operations_reader] to read the raw data from a CSV file. Step 2: Use [data_processing_parser] to parse the raw data into a structured format. Step 3: Use [data_processing_validator] to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use [data_processing_filter] to filter the validated data based on specified criteria, reducing the dataset to only what is necessary. Step 5: Use [data_processing_aggregator] to aggregate the filtered data for summary insights.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.8657771675564759
            },
            {
              "id": 1,
              "value": 0.7969517880430995
            },
            {
              "id": 2,
              "value": 0.24432500008179503
            },
            {
              "id": 3,
              "value": 0.06498425720774836
            },
            {
              "id": 4,
              "value": 0.06588861163921012
            },
            {
              "id": 5,
              "value": 0.7973452881094937
            },
            {
              "id": 6,
              "value": 0.7950098620459312
            },
            {
              "id": 7,
              "value": 0.4634803735251416
            },
            {
              "id": 8,
              "value": 0.8882463512847788
            },
            {
              "id": 9,
              "value": 0.5730873929098158
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 1.7758974222683708
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:42.424723",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f254bf68",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' to read the raw data from a specified file format (e.g., CSV or JSON). Step 2: Use the 'data_processing_parser' to parse the raw data into a structured format, extracting relevant fields for further processing. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' to selectively filter the validated data based on specified criteria to reduce the dataset size. Step 5: Finally, use the 'data_processing_aggregator' to aggregate the filtered data, summarizing key metrics or insights as required.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.07272534075928572
            },
            {
              "id": 1,
              "value": 0.9637457033641366
            },
            {
              "id": 2,
              "value": 0.07566690013586586
            },
            {
              "id": 3,
              "value": 0.8679178567981299
            },
            {
              "id": 4,
              "value": 0.10835677979195812
            },
            {
              "id": 5,
              "value": 0.45418990671343584
            },
            {
              "id": 6,
              "value": 0.6131156850787904
            },
            {
              "id": 7,
              "value": 0.2774545569632999
            },
            {
              "id": 8,
              "value": 0.7041599170321271
            },
            {
              "id": 9,
              "value": 0.19373795040005282
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:46.879960",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d4b39952",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file that contains raw data. Step 2: Use data_processing_parser to parse the raw data into a structured format, allowing for easy manipulation. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria to narrow down to relevant entries. Step 5: Use data_processing_aggregator to aggregate the filtered data for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.7957004541050473
            },
            {
              "id": 1,
              "value": 0.3072665325229319
            },
            {
              "id": 2,
              "value": 0.9284373670943251
            },
            {
              "id": 3,
              "value": 0.3237523678215183
            },
            {
              "id": 4,
              "value": 0.41620594987520687
            },
            {
              "id": 5,
              "value": 0.9355556458803731
            },
            {
              "id": 6,
              "value": 0.23102064898990227
            },
            {
              "id": 7,
              "value": 0.7033506724230723
            },
            {
              "id": 8,
              "value": 0.535466653160576
            },
            {
              "id": 9,
              "value": 0.4732405119603612
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:49.844963",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4cdcf9a9",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' to read raw data from a CSV file. This will retrieve the data needed for further processing. Step 2: Use the 'data_processing_parser' to parse the retrieved data into a structured format. This will ensure the data is organized correctly. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema, ensuring its correctness and integrity. Step 4: Use the 'data_processing_filter' to selectively filter the validated data based on specified criteria to reduce the dataset size. Step 5: Finally, use the 'data_processing_aggregator' to aggregate the filtered data for analysis, summarizing the key metrics needed for reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.9842363837640504
            },
            {
              "id": 1,
              "value": 0.2545348099282069
            },
            {
              "id": 2,
              "value": 0.17905656197923125
            },
            {
              "id": 3,
              "value": 0.9968177405316336
            },
            {
              "id": 4,
              "value": 0.9757202116738072
            },
            {
              "id": 5,
              "value": 0.0626340543895596
            },
            {
              "id": 6,
              "value": 0.7889951619197038
            },
            {
              "id": 7,
              "value": 0.33334882954929623
            },
            {
              "id": 8,
              "value": 0.6319261678378324
            },
            {
              "id": 9,
              "value": 0.4037120772614817
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:18.601747",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_bc0a0122",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a specified input file (e.g., CSV, JSON, or XML). Step 2: Use the data_processing_parser tool to parse the raw data obtained in Step 1 into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.938078486688581
            },
            {
              "id": 1,
              "value": 0.4339903588186056
            },
            {
              "id": 2,
              "value": 0.6637242658661571
            },
            {
              "id": 3,
              "value": 0.3750817641282731
            },
            {
              "id": 4,
              "value": 0.8295488864425484
            },
            {
              "id": 5,
              "value": 0.6814117014893746
            },
            {
              "id": 6,
              "value": 0.7672666435883057
            },
            {
              "id": 7,
              "value": 0.8525269182782338
            },
            {
              "id": 8,
              "value": 0.16104324890459987
            },
            {
              "id": 9,
              "value": 0.564681742828294
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:21.595073",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_74add623",
      "task_type": "data_pipeline",
      "description": "Step 1: Use [file_operations_reader] to read the raw data file, which can be in CSV, JSON, or XML format. This step will retrieve the data from the specified file. Step 2: Use [data_processing_parser] to parse the raw data into a structured format, making it easier to work with. Step 3: Use [data_processing_validator] to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use [data_processing_filter] to selectively filter the validated data based on specific criteria, reducing the dataset to only the relevant information. Step 5: Use [data_processing_aggregator] to aggregate the filtered data, summarizing it for further analysis. Step 6: Use [computation_analyzer] to analyze the aggregated data and generate statistical insights and trend analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.2685754103388517
            },
            {
              "id": 1,
              "value": 0.02755142969332225
            },
            {
              "id": 2,
              "value": 0.5209143030546663
            },
            {
              "id": 3,
              "value": 0.06409394553321335
            },
            {
              "id": 4,
              "value": 0.4946893220037317
            },
            {
              "id": 5,
              "value": 0.27675030894031516
            },
            {
              "id": 6,
              "value": 0.6072937171816322
            },
            {
              "id": 7,
              "value": 0.5017339422653464
            },
            {
              "id": 8,
              "value": 0.1204406756200741
            },
            {
              "id": 9,
              "value": 0.32859249570961824
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "computation_analyzer"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:26.702084",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8bebe58c",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read and retrieve the raw data from a specified CSV file. Step 2: Use the data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_aggregator to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.3687147074600474
            },
            {
              "id": 1,
              "value": 0.018927080137309815
            },
            {
              "id": 2,
              "value": 0.016938316533900877
            },
            {
              "id": 3,
              "value": 0.2310630497040963
            },
            {
              "id": 4,
              "value": 0.6395131857035449
            },
            {
              "id": 5,
              "value": 0.9806716410273442
            },
            {
              "id": 6,
              "value": 0.5591599587053869
            },
            {
              "id": 7,
              "value": 0.6730847328157822
            },
            {
              "id": 8,
              "value": 0.8117195687008066
            },
            {
              "id": 9,
              "value": 0.25802410208815174
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 8.594706497226525
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:29.595784",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_77e553b9",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from a specified file (e.g., CSV or JSON). Step 2: Use the data_processing_parser to parse the raw data into a structured format for easier processing. Step 3: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Finally, use the data_processing_aggregator to aggregate the filtered data for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.3154658576962215
            },
            {
              "id": 1,
              "value": 0.4601139312865711
            },
            {
              "id": 2,
              "value": 0.6321251034449632
            },
            {
              "id": 3,
              "value": 0.3866755576714098
            },
            {
              "id": 4,
              "value": 0.5483681181536414
            },
            {
              "id": 5,
              "value": 0.5355148767918244
            },
            {
              "id": 6,
              "value": 0.55815933145887
            },
            {
              "id": 7,
              "value": 0.2689568383358183
            },
            {
              "id": 8,
              "value": 0.3016364119185543
            },
            {
              "id": 9,
              "value": 0.589806434849423
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:32.034081",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4bd20f3c",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' to read the raw data from a specified file in CSV format. Step 2: Use the 'data_processing_parser' to parse the raw data into a structured format. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' to selectively filter the validated data based on specific criteria, reducing the dataset to only relevant entries. Step 5: Use the 'data_processing_aggregator' to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.2579170312767657
            },
            {
              "id": 1,
              "value": 0.9984340481343905
            },
            {
              "id": 2,
              "value": 0.6063818297443128
            },
            {
              "id": 3,
              "value": 0.43920763367077464
            },
            {
              "id": 4,
              "value": 0.9308187017631653
            },
            {
              "id": 5,
              "value": 0.37613003857230476
            },
            {
              "id": 6,
              "value": 0.03028998307042685
            },
            {
              "id": 7,
              "value": 0.11820408806205251
            },
            {
              "id": 8,
              "value": 0.12174489637649111
            },
            {
              "id": 9,
              "value": 0.22926521919788234
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:36.095189",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_89e5c13d",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read raw data from a CSV file containing user activity logs. This will retrieve the data necessary for processing. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format, ensuring that the data is organized for further analysis. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specific criteria, such as user activity within the last month. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data, summarizing key metrics such as total user activities and engagement levels. Step 6: Finally, use the computation_analyzer tool to analyze the aggregated results, generating statistical insights and providing trend analysis over the user activities.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.06360685072875016
            },
            {
              "id": 1,
              "value": 0.0771467256726025
            },
            {
              "id": 2,
              "value": 0.5339556284491644
            },
            {
              "id": 3,
              "value": 0.5401731011674568
            },
            {
              "id": 4,
              "value": 0.17726136526161385
            },
            {
              "id": 5,
              "value": 0.4687926615529854
            },
            {
              "id": 6,
              "value": 0.9093506664219045
            },
            {
              "id": 7,
              "value": 0.6568660502905093
            },
            {
              "id": 8,
              "value": 0.8482428730790925
            },
            {
              "id": 9,
              "value": 0.22406400986688124
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "computation_analyzer"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:40.322934",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_70602ffa",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a specified file in CSV format. Step 2: Use the data_processing_parser tool to parse the raw CSV data into a structured format, extracting relevant fields. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema, ensuring its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant entries. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data, summarizing key metrics as needed.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.22269538089093677
            },
            {
              "id": 1,
              "value": 0.7788766433065862
            },
            {
              "id": 2,
              "value": 0.6380111042928198
            },
            {
              "id": 3,
              "value": 0.2921318997592266
            },
            {
              "id": 4,
              "value": 0.013571992016059187
            },
            {
              "id": 5,
              "value": 0.8930180515173856
            },
            {
              "id": 6,
              "value": 0.9953999539966
            },
            {
              "id": 7,
              "value": 0.5815814965013032
            },
            {
              "id": 8,
              "value": 0.014155225617485723
            },
            {
              "id": 9,
              "value": 0.0017248160595956241
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:43.675722",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_b1ae8e6f",
      "task_type": "data_pipeline",
      "description": "Step 1: Use 'file_operations_reader' to read raw data from a CSV file into memory. Step 2: Use 'data_processing_parser' to parse the raw data retrieved from the file into a structured format. Step 3: Use 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use 'data_processing_filter' to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant entries. Step 5: Use 'data_processing_aggregator' to aggregate the filtered data, summarizing the key insights.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.631645851655757
            },
            {
              "id": 1,
              "value": 0.9751273304143814
            },
            {
              "id": 2,
              "value": 0.25209005784206573
            },
            {
              "id": 3,
              "value": 0.9446712149495312
            },
            {
              "id": 4,
              "value": 0.31575720635127424
            },
            {
              "id": 5,
              "value": 0.16498644907548343
            },
            {
              "id": 6,
              "value": 0.15800470254187637
            },
            {
              "id": 7,
              "value": 0.3810746479692717
            },
            {
              "id": 8,
              "value": 0.21590702258830252
            },
            {
              "id": 9,
              "value": 0.7584128156692075
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:47.047945",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_78f779ec",
      "task_type": "data_pipeline",
      "description": "Step 1: Use [file_operations_reader] to read the raw data from the input file, which can be in formats like CSV, JSON, or XML. Step 2: Use [data_processing_parser] to parse the raw data into a structured format, making it easier to work with. Step 3: Use [data_processing_validator] to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use [data_processing_filter] to apply filtering criteria on the validated data to reduce the dataset based on specific conditions. Step 5: Use [data_processing_aggregator] to aggregate the filtered data for summarization or reporting. Step 6: Use [data_processing_transformer] to transform the aggregated data into the desired output format, such as converting it from JSON to XML.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.7866958884935011
            },
            {
              "id": 1,
              "value": 0.1109163556315994
            },
            {
              "id": 2,
              "value": 0.01526105654441623
            },
            {
              "id": 3,
              "value": 0.8882651881197233
            },
            {
              "id": 4,
              "value": 0.269095563534426
            },
            {
              "id": 5,
              "value": 0.7589376887611209
            },
            {
              "id": 6,
              "value": 0.9771137157543458
            },
            {
              "id": 7,
              "value": 0.09881276958937435
            },
            {
              "id": 8,
              "value": 0.22426320611210337
            },
            {
              "id": 9,
              "value": 0.46345251066550974
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:50.032580",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_853880f0",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read raw data from a specified file (e.g., CSV, JSON, XML). Step 2: Use the data_processing_parser tool to parse the raw data into a structured format for easier access and manipulation. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria to reduce the dataset size. Step 5: Finally, use the data_processing_aggregator tool to aggregate the filtered data for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.2989459970410534
            },
            {
              "id": 1,
              "value": 0.41031072985445527
            },
            {
              "id": 2,
              "value": 0.98407046707597
            },
            {
              "id": 3,
              "value": 0.47083087753362907
            },
            {
              "id": 4,
              "value": 0.7579837607036191
            },
            {
              "id": 5,
              "value": 0.023672297976930712
            },
            {
              "id": 6,
              "value": 0.611030851329561
            },
            {
              "id": 7,
              "value": 0.5291036295572161
            },
            {
              "id": 8,
              "value": 0.6137890560649879
            },
            {
              "id": 9,
              "value": 0.4462717108990003
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:18.004633",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f154cbc3",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read and retrieve the raw data from a CSV file. Step 2: Use the 'data_processing_parser' tool to parse the raw data into a structured format, extracting necessary fields. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specific criteria to reduce the dataset to relevant information. Step 5: Finally, use the 'data_processing_aggregator' tool to aggregate the filtered data for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.4003569553814966
            },
            {
              "id": 1,
              "value": 0.6046969622904343
            },
            {
              "id": 2,
              "value": 0.7116371240018861
            },
            {
              "id": 3,
              "value": 0.5820470725849477
            },
            {
              "id": 4,
              "value": 0.8371389105776578
            },
            {
              "id": 5,
              "value": 0.2271604372893572
            },
            {
              "id": 6,
              "value": 0.5386980876064003
            },
            {
              "id": 7,
              "value": 0.669514727792166
            },
            {
              "id": 8,
              "value": 0.9335773580087143
            },
            {
              "id": 9,
              "value": 0.035832328092686216
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 2.652664725200934
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:21.718841",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e76931d4",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read raw data from a specified file in CSV format. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format, extracting relevant fields. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specific criteria, reducing the dataset to only include relevant records. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data, summarizing it according to specified metrics.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.6710390567195478
            },
            {
              "id": 1,
              "value": 0.6511623428651347
            },
            {
              "id": 2,
              "value": 0.8393127199284925
            },
            {
              "id": 3,
              "value": 0.5634637029213407
            },
            {
              "id": 4,
              "value": 0.6532704300823026
            },
            {
              "id": 5,
              "value": 0.7747749498445585
            },
            {
              "id": 6,
              "value": 0.06921325746489482
            },
            {
              "id": 7,
              "value": 0.8322839779065464
            },
            {
              "id": 8,
              "value": 0.06097721909861231
            },
            {
              "id": 9,
              "value": 0.7508337100485236
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:25.265495",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_99e6ea85",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data file in CSV format. Step 2: Use the data_processing_parser to parse the raw CSV data into a structured format (e.g., converting it into a JSON object). Step 3: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria (e.g., removing entries that don't meet certain thresholds). Step 5: Use the data_processing_aggregator to aggregate the filtered data for further analysis (e.g., summing values or counting occurrences). Step 6: Use the computation_analyzer to analyze the aggregated data and generate statistical insights.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.5684347410452378
            },
            {
              "id": 1,
              "value": 0.9957783248489185
            },
            {
              "id": 2,
              "value": 0.9368713204039785
            },
            {
              "id": 3,
              "value": 0.16364710416213002
            },
            {
              "id": 4,
              "value": 0.4096248835161649
            },
            {
              "id": 5,
              "value": 0.7536992303738776
            },
            {
              "id": 6,
              "value": 0.4714961125166275
            },
            {
              "id": 7,
              "value": 0.9250032432435866
            },
            {
              "id": 8,
              "value": 0.4807873225773913
            },
            {
              "id": 9,
              "value": 0.730645145111314
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "computation_analyzer"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:30.058926",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_6d943a20",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data file containing customer transactions in CSV format. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format for easier manipulation. Step 3: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure correctness and integrity. Step 4: Use the data_processing_filter tool to filter the valid data based on specific criteria, such as transactions above a certain amount. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data to summarize total transactions per customer. Step 6: Use the computation_analyzer tool to analyze the aggregated results and generate statistical insights.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.43766131501510963
            },
            {
              "id": 1,
              "value": 0.1464347449500002
            },
            {
              "id": 2,
              "value": 0.9328015306690488
            },
            {
              "id": 3,
              "value": 0.4369061098069684
            },
            {
              "id": 4,
              "value": 0.1615039484752343
            },
            {
              "id": 5,
              "value": 0.4445552483156988
            },
            {
              "id": 6,
              "value": 0.048906831087503155
            },
            {
              "id": 7,
              "value": 0.8986580851436728
            },
            {
              "id": 8,
              "value": 0.8538348010528796
            },
            {
              "id": 9,
              "value": 0.36751538331020206
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "computation_analyzer"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 6.498086889557559
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:33.275311",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_b586e4b4",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specific criteria. Step 5: Use the data_processing_aggregator to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.7714148453083141
            },
            {
              "id": 1,
              "value": 0.14005708220830604
            },
            {
              "id": 2,
              "value": 0.8958966377963832
            },
            {
              "id": 3,
              "value": 0.6019431717379146
            },
            {
              "id": 4,
              "value": 0.3317805711552766
            },
            {
              "id": 5,
              "value": 0.7690162658497999
            },
            {
              "id": 6,
              "value": 0.6818855084610943
            },
            {
              "id": 7,
              "value": 0.8007677381145811
            },
            {
              "id": 8,
              "value": 0.4439077224129676
            },
            {
              "id": 9,
              "value": 0.3068487470001724
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 6.3911375373734876,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:35.856506",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_04c9e72d",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data files in CSV format and retrieve the data. Step 2: Use the data_processing_parser to parse the raw data extracted from the previous step into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_aggregator to aggregate the filtered data for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.5227614158411874
            },
            {
              "id": 1,
              "value": 0.9385404758556898
            },
            {
              "id": 2,
              "value": 0.06290332374037899
            },
            {
              "id": 3,
              "value": 0.2495894280168215
            },
            {
              "id": 4,
              "value": 0.0673266595044334
            },
            {
              "id": 5,
              "value": 0.36847879033176467
            },
            {
              "id": 6,
              "value": 0.9006372643765677
            },
            {
              "id": 7,
              "value": 0.8403494979169426
            },
            {
              "id": 8,
              "value": 0.6924629690784034
            },
            {
              "id": 9,
              "value": 0.30937274874101117
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:41.025814",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_2d562f6c",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' to read the raw data from a specified file in CSV format. This will retrieve the contents of the file for further processing. Step 2: Use the 'data_processing_parser' to parse the retrieved raw data into a structured format, enabling easy access to individual data elements. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant entries. Step 5: Finally, use the 'data_processing_aggregator' to aggregate the filtered data, summarizing the results as needed for reporting or further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.9208935665800225
            },
            {
              "id": 1,
              "value": 0.41119556722254724
            },
            {
              "id": 2,
              "value": 0.3291565115013291
            },
            {
              "id": 3,
              "value": 0.7953999636894838
            },
            {
              "id": 4,
              "value": 0.9434001061129451
            },
            {
              "id": 5,
              "value": 0.8986059259631581
            },
            {
              "id": 6,
              "value": 0.6632759233424873
            },
            {
              "id": 7,
              "value": 0.36426488931919543
            },
            {
              "id": 8,
              "value": 0.02162280339559597
            },
            {
              "id": 9,
              "value": 0.3503230901054768
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "medium"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:45.107309",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8de6caa6",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read the raw data from a CSV file. Step 2: Use data_processing_parser to parse the raw data into a structured format (e.g., converting it into a JSON object). Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria (e.g., filtering out records that do not meet certain conditions). Step 5: Use data_processing_aggregator to aggregate the filtered data for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.9613365379198866
            },
            {
              "id": 1,
              "value": 0.23013502151543008
            },
            {
              "id": 2,
              "value": 0.8343672472835162
            },
            {
              "id": 3,
              "value": 0.8521287922987226
            },
            {
              "id": 4,
              "value": 0.0073586751615238866
            },
            {
              "id": 5,
              "value": 0.042882616748683544
            },
            {
              "id": 6,
              "value": 0.1541793458652021
            },
            {
              "id": 7,
              "value": 0.45572327954047265
            },
            {
              "id": 8,
              "value": 0.23345333095497112
            },
            {
              "id": 9,
              "value": 0.6131357477722749
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:47.569938",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f4833834",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a specified input file (e.g., CSV, JSON). Step 2: Use the data_processing_parser tool to parse the raw data into a structured format for easier processing. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specific criteria, reducing the dataset to only relevant entries. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.669581994198741
            },
            {
              "id": 1,
              "value": 0.03204985537725846
            },
            {
              "id": 2,
              "value": 0.059440219402732475
            },
            {
              "id": 3,
              "value": 0.7688170152479946
            },
            {
              "id": 4,
              "value": 0.7682345652657481
            },
            {
              "id": 5,
              "value": 0.4615931546994555
            },
            {
              "id": 6,
              "value": 0.8142786768932535
            },
            {
              "id": 7,
              "value": 0.33967985993813055
            },
            {
              "id": 8,
              "value": 0.8608969362483933
            },
            {
              "id": 9,
              "value": 0.7190736793342944
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:50.035524",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_65f73535",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader to read data from a specified CSV file. Step 2: Use the data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_transformer to convert the filtered data into a different format, such as JSON.",
      "inputs": {
        "input_data": {
          "data": [
            0.2110554494208614,
            0.8055691915312997,
            0.1414158640420975,
            0.2759605040833095,
            0.21697834325411736
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:17.903912",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d7b22579",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a CSV file. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.9976980253094858,
            0.930567167128632,
            0.26950850219233313,
            0.8523152891696898,
            0.4875953660286968
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 8.01483593703854
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:20.579505",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_93ce38f3",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data file containing raw data in CSV format. Step 2: Use data_processing_parser to parse the raw data into a structured format, extracting relevant fields from the CSV. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria, such as date range or specific categories. Step 5: Use data_processing_transformer to convert the filtered data from the structured format into JSON format for further use.",
      "inputs": {
        "input_data": {
          "data": [
            0.7459838487207313,
            0.8902032890380496,
            0.1972047725926216,
            0.20167624860326439,
            0.577422163178313
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:24.256332",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_79c32c4a",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a specified file (e.g., a CSV file). Step 2: Use data_processing_parser to parse the raw data into a structured format, making it easier to work with. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria, reducing the amount of processed data. Step 5: Use data_processing_transformer to convert the filtered data into a different format (e.g., from JSON to XML) for further use or storage.",
      "inputs": {
        "input_data": {
          "data": [
            0.5962611829709232,
            0.020610854806768497,
            0.8182714971760228,
            0.3218387816897379,
            0.23206292421568486
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:32.336275",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1df24ae8",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file containing raw data. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to filter the validated data based on specific criteria, such as removing any entries that do not meet the criteria. Step 5: Use data_processing_transformer to transform the filtered data into a different format, such as converting it from JSON to XML.",
      "inputs": {
        "input_data": {
          "data": [
            0.1903668718714574,
            0.6465166141918244,
            0.1387592097908149,
            0.14328037158023577,
            0.057291429556333595
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 8.988390558983388
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:35.225431",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d794abac",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read the data from the specified file (CSV, JSON, or XML format). Step 2: Use the data_processing_parser tool to parse the raw data into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, if needed. Step 5: Use the data_processing_transformer tool to transform the filtered data into a different format (if required).",
      "inputs": {
        "input_data": {
          "data": [
            0.9801505883764353,
            0.9245977899298324,
            0.27255493449176005,
            0.4278113202631588,
            0.5624380927677012
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 4.508378146605729
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:39.559917",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_91d66021",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read data from a specified file (e.g., a CSV file). Step 2: Use the data_processing_parser tool to parse the raw data obtained from the file into a structured format (e.g., converting it into a JSON object). Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria (e.g., only include entries that meet certain conditions). Step 5: Use the data_processing_transformer tool to transform the filtered data into a different format (e.g., converting the JSON data into XML format).",
      "inputs": {
        "input_data": {
          "data": [
            0.8979547920787464,
            0.22713060745678737,
            0.8985117073553441,
            0.832420085632679,
            0.1381444329816065
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:42.818750",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_31a2d5c5",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a specified file (e.g., a CSV file). Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the data based on specified criteria to obtain relevant records. Step 5: Use data_processing_transformer to transform the filtered data into a different format (e.g., from JSON to XML) if needed.",
      "inputs": {
        "input_data": {
          "data": [
            0.5192475522503813,
            0.6477508421868856,
            0.013189712511954665,
            0.2239848226873572,
            0.6621424989591829
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 4.649465892509767
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:45.257594",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_aa60f80f",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a specified file (e.g., CSV, JSON, or XML). Step 2: Use data_processing_parser to parse the raw data retrieved from the file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.5491363250328747,
            0.15762724979651632,
            0.4226426901606901,
            0.2947472117409897,
            0.2642401071233257
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 9.042520495967686,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:46.996657",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_58d4b66d",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.7879707129472974,
            0.7895527925693032,
            0.09304094857891154,
            0.2758270810964546,
            0.5416055507252349
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:50.724623",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1a4238c5",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a source file (e.g., a CSV or JSON file). Step 2: Use the data_processing_parser tool to parse the raw data into a structured format for further processing. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Finally, use the data_processing_aggregator tool to aggregate the filtered data for analysis or reporting purposes.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.07219976871045553
            },
            {
              "id": 1,
              "value": 0.47673895009722345
            },
            {
              "id": 2,
              "value": 0.6012212620079468
            },
            {
              "id": 3,
              "value": 0.25109228255246185
            },
            {
              "id": 4,
              "value": 0.026850913987901515
            },
            {
              "id": 5,
              "value": 0.01950398962851907
            },
            {
              "id": 6,
              "value": 0.10391579164077702
            },
            {
              "id": 7,
              "value": 0.8177734617479273
            },
            {
              "id": 8,
              "value": 0.2871093522961441
            },
            {
              "id": 9,
              "value": 0.5723356614918815
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:18.600090",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e12a2f08",
      "task_type": "data_pipeline",
      "description": "Step 1: Use [file_operations_reader] to read raw data from a specified file (e.g., CSV or JSON). Step 2: Use [data_processing_parser] to parse the retrieved raw data into a structured format. Step 3: Use [data_processing_validator] to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use [data_processing_filter] to selectively filter the validated data based on specific criteria. Step 5: Use [data_processing_aggregator] to aggregate the filtered data, summarizing key insights for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.6138341030262624
            },
            {
              "id": 1,
              "value": 0.35324583247140273
            },
            {
              "id": 2,
              "value": 0.44261991343039175
            },
            {
              "id": 3,
              "value": 0.12984020237618132
            },
            {
              "id": 4,
              "value": 0.2959577892715154
            },
            {
              "id": 5,
              "value": 0.8029951013274017
            },
            {
              "id": 6,
              "value": 0.3886328525529652
            },
            {
              "id": 7,
              "value": 0.14150725425567967
            },
            {
              "id": 8,
              "value": 0.18642228872231947
            },
            {
              "id": 9,
              "value": 0.7522297508874504
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:21.347762",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_bb4b0c0a",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read the raw data from a specified file format (e.g., CSV or JSON). Step 2: Use data_processing_parser to parse the raw data into a structured format, extracting key data elements. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_aggregator to aggregate the filtered data for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.285414426362935
            },
            {
              "id": 1,
              "value": 0.6914184738717881
            },
            {
              "id": 2,
              "value": 0.04213993432990937
            },
            {
              "id": 3,
              "value": 0.12059524896822638
            },
            {
              "id": 4,
              "value": 0.012891984567195647
            },
            {
              "id": 5,
              "value": 0.6264305133070501
            },
            {
              "id": 6,
              "value": 0.830433985179251
            },
            {
              "id": 7,
              "value": 0.33540912502118625
            },
            {
              "id": 8,
              "value": 0.6276251218349402
            },
            {
              "id": 9,
              "value": 0.3658912267093768
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:23.942467",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_39a04498",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read the raw data from a specified input file in CSV format. Step 2: Use the 'data_processing_parser' tool to parse the raw data into a structured format, extracting relevant fields. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the 'data_processing_filter' tool to filter the validated data based on specific criteria, narrowing down the dataset to only the required entries. Step 5: Use the 'data_processing_aggregator' tool to aggregate the filtered data, providing summary statistics or combined results as needed.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.31488707447193987
            },
            {
              "id": 1,
              "value": 0.8663234836006043
            },
            {
              "id": 2,
              "value": 0.32814171161624495
            },
            {
              "id": 3,
              "value": 0.24298540871738894
            },
            {
              "id": 4,
              "value": 0.23206713268805723
            },
            {
              "id": 5,
              "value": 0.8677333061520965
            },
            {
              "id": 6,
              "value": 0.9640748099119667
            },
            {
              "id": 7,
              "value": 0.11785780477731411
            },
            {
              "id": 8,
              "value": 0.4238110406809741
            },
            {
              "id": 9,
              "value": 0.53957035521237
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:27.164045",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e7276e67",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from a specified file in CSV format. Step 2: Use the data_processing_parser to parse the raw data into a structured format, focusing on extracting and organizing the data. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specific criteria to reduce the dataset size. Step 5: Use the data_processing_aggregator to aggregate the filtered data, summarizing key metrics as needed.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.33308620447479764
            },
            {
              "id": 1,
              "value": 0.6496281765622748
            },
            {
              "id": 2,
              "value": 0.4266638439315331
            },
            {
              "id": 3,
              "value": 0.05026494634800416
            },
            {
              "id": 4,
              "value": 0.8834598246263979
            },
            {
              "id": 5,
              "value": 0.6541631291044583
            },
            {
              "id": 6,
              "value": 0.6061274551543799
            },
            {
              "id": 7,
              "value": 0.8435618661617241
            },
            {
              "id": 8,
              "value": 0.13325394554109926
            },
            {
              "id": 9,
              "value": 0.3395987923794238
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:31.204442",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_93fdb6be",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data files in CSV format. Step 2: Use the data_processing_parser to parse the raw data into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_aggregator to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.24743793340868414
            },
            {
              "id": 1,
              "value": 0.7882820376158075
            },
            {
              "id": 2,
              "value": 0.11248780071471665
            },
            {
              "id": 3,
              "value": 0.9993191585754303
            },
            {
              "id": 4,
              "value": 0.731295555856174
            },
            {
              "id": 5,
              "value": 0.6972200702037148
            },
            {
              "id": 6,
              "value": 0.7947766332794891
            },
            {
              "id": 7,
              "value": 0.16332523983803193
            },
            {
              "id": 8,
              "value": 0.03879366511073956
            },
            {
              "id": 9,
              "value": 0.43505146131507566
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 0.5053062678495263
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:36.201338",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_ecf2080e",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read the raw data from a specified file in CSV format. Step 2: Use data_processing_parser to parse the raw data into a structured format, extracting relevant fields. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria (e.g., only include records that meet certain conditions). Step 5: Use data_processing_aggregator to aggregate the filtered data, summarizing key metrics such as totals or averages.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.9997657060338611
            },
            {
              "id": 1,
              "value": 0.9486767570873763
            },
            {
              "id": 2,
              "value": 0.7961970061205487
            },
            {
              "id": 3,
              "value": 0.06884502982614416
            },
            {
              "id": 4,
              "value": 0.6792265413755522
            },
            {
              "id": 5,
              "value": 0.4785146783728065
            },
            {
              "id": 6,
              "value": 0.09508378329717226
            },
            {
              "id": 7,
              "value": 0.2409015750368827
            },
            {
              "id": 8,
              "value": 0.3638257829856445
            },
            {
              "id": 9,
              "value": 0.8146816867248264
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "medium"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:38.927708",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_b4155700",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read raw data from a specified CSV file. Step 2: Use the 'data_processing_parser' tool to parse the raw data into a structured format, extracting relevant fields. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the 'data_processing_filter' tool to filter the validated data based on specific criteria to reduce the dataset size. Step 5: Use the 'data_processing_aggregator' tool to aggregate the filtered data, producing summary statistics or combining data points as needed.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.020601157476351273
            },
            {
              "id": 1,
              "value": 0.45401600528332264
            },
            {
              "id": 2,
              "value": 0.23175059618317584
            },
            {
              "id": 3,
              "value": 0.40866936961144995
            },
            {
              "id": 4,
              "value": 0.4707760407603465
            },
            {
              "id": 5,
              "value": 0.510674162770612
            },
            {
              "id": 6,
              "value": 0.726164818495054
            },
            {
              "id": 7,
              "value": 0.08823265507853995
            },
            {
              "id": 8,
              "value": 0.6425475255046982
            },
            {
              "id": 9,
              "value": 0.9199693642828576
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 8.506855523317764
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:43.699401",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_73804901",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from a specified file. Step 2: Use the data_processing_parser to parse the raw data into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter to selectively filter the validated data based on specific criteria. Step 5: Use the data_processing_aggregator to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.6096873739785803
            },
            {
              "id": 1,
              "value": 0.20245678930937516
            },
            {
              "id": 2,
              "value": 0.6324919847743284
            },
            {
              "id": 3,
              "value": 0.7173837368461793
            },
            {
              "id": 4,
              "value": 0.33285293379333936
            },
            {
              "id": 5,
              "value": 0.6925725381793394
            },
            {
              "id": 6,
              "value": 0.1638026843015984
            },
            {
              "id": 7,
              "value": 0.4895837391577442
            },
            {
              "id": 8,
              "value": 0.5064244013039698
            },
            {
              "id": 9,
              "value": 0.04482871432616842
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:46.914404",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1fe81319",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a specified file (e.g., CSV, JSON). Step 2: Use the data_processing_parser tool to parse the raw data into a structured format, extracting the necessary fields. Step 3: Use the data_processing_validator tool to validate the parsed data against a predefined schema, ensuring data integrity and correctness. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria (e.g., removing any records that do not meet certain conditions). Step 5: Use the data_processing_aggregator tool to aggregate the filtered data, summarizing it as needed for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.12493021247570302
            },
            {
              "id": 1,
              "value": 0.7626994873778421
            },
            {
              "id": 2,
              "value": 0.09940052363952034
            },
            {
              "id": 3,
              "value": 0.8301673486597754
            },
            {
              "id": 4,
              "value": 0.8805556337447897
            },
            {
              "id": 5,
              "value": 0.2818286876119034
            },
            {
              "id": 6,
              "value": 0.518919121492846
            },
            {
              "id": 7,
              "value": 0.7988842688626112
            },
            {
              "id": 8,
              "value": 0.8300882017721867
            },
            {
              "id": 9,
              "value": 0.7153156861257282
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:50.940000",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_659cfa90",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read the raw data from a CSV file. Step 2: Use data_processing_parser to parse the raw data into a structured format, extracting relevant fields. Step 3: Use data_processing_validator to validate the structured data against a predefined schema, ensuring its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specific criteria, such as removing entries with null values. Step 5: Use data_processing_aggregator to aggregate the filtered data to produce summary statistics or combined results.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.5276096395331934
            },
            {
              "id": 1,
              "value": 0.7785734857123671
            },
            {
              "id": 2,
              "value": 0.32495848178114484
            },
            {
              "id": 3,
              "value": 0.5383544275918736
            },
            {
              "id": 4,
              "value": 0.8327782544638807
            },
            {
              "id": 5,
              "value": 0.8448879532625151
            },
            {
              "id": 6,
              "value": 0.9564372510508897
            },
            {
              "id": 7,
              "value": 0.32622539019450003
            },
            {
              "id": 8,
              "value": 0.3687590573599683
            },
            {
              "id": 9,
              "value": 0.6735899356609504
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:17.982104",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_2546ad03",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read raw data from a CSV file. This will retrieve the data necessary for processing. Step 2: Use the 'data_processing_parser' tool to parse the raw data from the CSV file into a structured format, making it easier to handle. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema, ensuring its correctness and integrity. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant information. Step 5: Use the 'data_processing_aggregator' tool to aggregate the filtered data, summarizing it for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.22385617986573314
            },
            {
              "id": 1,
              "value": 0.3990377142634771
            },
            {
              "id": 2,
              "value": 0.5043890933547736
            },
            {
              "id": 3,
              "value": 0.5864591883827849
            },
            {
              "id": 4,
              "value": 0.49475828405945466
            },
            {
              "id": 5,
              "value": 0.7232776658796718
            },
            {
              "id": 6,
              "value": 0.9071611609180826
            },
            {
              "id": 7,
              "value": 0.022935513276639896
            },
            {
              "id": 8,
              "value": 0.8266631645686142
            },
            {
              "id": 9,
              "value": 0.5904867487783603
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:21.608445",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_bd85850f",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data file (in CSV format) from the specified location. Step 2: Use the data_processing_parser tool to parse the raw CSV data into a structured format, extracting relevant fields. Step 3: Use the data_processing_validator tool to validate the structured data against the predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria (e.g., only include records from a certain date range). Step 5: Use the data_processing_aggregator tool to aggregate the filtered data based on specific metrics (e.g., total sales per region).",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.12450176159218529
            },
            {
              "id": 1,
              "value": 0.10746981914498166
            },
            {
              "id": 2,
              "value": 0.049820123349919476
            },
            {
              "id": 3,
              "value": 0.411270730036702
            },
            {
              "id": 4,
              "value": 0.9547598484069068
            },
            {
              "id": 5,
              "value": 0.5786235345616229
            },
            {
              "id": 6,
              "value": 0.42367451388532695
            },
            {
              "id": 7,
              "value": 0.4648786234300749
            },
            {
              "id": 8,
              "value": 0.20728177386410107
            },
            {
              "id": 9,
              "value": 0.15939620698375057
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 0.24042579329693195
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:25.616619",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5d7b3480",
      "task_type": "data_pipeline",
      "description": "Step 1: Use [file_operations_reader] to read the raw data from a specified file (e.g., CSV, JSON). Step 2: Use [data_processing_parser] to parse the raw data into a structured format, extracting relevant fields for analysis. Step 3: Use [data_processing_validator] to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use [data_processing_filter] to selectively filter the validated data based on specific criteria, reducing the dataset to only the necessary information. Step 5: Use [data_processing_aggregator] to aggregate the filtered data, summarizing it for reporting or further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.12193487687262039
            },
            {
              "id": 1,
              "value": 0.8525797203693993
            },
            {
              "id": 2,
              "value": 0.1175463360895811
            },
            {
              "id": 3,
              "value": 0.37030053033536914
            },
            {
              "id": 4,
              "value": 0.026055549954203228
            },
            {
              "id": 5,
              "value": 0.17062991327532084
            },
            {
              "id": 6,
              "value": 0.7219500308004262
            },
            {
              "id": 7,
              "value": 0.053877437053957755
            },
            {
              "id": 8,
              "value": 0.8049831388288791
            },
            {
              "id": 9,
              "value": 0.9078080851178686
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:30.143174",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d8206d13",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a CSV file. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.8301455812562145
            },
            {
              "id": 1,
              "value": 0.053842471439275896
            },
            {
              "id": 2,
              "value": 0.2598991581552994
            },
            {
              "id": 3,
              "value": 0.13888224273474614
            },
            {
              "id": 4,
              "value": 0.9446051365687993
            },
            {
              "id": 5,
              "value": 0.5791182853368448
            },
            {
              "id": 6,
              "value": 0.9013520416106431
            },
            {
              "id": 7,
              "value": 0.8324492084706854
            },
            {
              "id": 8,
              "value": 0.6654442085053024
            },
            {
              "id": 9,
              "value": 0.1213944755152141
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:32.756141",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9a1c2b57",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a specified file (e.g., CSV or JSON format). Step 2: Use the data_processing_parser tool to parse the raw data into a structured format, extracting relevant fields for further processing. Step 3: Use the data_processing_filter tool to selectively filter the parsed data based on specified criteria, reducing the dataset to only the necessary records. Step 4: Use the data_processing_validator tool to validate the filtered data against a predefined schema, ensuring the data's correctness and integrity. Step 5: Use the data_processing_aggregator tool to aggregate the validated data, summarizing it based on certain attributes for analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.42275966953046606
            },
            {
              "id": 1,
              "value": 0.20252566101772007
            },
            {
              "id": 2,
              "value": 0.40621631053171137
            },
            {
              "id": 3,
              "value": 0.4370645150043413
            },
            {
              "id": 4,
              "value": 0.7377108522503432
            },
            {
              "id": 5,
              "value": 0.21947592531044113
            },
            {
              "id": 6,
              "value": 0.8398648295745003
            },
            {
              "id": 7,
              "value": 0.32233023752831647
            },
            {
              "id": 8,
              "value": 0.5723057996309776
            },
            {
              "id": 9,
              "value": 0.36754415573104626
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_filter",
        "data_processing_validator",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "medium"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:36.118954",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_07f587c6",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' to read raw data from a file in CSV format. Step 2: Use the 'data_processing_parser' to parse the raw data into a structured format, extracting relevant fields. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the 'data_processing_filter' to filter the validated data based on specific criteria, reducing the dataset size. Step 5: Use the 'data_processing_aggregator' to aggregate the filtered data for further analysis. Step 6: Finally, use the 'computation_analyzer' to analyze the aggregated data and generate statistical insights.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.4767566518423274
            },
            {
              "id": 1,
              "value": 0.1483176331193209
            },
            {
              "id": 2,
              "value": 0.7277028284503633
            },
            {
              "id": 3,
              "value": 0.0022386887157870294
            },
            {
              "id": 4,
              "value": 0.0038024000045152606
            },
            {
              "id": 5,
              "value": 0.8540950904686191
            },
            {
              "id": 6,
              "value": 0.9629380931995022
            },
            {
              "id": 7,
              "value": 0.3269955407772842
            },
            {
              "id": 8,
              "value": 0.16824611342571305
            },
            {
              "id": 9,
              "value": 0.03495450092676722
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "computation_analyzer"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 4.3927524051879585
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:39.188406",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_fb810d76",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read raw data from a CSV file. This will retrieve the data into a manageable format. Step 2: Use the data_processing_parser tool to parse the retrieved data into a structured format, allowing for easier manipulation in later steps. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to filter the validated data based on specific criteria, reducing the dataset to only the relevant records. Step 5: Finally, use the data_processing_aggregator tool to aggregate the filtered data, summarizing key metrics and insights from the dataset.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.4895828098065752
            },
            {
              "id": 1,
              "value": 0.3288182633188824
            },
            {
              "id": 2,
              "value": 0.6209573404281069
            },
            {
              "id": 3,
              "value": 0.04976410194205205
            },
            {
              "id": 4,
              "value": 0.2008001121392058
            },
            {
              "id": 5,
              "value": 0.9089394749226397
            },
            {
              "id": 6,
              "value": 0.21038266281171958
            },
            {
              "id": 7,
              "value": 0.918377988358681
            },
            {
              "id": 8,
              "value": 0.2946328375407623
            },
            {
              "id": 9,
              "value": 0.2724536587412827
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:43.435840",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5209eedc",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the `file_operations_reader` to read the raw data from a CSV file. Step 2: Use the `data_processing_parser` to parse the raw data into a structured format. Step 3: Use the `data_processing_validator` to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the `data_processing_filter` to filter the validated data based on specified criteria to reduce the dataset size. Step 5: Use the `data_processing_aggregator` to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.9187510008190837
            },
            {
              "id": 1,
              "value": 0.8692077216416846
            },
            {
              "id": 2,
              "value": 0.1391451254297862
            },
            {
              "id": 3,
              "value": 0.0014886336751048779
            },
            {
              "id": 4,
              "value": 0.47002445406728544
            },
            {
              "id": 5,
              "value": 0.3110455612893632
            },
            {
              "id": 6,
              "value": 0.7338217111882342
            },
            {
              "id": 7,
              "value": 0.8701384396610228
            },
            {
              "id": 8,
              "value": 0.8507154071978361
            },
            {
              "id": 9,
              "value": 0.9573246000288993
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:46.625407",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0a8ce84e",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read the raw data from a source file (e.g., CSV or JSON format). Step 2: Use data_processing_parser to parse the retrieved raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_aggregator to aggregate the filtered data for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.6533085667145133
            },
            {
              "id": 1,
              "value": 0.44946504879567317
            },
            {
              "id": 2,
              "value": 0.18648221130784837
            },
            {
              "id": 3,
              "value": 0.9643203234504784
            },
            {
              "id": 4,
              "value": 0.07661328592065997
            },
            {
              "id": 5,
              "value": 0.745204480200129
            },
            {
              "id": 6,
              "value": 0.15880388103993703
            },
            {
              "id": 7,
              "value": 0.1444794759489486
            },
            {
              "id": 8,
              "value": 0.9128581475514513
            },
            {
              "id": 9,
              "value": 0.6364871127176668
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 0.11592881630801102,
        "priority": "high"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:51.435846",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_3f638130",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read raw data from a specified source file in CSV format. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format, extracting relevant fields. Step 3: Use the data_processing_filter tool to apply specific criteria to filter the structured data, retaining only the necessary records. Step 4: Use the data_processing_validator tool to validate the filtered data against a predefined schema to ensure its correctness. Step 5: Use the data_processing_aggregator tool to aggregate the valid data into a summary report.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.5246287201997534
            },
            {
              "id": 1,
              "value": 0.8999398901699809
            },
            {
              "id": 2,
              "value": 0.7634064909612166
            },
            {
              "id": 3,
              "value": 0.2419189782590545
            },
            {
              "id": 4,
              "value": 0.24222360019908473
            },
            {
              "id": 5,
              "value": 0.5963315726575115
            },
            {
              "id": 6,
              "value": 0.08130271335968298
            },
            {
              "id": 7,
              "value": 0.03625112511755535
            },
            {
              "id": 8,
              "value": 0.9728592980923239
            },
            {
              "id": 9,
              "value": 0.8181946591860736
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_filter",
        "data_processing_validator",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "medium"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:17.788548",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8fe24eee",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a specified input file (e.g., a CSV file). Step 2: Use the data_processing_parser tool to parse the raw data into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.8368623826169351
            },
            {
              "id": 1,
              "value": 0.18844748394243127
            },
            {
              "id": 2,
              "value": 0.2608149495879115
            },
            {
              "id": 3,
              "value": 0.021520718141375017
            },
            {
              "id": 4,
              "value": 0.3166592362999313
            },
            {
              "id": 5,
              "value": 0.7055031736341772
            },
            {
              "id": 6,
              "value": 0.6342942135160505
            },
            {
              "id": 7,
              "value": 0.20525678443452589
            },
            {
              "id": 8,
              "value": 0.038739913620766675
            },
            {
              "id": 9,
              "value": 0.3265334252034482
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 3.869188923874931
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:21.153580",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1d79a8b7",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a specified file (e.g., CSV, JSON, or XML format). Step 2: Use the data_processing_parser tool to parse the raw data into a structured format, extracting relevant fields for further processing. Step 3: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, reducing the dataset to only include relevant records. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data, summarizing it in a meaningful way for analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.598518729764982
            },
            {
              "id": 1,
              "value": 0.3019004173309111
            },
            {
              "id": 2,
              "value": 0.6356420674435114
            },
            {
              "id": 3,
              "value": 0.16181639680298876
            },
            {
              "id": 4,
              "value": 0.025557299302630954
            },
            {
              "id": 5,
              "value": 0.0011243506575941975
            },
            {
              "id": 6,
              "value": 0.4867323336146334
            },
            {
              "id": 7,
              "value": 0.46165321656580605
            },
            {
              "id": 8,
              "value": 0.7004356541908842
            },
            {
              "id": 9,
              "value": 0.5354982114928827
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 8.060225079426267
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:24.171494",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_406a5d4e",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a specified file (CSV, JSON, or XML format). Step 2: Use the data_processing_parser tool to parse the raw data into a structured format for easier manipulation. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria to reduce the dataset to relevant information. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data, summarizing it for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.7700588019654028
            },
            {
              "id": 1,
              "value": 0.35769216271769366
            },
            {
              "id": 2,
              "value": 0.27454543375046414
            },
            {
              "id": 3,
              "value": 0.26640507158406435
            },
            {
              "id": 4,
              "value": 0.747363336945891
            },
            {
              "id": 5,
              "value": 0.49155817959396686
            },
            {
              "id": 6,
              "value": 0.6372492197003558
            },
            {
              "id": 7,
              "value": 0.17644536281458234
            },
            {
              "id": 8,
              "value": 0.29148527389807655
            },
            {
              "id": 9,
              "value": 0.3044168557432808
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:28.244313",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_2aa33c2e",
      "task_type": "data_pipeline",
      "description": "Step 1: Use 'file_operations_reader' to read the raw data from a specified source file (e.g., CSV, JSON). Step 2: Use 'data_processing_parser' to parse the raw data into a structured format. Step 3: Use 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use 'data_processing_filter' to selectively filter the validated data based on specified criteria. Step 5: Finally, use 'data_processing_aggregator' to aggregate the filtered data for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.6288643955637693
            },
            {
              "id": 1,
              "value": 0.5545259587985439
            },
            {
              "id": 2,
              "value": 0.740680549342546
            },
            {
              "id": 3,
              "value": 0.6734740139970541
            },
            {
              "id": 4,
              "value": 0.348776465103261
            },
            {
              "id": 5,
              "value": 0.31611448433409506
            },
            {
              "id": 6,
              "value": 0.9752547751322267
            },
            {
              "id": 7,
              "value": 0.4791594082580434
            },
            {
              "id": 8,
              "value": 0.3620054273860762
            },
            {
              "id": 9,
              "value": 0.3688098838599577
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "high"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:32.101656",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8472d822",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data files (e.g., CSV, JSON, or XML) from the specified input directory. Step 2: Use the data_processing_parser to parse the raw data into a structured format, extracting relevant fields and organizing the data for processing. Step 3: Use the data_processing_validator to validate the structured data against a defined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria, reducing the dataset to only relevant entries. Step 5: Use the data_processing_aggregator to aggregate the filtered data for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.0425557486320296
            },
            {
              "id": 1,
              "value": 0.15323810167190732
            },
            {
              "id": 2,
              "value": 0.21052997891245528
            },
            {
              "id": 3,
              "value": 0.9115601031748354
            },
            {
              "id": 4,
              "value": 0.3923243351996627
            },
            {
              "id": 5,
              "value": 0.6385418703749064
            },
            {
              "id": 6,
              "value": 0.7171843975964092
            },
            {
              "id": 7,
              "value": 0.4390819762284667
            },
            {
              "id": 8,
              "value": 0.11993362273182051
            },
            {
              "id": 9,
              "value": 0.1930524622938553
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:35.164484",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_84310f14",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data file in CSV format. Step 2: Use the data_processing_parser tool to parse the retrieved data into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema, ensuring its correctness. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specific criteria. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.8638135461314194
            },
            {
              "id": 1,
              "value": 0.11196838308928103
            },
            {
              "id": 2,
              "value": 0.9676354898701246
            },
            {
              "id": 3,
              "value": 0.23767906253719207
            },
            {
              "id": 4,
              "value": 0.762078562212152
            },
            {
              "id": 5,
              "value": 0.08455105422599296
            },
            {
              "id": 6,
              "value": 0.7674306125160992
            },
            {
              "id": 7,
              "value": 0.10059740307963039
            },
            {
              "id": 8,
              "value": 0.1823325648726053
            },
            {
              "id": 9,
              "value": 0.885685297255192
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 0.5319490539404401
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:37.993882",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_99072be0",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read raw data from a specified input file in CSV format. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format, ensuring the data is organized properly. Step 3: Use the data_processing_validator tool to validate the structured data against a defined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, reducing the dataset to only relevant entries. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data, summarizing key metrics or insights as required.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.14672354282568179
            },
            {
              "id": 1,
              "value": 0.4385065615474649
            },
            {
              "id": 2,
              "value": 0.631535393453752
            },
            {
              "id": 3,
              "value": 0.3593227347500262
            },
            {
              "id": 4,
              "value": 0.6743049934780747
            },
            {
              "id": 5,
              "value": 0.23730086856036048
            },
            {
              "id": 6,
              "value": 0.13478816384667636
            },
            {
              "id": 7,
              "value": 0.12915980595280718
            },
            {
              "id": 8,
              "value": 0.11419040731482222
            },
            {
              "id": 9,
              "value": 0.45079084720353657
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:42.328701",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_702ee95e",
      "task_type": "data_pipeline",
      "description": "Step 1: Use [file_operations_reader] to read the raw data from a specified file in CSV format. Step 2: Use [data_processing_parser] to parse the raw CSV data into a structured format, organizing it into rows and columns. Step 3: Use [data_processing_validator] to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use [data_processing_filter] to filter the validated data based on specified criteria, removing any unwanted records. Step 5: Use [data_processing_aggregator] to aggregate the filtered data to summarize key metrics or statistics.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.048322620609580924
            },
            {
              "id": 1,
              "value": 0.18412305589022349
            },
            {
              "id": 2,
              "value": 0.2570662028590265
            },
            {
              "id": 3,
              "value": 0.7959927323097356
            },
            {
              "id": 4,
              "value": 0.10369263163021891
            },
            {
              "id": 5,
              "value": 0.8073528018301018
            },
            {
              "id": 6,
              "value": 0.6220946654260724
            },
            {
              "id": 7,
              "value": 0.2450430648338734
            },
            {
              "id": 8,
              "value": 0.4151867748117801
            },
            {
              "id": 9,
              "value": 0.6521890087052418
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 8.823368310543598,
        "priority": "high"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:48.058935",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_cc234bb0",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a specified input file (e.g., CSV or JSON format). Step 2: Use the data_processing_parser tool to parse the raw data into a structured format, ensuring the data is organized correctly. Step 3: Use the data_processing_validator tool to validate the parsed data against a predefined schema, checking for compliance and correctness. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria to focus on relevant information. Step 5: Finally, use the data_processing_aggregator tool to aggregate the filtered data, summarizing the relevant insights for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.29028837650916706
            },
            {
              "id": 1,
              "value": 0.8190768440264634
            },
            {
              "id": 2,
              "value": 0.991315151471735
            },
            {
              "id": 3,
              "value": 0.1844591827403521
            },
            {
              "id": 4,
              "value": 0.5341846813788186
            },
            {
              "id": 5,
              "value": 0.9186909114083601
            },
            {
              "id": 6,
              "value": 0.042485681073423764
            },
            {
              "id": 7,
              "value": 0.828235827564791
            },
            {
              "id": 8,
              "value": 0.1876762154160836
            },
            {
              "id": 9,
              "value": 0.11729617909529289
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:51.573754",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a28237fd",
      "task_type": "api_integration",
      "description": "Step 1: Use the `network_fetcher` to retrieve raw data from a specified API endpoint. Step 2: Use the `data_processing_parser` to parse the raw data into a structured format, making it easier to work with. Step 3: Use the `data_processing_validator` to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the `data_processing_transformer` to transform the validated data into a different format if necessary. Step 5: Finally, use the `network_poster` to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "high"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:16.682151",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_24d287a9",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the retrieved raw data into a structured format (e.g., JSON). Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into another format (e.g., from JSON to XML). Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 9.88859383194445
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:18.993247",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_55b88951",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to a specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:21.469465",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0df4b9c0",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against the predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to the specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:26.264754",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_2e650417",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: If the data is valid, use network_poster to send the validated data to the specified destination API endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 3.9897205701324596,
        "priority": "medium"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:28.979255",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_935d3e60",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher to retrieve data from a specified API endpoint. This will provide the raw data needed for processing. Step 2: Use the data_processing_parser to parse the raw data retrieved from the API into a structured format. This organizes the data into a manageable structure for further processing. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema, ensuring that the data meets the required standards for correctness and integrity. Step 4: Use the data_processing_transformer to transform the validated data into a different format, such as converting it from JSON to XML, as needed for the next stage. Step 5: Finally, use the network_poster to send the transformed data to a specified destination over the network, completing the API integration process.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "medium"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:32.202162",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0eef6c78",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: If the data is valid, use network_poster to send the validated data to a specified destination endpoint. Step 5: In case of any issues during validation, use file_operations_reader to read a fallback file containing default data.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "network_poster",
        "file_operations_reader"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:34.890911",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_50f0f3af",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 5.12475256227465
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:38.586003",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c66cb855",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher to retrieve data from a specified API endpoint. This will provide the raw data necessary for further processing. Step 2: Use the data_processing_parser to parse the raw data obtained from the API into a structured format, making it easier to work with. Step 3: Use the data_processing_validator to check the structured data against a predefined schema to ensure that it meets the required compliance. Step 4: If the data passes validation, use the network_poster to send the validated data to the designated destination over the network. If the data fails validation, log the error for review.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "medium"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:43.740272",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d25af065",
      "task_type": "api_integration",
      "description": "Step 1: Use the 'network_fetcher' to retrieve data from a specified API endpoint. Step 2: Use the 'data_processing_parser' to parse the raw data retrieved into a structured format. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_transformer' to transform the validated data into a desired output format (e.g., from JSON to XML). Step 5: Use the 'network_poster' to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:52.432658",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e16ac8db",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read the raw data from a CSV file. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specific criteria. Step 5: Use data_processing_aggregator to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.028660147620544474
            },
            {
              "id": 1,
              "value": 0.04911071904700459
            },
            {
              "id": 2,
              "value": 0.33324222458809316
            },
            {
              "id": 3,
              "value": 0.9387583125756581
            },
            {
              "id": 4,
              "value": 0.017301387594728546
            },
            {
              "id": 5,
              "value": 0.9409583439116619
            },
            {
              "id": 6,
              "value": 0.27345501785391957
            },
            {
              "id": 7,
              "value": 0.5555101560528481
            },
            {
              "id": 8,
              "value": 0.6848154698113594
            },
            {
              "id": 9,
              "value": 0.030230373897936924
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:18.856167",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1f0a0a83",
      "task_type": "data_pipeline",
      "description": "Step 1: Use 'file_operations_reader' to read raw data from a specified input file in CSV format. Step 2: Use 'data_processing_parser' to parse the raw CSV data into a structured format for easier handling. Step 3: Use 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use 'data_processing_filter' to selectively filter the valid data based on specific criteria, reducing the dataset to only the relevant information. Step 5: Use 'data_processing_aggregator' to aggregate the filtered data for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.8210137189242178
            },
            {
              "id": 1,
              "value": 0.8254525267762045
            },
            {
              "id": 2,
              "value": 0.8566371463263166
            },
            {
              "id": 3,
              "value": 0.665532335399197
            },
            {
              "id": 4,
              "value": 0.7760226044632447
            },
            {
              "id": 5,
              "value": 0.2403670457473086
            },
            {
              "id": 6,
              "value": 0.6496491379574518
            },
            {
              "id": 7,
              "value": 0.8324846842213777
            },
            {
              "id": 8,
              "value": 0.34521929976497046
            },
            {
              "id": 9,
              "value": 0.4505588418411042
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:22.068253",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_ef28b159",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data file (in CSV format) and retrieve the data. Step 2: Use the data_processing_parser tool to parse the raw data from the file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.9279375917564779
            },
            {
              "id": 1,
              "value": 0.5328788646647409
            },
            {
              "id": 2,
              "value": 0.2621960178422671
            },
            {
              "id": 3,
              "value": 0.20471302535128622
            },
            {
              "id": 4,
              "value": 0.25663057595953254
            },
            {
              "id": 5,
              "value": 0.6852764564526249
            },
            {
              "id": 6,
              "value": 0.6063415516418909
            },
            {
              "id": 7,
              "value": 0.22989658295267357
            },
            {
              "id": 8,
              "value": 0.8374136319212956
            },
            {
              "id": 9,
              "value": 0.4502564118841881
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:26.098352",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_7c9831db",
      "task_type": "data_pipeline",
      "description": "Step 1: Use 'file_operations_reader' to read raw data from the input file (in formats such as CSV, JSON, or XML). Step 2: Use 'data_processing_parser' to parse the raw data into a structured format, enabling easier manipulation and analysis. Step 3: Use 'data_processing_validator' to validate the structured data against a predefined schema, ensuring data integrity and correctness. Step 4: Use 'data_processing_filter' to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant records. Step 5: Use 'data_processing_aggregator' to aggregate the filtered data, preparing it for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.15200580239482075
            },
            {
              "id": 1,
              "value": 0.22562755008655644
            },
            {
              "id": 2,
              "value": 0.15109086246602366
            },
            {
              "id": 3,
              "value": 0.8120291618549151
            },
            {
              "id": 4,
              "value": 0.40743490605765376
            },
            {
              "id": 5,
              "value": 0.5995055470681213
            },
            {
              "id": 6,
              "value": 0.9338521453473495
            },
            {
              "id": 7,
              "value": 0.3096543428146169
            },
            {
              "id": 8,
              "value": 0.26451206866788757
            },
            {
              "id": 9,
              "value": 0.7778524041943159
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "medium"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:29.851997",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f220760f",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read raw data from a specified file in CSV format. This step will retrieve the necessary data for processing. Step 2: Use the 'data_processing_parser' tool to parse the raw data into a structured format, making it easier to work with. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specific criteria that are relevant to the analysis. Step 5: Finally, use the 'data_processing_aggregator' tool to aggregate the filtered data, summarizing it for further insights or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.4138661264409851
            },
            {
              "id": 1,
              "value": 0.3030071469546779
            },
            {
              "id": 2,
              "value": 0.04579087378911684
            },
            {
              "id": 3,
              "value": 0.1699118025320836
            },
            {
              "id": 4,
              "value": 0.5383645197414976
            },
            {
              "id": 5,
              "value": 0.7271753365423391
            },
            {
              "id": 6,
              "value": 0.5026204598422271
            },
            {
              "id": 7,
              "value": 0.08444567838338213
            },
            {
              "id": 8,
              "value": 0.2804774097972963
            },
            {
              "id": 9,
              "value": 0.09384493957226703
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 4.340373605555487
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:34.760832",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f7b91469",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read the raw data from a specified file (e.g., CSV or JSON format). Step 2: Use the 'data_processing_parser' tool to parse the raw data into a structured format, ensuring the data is ready for further processing. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant information. Step 5: Finally, use the 'data_processing_aggregator' tool to aggregate the filtered data, summarizing the results for analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.7344049294188157
            },
            {
              "id": 1,
              "value": 0.7347424742866986
            },
            {
              "id": 2,
              "value": 0.9624012346521458
            },
            {
              "id": 3,
              "value": 0.995580152702221
            },
            {
              "id": 4,
              "value": 0.8221757875015431
            },
            {
              "id": 5,
              "value": 0.15109593116721232
            },
            {
              "id": 6,
              "value": 0.8269907240570887
            },
            {
              "id": 7,
              "value": 0.7811021793014492
            },
            {
              "id": 8,
              "value": 0.6025428950352033
            },
            {
              "id": 9,
              "value": 0.7330092525980689
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "medium"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:37.806224",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_931141a1",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' to read the raw data from a specified file (e.g., CSV, JSON). This step retrieves the data and prepares it for parsing. Step 2: Use the 'data_processing_parser' to parse the raw data into a structured format, extracting relevant information from the input data. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema, ensuring that the data is correct and meets the required standards. Step 4: Use the 'data_processing_filter' to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant entries. Step 5: Use the 'data_processing_aggregator' to aggregate the filtered data, summarizing key metrics or insights that can be derived from the dataset.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.9684595985402986
            },
            {
              "id": 1,
              "value": 0.47285440851314897
            },
            {
              "id": 2,
              "value": 0.7121197771349693
            },
            {
              "id": 3,
              "value": 0.7604685910739015
            },
            {
              "id": 4,
              "value": 0.17115653850982604
            },
            {
              "id": 5,
              "value": 0.9465965725984286
            },
            {
              "id": 6,
              "value": 0.16830525062487622
            },
            {
              "id": 7,
              "value": 0.1617708980963054
            },
            {
              "id": 8,
              "value": 0.5444092114412993
            },
            {
              "id": 9,
              "value": 0.09591022828490225
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:41.194252",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_2ce933ff",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a CSV file. This tool will retrieve the data and handle any errors during the read operation. Step 2: Use the data_processing_parser tool to parse the raw data from the file into a structured format. This step will ensure that the data is organized correctly for further processing. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema. This will check the data for correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. This will help reduce the dataset to only the relevant information. Step 5: Finally, use the data_processing_aggregator tool to aggregate the filtered data to derive meaningful insights or summaries.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.5330877767430858
            },
            {
              "id": 1,
              "value": 0.24271645830678623
            },
            {
              "id": 2,
              "value": 0.6643493352566463
            },
            {
              "id": 3,
              "value": 0.313442094781358
            },
            {
              "id": 4,
              "value": 0.5469620076059114
            },
            {
              "id": 5,
              "value": 0.48773575546061465
            },
            {
              "id": 6,
              "value": 0.3282834198801732
            },
            {
              "id": 7,
              "value": 0.7157371074984902
            },
            {
              "id": 8,
              "value": 0.005876623868894759
            },
            {
              "id": 9,
              "value": 0.5414065633638702
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:45.581065",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d470c315",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' to read the raw data from a CSV file. Step 2: Use the 'data_processing_parser' to parse the raw data into a structured format. Step 3: Use the 'data_processing_validator' to validate the structured data against the predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' to selectively filter the validated data based on specified criteria. Step 5: Finally, use the 'data_processing_aggregator' to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.5544464864209364
            },
            {
              "id": 1,
              "value": 0.7856932799343573
            },
            {
              "id": 2,
              "value": 0.46802570561315593
            },
            {
              "id": 3,
              "value": 0.4696438006819773
            },
            {
              "id": 4,
              "value": 0.8023218407171959
            },
            {
              "id": 5,
              "value": 0.19665327723445292
            },
            {
              "id": 6,
              "value": 0.348481284723683
            },
            {
              "id": 7,
              "value": 0.1258789548364122
            },
            {
              "id": 8,
              "value": 0.1068879175501517
            },
            {
              "id": 9,
              "value": 0.8456879821551839
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:48.939088",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_fe4010d0",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a specified input file (e.g., CSV or JSON format). This will retrieve the data for processing. Step 2: Use the data_processing_parser tool to parse the retrieved raw data into a structured format. This will help in organizing the data for further processing. Step 3: Use the data_processing_validator tool to validate the parsed data against a defined schema to ensure its correctness and integrity. This step is crucial for maintaining data quality. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. This will reduce the dataset to only relevant information. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for analysis. This will summarize the data and prepare it for reporting or further computation.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.19110545079025376
            },
            {
              "id": 1,
              "value": 0.9967675546117539
            },
            {
              "id": 2,
              "value": 0.522885819937246
            },
            {
              "id": 3,
              "value": 0.41883658974977434
            },
            {
              "id": 4,
              "value": 0.19926732783415757
            },
            {
              "id": 5,
              "value": 0.04981050761181449
            },
            {
              "id": 6,
              "value": 0.18137761416532228
            },
            {
              "id": 7,
              "value": 0.19275858200350537
            },
            {
              "id": 8,
              "value": 0.6649535276396279
            },
            {
              "id": 9,
              "value": 0.15226220372226196
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:52.816850",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_b072928b",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser tool to parse the raw data retrieved from the file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.607571133163575
            },
            {
              "id": 1,
              "value": 0.8694878283197774
            },
            {
              "id": 2,
              "value": 0.685248620049124
            },
            {
              "id": 3,
              "value": 0.017447619570808093
            },
            {
              "id": 4,
              "value": 0.8934695495651315
            },
            {
              "id": 5,
              "value": 0.7918965998134273
            },
            {
              "id": 6,
              "value": 0.6242388523475754
            },
            {
              "id": 7,
              "value": 0.6237636315744266
            },
            {
              "id": 8,
              "value": 0.3775750864622597
            },
            {
              "id": 9,
              "value": 0.9267802085356135
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:18.666565",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_69e1cf04",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read raw data from a specified file in CSV format. Step 2: Use the 'data_processing_parser' tool to parse the read data into a structured format, extracting relevant fields. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specific criteria, reducing the dataset to only the necessary entries. Step 5: Use the 'data_processing_aggregator' tool to aggregate the filtered data, summarizing key metrics for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.14287152134783443
            },
            {
              "id": 1,
              "value": 0.6583910004615984
            },
            {
              "id": 2,
              "value": 0.17841280433770956
            },
            {
              "id": 3,
              "value": 0.04628355097487791
            },
            {
              "id": 4,
              "value": 0.5306036057938741
            },
            {
              "id": 5,
              "value": 0.6741218578350097
            },
            {
              "id": 6,
              "value": 0.31122851573087273
            },
            {
              "id": 7,
              "value": 0.6257697732743283
            },
            {
              "id": 8,
              "value": 0.8774120474441363
            },
            {
              "id": 9,
              "value": 0.3483310475752849
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:27.760367",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_65437d17",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data file, which may be in CSV, JSON, or XML format. This will retrieve the data for further processing. Step 2: After reading the data, use the data_processing_parser tool to parse the retrieved raw data into a structured format. This tool will handle the different formats and ensure the data is organized. Step 3: Next, apply the data_processing_validator tool to validate the structured data against a predefined schema. This will ensure that the data meets the necessary quality and integrity requirements. Step 4: If the data is valid, proceed to use the data_processing_filter tool to selectively filter the data based on specified criteria, reducing the dataset to only the relevant information needed for analysis. Step 5: Finally, utilize the data_processing_aggregator tool to aggregate the filtered data, summarizing it into meaningful insights for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.6164355566216418
            },
            {
              "id": 1,
              "value": 0.271492089894399
            },
            {
              "id": 2,
              "value": 0.8983950205739496
            },
            {
              "id": 3,
              "value": 0.3266081774619205
            },
            {
              "id": 4,
              "value": 0.8062753339744453
            },
            {
              "id": 5,
              "value": 0.9554662383361762
            },
            {
              "id": 6,
              "value": 0.38478441803034213
            },
            {
              "id": 7,
              "value": 0.35151315237910274
            },
            {
              "id": 8,
              "value": 0.2919256685672087
            },
            {
              "id": 9,
              "value": 0.8857455602034215
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "medium"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:31.731234",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_7c5b31e9",
      "task_type": "data_pipeline",
      "description": "Step 1: Use [file_operations_reader] to read the raw data from a CSV file. Step 2: Use [data_processing_parser] to parse the raw data into a structured format. Step 3: Use [data_processing_validator] to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use [data_processing_filter] to filter the validated data based on specified criteria. Step 5: Use [data_processing_aggregator] to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.9639864701556484
            },
            {
              "id": 1,
              "value": 0.3871218810522533
            },
            {
              "id": 2,
              "value": 0.15815754681577388
            },
            {
              "id": 3,
              "value": 0.35316984033629983
            },
            {
              "id": 4,
              "value": 0.136324250081158
            },
            {
              "id": 5,
              "value": 0.15032975014463335
            },
            {
              "id": 6,
              "value": 0.45686534203401197
            },
            {
              "id": 7,
              "value": 0.4783063850025546
            },
            {
              "id": 8,
              "value": 0.2604529897531722
            },
            {
              "id": 9,
              "value": 0.8621563577274792
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 8.331246908977171
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:34.192122",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_b5ce7c10",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a CSV file. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Finally, use the data_processing_aggregator tool to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.3065323027731056
            },
            {
              "id": 1,
              "value": 0.16922156940973254
            },
            {
              "id": 2,
              "value": 0.008205883571846373
            },
            {
              "id": 3,
              "value": 0.30442861289573
            },
            {
              "id": 4,
              "value": 0.5445895755472835
            },
            {
              "id": 5,
              "value": 0.5400932667927966
            },
            {
              "id": 6,
              "value": 0.9676645530765123
            },
            {
              "id": 7,
              "value": 0.06364665242176681
            },
            {
              "id": 8,
              "value": 0.4674792383587898
            },
            {
              "id": 9,
              "value": 0.23252930336196753
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 2.8812440813429716
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:36.568522",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9b1fffd2",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from a specified input file (e.g., CSV or JSON). Step 2: Use the data_processing_parser to parse the raw data into a structured format, ensuring that it is ready for further processing. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant entries. Step 5: Use the data_processing_aggregator to aggregate the filtered data, summarizing it into a more concise format for analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.302985016726005
            },
            {
              "id": 1,
              "value": 0.16202995179463353
            },
            {
              "id": 2,
              "value": 0.12286536670527692
            },
            {
              "id": 3,
              "value": 0.9124446524936203
            },
            {
              "id": 4,
              "value": 0.09252578370424058
            },
            {
              "id": 5,
              "value": 0.31844888835696517
            },
            {
              "id": 6,
              "value": 0.7216425550702871
            },
            {
              "id": 7,
              "value": 0.5348943569220148
            },
            {
              "id": 8,
              "value": 0.01573534239972607
            },
            {
              "id": 9,
              "value": 0.8145331008368253
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:39.386367",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e96ed29c",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read raw data from a specified file, such as a CSV or JSON file. This will retrieve the data needed for processing. Step 2: Use the 'data_processing_parser' tool to parse the raw data retrieved from the file into a structured format. This step ensures that the data is organized and ready for further processing. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema, confirming that it meets the required standards for correctness and integrity. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specified criteria, allowing only the relevant data to proceed. Step 5: Finally, use the 'data_processing_aggregator' tool to aggregate the filtered data, summarizing it into a meaningful format for analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.021474687331650255
            },
            {
              "id": 1,
              "value": 0.7733256737156595
            },
            {
              "id": 2,
              "value": 0.19283456239420427
            },
            {
              "id": 3,
              "value": 0.8598574523344106
            },
            {
              "id": 4,
              "value": 0.0760334728653923
            },
            {
              "id": 5,
              "value": 0.3686971953484226
            },
            {
              "id": 6,
              "value": 0.2874329414785973
            },
            {
              "id": 7,
              "value": 0.4415610571785782
            },
            {
              "id": 8,
              "value": 0.0002479782792358387
            },
            {
              "id": 9,
              "value": 0.9300672218779787
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:43.227084",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0032c1d7",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a CSV file. This will extract the data into a readable format. Step 2: Use the data_processing_parser tool to parse the raw data retrieved from the CSV file and structure it into a defined format (e.g., JSON). Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its integrity and correctness. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specific criteria (e.g., removing entries that do not meet certain conditions). Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for analysis, summarizing key metrics or insights.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.9775062539084701
            },
            {
              "id": 1,
              "value": 0.8456756376938275
            },
            {
              "id": 2,
              "value": 0.2341486955923756
            },
            {
              "id": 3,
              "value": 0.5263551623786926
            },
            {
              "id": 4,
              "value": 0.02375050056661787
            },
            {
              "id": 5,
              "value": 0.8971134036872837
            },
            {
              "id": 6,
              "value": 0.3545436826741821
            },
            {
              "id": 7,
              "value": 0.9031174550752942
            },
            {
              "id": 8,
              "value": 0.922510453997768
            },
            {
              "id": 9,
              "value": 0.29337901167601277
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:47.039301",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a481b39c",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read and retrieve raw data from a specified input file in CSV format. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format suitable for processing. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.6133686283844857
            },
            {
              "id": 1,
              "value": 0.2229171609292544
            },
            {
              "id": 2,
              "value": 0.7477052237479712
            },
            {
              "id": 3,
              "value": 0.5158664253003256
            },
            {
              "id": 4,
              "value": 0.3452789185908973
            },
            {
              "id": 5,
              "value": 0.19367283608014363
            },
            {
              "id": 6,
              "value": 0.46581305133538153
            },
            {
              "id": 7,
              "value": 0.6351608172410589
            },
            {
              "id": 8,
              "value": 0.17011994993765067
            },
            {
              "id": 9,
              "value": 0.3743421468284458
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 1.2779237426601961
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:49.689053",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_7d44019e",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from a specified file format (e.g., CSV or JSON). Step 2: Use the data_processing_parser to parse the read data into a structured format. Step 3: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Finally, use the data_processing_aggregator to aggregate the filtered data for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.03841641471813584
            },
            {
              "id": 1,
              "value": 0.36199518606605585
            },
            {
              "id": 2,
              "value": 0.746034101981201
            },
            {
              "id": 3,
              "value": 0.4695253137124926
            },
            {
              "id": 4,
              "value": 0.9539168154479143
            },
            {
              "id": 5,
              "value": 0.45678306571950045
            },
            {
              "id": 6,
              "value": 0.6965566982354685
            },
            {
              "id": 7,
              "value": 0.07957947528579734
            },
            {
              "id": 8,
              "value": 0.653197842672785
            },
            {
              "id": 9,
              "value": 0.11555381725048308
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:52.992883",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e2e4b980",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a different format if necessary. Step 5: Finally, use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 0.729067837802623
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:17.370384",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_18d0002b",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a different format if necessary. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:20.094581",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d5048919",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired format for further use. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 7.962184847780461,
        "priority": "medium"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:23.078301",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_03173146",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher to retrieve data from a specified API endpoint. Step 2: Use the data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_transformer to convert the validated data into a different format if necessary. Step 5: Use the network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 0.6759389808959279
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:30.438933",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a9f7da69",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to a specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 9.410676867006352
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:33.044051",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f889bd8a",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data obtained from the API into a structured format (e.g., JSON). Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a required format (e.g., convert JSON to XML). Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "high"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:37.340664",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_b41ed316",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired format. Step 5: Use network_poster to send the transformed data to the specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:39.640211",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_49fe82cc",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. This will involve specifying the URL of the API to fetch the required data. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format, making it easier to work with. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: If the data is valid, use network_poster to send the validated data to a specified destination over the network. If the data is not valid, you may need to adjust the data or handle errors accordingly.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:45.831582",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0054d7f9",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from an external API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "high"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:49.527144",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_13c0622d",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve raw data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to the specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:53.082855",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_cad33be7",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read and retrieve the raw data from a specified file (e.g., a CSV or JSON file). Step 2: Use the data_processing_parser tool to parse the retrieved raw data into a structured format, extracting relevant information. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria (e.g., filtering out records that do not meet certain conditions). Step 5: Use the data_processing_aggregator tool to aggregate the filtered data, summarizing it for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.08541831217458784
            },
            {
              "id": 1,
              "value": 0.5256276040499706
            },
            {
              "id": 2,
              "value": 0.9206198457639437
            },
            {
              "id": 3,
              "value": 0.588907448399059
            },
            {
              "id": 4,
              "value": 0.6337899447110928
            },
            {
              "id": 5,
              "value": 0.7161742361633759
            },
            {
              "id": 6,
              "value": 0.7630463462123573
            },
            {
              "id": 7,
              "value": 0.08422312799315235
            },
            {
              "id": 8,
              "value": 0.9695420077153841
            },
            {
              "id": 9,
              "value": 0.8756221498731273
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:19.917243",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_038f7373",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read raw data from a CSV file. Step 2: Use the data_processing_parser to parse the retrieved data into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_aggregator to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.5684577571819305
            },
            {
              "id": 1,
              "value": 0.39827143833469747
            },
            {
              "id": 2,
              "value": 0.01665039482373254
            },
            {
              "id": 3,
              "value": 0.922944173295668
            },
            {
              "id": 4,
              "value": 0.7564217604207296
            },
            {
              "id": 5,
              "value": 0.445731684406525
            },
            {
              "id": 6,
              "value": 0.9609324475776791
            },
            {
              "id": 7,
              "value": 0.10585850909936456
            },
            {
              "id": 8,
              "value": 0.9360713323075889
            },
            {
              "id": 9,
              "value": 0.5712957203761566
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:22.944081",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_403cc12d",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from a CSV file. Step 2: Use the data_processing_parser to parse the raw CSV data into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_aggregator to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.18445814202285238
            },
            {
              "id": 1,
              "value": 0.2003953655299483
            },
            {
              "id": 2,
              "value": 0.4877247615909569
            },
            {
              "id": 3,
              "value": 0.5858864448957424
            },
            {
              "id": 4,
              "value": 0.08646190304510382
            },
            {
              "id": 5,
              "value": 0.3433636229295213
            },
            {
              "id": 6,
              "value": 0.42788384442316674
            },
            {
              "id": 7,
              "value": 0.28198799228259397
            },
            {
              "id": 8,
              "value": 0.568213728220425
            },
            {
              "id": 9,
              "value": 0.21927787124548392
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 2.1208784062020625
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:26.704735",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_965fe6d9",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read the raw data file in CSV format. Step 2: Use data_processing_parser to parse the raw CSV data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_aggregator to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.8057946994912711
            },
            {
              "id": 1,
              "value": 0.5073948990783903
            },
            {
              "id": 2,
              "value": 0.5212349543131028
            },
            {
              "id": 3,
              "value": 0.5929507428767377
            },
            {
              "id": 4,
              "value": 0.896865668575201
            },
            {
              "id": 5,
              "value": 0.27480794536501474
            },
            {
              "id": 6,
              "value": 0.8291781463521044
            },
            {
              "id": 7,
              "value": 0.9567906925644393
            },
            {
              "id": 8,
              "value": 0.18777172342084503
            },
            {
              "id": 9,
              "value": 0.4084151669366516
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:30.635490",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_b17c1e8a",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a specified input file (e.g., CSV, JSON). Step 2: Use the data_processing_parser tool to parse the raw data retrieved from the file and structure it into a defined format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specific criteria to reduce the dataset to the relevant information. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.4582902010585722
            },
            {
              "id": 1,
              "value": 0.16370746500701183
            },
            {
              "id": 2,
              "value": 0.6101829772262175
            },
            {
              "id": 3,
              "value": 0.6606602879202788
            },
            {
              "id": 4,
              "value": 0.6969692201877331
            },
            {
              "id": 5,
              "value": 0.048891027011592314
            },
            {
              "id": 6,
              "value": 0.7438854665884915
            },
            {
              "id": 7,
              "value": 0.45541938344980226
            },
            {
              "id": 8,
              "value": 0.5799533492413601
            },
            {
              "id": 9,
              "value": 0.1311183050820246
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 5.485849022383951
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:34.412786",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_907b7783",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read data from a specified file, such as a CSV or JSON file, to retrieve the raw data. Step 2: Use the 'data_processing_parser' tool to parse the raw data retrieved in Step 1 into a structured format that can be easily manipulated. Step 3: Use the 'data_processing_validator' tool to validate the structured data from Step 2 against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specified criteria to reduce the dataset size. Step 5: Finally, use the 'data_processing_aggregator' tool to aggregate the filtered data from Step 4, providing summarized insights or metrics based on the requirements.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.32687712077676745
            },
            {
              "id": 1,
              "value": 0.6881079921585758
            },
            {
              "id": 2,
              "value": 0.2776424876932355
            },
            {
              "id": 3,
              "value": 0.7456530082457169
            },
            {
              "id": 4,
              "value": 0.7658023802791158
            },
            {
              "id": 5,
              "value": 0.5314215861293987
            },
            {
              "id": 6,
              "value": 0.6377681115511944
            },
            {
              "id": 7,
              "value": 0.25497960445573553
            },
            {
              "id": 8,
              "value": 0.9837303656897414
            },
            {
              "id": 9,
              "value": 0.6270554510384924
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:41.077500",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4186c618",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read raw data from a specified file (e.g., CSV or JSON). Step 2: Use data_processing_parser to parse the raw data into a structured format for easier manipulation. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria to reduce the dataset. Step 5: Use data_processing_aggregator to aggregate the filtered data for summarization and further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.4401157834866981
            },
            {
              "id": 1,
              "value": 0.14534439021189183
            },
            {
              "id": 2,
              "value": 0.5717935610808818
            },
            {
              "id": 3,
              "value": 0.991604503205954
            },
            {
              "id": 4,
              "value": 0.8234072380701695
            },
            {
              "id": 5,
              "value": 0.44583941613572586
            },
            {
              "id": 6,
              "value": 0.329415016701167
            },
            {
              "id": 7,
              "value": 0.9420300822100814
            },
            {
              "id": 8,
              "value": 0.7945583878528137
            },
            {
              "id": 9,
              "value": 0.2929074143192756
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:44.359443",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_216fa048",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read the raw data from a CSV file. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the data based on specified criteria, reducing the dataset to only relevant entries. Step 5: Use data_processing_aggregator to aggregate the filtered data for analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.050050254707988184
            },
            {
              "id": 1,
              "value": 0.2123641774940206
            },
            {
              "id": 2,
              "value": 0.09203233401321365
            },
            {
              "id": 3,
              "value": 0.6187599937353854
            },
            {
              "id": 4,
              "value": 0.09780600171018028
            },
            {
              "id": 5,
              "value": 0.2150631321772477
            },
            {
              "id": 6,
              "value": 0.9693268350166114
            },
            {
              "id": 7,
              "value": 0.022897836744296574
            },
            {
              "id": 8,
              "value": 0.4774571216256164
            },
            {
              "id": 9,
              "value": 0.6486978307221959
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 8.585595405110414
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:46.614547",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c2095f5c",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from a specified file format (e.g., CSV, JSON, XML). Step 2: Use the data_processing_parser to parse the raw data into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_aggregator to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.8994911660136475
            },
            {
              "id": 1,
              "value": 0.18474211674245555
            },
            {
              "id": 2,
              "value": 0.7421789584602261
            },
            {
              "id": 3,
              "value": 0.3319251051624289
            },
            {
              "id": 4,
              "value": 0.3133392836615071
            },
            {
              "id": 5,
              "value": 0.6901862427957359
            },
            {
              "id": 6,
              "value": 0.3854015040957115
            },
            {
              "id": 7,
              "value": 0.80396873618357
            },
            {
              "id": 8,
              "value": 0.42486896701254806
            },
            {
              "id": 9,
              "value": 0.09429415668700514
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:50.257790",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c2741980",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read the raw data file in CSV format. Step 2: Use data_processing_parser to parse the raw data retrieved from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria to reduce the dataset size. Step 5: Use data_processing_aggregator to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.20017561813230478
            },
            {
              "id": 1,
              "value": 0.6504955000797863
            },
            {
              "id": 2,
              "value": 0.25466131461125563
            },
            {
              "id": 3,
              "value": 0.9552051595849697
            },
            {
              "id": 4,
              "value": 0.10359088276106854
            },
            {
              "id": 5,
              "value": 0.9020495368742898
            },
            {
              "id": 6,
              "value": 0.11507529641372705
            },
            {
              "id": 7,
              "value": 0.9977542076795192
            },
            {
              "id": 8,
              "value": 0.6305536595133869
            },
            {
              "id": 9,
              "value": 0.9679538878555097
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 3.9983891311734263,
        "priority": "medium"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:53.099031",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0c5805bc",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a specified file. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_transformer to convert the filtered data from its current format to a desired output format.",
      "inputs": {
        "input_data": {
          "data": [
            0.7409810881289771,
            0.9673196742266231,
            0.4865885100849605,
            0.16906800704568203,
            0.7226773895783245
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 6.627567235306085
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:16.932999",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4bcd435f",
      "task_type": "basic_task",
      "description": "Step 1: Use the 'file_operations_reader' to read data from a CSV file. Step 2: Use the 'data_processing_parser' to parse the raw data extracted from the CSV file into a structured format. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.6785257086181975,
            0.7062125753246493,
            0.8931299483987031,
            0.2734513839525541,
            0.03309385481920979
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:18.973242",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_6f49c9d0",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read the input data file (in CSV format). Step 2: Use the data_processing_parser tool to parse the raw data into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_transformer tool to transform the filtered data into a desired output format (for example, converting it from JSON to XML).",
      "inputs": {
        "input_data": {
          "data": [
            0.513026568978077,
            0.30418208883009157,
            0.8330927980136087,
            0.554676952614521,
            0.06097851193059234
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:22.441087",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_50eb1702",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a CSV file. Step 2: Use data_processing_parser to parse the raw CSV data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to filter the validated data based on specified criteria. Step 5: Use data_processing_transformer to transform the filtered data into a JSON format.",
      "inputs": {
        "input_data": {
          "data": [
            0.17599369325114,
            0.3310244141599856,
            0.13146915163696282,
            0.07235346071040527,
            0.09990748006569317
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:27.078105",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_21ea0571",
      "task_type": "basic_task",
      "description": "Step 1: Use the 'file_operations_reader' tool to read a CSV file containing raw data. This will allow you to retrieve the data from the file. Step 2: Use the 'data_processing_parser' tool to parse the raw data from the CSV file into a structured format. This will help organize the data for easier processing. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema. This step ensures that the data is correct and meets the required standards. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specific criteria. This will help you focus on the relevant data you need. Step 5: Finally, use the 'data_processing_transformer' tool to convert the filtered data into a different format, such as JSON, for further use or storage.",
      "inputs": {
        "input_data": {
          "data": [
            0.9880847195657574,
            0.30433131873464225,
            0.9357085079481954,
            0.20412531927760857,
            0.9249248744499732
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:34.544621",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a10ac122",
      "task_type": "basic_task",
      "description": "Step 1: Use the 'file_operations_reader' tool to read a CSV file containing raw data. Step 2: Use the 'data_processing_parser' tool to parse the raw data from the CSV file into a structured format. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specific criteria. Step 5: Use the 'data_processing_transformer' tool to convert the filtered data into a different format, such as JSON.",
      "inputs": {
        "input_data": {
          "data": [
            0.39387702946490644,
            0.055788395810501656,
            0.6366514600287387,
            0.26897987444082805,
            0.7745299392049786
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 6.086353930175774,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:39.247968",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d30f42a7",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a CSV file and retrieve its contents. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.5260513288304661,
            0.3279098301781376,
            0.8555092589407232,
            0.9580186442042671,
            0.7774815533196482
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 6.515248161700061
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:41.296748",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d175156e",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read the data from a specified file in CSV format. Step 2: Use the data_processing_parser tool to parse the raw CSV data into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to filter the validated data based on specified criteria, such as removing any entries that do not meet certain conditions.",
      "inputs": {
        "input_data": {
          "data": [
            0.4407597226549471,
            0.015078358332007213,
            0.7196928850617076,
            0.17431531065827743,
            0.38363155637451307
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 3.976716136635316,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:43.739826",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a3dabf1d",
      "task_type": "basic_task",
      "description": "Step 1: Use the 'file_operations_reader' tool to read the data from a specified CSV file. Step 2: Use the 'data_processing_parser' tool to parse the raw data retrieved from the CSV file into a structured format. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Optionally, use the 'data_processing_filter' tool to selectively filter the validated data based on specified criteria if needed. Step 5: Finally, use the 'data_processing_transformer' tool to transform the filtered data into a different format, such as converting it from JSON to XML.",
      "inputs": {
        "input_data": {
          "data": [
            0.2061235447619646,
            0.5059352408120362,
            0.9495285612349855,
            0.09458626867164766,
            0.5868375006509343
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:47.401493",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d7d1ddc5",
      "task_type": "basic_task",
      "description": "Step 1: Use the 'file_operations_reader' tool to read data from a specified input file. This will allow you to retrieve the raw data you need. Step 2: Use the 'data_processing_parser' tool to parse the raw data into a structured format. This step will help in organizing the data for further processing. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema. This ensures that the data is correct and meets the necessary requirements. Step 4: Use the 'data_processing_filter' tool to selectively filter the valid data based on specified criteria. This will help in narrowing down the data to only what is needed. Step 5: Use the 'data_processing_transformer' tool to transform the filtered data into your desired output format, completing the data processing workflow.",
      "inputs": {
        "input_data": {
          "data": [
            0.24251123409562303,
            0.4762443204047059,
            0.38300802144032464,
            0.14571724697332722,
            0.14059621400948807
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:53.154529",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_76b5f83d",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read the raw data files in CSV format. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_aggregator to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.025707767838804685
            },
            {
              "id": 1,
              "value": 0.3586771866585
            },
            {
              "id": 2,
              "value": 0.14045383243685017
            },
            {
              "id": 3,
              "value": 0.06780506873726466
            },
            {
              "id": 4,
              "value": 0.22577088627335662
            },
            {
              "id": 5,
              "value": 0.5092953746934473
            },
            {
              "id": 6,
              "value": 0.10149495187001611
            },
            {
              "id": 7,
              "value": 0.13610569551339546
            },
            {
              "id": 8,
              "value": 0.8549230196234927
            },
            {
              "id": 9,
              "value": 0.4444472431227572
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 4.860616578942953
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:18.249281",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_bd8b32ce",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read the raw data from the specified input file (e.g., CSV or JSON). Step 2: Use data_processing_parser to parse the raw data into a structured format, extracting relevant fields. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the structured data based on specified criteria, removing any unnecessary records. Step 5: Use data_processing_aggregator to aggregate the filtered data for summary insights, such as totals or averages.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.45567356542427095
            },
            {
              "id": 1,
              "value": 0.7532776627672968
            },
            {
              "id": 2,
              "value": 0.18958291980792075
            },
            {
              "id": 3,
              "value": 0.7878187017208778
            },
            {
              "id": 4,
              "value": 0.5957309018287492
            },
            {
              "id": 5,
              "value": 0.5293093899065094
            },
            {
              "id": 6,
              "value": 0.5216735370028367
            },
            {
              "id": 7,
              "value": 0.022115631888932064
            },
            {
              "id": 8,
              "value": 0.8718541193866086
            },
            {
              "id": 9,
              "value": 0.8030739798326296
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:20.942472",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_20484bf3",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read raw data from a specified file format (e.g., CSV or JSON). This step will retrieve the unstructured data needed for processing. Step 2: Use the 'data_processing_parser' tool to parse the raw data into a structured format. This will help in organizing the data for further processing. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness and integrity. This step is crucial to filter out any invalid data. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specified criteria. This will help in reducing the dataset to only relevant information. Step 5: Finally, use the 'data_processing_aggregator' tool to aggregate the filtered data to summarize or compile the information as needed for analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.0007320794357577043
            },
            {
              "id": 1,
              "value": 0.47183855860054347
            },
            {
              "id": 2,
              "value": 0.45222512122654246
            },
            {
              "id": 3,
              "value": 0.5940329373705424
            },
            {
              "id": 4,
              "value": 0.7235333775926477
            },
            {
              "id": 5,
              "value": 0.49692296926021706
            },
            {
              "id": 6,
              "value": 0.16355338248617357
            },
            {
              "id": 7,
              "value": 0.42243786745389056
            },
            {
              "id": 8,
              "value": 0.13532407164771199
            },
            {
              "id": 9,
              "value": 0.9985029885422999
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 0.7984476687442804
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:25.737871",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_fab2db12",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a specified file format (e.g., CSV or JSON). Step 2: Use the data_processing_parser tool to parse the retrieved raw data into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.4478086530010382
            },
            {
              "id": 1,
              "value": 0.8738541999895477
            },
            {
              "id": 2,
              "value": 0.7811181686734873
            },
            {
              "id": 3,
              "value": 0.8554974771518826
            },
            {
              "id": 4,
              "value": 0.47097163370304973
            },
            {
              "id": 5,
              "value": 0.8012757802810129
            },
            {
              "id": 6,
              "value": 0.9208331417825742
            },
            {
              "id": 7,
              "value": 0.11640314689675002
            },
            {
              "id": 8,
              "value": 0.4664986890967364
            },
            {
              "id": 9,
              "value": 0.6740803208585178
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:28.658392",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_6726b1a8",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the [file_operations_reader] to read the raw data file, retrieving the data in its original format (e.g., CSV or JSON). Step 2: Use the [data_processing_parser] to parse the retrieved raw data into a structured format, organizing it for further processing. Step 3: Use the [data_processing_validator] to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the [data_processing_filter] to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant information. Step 5: Use the [data_processing_aggregator] to aggregate the filtered data, summarizing key metrics or insights from the dataset.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.9068130995554131
            },
            {
              "id": 1,
              "value": 0.7061362334403725
            },
            {
              "id": 2,
              "value": 0.8923001473771711
            },
            {
              "id": 3,
              "value": 0.395378933629179
            },
            {
              "id": 4,
              "value": 0.37556457026047574
            },
            {
              "id": 5,
              "value": 0.3449288430885715
            },
            {
              "id": 6,
              "value": 0.18617498988001857
            },
            {
              "id": 7,
              "value": 0.6253041581309216
            },
            {
              "id": 8,
              "value": 0.8364115528678414
            },
            {
              "id": 9,
              "value": 0.3804907240269858
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:34.922440",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5afb0400",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read the raw data from the input files in CSV format. Step 2: Use the 'data_processing_parser' tool to parse the raw data into a structured format, extracting necessary fields and organizing the data. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specific criteria, reducing the dataset to only relevant entries. Step 5: Use the 'data_processing_aggregator' tool to aggregate the filtered data to summarize key metrics or insights.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.10793969282641802
            },
            {
              "id": 1,
              "value": 0.27077490721546893
            },
            {
              "id": 2,
              "value": 0.7493955889442027
            },
            {
              "id": 3,
              "value": 0.48276424671683893
            },
            {
              "id": 4,
              "value": 0.3210412174799384
            },
            {
              "id": 5,
              "value": 0.5715088428045559
            },
            {
              "id": 6,
              "value": 0.6270064603834093
            },
            {
              "id": 7,
              "value": 0.5487112816955997
            },
            {
              "id": 8,
              "value": 0.8144337722728359
            },
            {
              "id": 9,
              "value": 0.3828967526778614
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:39.947572",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_10310ece",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read and retrieve raw data from a CSV file. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a defined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_aggregator to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.8163102591561798
            },
            {
              "id": 1,
              "value": 0.16808571050874832
            },
            {
              "id": 2,
              "value": 0.48483840486372554
            },
            {
              "id": 3,
              "value": 0.7641560152713871
            },
            {
              "id": 4,
              "value": 0.5816624265570772
            },
            {
              "id": 5,
              "value": 0.7968935051399841
            },
            {
              "id": 6,
              "value": 0.8621374406741472
            },
            {
              "id": 7,
              "value": 0.9651058545271776
            },
            {
              "id": 8,
              "value": 0.5717244585384443
            },
            {
              "id": 9,
              "value": 0.9521978378720456
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 7.799051268875955
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:42.472797",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_63a975a8",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from the input file in CSV format. Step 2: Use the data_processing_parser tool to parse the raw CSV data into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to filter the validated data based on specified criteria (e.g., only include records that meet certain conditions). Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for summarization and reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.7603659583141653
            },
            {
              "id": 1,
              "value": 0.2874168677000043
            },
            {
              "id": 2,
              "value": 0.9047637416622464
            },
            {
              "id": 3,
              "value": 0.007108556007925149
            },
            {
              "id": 4,
              "value": 0.8251948685778588
            },
            {
              "id": 5,
              "value": 0.47509876747931035
            },
            {
              "id": 6,
              "value": 0.24667591434890868
            },
            {
              "id": 7,
              "value": 0.616901694483969
            },
            {
              "id": 8,
              "value": 0.8322431668055097
            },
            {
              "id": 9,
              "value": 0.8979301246047868
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:45.709190",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_36060e45",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' to read raw data from a specified file. This step will retrieve the data in its original format (e.g., CSV, JSON). Step 2: Use the 'data_processing_parser' to parse the raw data into a structured format, making it easier to work with. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema, ensuring its correctness and integrity. Step 4: Use the 'data_processing_filter' to selectively filter the validated data based on specified criteria, reducing the dataset to only relevant information. Step 5: Finally, use the 'data_processing_aggregator' to aggregate the filtered data for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.13971796725927155
            },
            {
              "id": 1,
              "value": 0.20132997199060532
            },
            {
              "id": 2,
              "value": 0.67218602855377
            },
            {
              "id": 3,
              "value": 0.554239125006416
            },
            {
              "id": 4,
              "value": 0.08701258783033816
            },
            {
              "id": 5,
              "value": 0.801416373801651
            },
            {
              "id": 6,
              "value": 0.2735996373851438
            },
            {
              "id": 7,
              "value": 0.2642379177527422
            },
            {
              "id": 8,
              "value": 0.27556431012624494
            },
            {
              "id": 9,
              "value": 0.6760142162523815
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 6.657205441139694,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:50.025856",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_6f99c3ef",
      "task_type": "data_pipeline",
      "description": "Step 1: Use [file_operations_reader] to read the raw data from a specified file (e.g., CSV or JSON format). Step 2: Use [data_processing_parser] to parse the raw data into a structured format, extracting the necessary fields. Step 3: Use [data_processing_validator] to validate the structured data against a predefined schema to ensure its accuracy and integrity. Step 4: Use [data_processing_filter] to filter the validated data based on specific criteria, reducing the dataset to the most relevant entries. Step 5: Use [data_processing_aggregator] to aggregate the filtered data, summarizing the results for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.9054435655768979
            },
            {
              "id": 1,
              "value": 0.20660506419943603
            },
            {
              "id": 2,
              "value": 0.3353220180263
            },
            {
              "id": 3,
              "value": 0.9942266013878436
            },
            {
              "id": 4,
              "value": 0.7136763284342766
            },
            {
              "id": 5,
              "value": 0.23144539614694914
            },
            {
              "id": 6,
              "value": 0.28873736223296975
            },
            {
              "id": 7,
              "value": 0.3701375376420656
            },
            {
              "id": 8,
              "value": 0.07797825050345819
            },
            {
              "id": 9,
              "value": 0.6559463139776927
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:53.238075",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_68b3450d",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher to retrieve data from a specified API endpoint. Step 2: Use the data_processing_parser to parse the raw data received from the API into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_transformer to transform the validated data into a different format as required. Step 5: Use the network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:17.526675",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_56dfac80",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to the specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 3.4302621608013384
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:20.290077",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_24dbd30c",
      "task_type": "api_integration",
      "description": "Step 1: Use the 'network_fetcher' to retrieve data from a specified API endpoint that provides raw data in JSON format. Step 2: Use the 'data_processing_parser' to parse the raw JSON data into a structured format that can be easily manipulated. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_transformer' to transform the validated data into a different format, such as XML, for further processing or storage. Step 5: Use the 'network_poster' to send the transformed data to a specified destination endpoint over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:23.624099",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a2d96ac3",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a desired output format (e.g., from JSON to XML). Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "medium"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:31.596105",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5831a561",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:34.513419",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_3c6ca1b5",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve raw data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to transform the validated data into a different format as required. Step 5: Use network_poster to send the transformed data to a specified destination.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:36.859637",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_442b443f",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the valid data into a desired output format. Step 5: Use network_poster to send the transformed data to a specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:39.073957",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_fa755ebb",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to convert the validated data into the desired output format (e.g., from JSON to XML). Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:45.849874",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f3d88006",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher to retrieve raw data from a specified API endpoint. Step 2: Use the data_processing_parser to parse the raw data into a structured format, such as JSON. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: If the data is valid, use the network_poster to send the validated data to a specified destination endpoint. Step 5: If the data is invalid, log the error and use the data_processing_transformer to convert the data into an alternative format if necessary.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "network_poster",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:49.999191",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d93905db",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to the specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:53.397119",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_71c17b0d",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a specified file (e.g., a CSV file). Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_transformer to convert the filtered data into a desired output format (e.g., from JSON to XML).",
      "inputs": {
        "input_data": {
          "data": [
            0.22310049600289905,
            0.6690708696750333,
            0.9037370806798709,
            0.5926406633196193,
            0.3024538979492398
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 8.042004165215193
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:17.476797",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0a61af30",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader to read data from a specified file (e.g., a CSV file). Step 2: Use the data_processing_parser to parse the raw data into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_transformer to transform the filtered data into a different format (e.g., from JSON to XML).",
      "inputs": {
        "input_data": {
          "data": [
            0.3928408880019121,
            0.3789112047139429,
            0.023741758840409743,
            0.6243969338370491,
            0.9551920002755333
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:25.358373",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_60bb328b",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.05792724191652099,
            0.3658376302700995,
            0.041308152645759044,
            0.34138334093128697,
            0.16527257143470053
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 0.5407106745674428,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:28.278427",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_ddb551ff",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a specified file (e.g., CSV or JSON format). Step 2: Use data_processing_parser to parse the raw data into a structured format for easier manipulation. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specific criteria (e.g., filtering out unnecessary entries). Step 5: Use data_processing_transformer to convert the filtered data into the desired output format (e.g., from JSON to XML).",
      "inputs": {
        "input_data": {
          "data": [
            0.9422193425023452,
            0.002737361781129266,
            0.9177616142879107,
            0.12417148301076208,
            0.5785931295957013
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 1.1882899213139764,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:35.215828",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_485663ed",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read a CSV file containing raw data. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_transformer to transform the filtered data into a different format, such as JSON.",
      "inputs": {
        "input_data": {
          "data": [
            0.9203906775061738,
            0.9449710552196113,
            0.8200044035245283,
            0.941301242819088,
            0.15064745118971767
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "low"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:38.484739",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c9b200cd",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a specified file (e.g., CSV, JSON, or XML format). Step 2: Use data_processing_parser to parse the raw data retrieved from the file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.16843673375462487,
            0.35211660732940586,
            0.15741118638460538,
            0.8342701481276252,
            0.17284735463964684
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:40.494855",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_3b331689",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file containing raw data. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to filter the validated data based on specific criteria to focus on relevant information. Step 5: Use data_processing_transformer to convert the filtered data into a different format, such as JSON, for further use.",
      "inputs": {
        "input_data": {
          "data": [
            0.5453031493101518,
            0.7060184672613694,
            0.24588645631576045,
            0.8507271440580523,
            0.24939117380557274
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 9.123075098896129
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:43.451167",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4781a623",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data file containing the raw data. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the valid data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.9089036098202841,
            0.5432964512255076,
            0.49087123849577474,
            0.4946290178187557,
            0.4025141181667158
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:46.584114",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_81faa541",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read data from a specified file (for example, a CSV file). Step 2: Use the data_processing_parser tool to parse the raw data retrieved from the file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.21486606088064342,
            0.4535974678880187,
            0.06334312477823467,
            0.3901570662202276,
            0.47481444380623805
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:49.274295",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_74e39be1",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a specified input file (e.g., a CSV or JSON file). Step 2: Use data_processing_parser to parse the raw data retrieved from the file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.1884435035466293,
            0.0783062395312557,
            0.6029891674014494,
            0.30549569014868216,
            0.11870851751296096
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:53.436681",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c138186b",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a different format as required (e.g., from JSON to XML). Step 5: Use network_poster to send the transformed data to the specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:19.845219",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_bdc38778",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data fetched from the API into a structured format. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: If the data is valid, use network_poster to send the validated data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 5.1434123039331405
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:22.081088",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_50289824",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema for correctness. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to a specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 9.175727068743797,
        "priority": "medium"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:28.787299",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_fc8a2c6e",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:31.413378",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_94046053",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to the specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:34.003550",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_ba3aed5e",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher to retrieve raw data from the specified API endpoint. Step 2: Use the data_processing_parser to parse the raw data into a structured format. Step 3: Use the data_processing_validator to validate the structured data against the predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_transformer to transform the validated data into the desired output format. Step 5: Use the network_poster to send the transformed data to the specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:36.701116",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5c279f84",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to the specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:39.038324",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d0544604",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to the specified destination.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:45.837634",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8b98b317",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data obtained from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a different format if required. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:49.629606",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4292aebb",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher tool to retrieve data from a specified API endpoint that provides the raw data needed for processing. Step 2: Use the data_processing_parser tool to parse the raw data retrieved from the API into a structured format such as JSON. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_transformer tool to transform the validated data into another format (e.g., XML) if required. Step 5: Use the network_poster tool to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 5.489191784721759
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:53.463836",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4a6ffe5f",
      "task_type": "data_pipeline",
      "description": "Step 1: Use [file_operations_reader] to read raw data from a source file in CSV format. Step 2: Use [data_processing_parser] to parse the raw CSV data into a structured format. Step 3: Use [data_processing_validator] to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use [data_processing_filter] to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant entries. Step 5: Use [data_processing_aggregator] to aggregate the filtered data to summarize key metrics or insights.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.8675309603803962
            },
            {
              "id": 1,
              "value": 0.8504730936642303
            },
            {
              "id": 2,
              "value": 0.8602458882919212
            },
            {
              "id": 3,
              "value": 0.6852850656040282
            },
            {
              "id": 4,
              "value": 0.2608426884514019
            },
            {
              "id": 5,
              "value": 0.23959200051405904
            },
            {
              "id": 6,
              "value": 0.7833070971446409
            },
            {
              "id": 7,
              "value": 0.20813450455553661
            },
            {
              "id": 8,
              "value": 0.7305687244623322
            },
            {
              "id": 9,
              "value": 0.43820139202327
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:20.299385",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_05d838b0",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read raw data files from the specified source location in either CSV or JSON format. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format, allowing for easier processing in the subsequent steps. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema, ensuring the data's correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant entries. Step 5: Finally, use the data_processing_aggregator tool to aggregate the filtered data, summarizing it in a desired format for analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.18947125121297947
            },
            {
              "id": 1,
              "value": 0.8177393397641537
            },
            {
              "id": 2,
              "value": 0.5534772016204844
            },
            {
              "id": 3,
              "value": 0.6850493492210367
            },
            {
              "id": 4,
              "value": 0.4020872022454297
            },
            {
              "id": 5,
              "value": 0.5556962346184489
            },
            {
              "id": 6,
              "value": 0.02178087907003201
            },
            {
              "id": 7,
              "value": 0.12492862063315124
            },
            {
              "id": 8,
              "value": 0.6141275300089223
            },
            {
              "id": 9,
              "value": 0.5016797929331986
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:24.264555",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_64e80d9a",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read a CSV file containing raw data. This will retrieve the data and make it available for processing. Step 2: Use the 'data_processing_parser' tool to parse the retrieved raw data from the CSV file into a structured format. This step will ensure that the data is organized correctly for further analysis. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema. This step checks for data integrity and correctness. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specific criteria. This will help reduce the dataset to only the relevant information needed for analysis. Step 5: Finally, use the 'data_processing_aggregator' tool to aggregate the filtered data, summarizing it into meaningful insights that can be used for reporting or further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.23924904804066416
            },
            {
              "id": 1,
              "value": 0.4414057558809613
            },
            {
              "id": 2,
              "value": 0.05639364359537502
            },
            {
              "id": 3,
              "value": 0.0830223526337015
            },
            {
              "id": 4,
              "value": 0.29519430305628336
            },
            {
              "id": 5,
              "value": 0.8785046071502335
            },
            {
              "id": 6,
              "value": 0.31507563072680234
            },
            {
              "id": 7,
              "value": 0.3975085639699866
            },
            {
              "id": 8,
              "value": 0.010193906531181463
            },
            {
              "id": 9,
              "value": 0.12055693816839486
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:31.740046",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9dbdea84",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file containing raw information. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_aggregator to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.08267885043502521
            },
            {
              "id": 1,
              "value": 0.5551968251610004
            },
            {
              "id": 2,
              "value": 0.7860851502194046
            },
            {
              "id": 3,
              "value": 0.708899316766449
            },
            {
              "id": 4,
              "value": 0.9557120509811877
            },
            {
              "id": 5,
              "value": 0.3142949084732187
            },
            {
              "id": 6,
              "value": 0.4697978901233064
            },
            {
              "id": 7,
              "value": 0.6922291760710011
            },
            {
              "id": 8,
              "value": 0.6608894161906974
            },
            {
              "id": 9,
              "value": 0.7539780233896721
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:34.078128",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0ef5d81b",
      "task_type": "data_pipeline",
      "description": "Step 1: Use [file_operations_reader] to read the raw data from a CSV file. Step 2: Use [data_processing_parser] to parse the raw data into a structured format. Step 3: Use [data_processing_validator] to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use [data_processing_filter] to selectively filter the validated data based on specified criteria. Step 5: Use [data_processing_aggregator] to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.7478163238233632
            },
            {
              "id": 1,
              "value": 0.5369352677131626
            },
            {
              "id": 2,
              "value": 0.28446521996088936
            },
            {
              "id": 3,
              "value": 0.20668618270239425
            },
            {
              "id": 4,
              "value": 0.5067847522096522
            },
            {
              "id": 5,
              "value": 0.9975096752559569
            },
            {
              "id": 6,
              "value": 0.05507109013677114
            },
            {
              "id": 7,
              "value": 0.974202594420306
            },
            {
              "id": 8,
              "value": 0.7218086736477791
            },
            {
              "id": 9,
              "value": 0.9106839979530512
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:36.461644",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0c5e1719",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser tool to parse the raw data retrieved from the CSV file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.6556570829790912
            },
            {
              "id": 1,
              "value": 0.5398417062299664
            },
            {
              "id": 2,
              "value": 0.5664469408630066
            },
            {
              "id": 3,
              "value": 0.6149941448913345
            },
            {
              "id": 4,
              "value": 0.7132936808853964
            },
            {
              "id": 5,
              "value": 0.7252739877136012
            },
            {
              "id": 6,
              "value": 0.960687544757441
            },
            {
              "id": 7,
              "value": 0.6569178571983723
            },
            {
              "id": 8,
              "value": 0.38666540021958307
            },
            {
              "id": 9,
              "value": 0.9135332166868648
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:39.001145",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_992f7e16",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a specified file format, such as CSV or JSON. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format, ensuring that the data is organized properly. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant information. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data, summarizing it as needed for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.012805156618351687
            },
            {
              "id": 1,
              "value": 0.2481448070440202
            },
            {
              "id": 2,
              "value": 0.6270541062058961
            },
            {
              "id": 3,
              "value": 0.9637729642028505
            },
            {
              "id": 4,
              "value": 0.32446393882268865
            },
            {
              "id": 5,
              "value": 0.042407462189654854
            },
            {
              "id": 6,
              "value": 0.4578707077590243
            },
            {
              "id": 7,
              "value": 0.7871166565457696
            },
            {
              "id": 8,
              "value": 0.9353054526265295
            },
            {
              "id": 9,
              "value": 0.40560977143541066
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:43.250259",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_107e66f7",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read raw data from a specified input file in CSV format. Step 2: Use the data_processing_parser tool to parse the raw data retrieved from the file into a structured format, extracting relevant fields. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant entries. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.5616043311971496
            },
            {
              "id": 1,
              "value": 0.08493984965131973
            },
            {
              "id": 2,
              "value": 0.16273022171770124
            },
            {
              "id": 3,
              "value": 0.3562588953810537
            },
            {
              "id": 4,
              "value": 0.04442118883518065
            },
            {
              "id": 5,
              "value": 0.8149426975449287
            },
            {
              "id": 6,
              "value": 0.9075375293755329
            },
            {
              "id": 7,
              "value": 0.5926203954936651
            },
            {
              "id": 8,
              "value": 0.6959733076374707
            },
            {
              "id": 9,
              "value": 0.515213427111471
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:46.582465",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9e9f19f8",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read the raw data from a specified file format (e.g., CSV or JSON). Step 2: Use the 'data_processing_parser' tool to parse the raw data into a structured format, extracting relevant information. Step 3: Use the 'data_processing_validator' tool to validate the parsed data against a predefined schema, ensuring its correctness and integrity. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specific criteria to reduce the dataset size. Step 5: Finally, use the 'data_processing_aggregator' tool to aggregate the filtered data for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.6704591864800341
            },
            {
              "id": 1,
              "value": 0.5526072494201643
            },
            {
              "id": 2,
              "value": 0.5252675741251367
            },
            {
              "id": 3,
              "value": 0.9038367370696777
            },
            {
              "id": 4,
              "value": 0.5004425840994622
            },
            {
              "id": 5,
              "value": 0.4859544294964876
            },
            {
              "id": 6,
              "value": 0.01821129065573268
            },
            {
              "id": 7,
              "value": 0.9997579486576624
            },
            {
              "id": 8,
              "value": 0.28318680194692536
            },
            {
              "id": 9,
              "value": 0.2758547526932962
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "medium"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:49.565430",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5765becc",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read the raw data file, which could be in formats like CSV, JSON, or XML. Step 2: Use the 'data_processing_parser' tool to parse the raw data retrieved from the file into a structured format. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specified criteria to reduce the dataset size. Step 5: Finally, use the 'data_processing_aggregator' tool to aggregate the filtered data to generate summarized insights.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.49691276960356234
            },
            {
              "id": 1,
              "value": 0.7489701922423163
            },
            {
              "id": 2,
              "value": 0.16011185021467256
            },
            {
              "id": 3,
              "value": 0.3442836332429665
            },
            {
              "id": 4,
              "value": 0.5177600679304146
            },
            {
              "id": 5,
              "value": 0.14466723921266844
            },
            {
              "id": 6,
              "value": 0.5502598694792785
            },
            {
              "id": 7,
              "value": 0.6431497587569799
            },
            {
              "id": 8,
              "value": 0.9436326113952012
            },
            {
              "id": 9,
              "value": 0.31071490858320794
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 5.821800895370152
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:53.551585",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_b085ca49",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to the specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:16.599781",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f67b9526",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data obtained from the API into a structured format. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:19.438561",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_7a4b6557",
      "task_type": "api_integration",
      "description": "Step 1: Use the 'network_fetcher' tool to retrieve data from a specified API endpoint. Step 2: Use the 'data_processing_parser' tool to parse the raw data retrieved from the API into a structured format, such as JSON. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_transformer' tool to transform the validated data into another format, such as XML, if required. Step 5: Finally, use the 'network_poster' tool to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 9.980823339842582
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:22.325818",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_81c90991",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format (e.g., JSON to XML). Step 5: Use network_poster to send the transformed data to the specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:28.373069",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_85ef2a27",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the retrieved raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a defined schema to ensure its correctness. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 6.481150883294674
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:30.641306",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_cd68f69d",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a different format if necessary. Step 5: Use network_poster to send the final transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:32.891543",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a0c8af27",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data received from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a defined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a different format as required. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:35.653261",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_96f8c339",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format (e.g., from JSON to XML). Step 5: Use network_poster to send the transformed data to the specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 6.183947328910699
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:39.668372",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_cad400ea",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format (e.g., converting JSON data to XML). Step 5: Use network_poster to send the transformed data to the specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:48.380150",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8b2c2246",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher to retrieve data from the specified API endpoint. Step 2: Use the data_processing_parser to parse the raw data received from the API into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_transformer to transform the validated data into the desired format (e.g., converting JSON to XML). Step 5: Use the network_poster to send the transformed data to the specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:54.357725",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_6174c64b",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a specified file. Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria for further analysis.",
      "inputs": {
        "input_data": {
          "data": [
            0.8660421102771055,
            0.1760086094447606,
            0.9471702783529767,
            0.36461826162821676,
            0.31472928179684934
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:17.147447",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_cac4fa08",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data file containing information in CSV format. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.6269510180632775,
            0.2668404875545286,
            0.004402339390159438,
            0.6513372354674537,
            0.057971608287606946
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:19.803211",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c08bc6a8",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.9752513126618356,
            0.8212878994870122,
            0.7728387437677745,
            0.591462309431193,
            0.794440150941702
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:22.463728",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a0b8ffd1",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read a CSV file containing raw data. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.17963729102598514,
            0.6658298089564649,
            0.8792831196883583,
            0.025719376068673472,
            0.05358174697327345
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:25.843035",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a69888aa",
      "task_type": "basic_task",
      "description": "Step 1: Use the 'file_operations_reader' to read the data from a CSV file. Step 2: Use the 'data_processing_parser' to parse the raw data from the CSV into a structured format. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' to selectively filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.13054488540081532,
            0.948180046518577,
            0.5153986510058958,
            0.12464768554034378,
            0.04513152496197281
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 3.7491520975051444
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:36.156285",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_81e8663e",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a specified input file (e.g., a CSV file). Step 2: Use data_processing_parser to parse the raw data retrieved from the file into a structured format (e.g., converting CSV data into a structured table). Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria (e.g., keeping only rows that meet certain conditions). Step 5: Use data_processing_transformer to convert the filtered data into a desired output format (e.g., from JSON to XML).",
      "inputs": {
        "input_data": {
          "data": [
            0.30555542304122674,
            0.002114643839929209,
            0.7412130682229433,
            0.9874391746545433,
            0.4051730014860617
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 9.028183875546928
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:41.049049",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d0abf866",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a CSV file. Step 2: Use data_processing_parser to parse the raw data from the CSV into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.5426174372922056,
            0.31795466664205074,
            0.13998441342454648,
            0.49452528753119607,
            0.37868096762987247
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:43.388764",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_44f99f94",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader to read data from a CSV file. Step 2: Use the data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.7053482881666145,
            0.5570816890568944,
            0.6565801265259396,
            0.02670292870690305,
            0.7068956691407011
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:48.080407",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_33b16957",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read a CSV file containing raw data. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.5456611943884419,
            0.09920743429018908,
            0.5900889177358425,
            0.024330244088320074,
            0.42056920933521114
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:51.914125",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_76dd324a",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a specified input file (e.g., CSV, JSON). Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a defined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_transformer to transform the filtered data into a desired output format (e.g., convert it from JSON to XML).",
      "inputs": {
        "input_data": {
          "data": [
            0.046621093685497716,
            0.845226913398233,
            0.5441978847187429,
            0.2488081563039931,
            0.19942724615189555
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:55.788987",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0160b575",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to convert the validated data into a different format if required. Step 5: Use network_poster to send the transformed data to a specified destination endpoint.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 8.053761314764358
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:24.153630",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_02d4a0ee",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher to retrieve data from a specified API endpoint. Step 2: Use the data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: If the data is valid, use the network_poster to send the validated data to a specified destination over the network. Step 5: Finally, use the file_operations_scanner to scan any relevant files for patterns or anomalies, ensuring the overall data quality is maintained.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "network_poster",
        "file_operations_scanner"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:26.880132",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9e0c781b",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher to retrieve data from an external API endpoint. Step 2: Use the data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_transformer to convert the validated data into a different format, such as transforming JSON data into XML format. Step 5: Use the network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:30.176118",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_cb18d8cd",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data received from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a different format as required by the destination. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:34.999788",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_22c50113",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from an external API endpoint. Step 2: Use data_processing_parser to parse the raw data obtained from the API into a structured format. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to transform the validated data from JSON format to XML format. Step 5: Use network_poster to send the transformed XML data to a specified destination endpoint over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:37.130850",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_7200d216",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a different format if necessary. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:39.905967",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_21b21c24",
      "task_type": "api_integration",
      "description": "Step 1: Use the network_fetcher to retrieve raw data from the specified API endpoint. Step 2: Use the data_processing_parser to parse the retrieved raw data into a structured format. Step 3: Use the data_processing_validator to validate the structured data against the defined schema to ensure its correctness. Step 4: If the data is valid, use the network_poster to send the validated data to the specified destination endpoint. Step 5: If needed, use the data_processing_transformer to convert the data into a different format before posting it.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "network_poster",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 2.023320911967598
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:44.278979",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f3c2360f",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into the desired output format. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 4.537677107339409,
        "priority": "medium"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:51.716239",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_fa11cc7f",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from a specified API endpoint. Step 2: Use data_processing_parser to parse the raw data retrieved into a structured format. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_transformer to transform the validated data into a different format if necessary. Step 5: Use network_poster to send the transformed data to a specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:54.286846",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_2da1f9e9",
      "task_type": "api_integration",
      "description": "Step 1: Use network_fetcher to retrieve data from the specified API source. Step 2: Use data_processing_parser to parse the raw data retrieved from the API into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_transformer to transform the validated data into a different format as required. Step 5: Use network_poster to send the transformed data to the specified destination over the network.",
      "inputs": {
        "api_endpoints": [
          "https://api.example.com/data",
          "https://api.example.com/submit"
        ],
        "auth_credentials": {
          "api_key": "sample_key_123",
          "secret": "sample_secret"
        },
        "validation_schema": {
          "type": "object",
          "required": [
            "id",
            "value"
          ]
        }
      },
      "expected_outputs": {
        "api_response": {
          "status": 200,
          "data": {}
        },
        "validated_data": {
          "valid": true,
          "errors": []
        }
      },
      "required_tools": [
        "network_fetcher",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_transformer",
        "network_poster"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "api_integration",
        "generated_at": "2025-07-09T00:25:57.142440",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_188774d7",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a specified file format (CSV, JSON, or XML). Step 2: Use the data_processing_parser tool to parse the raw data into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the valid data based on specified criteria. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data, providing summarized insights.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.5692240406770751
            },
            {
              "id": 1,
              "value": 0.2476448018546915
            },
            {
              "id": 2,
              "value": 0.2047737816563252
            },
            {
              "id": 3,
              "value": 0.10161833597183079
            },
            {
              "id": 4,
              "value": 0.4404976206283925
            },
            {
              "id": 5,
              "value": 0.4439161305598823
            },
            {
              "id": 6,
              "value": 0.44605026621999533
            },
            {
              "id": 7,
              "value": 0.6587355619890308
            },
            {
              "id": 8,
              "value": 0.9616651395991713
            },
            {
              "id": 9,
              "value": 0.2811889232326611
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 1.7230851775695428,
        "priority": "medium"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:19.322854",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_63986edb",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read and retrieve raw data from a specified CSV file. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Finally, use the data_processing_aggregator tool to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.6301914955612236
            },
            {
              "id": 1,
              "value": 0.5503695120667973
            },
            {
              "id": 2,
              "value": 0.08538346482951109
            },
            {
              "id": 3,
              "value": 0.14580778670752215
            },
            {
              "id": 4,
              "value": 0.1080798387339168
            },
            {
              "id": 5,
              "value": 0.5433953664417215
            },
            {
              "id": 6,
              "value": 0.7488789140868122
            },
            {
              "id": 7,
              "value": 0.267574570450253
            },
            {
              "id": 8,
              "value": 0.40529105654634434
            },
            {
              "id": 9,
              "value": 0.7229359085999445
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:22.220565",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_78fdf149",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a specified source file (e.g., CSV or JSON). Step 2: Use the data_processing_parser tool to parse the raw data into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.15911756632629226
            },
            {
              "id": 1,
              "value": 0.4365105086722545
            },
            {
              "id": 2,
              "value": 0.6270811161035917
            },
            {
              "id": 3,
              "value": 0.532700987170416
            },
            {
              "id": 4,
              "value": 0.7474113678730413
            },
            {
              "id": 5,
              "value": 0.41631694368215166
            },
            {
              "id": 6,
              "value": 0.31716088913414664
            },
            {
              "id": 7,
              "value": 0.17251736321537015
            },
            {
              "id": 8,
              "value": 0.8041649757518335
            },
            {
              "id": 9,
              "value": 0.4541831236594813
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:25.263700",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_3fb0ac2b",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from a specified file format (e.g., CSV or JSON). This tool will retrieve the data and prepare it for processing. Step 2: Use the data_processing_parser to parse the raw data into a structured format. This tool will extract relevant information from the raw data and organize it. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. This step checks whether the data meets the required standards. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria (e.g., specific values or ranges). This tool will reduce the dataset to only the necessary information. Step 5: Use the data_processing_aggregator to aggregate the filtered data based on certain metrics or dimensions, providing a summarized view of the data.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.40653542567164236
            },
            {
              "id": 1,
              "value": 0.26184818357122863
            },
            {
              "id": 2,
              "value": 0.07290182963816894
            },
            {
              "id": 3,
              "value": 0.9613743485416171
            },
            {
              "id": 4,
              "value": 0.06781771772872414
            },
            {
              "id": 5,
              "value": 0.3511404101935329
            },
            {
              "id": 6,
              "value": 0.21518371378515788
            },
            {
              "id": 7,
              "value": 0.5331766550984823
            },
            {
              "id": 8,
              "value": 0.9339026100541864
            },
            {
              "id": 9,
              "value": 0.8073698385111922
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "medium"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:28.785664",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_320882ec",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser to parse the retrieved raw data into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_aggregator to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.9467210734863465
            },
            {
              "id": 1,
              "value": 0.38603876017598293
            },
            {
              "id": 2,
              "value": 0.8172538293982438
            },
            {
              "id": 3,
              "value": 0.31273566796462104
            },
            {
              "id": 4,
              "value": 0.05182184729461348
            },
            {
              "id": 5,
              "value": 0.48840736611256597
            },
            {
              "id": 6,
              "value": 0.1953978684810489
            },
            {
              "id": 7,
              "value": 0.5179614691683038
            },
            {
              "id": 8,
              "value": 0.03686396828211136
            },
            {
              "id": 9,
              "value": 0.6911438661062268
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 8.727596909185875
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:34.213460",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8738490b",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' to read raw data from a specified file in CSV format. Step 2: Use the 'data_processing_parser' to parse the raw data from the CSV file into a structured format, making it easier to work with. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' to selectively filter the validated data based on specified criteria to reduce the dataset size. Step 5: Use the 'data_processing_aggregator' to aggregate the filtered data for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.8847808763853374
            },
            {
              "id": 1,
              "value": 0.8884112865968116
            },
            {
              "id": 2,
              "value": 0.2962579825039906
            },
            {
              "id": 3,
              "value": 0.5533244579947728
            },
            {
              "id": 4,
              "value": 0.833957072408245
            },
            {
              "id": 5,
              "value": 0.18219876591152917
            },
            {
              "id": 6,
              "value": 0.4186891500786529
            },
            {
              "id": 7,
              "value": 0.010011048879072781
            },
            {
              "id": 8,
              "value": 0.7907524333997888
            },
            {
              "id": 9,
              "value": 0.1313091713308805
            }
          ]
        },
        "output_format": "xml",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 0.40440847107310085,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:39.486234",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_92ad2242",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read the raw data from a CSV file. This will retrieve the data and prepare it for processing. Step 2: Use the 'data_processing_parser' tool to parse the raw data from the CSV file into a structured format, making it easier to work with. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema, ensuring that the data integrity is maintained. Step 4: Use the 'data_processing_filter' tool to filter the validated data based on specified criteria, allowing us to focus on relevant subsets of the data. Step 5: Finally, use the 'data_processing_aggregator' tool to aggregate the filtered data, providing summarized insights for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.9576910367226421
            },
            {
              "id": 1,
              "value": 0.7425118721159077
            },
            {
              "id": 2,
              "value": 0.6980461346360473
            },
            {
              "id": 3,
              "value": 0.38024302703635127
            },
            {
              "id": 4,
              "value": 0.11473692339238084
            },
            {
              "id": 5,
              "value": 0.701317966303751
            },
            {
              "id": 6,
              "value": 0.30484772914881664
            },
            {
              "id": 7,
              "value": 0.8793214645893431
            },
            {
              "id": 8,
              "value": 0.019218819216152316
            },
            {
              "id": 9,
              "value": 0.1866289966076483
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:43.131999",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_bce3170b",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from a specified file (e.g., CSV or JSON format). Step 2: Use the data_processing_parser to parse the raw data into a structured format for further processing. Step 3: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria to reduce the dataset size. Step 5: Use the data_processing_aggregator to aggregate the filtered data for final analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.1531258151721112
            },
            {
              "id": 1,
              "value": 0.25333447628201156
            },
            {
              "id": 2,
              "value": 0.4984375389248682
            },
            {
              "id": 3,
              "value": 0.09642440092715554
            },
            {
              "id": 4,
              "value": 0.2504200078764832
            },
            {
              "id": 5,
              "value": 0.8809954539918718
            },
            {
              "id": 6,
              "value": 0.8268798835092492
            },
            {
              "id": 7,
              "value": 0.4651480367446268
            },
            {
              "id": 8,
              "value": 0.36453239781391056
            },
            {
              "id": 9,
              "value": 0.8498781039199483
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:49.805002",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_53a3a05f",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from a CSV file. Step 2: Use the data_processing_parser to parse the raw data into a structured format, extracting the relevant fields. Step 3: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to filter the validated data based on specific criteria to reduce the dataset to only the necessary entries. Step 5: Use the data_processing_aggregator to aggregate the filtered data, summarizing key metrics or statistics as needed.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.9124871095132815
            },
            {
              "id": 1,
              "value": 0.2897611442630409
            },
            {
              "id": 2,
              "value": 0.07292304071212852
            },
            {
              "id": 3,
              "value": 0.14373753215894036
            },
            {
              "id": 4,
              "value": 0.8358724191656748
            },
            {
              "id": 5,
              "value": 0.27228098908650566
            },
            {
              "id": 6,
              "value": 0.46563068635353266
            },
            {
              "id": 7,
              "value": 0.350902816840019
            },
            {
              "id": 8,
              "value": 0.21542250525635032
            },
            {
              "id": 9,
              "value": 0.8536141212321692
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 7.803574078579115
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:53.113251",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_aea5a296",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' to read the raw data files in CSV format from the specified directory. Step 2: Use the 'data_processing_parser' to parse the raw data retrieved in the previous step into a structured format. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the 'data_processing_filter' to filter the validated data based on specified criteria, retaining only the relevant records. Step 5: Use the 'data_processing_aggregator' to aggregate the filtered data based on defined metrics, preparing it for analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.14060506659570104
            },
            {
              "id": 1,
              "value": 0.27167591143376113
            },
            {
              "id": 2,
              "value": 0.2032787155289596
            },
            {
              "id": 3,
              "value": 0.6096462804566762
            },
            {
              "id": 4,
              "value": 0.14988695650910744
            },
            {
              "id": 5,
              "value": 0.014141762722117424
            },
            {
              "id": 6,
              "value": 0.45518029738201693
            },
            {
              "id": 7,
              "value": 0.5234273936175717
            },
            {
              "id": 8,
              "value": 0.21440419392386834
            },
            {
              "id": 9,
              "value": 0.5169498537256125
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:57.994782",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f6baf36c",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read and retrieve data from the input file, which could be in CSV, JSON, or XML format. Step 2: Use data_processing_parser to parse the raw data into a structured format for easier processing. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria to reduce the dataset to relevant entries. Step 5: Use data_processing_aggregator to aggregate the filtered data for summarization or further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.9149223948544691
            },
            {
              "id": 1,
              "value": 0.6297646149612873
            },
            {
              "id": 2,
              "value": 0.4447161020099639
            },
            {
              "id": 3,
              "value": 0.837016772473546
            },
            {
              "id": 4,
              "value": 0.18324401727743034
            },
            {
              "id": 5,
              "value": 0.3627863148360426
            },
            {
              "id": 6,
              "value": 0.3185088373400514
            },
            {
              "id": 7,
              "value": 0.9419046113954194
            },
            {
              "id": 8,
              "value": 0.6658016547001269
            },
            {
              "id": 9,
              "value": 0.1571563390649764
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "high"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:19.158698",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8468a0c7",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read raw data from a specified file in CSV format. Step 2: Use the data_processing_parser tool to parse the raw CSV data into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specific criteria. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.4441471960410279
            },
            {
              "id": 1,
              "value": 0.8633505504427961
            },
            {
              "id": 2,
              "value": 0.5706107341554957
            },
            {
              "id": 3,
              "value": 0.9314035790050424
            },
            {
              "id": 4,
              "value": 0.018200678233158407
            },
            {
              "id": 5,
              "value": 0.19585627181060095
            },
            {
              "id": 6,
              "value": 0.23564676276535268
            },
            {
              "id": 7,
              "value": 0.276050914677925
            },
            {
              "id": 8,
              "value": 0.45418923194760785
            },
            {
              "id": 9,
              "value": 0.7563269434178042
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:25.678657",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e670887d",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read the raw data from the specified input file (e.g., CSV or JSON). Step 2: Use data_processing_parser to parse the raw data into a structured format suitable for processing. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria to reduce the dataset size. Step 5: Use data_processing_aggregator to aggregate the filtered data for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.457792571092962
            },
            {
              "id": 1,
              "value": 0.31264841337199656
            },
            {
              "id": 2,
              "value": 0.7659478021352665
            },
            {
              "id": 3,
              "value": 0.18492261627542972
            },
            {
              "id": 4,
              "value": 0.6903761772191231
            },
            {
              "id": 5,
              "value": 0.9592804054488094
            },
            {
              "id": 6,
              "value": 0.47361671283690565
            },
            {
              "id": 7,
              "value": 0.14884567601303123
            },
            {
              "id": 8,
              "value": 0.3465158535862979
            },
            {
              "id": 9,
              "value": 0.7836292365723919
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:29.074419",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0516ceb5",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from a specified file format (e.g., CSV, JSON). Step 2: Use the data_processing_parser to parse the raw data into a structured format for easier manipulation. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Finally, use the data_processing_aggregator to aggregate the filtered data for further analysis or reporting.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.8154907756196929
            },
            {
              "id": 1,
              "value": 0.6676392764097476
            },
            {
              "id": 2,
              "value": 0.16420784010135214
            },
            {
              "id": 3,
              "value": 0.7766765472957051
            },
            {
              "id": 4,
              "value": 0.8505257063421048
            },
            {
              "id": 5,
              "value": 0.8002743049387521
            },
            {
              "id": 6,
              "value": 0.2565212976716921
            },
            {
              "id": 7,
              "value": 0.7493727762219187
            },
            {
              "id": 8,
              "value": 0.8001820604384231
            },
            {
              "id": 9,
              "value": 0.24151697231878622
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "max_cost": 1.5230280585267917,
        "priority": "high"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:34.697230",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_164454cf",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read the raw data from input files in various formats (CSV, JSON, XML). Step 2: Use data_processing_parser to parse the raw data into a structured format for further processing. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria to reduce the dataset size. Step 5: Use data_processing_aggregator to aggregate the filtered data for summarization or further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.03834867205750869
            },
            {
              "id": 1,
              "value": 0.6312436480134612
            },
            {
              "id": 2,
              "value": 0.7114562927255865
            },
            {
              "id": 3,
              "value": 0.7027110253045209
            },
            {
              "id": 4,
              "value": 0.6552179197527328
            },
            {
              "id": 5,
              "value": 0.5257566123460783
            },
            {
              "id": 6,
              "value": 0.26825056917676293
            },
            {
              "id": 7,
              "value": 0.9843878091592362
            },
            {
              "id": 8,
              "value": 0.6126910693650103
            },
            {
              "id": 9,
              "value": 0.7204087572410454
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:37.437047",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_02735a67",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read and retrieve raw data from a specified file format, such as CSV or JSON. Step 2: Use the data_processing_parser to parse the raw data into a structured format, extracting relevant fields as necessary. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant entries. Step 5: Use the data_processing_aggregator to aggregate the filtered data, summarizing it as required for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.8560765646678828
            },
            {
              "id": 1,
              "value": 0.9130475976839477
            },
            {
              "id": 2,
              "value": 0.6784177915772551
            },
            {
              "id": 3,
              "value": 0.6855449986672777
            },
            {
              "id": 4,
              "value": 0.5502611939252108
            },
            {
              "id": 5,
              "value": 0.7347962057749341
            },
            {
              "id": 6,
              "value": 0.4651677309882858
            },
            {
              "id": 7,
              "value": 0.8010305192957108
            },
            {
              "id": 8,
              "value": 0.35058192068040195
            },
            {
              "id": 9,
              "value": 0.5109992740557816
            }
          ]
        },
        "output_format": "json",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:41.356988",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_614508f6",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read the raw data from a specified input file format (e.g., CSV, JSON). Step 2: Use data_processing_parser to parse the raw data into a structured format, ensuring the data is organized correctly. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria, reducing the dataset for further processing. Step 5: Finally, use data_processing_aggregator to aggregate the filtered data, summarizing the key insights or metrics as needed.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.5673755389012644
            },
            {
              "id": 1,
              "value": 0.677040081892301
            },
            {
              "id": 2,
              "value": 0.35826622459689583
            },
            {
              "id": 3,
              "value": 0.553875103995283
            },
            {
              "id": 4,
              "value": 0.0818587606590887
            },
            {
              "id": 5,
              "value": 0.692587873521823
            },
            {
              "id": 6,
              "value": 0.7703087321738592
            },
            {
              "id": 7,
              "value": 0.8701543013492804
            },
            {
              "id": 8,
              "value": 0.5237470257874668
            },
            {
              "id": 9,
              "value": 0.7879086923190869
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:46.383104",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_33e70b69",
      "task_type": "data_pipeline",
      "description": "Step 1: Use file_operations_reader to read raw data from a specified file format (e.g., CSV or JSON). Step 2: Use data_processing_parser to parse the raw data into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_aggregator to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.1280269220307405
            },
            {
              "id": 1,
              "value": 0.24314000103030053
            },
            {
              "id": 2,
              "value": 0.7178381212911791
            },
            {
              "id": 3,
              "value": 0.44754336349764234
            },
            {
              "id": 4,
              "value": 0.7064683299895166
            },
            {
              "id": 5,
              "value": 0.27294683488500504
            },
            {
              "id": 6,
              "value": 0.26934717202328773
            },
            {
              "id": 7,
              "value": 0.38580516534555964
            },
            {
              "id": 8,
              "value": 0.3094237995114837
            },
            {
              "id": 9,
              "value": 0.01289659135195309
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9,
        "priority": "low"
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:48.676853",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_3e55749c",
      "task_type": "data_pipeline",
      "description": "Step 1: Use `file_operations_reader` to read the raw data from a specified file (e.g., CSV or JSON format). Step 2: Use `data_processing_parser` to parse the raw data into a structured format, extracting the necessary fields for further processing. Step 3: Use `data_processing_validator` to validate the structured data against a predefined schema, ensuring it meets the required standards. Step 4: Use `data_processing_filter` to selectively filter the validated data based on specified criteria (e.g., removing invalid entries or filtering for specific values). Step 5: Use `data_processing_aggregator` to aggregate the filtered data, generating summary statistics or combining data points as needed.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.006964277606939739
            },
            {
              "id": 1,
              "value": 0.00245521227595058
            },
            {
              "id": 2,
              "value": 0.8643229871261858
            },
            {
              "id": 3,
              "value": 0.5281753501262977
            },
            {
              "id": 4,
              "value": 0.49677901909668476
            },
            {
              "id": 5,
              "value": 0.41314631811735036
            },
            {
              "id": 6,
              "value": 0.28750771478431747
            },
            {
              "id": 7,
              "value": 0.755387778649837
            },
            {
              "id": 8,
              "value": 0.6985935877544404
            },
            {
              "id": 9,
              "value": 0.05516990945626565
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:54.124319",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_2b41da05",
      "task_type": "data_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data file (in CSV format) and retrieve its contents. Step 2: Use the data_processing_parser to parse the retrieved raw data into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_aggregator to aggregate the filtered data for further analysis.",
      "inputs": {
        "raw_data": {
          "records": [
            {
              "id": 0,
              "value": 0.04668614239294433
            },
            {
              "id": 1,
              "value": 0.8989029206204459
            },
            {
              "id": 2,
              "value": 0.7826646786595739
            },
            {
              "id": 3,
              "value": 0.2008583577487918
            },
            {
              "id": 4,
              "value": 0.30862772749263234
            },
            {
              "id": 5,
              "value": 0.9558443303777792
            },
            {
              "id": 6,
              "value": 0.8670667832079235
            },
            {
              "id": 7,
              "value": 0.23663484481653219
            },
            {
              "id": 8,
              "value": 0.6564810538223297
            },
            {
              "id": 9,
              "value": 0.08842031126222172
            }
          ]
        },
        "output_format": "csv",
        "transformation_rules": {
          "normalize": true,
          "format": "json"
        }
      },
      "expected_outputs": {
        "parsed_data": {
          "structured": true,
          "record_count": 10
        },
        "transformed_data": {
          "format": "normalized",
          "records": []
        },
        "output_file": {
          "path": "/output/result.json",
          "size": 1024
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 300,
        "max_retries": 2,
        "required_confidence": 0.9
      },
      "complexity": "medium",
      "metadata": {
        "template": "data_pipeline",
        "generated_at": "2025-07-09T00:25:58.274620",
        "timeout": 300,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d80fe1df",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser to parse the raw data from the CSV into a structured JSON format. Step 3: Use the data_processing_validator to validate the structured JSON data against a predefined schema to ensure data integrity. Step 4: Use the data_processing_filter to filter the validated data based on specific criteria (e.g., removing entries that do not meet certain thresholds). Step 5: Use the data_processing_transformer to convert the filtered JSON data into XML format for further processing. Step 6: Use the file_operations_writer to write the final XML output to a specified file.",
      "inputs": {
        "input_data": {
          "data": [
            0.2820431512794638,
            0.9451858582600312,
            0.43368583717659137,
            0.6765864464455483,
            0.10968715463056156
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:18.704479",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a1766979",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from a specified CSV file. This will retrieve the data for further processing. Step 2: Use data_processing_parser to parse the raw data from the CSV format into a structured format, such as JSON. This step will help in organizing the data for analysis. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. This is essential for maintaining data quality. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria, which will help reduce the dataset to only the relevant entries. Step 5: Use computation_analyzer to analyze the filtered data, generating statistical insights and trend analysis. This will summarize the findings from the dataset. Step 6: Finally, use file_operations_writer to write the analyzed results into a new report file, saving the insights for future reference.",
      "inputs": {
        "input_data": {
          "data": [
            0.9920039952552306,
            0.3423186084086205,
            0.07802085473697762,
            0.10791217643786266,
            0.30507212972181996
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "computation_analyzer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "priority": "low"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:23.987862",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1497fae2",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser to parse the raw data into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specific criteria to reduce the dataset to relevant entries. Step 5: Use the data_processing_transformer to convert the filtered data into JSON format for further processing. Step 6: Finally, use the file_operations_writer to write the transformed JSON data to a new file for storage.",
      "inputs": {
        "input_data": {
          "data": [
            0.058401256307446414,
            0.8092942399340617,
            0.6404045275423369,
            0.7127262816519461,
            0.0021833334123190218
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "priority": "medium"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:28.241978",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_b71120e4",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from a CSV file containing information about customer transactions. Step 2: Use the data_processing_parser to parse the retrieved CSV data into a structured JSON format for easier processing. Step 3: Use the data_processing_validator to validate the structured JSON data against a predefined schema to ensure data integrity and correctness. Step 4: Use the data_processing_filter to selectively filter the validated data based on specific criteria, such as transactions over a certain amount. Step 5: Use the data_processing_aggregator to aggregate the filtered data to compute the total transaction amounts per customer. Step 6: Use the file_operations_writer to write the aggregated results back into a new JSON file for reporting purposes.",
      "inputs": {
        "input_data": {
          "data": [
            0.3871470164111033,
            0.5583121026590456,
            0.895305972334919,
            0.8625090515814703,
            0.05738406271551966
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:33.911664",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_2e3de2c6",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read raw data from a specified input file in CSV format. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specific criteria, reducing the dataset to only the relevant entries. Step 5: Use the data_processing_transformer tool to transform the filtered data from the structured format into JSON format. Step 6: Use the file_operations_writer tool to write the transformed JSON data to an output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.24598285102391482,
            0.49999818936760565,
            0.4278954618395159,
            0.9063033382965097,
            0.0809320052471828
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "priority": "medium"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:38.392834",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_efc5d140",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read and retrieve data from a CSV file that contains raw data. Step 2: Use the data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_transformer to transform the filtered data into JSON format for further analysis. Step 6: Use the computation_analyzer to analyze the transformed JSON data and generate statistical insights and trend analysis.",
      "inputs": {
        "input_data": {
          "data": [
            0.11089239462707268,
            0.5189180910370046,
            0.23516906316346087,
            0.3359896775955473,
            0.22060344319582958
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "computation_analyzer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 9.278697761862244
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:42.263423",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5ef37ed9",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser to parse the raw data from the CSV file into a structured format (e.g., JSON). Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria (e.g., removing entries with null values). Step 5: Use the data_processing_aggregator to aggregate the filtered data for further analysis (e.g., summing a specific column). Step 6: Use the file_operations_writer to write the aggregated results to a new output CSV file.",
      "inputs": {
        "input_data": {
          "data": [
            0.9218104922045808,
            0.748207781728167,
            0.1051657007678537,
            0.16046557286905638,
            0.5966216781904903
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 2.4228034782903487
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:46.181349",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_37d8f0b4",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from a CSV file. Step 2: Use the data_processing_parser to parse the raw data into a structured format (e.g., JSON). Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria (e.g., removing entries that do not meet certain thresholds). Step 5: Use the data_processing_aggregator to aggregate the filtered data for further analysis. Step 6: Use the file_operations_writer to write the aggregated data back to a new CSV file for storage.",
      "inputs": {
        "input_data": {
          "data": [
            0.02847100628827226,
            0.483086096464594,
            0.3962447402481425,
            0.8218845503552824,
            0.5624285030193279
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "priority": "medium"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:51.434743",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_871ecb8f",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use file_operations_reader to read the raw data from a CSV file that contains user information. Step 2: Use data_processing_parser to parse the raw CSV data into a structured format, extracting relevant fields such as user ID, name, and email. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure that all required fields are present and correctly formatted. Step 4: Use data_processing_filter to selectively filter out any invalid records based on the validation results, ensuring that only correct entries remain. Step 5: Use data_processing_transformer to convert the filtered structured data from JSON format to XML format, preparing it for output. Step 6: Finally, use file_operations_writer to write the transformed XML data back into a new XML file for further use.",
      "inputs": {
        "input_data": {
          "data": [
            0.35217548256246034,
            0.22524295061285848,
            0.041738685916883345,
            0.4817754921489117,
            0.15596961024106648
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:55.741642",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_56275063",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from a CSV file. Step 2: Use the data_processing_parser to parse the raw data into a structured format like JSON. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria to reduce the dataset to only the relevant entries. Step 5: Use the data_processing_transformer to transform the filtered data from JSON format to XML format for further processing. Step 6: Use the file_operations_writer to write the transformed XML data into a new file for storage or further use.",
      "inputs": {
        "input_data": {
          "data": [
            0.09776533558671407,
            0.6068652642070684,
            0.8141656602569705,
            0.3290566321084868,
            0.2624442675456866
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "priority": "medium"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:59.036399",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9c31b9de",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from a raw CSV file containing sales data. Step 2: Use the data_processing_parser to parse the raw CSV data into a structured format (e.g., JSON). Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to filter the validated data based on specific criteria, such as sales above a certain threshold. Step 5: Use the data_processing_aggregator to aggregate the filtered data, calculating total sales per product category. Step 6: Finally, use the file_operations_writer to write the aggregated results into a new CSV file for reporting purposes.",
      "inputs": {
        "input_data": {
          "data": [
            0.20509108447901003,
            0.40357065930388314,
            0.33355928431035253,
            0.42084770163477314,
            0.7796606355912457
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "priority": "low"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:18.738670",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_ad43ed89",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a specified input file in CSV format. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format (like JSON) for easier manipulation. Step 3: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria to reduce the dataset size. Step 5: Use the data_processing_transformer tool to transform the filtered data from JSON format to XML format, preparing it for the next step. Step 6: Finally, use the file_operations_writer tool to write the transformed XML data to an output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.26014933039627164,
            0.6620734579316413,
            0.1994589824188192,
            0.5207296552731594,
            0.051183854022260555
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:23.285723",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_be16d5c2",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use file_operations_reader to read data from a specified input file in CSV format. Step 2: Use data_processing_parser to parse the raw CSV data into a structured JSON format. Step 3: Use data_processing_validator to validate the structured JSON data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated JSON data based on specific criteria, such as removing entries that do not meet certain conditions. Step 5: Use data_processing_transformer to convert the filtered JSON data into XML format for output. Step 6: Use file_operations_writer to write the transformed XML data to a specified output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.5056860184191988,
            0.8731889213484924,
            0.859229241148528,
            0.8851358382565977,
            0.46714033836683244
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:27.610360",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_439df8c8",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser to parse the raw data into a structured JSON format for easier handling. Step 3: Use the data_processing_validator to validate the structured JSON data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter to selectively filter the validated data based on specific criteria, reducing the dataset to only relevant entries. Step 5: Use the computation_analyzer to analyze the filtered data and generate statistical insights, including trend analysis. Step 6: Finally, use the file_operations_writer to write the analysis results to a new JSON file for reporting purposes.",
      "inputs": {
        "input_data": {
          "data": [
            0.32503207325525907,
            0.9060487279624685,
            0.20822844445501476,
            0.6376700805505159,
            0.874267419026347
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "computation_analyzer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:32.980984",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_956ca559",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a specified input file in CSV format. Step 2: Use the data_processing_parser tool to parse the raw CSV data into a structured format (e.g., JSON). Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant entries. Step 5: Use the computation_calculator tool to perform mathematical calculations on the filtered data, such as calculating averages or sums of specific fields. Step 6: Use the file_operations_writer tool to write the final processed results into a new output file in JSON format.",
      "inputs": {
        "input_data": {
          "data": [
            0.26830454247662294,
            0.22415641432530364,
            0.2328095542396107,
            0.12574936704768058,
            0.8027542891504854
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "computation_calculator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 5.531672546738558
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:38.108448",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_2ee4a4d8",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a specified input file in CSV format. This will retrieve the raw data needed for processing. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format. This step will convert the CSV data into a JSON format for easier manipulation. Step 3: Use the data_processing_validator tool to validate the structured JSON data against a predefined schema to ensure its correctness and integrity. Step 4: If the data is valid, use the data_processing_filter tool to selectively filter the JSON data based on specific criteria, ensuring only the relevant data is retained. Step 5: Finally, use the file_operations_writer tool to write the filtered JSON data to a new output file, saving the processed results for future use.",
      "inputs": {
        "input_data": {
          "data": [
            0.4017155373219574,
            0.486569009586027,
            0.26169076251135404,
            0.8610877130598374,
            0.42591424600711625
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 3.4186709568946902,
        "priority": "medium"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:42.457946",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4526d46a",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read raw data from a specified CSV file. Step 2: Use the data_processing_parser to parse the raw CSV data into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter to filter the validated data based on specified criteria to reduce the dataset. Step 5: Use the data_processing_transformer to convert the filtered data from JSON format to XML format. Step 6: Use the file_operations_writer to write the transformed XML data to a new file.",
      "inputs": {
        "input_data": {
          "data": [
            0.2625940113688824,
            0.9573197978566912,
            0.20204596917622453,
            0.5681848474619993,
            0.2517806187071926
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 3.336786746644035
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:46.003197",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_ec4b0a84",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data file in CSV format and retrieve its contents. Step 2: Use the data_processing_parser to parse the retrieved CSV data into a structured format, such as JSON. Step 3: Use the data_processing_validator to validate the structured JSON data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specific criteria to reduce the dataset size. Step 5: Use the data_processing_aggregator to aggregate the filtered data for further analysis. Step 6: Use the file_operations_writer to write the aggregated data into a new output file in JSON format.",
      "inputs": {
        "input_data": {
          "data": [
            0.5710676268352255,
            0.32983242633213283,
            0.7937313250153678,
            0.6063996318364123,
            0.7631361838115391
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 6.185899722265738,
        "priority": "low"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:51.446111",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a9e571b2",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' to read the raw data from a specified file. This tool will support formats like CSV or JSON for retrieving data. Step 2: Use the 'data_processing_parser' to parse the retrieved raw data into a structured format. This tool will ensure the data is organized and ready for further processing. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema, ensuring that the data meets the required standards for accuracy and integrity. Step 4: Use the 'data_processing_filter' to apply specific criteria to the validated data, selectively filtering out any unnecessary information. Step 5: Use the 'data_processing_aggregator' to aggregate the filtered data, summarizing it for further insights or analysis. Step 6: Finally, use the 'file_operations_writer' to write the aggregated data back to a file in the desired format, ensuring that the results are stored for future use.",
      "inputs": {
        "input_data": {
          "data": [
            0.12691586642642683,
            0.5222564517720867,
            0.6701529383269915,
            0.03498869554242778,
            0.26590645890662135
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 1.473597254246639,
        "priority": "low"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:56.599907",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_21c5c17d",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read raw data from a CSV file containing sales transactions. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format, extracting relevant fields such as transaction ID, amount, and date. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure it meets the required standards. Step 4: Use the data_processing_filter tool to filter out any transactions that do not meet specific criteria, such as transactions below a certain amount or with missing information. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data, calculating total sales and number of transactions per day. Step 6: Use the file_operations_writer tool to write the aggregated results to a new CSV file for further analysis.",
      "inputs": {
        "input_data": {
          "data": [
            0.8140623766020533,
            0.5652035619044917,
            0.5059603563276021,
            0.015662054375529255,
            0.48595209581160304
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:26:00.366452",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_611e754c",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a source file (e.g., a CSV file) and retrieve the raw data. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format, extracting relevant fields and organizing the data. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, reducing the dataset size for further analysis. Step 5: Use the data_processing_transformer tool to transform the filtered data into a different format (e.g., from JSON to XML) for reporting purposes. Step 6: Finally, use the file_operations_writer tool to write the transformed data into a new output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.20861075504924664,
            0.006823436337690336,
            0.227076036540253,
            0.222980940747041,
            0.070373364714992
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "priority": "medium"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:19.270455",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_07a7f4d8",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a CSV file containing customer information. Step 2: Use the data_processing_parser tool to parse the raw CSV data into a structured format (e.g., JSON). Step 3: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure all required fields are present and correctly formatted. Step 4: Use the data_processing_filter tool to selectively filter the valid data based on specific criteria, such as only including customers from a certain region. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data to calculate the total number of customers per region. Step 6: Finally, use the file_operations_writer tool to write the aggregated results into a new CSV file for reporting purposes.",
      "inputs": {
        "input_data": {
          "data": [
            0.5997454182684226,
            0.5229429677363319,
            0.20965910249929254,
            0.5170869578854915,
            0.390657128025231
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:24.167633",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_2e97d281",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use file_operations_reader to read the raw data from a CSV file into a structured format. Step 2: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 3: Use data_processing_filter to selectively filter the valid data based on specified criteria (e.g., removing entries that do not meet certain conditions). Step 4: Use data_processing_transformer to convert the filtered data from CSV format to JSON format for easier manipulation. Step 5: Use computation_calculator to perform any necessary mathematical calculations on the transformed JSON data, such as calculating averages or sums. Step 6: Finally, use file_operations_writer to write the processed and calculated data to a new JSON file for further use.",
      "inputs": {
        "input_data": {
          "data": [
            0.8018732420706146,
            0.13410604186278463,
            0.47864340276828643,
            0.12707089924212156,
            0.8113720146504116
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "computation_calculator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:28.633620",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_24aca89d",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use file_operations_reader to read and retrieve raw data from a source file in CSV format. Step 2: Use data_processing_parser to parse the raw CSV data into a structured format (e.g., JSON). Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria, keeping only the relevant entries. Step 5: Use data_processing_transformer to transform the filtered data from JSON format to XML format. Step 6: Use file_operations_writer to write the transformed XML data to a new output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.6711592691988387,
            0.8110147516253081,
            0.18193228766411784,
            0.18604589143485628,
            0.4793068201971237
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 6.153225955519424
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:33.746226",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d4cbbb4f",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a specified input file in CSV format. Step 2: Use the data_processing_parser tool to parse the raw data extracted from the input file into a structured format (e.g., JSON). Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specific criteria (e.g., removing entries with null values). Step 5: Use the data_processing_aggregator tool to aggregate the filtered data into a summary format. Step 6: Use the file_operations_writer tool to write the aggregated data into an output file in JSON format.",
      "inputs": {
        "input_data": {
          "data": [
            0.022442614808182215,
            0.7620459955666681,
            0.6437152869648107,
            0.6187285257812283,
            0.5273493315265081
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 5.7003490999504125
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:38.581326",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_ddfb4cdd",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured JSON format. Step 3: Use the data_processing_validator tool to validate the structured JSON data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated JSON data based on specified criteria, reducing the dataset to only the relevant information. Step 5: Use the data_processing_transformer tool to transform the filtered JSON data into XML format for further processing. Step 6: Use the file_operations_writer tool to write the transformed XML data to a new file for storage.",
      "inputs": {
        "input_data": {
          "data": [
            0.12037750346285203,
            0.8548115892117031,
            0.49601363292404654,
            0.13620797558552644,
            0.32647616575763416
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 6.235251538234687
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:42.739101",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a701bf17",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from a CSV file. Step 2: Use the data_processing_parser to parse the raw data into a structured JSON format. Step 3: Use the data_processing_validator to validate the structured JSON data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_transformer to convert the filtered data from JSON format to XML format. Step 6: Use the file_operations_writer to write the final XML formatted data to a new file.",
      "inputs": {
        "input_data": {
          "data": [
            0.4703095734528906,
            0.025092313139920996,
            0.15562118781819156,
            0.5306834881808209,
            0.3535379584048518
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "priority": "high"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:46.196672",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_6fc182d9",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use file_operations_reader to read data from a specified input file in CSV format. Step 2: Use data_processing_parser to parse the raw data into a structured format for further processing. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to filter the validated data based on specified criteria, reducing the dataset to the most relevant entries. Step 5: Use data_processing_transformer to convert the filtered data from its current format to JSON format for output. Step 6: Use file_operations_writer to write the transformed JSON data to a new output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.2590483839832298,
            0.6190190005297298,
            0.398664866042282,
            0.16776065556519315,
            0.5772792787737603
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:51.432728",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d50cf81e",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from a specified input file, such as a CSV or JSON file, and retrieve the raw data. Step 2: Use the data_processing_parser to parse the raw data from the previous step into a structured format that can be easily manipulated. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specific criteria to reduce the dataset size for further analysis. Step 5: Use the computation_analyzer to analyze the filtered data, generating statistical insights and trend analysis based on the remaining data. Step 6: Finally, use the file_operations_writer to write the analysis results to a specified output file, such as a new CSV or JSON file.",
      "inputs": {
        "input_data": {
          "data": [
            0.7294089772484859,
            0.5790198449647816,
            0.14276746353956293,
            0.4422346044812079,
            0.8630124205628239
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "computation_analyzer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 3.888140866002552
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:55.805579",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_caf4db46",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read raw data from a CSV file into a structured format. Step 2: Use the data_processing_parser tool to parse the raw data and convert it into a structured JSON format. Step 3: Use the data_processing_validator tool to validate the structured JSON data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, reducing the dataset size. Step 5: Use the data_processing_transformer tool to transform the filtered JSON data into XML format for further processing. Step 6: Use the file_operations_writer tool to write the transformed XML data to a new file for storage.",
      "inputs": {
        "input_data": {
          "data": [
            0.07884497455252593,
            0.10012494923273085,
            0.865285180423771,
            0.4613045512513074,
            0.0620273787913167
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 4.831938308488794
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:26:00.722022",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5ef10543",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a source file in CSV format. This will retrieve the raw data needed for processing. Step 2: Use the data_processing_parser tool to parse the raw CSV data into a structured format such as JSON. This will help in organizing the data for further processing. Step 3: Use the data_processing_validator tool to validate the structured JSON data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant entries. Step 5: Use the data_processing_transformer tool to transform the filtered data from JSON format to XML format. Step 6: Use the file_operations_writer tool to write the transformed XML data to an output file, completing the data processing pipeline.",
      "inputs": {
        "input_data": {
          "data": [
            0.9806051494398433,
            0.6417718743649432,
            0.5029230090518585,
            0.8229859741729284,
            0.9247076244583897
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:19.652457",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_b67d7743",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use file_operations_reader to read raw data from a CSV file. Step 2: Use data_processing_parser to parse the raw CSV data into a structured format (e.g., JSON). Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria (e.g., removing entries with null values). Step 5: Use data_processing_transformer to convert the filtered JSON data into XML format for output. Step 6: Use file_operations_writer to write the transformed XML data into a new file.",
      "inputs": {
        "input_data": {
          "data": [
            0.8944799857627761,
            0.19424282638057677,
            0.8898952156242513,
            0.17243422254812857,
            0.976889379149348
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 5.351501256587006
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:23.847597",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_2df9c151",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specific criteria. Step 5: Use the data_processing_transformer tool to transform the filtered data into a different format, such as converting it from JSON to XML. Step 6: Use the file_operations_writer tool to write the transformed data into a new file.",
      "inputs": {
        "input_data": {
          "data": [
            0.9557852349638365,
            0.4743117450639036,
            0.10695862780403742,
            0.8430079037375339,
            0.13533430416447767
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 8.584307316898455,
        "priority": "low"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:28.192961",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_ddf5412c",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from a CSV file containing user information. Step 2: Use the data_processing_parser to parse the raw CSV data into a structured JSON format. Step 3: Use the data_processing_validator to validate the structured JSON data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to filter the validated data based on specified criteria, such as age greater than 18. Step 5: Use the data_processing_aggregator to aggregate the filtered data to summarize user statistics, such as the total count of users by age group. Step 6: Finally, use the file_operations_writer to write the aggregated statistics to a new output file in JSON format.",
      "inputs": {
        "input_data": {
          "data": [
            0.19389244590768717,
            0.010857529076947259,
            0.12014738210833609,
            0.2794037224468059,
            0.7107090554478755
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 8.846590506628509
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:33.913426",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_02c89c5f",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use [file_operations_reader] to read the raw data from a CSV file into a structured format. Step 2: Use [data_processing_parser] to parse the read data and convert it into a structured JSON format. Step 3: Use [data_processing_validator] to validate the JSON data against a predefined schema to ensure its correctness and integrity. Step 4: Use [data_processing_filter] to selectively filter the validated JSON data based on specified criteria, reducing the dataset to only the relevant information. Step 5: Use [data_processing_transformer] to transform the filtered JSON data into XML format for further processing. Step 6: Use [file_operations_writer] to write the transformed XML data into a new output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.8477411484283921,
            0.07956571483656949,
            0.487974176096413,
            0.23898066179948196,
            0.09118108663601698
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:38.559028",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_3b1f2cfb",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file. This will retrieve the raw data that needs to be processed. Step 2: Use the data_processing_parser tool to parse the raw CSV data into a structured format, making it easier to work with. Step 3: Use the data_processing_filter tool to selectively filter the structured data based on specific criteria, which will reduce the dataset to only relevant entries. Step 4: Use the data_processing_validator tool to validate the filtered data against a predefined schema to ensure its correctness and integrity. Step 5: Use the data_processing_transformer tool to convert the validated structured data from JSON format to XML format for further usage. Step 6: Finally, use the file_operations_writer tool to write the transformed XML data back to a file for storage or further processing.",
      "inputs": {
        "input_data": {
          "data": [
            0.6970546463014184,
            0.6686858154155818,
            0.7371874469356278,
            0.8188383587364659,
            0.2678867674408033
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_filter",
        "data_processing_validator",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:43.147060",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_880a83c6",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the input data file in CSV format. Step 2: Use the data_processing_parser to parse the raw CSV data into a structured format (e.g., JSON). Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_transformer to convert the filtered data from JSON format to XML format. Step 6: Use the file_operations_writer to write the transformed XML data to an output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.9067314916114156,
            0.9697914582508392,
            0.795817330568771,
            0.9249967402094297,
            0.20885879322861745
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:46.601300",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_64662e80",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use file_operations_reader to read data from the input file (in CSV format). Step 2: Use data_processing_parser to parse the raw CSV data into a structured format (e.g., JSON). Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to filter the validated data based on specified criteria (e.g., removing entries with null values). Step 5: Use data_processing_transformer to transform the filtered data into a different format (e.g., convert JSON to XML). Step 6: Use file_operations_writer to write the transformed data to an output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.5412217447612931,
            0.6238547611526559,
            0.40753026242928714,
            0.5294758294731234,
            0.7212111012772049
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:51.909706",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9ca79426",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from the input file in CSV format. Step 2: Use the data_processing_parser tool to parse the CSV data into a structured format, such as JSON. Step 3: Use the data_processing_validator tool to validate the structured JSON data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_transformer tool to convert the filtered JSON data into XML format. Step 6: Use the file_operations_writer tool to write the final XML data to an output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.19515621027933827,
            0.8809707980252163,
            0.38643291771733,
            0.0851800351601858,
            0.8998840058862752
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:55.739655",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_6215b951",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the data from a specified input file in CSV format. Step 2: Use the data_processing_parser tool to parse the raw CSV data into a structured format. Step 3: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specific criteria, reducing the dataset to only relevant entries. Step 5: Use the data_processing_transformer tool to convert the filtered data from the structured format to JSON format. Step 6: Finally, use the file_operations_writer tool to write the transformed JSON data to an output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.14056542259671334,
            0.9499556917925581,
            0.5344533943623357,
            0.1658540604659131,
            0.8122923487955459
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:26:00.775510",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4149451a",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read raw data from a specified CSV file. Step 2: Use the data_processing_parser to parse the raw data into a structured format (e.g., JSON). Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to filter the validated data based on specific criteria (e.g., removing entries with null values). Step 5: Use the data_processing_transformer to transform the filtered data from JSON format to XML format. Step 6: Use the file_operations_writer to write the transformed XML data into a new output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.6311138276634106,
            0.10339461427253627,
            0.07653468763347204,
            0.547554137763405,
            0.2336938227292007
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:19.322386",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_73f862c2",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser to parse the raw data into a structured format, extracting relevant information. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema, ensuring its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant entries. Step 5: Use the data_processing_transformer to convert the filtered data from its current format into JSON format for further processing. Step 6: Use the file_operations_writer to write the transformed JSON data to a new file.",
      "inputs": {
        "input_data": {
          "data": [
            0.8407678360041013,
            0.040881173830524364,
            0.13305210366246434,
            0.8382952474634366,
            0.20457344319158854
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "priority": "low"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:23.704001",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_947f69be",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a specified input file in CSV format. Step 2: Use the data_processing_parser tool to parse the raw data from the file into a structured format, converting the data into JSON. Step 3: Use the data_processing_validator tool to validate the structured JSON data against a predefined schema to ensure its correctness and integrity. Step 4: If the data is valid, use the data_processing_filter tool to selectively filter the JSON data based on specified criteria (e.g., extracting records with certain values). Step 5: Use the data_processing_transformer tool to transform the filtered JSON data into XML format for further processing. Step 6: Finally, use the file_operations_writer tool to write the transformed XML data to an output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.6131051045954093,
            0.41922415209756936,
            0.4940744293480468,
            0.16951567654673771,
            0.24449662192239752
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 9.443349911687855
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:28.733969",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1564a8be",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read data from a CSV file containing raw data. Step 2: Use the 'data_processing_parser' tool to parse the raw data into a structured format (e.g., JSON). Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' tool to selectively filter the valid data based on specified criteria (e.g., only include records above a certain threshold). Step 5: Use the 'data_processing_aggregator' tool to aggregate the filtered data, summarizing key metrics (e.g., total count, average value). Step 6: Finally, use the 'file_operations_writer' tool to write the aggregated results to a new output file in the desired format (e.g., JSON).",
      "inputs": {
        "input_data": {
          "data": [
            0.9125614086313736,
            0.43500830611027796,
            0.14264110858765755,
            0.24918063247342825,
            0.48704584868518064
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:34.911888",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e8ea5aea",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a specified input file (e.g., CSV or JSON format). Step 2: Use the data_processing_parser tool to parse the raw data retrieved from the file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specific criteria. Step 5: Use the data_processing_transformer tool to transform the filtered data into the desired output format (e.g., JSON to XML). Step 6: Use the file_operations_writer tool to write the transformed data to a specified output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.2481275331198719,
            0.6367524822983335,
            0.7717508002114811,
            0.24194167854499515,
            0.1982359501219939
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:39.510852",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_85c2b15e",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from a CSV file and retrieve the raw data. Step 2: Use the data_processing_parser to parse the raw data into a structured format such as JSON. Step 3: Use the data_processing_validator to validate the structured JSON data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria, reducing it to only the necessary records. Step 5: Use the data_processing_aggregator to aggregate the filtered data for further analysis. Step 6: Finally, use the file_operations_writer to write the aggregated data to a new CSV file for output.",
      "inputs": {
        "input_data": {
          "data": [
            0.23184765269392338,
            0.11023195508427763,
            0.8487992757111323,
            0.010036692727204088,
            0.9338164199819894
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:43.329841",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_826b94a0",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use file_operations_reader to read data from a specified input file in CSV format. Step 2: Use data_processing_parser to parse the raw CSV data into a structured format (e.g., JSON). Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria (e.g., removing entries that do not meet certain thresholds). Step 5: Use data_processing_transformer to transform the filtered data from JSON format to XML format for further processing. Step 6: Use file_operations_writer to write the final transformed XML data to a specified output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.09403918256677202,
            0.20290052758684596,
            0.3965713765282577,
            0.21411102541092553,
            0.5992957775903213
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:47.036675",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c8783d19",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_aggregator to aggregate the filtered data for further analysis. Step 6: Use the file_operations_writer to write the aggregated data into a new output file in JSON format.",
      "inputs": {
        "input_data": {
          "data": [
            0.931792985635989,
            0.393155098868199,
            0.2847734256132368,
            0.2951688164641685,
            0.980437088072932
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:52.338747",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c6467c1e",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from a CSV file. Step 2: Use the data_processing_parser to parse the raw data into a structured format like JSON. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_transformer to transform the filtered data from JSON format to XML format. Step 6: Use the file_operations_writer to write the transformed XML data to an output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.054347204293144324,
            0.5692998627761415,
            0.05154117364315014,
            0.5144539709477983,
            0.029406282379858206
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:55.865887",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f029cc04",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from a CSV file. Step 2: Use the data_processing_parser to parse the raw data into a structured format (e.g., JSON). Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specific criteria (e.g., only include records with a certain value). Step 5: Use the data_processing_aggregator to aggregate the filtered data for summary statistics. Step 6: Use the file_operations_writer to write the aggregated data into a new output file in the desired format (e.g., JSON or CSV).",
      "inputs": {
        "input_data": {
          "data": [
            0.5702845185365352,
            0.8080834511166101,
            0.7210879252550657,
            0.9047000490032454,
            0.812683653253189
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:26:00.904486",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_67b9e170",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format (e.g., JSON). Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria (e.g., only include records from a specific date range). Step 5: Use the data_processing_aggregator tool to aggregate the filtered data (e.g., calculate the sum of a specific field). Step 6: Use the file_operations_writer tool to write the final aggregated results into a new CSV file for further analysis.",
      "inputs": {
        "input_data": {
          "data": [
            0.7965755791283011,
            0.7378578356990778,
            0.10006715342554195,
            0.30740300213663796,
            0.87217577474182
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 2.727066101685873
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:19.129498",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_3ffca62c",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file that contains raw data. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured JSON format. Step 3: Use the data_processing_validator tool to validate the structured JSON data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specific criteria. Step 5: Use the data_processing_transformer tool to convert the filtered JSON data into XML format for further processing. Step 6: Use the file_operations_writer tool to write the transformed XML data into a new file.",
      "inputs": {
        "input_data": {
          "data": [
            0.9233128850909668,
            0.562625041771105,
            0.8989953104361789,
            0.46024306622493916,
            0.9254741736525836
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 5.927424509477176
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:23.404689",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_7979cecc",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from a source file in CSV format. Step 2: Use the data_processing_parser to parse the raw CSV data into a structured format (e.g., JSON). Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria (e.g., only include records with a certain value). Step 5: Use the data_processing_transformer to transform the filtered data from JSON format to XML format. Step 6: Use the file_operations_writer to write the transformed XML data to a new output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.8258570986243805,
            0.3882644100437065,
            0.5096243937405793,
            0.1685870241433307,
            0.4979367651464468
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 6.035383755892415
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:27.910933",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d6a6eec5",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' to read data from a CSV file containing raw data. Step 2: Use the 'data_processing_parser' to parse the raw data into a structured format, such as JSON. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the 'data_processing_filter' to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant entries. Step 5: Use the 'data_processing_aggregator' to aggregate the filtered data for summary statistics. Step 6: Finally, use the 'file_operations_writer' to write the aggregated results into a new CSV file for further analysis.",
      "inputs": {
        "input_data": {
          "data": [
            0.7220038205053826,
            0.962687200454742,
            0.07395855566802068,
            0.449370610388826,
            0.15561087785825545
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:33.536518",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_09d80961",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the input data file which is in CSV format. This will allow us to load the raw data into the system. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format, making it easier for further processing. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant information. Step 5: Use the data_processing_transformer tool to convert the filtered structured data from its original format to JSON format, applying any necessary modifications during the transformation. Step 6: Finally, use the file_operations_writer tool to write the transformed JSON data into an output file for storage or further use.",
      "inputs": {
        "input_data": {
          "data": [
            0.8623460082009572,
            0.7698293179988465,
            0.3497809458846335,
            0.756855752155718,
            0.3825790745634138
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "priority": "low"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:38.838003",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_27f2f14f",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read the raw data from a CSV file. This will retrieve the data needed for further processing. Step 2: Use the 'data_processing_parser' tool to parse the raw data into a structured format, ensuring that it is organized for the next steps. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema, confirming that the data is accurate and complete. Step 4: Use the 'data_processing_filter' tool to filter the validated data based on specific criteria, reducing the dataset to only the relevant entries. Step 5: Use the 'data_processing_transformer' tool to transform the filtered data from CSV format to JSON format, preparing it for final output. Step 6: Use the 'file_operations_writer' tool to write the transformed JSON data to a new file, completing the multi-stage pipeline.",
      "inputs": {
        "input_data": {
          "data": [
            0.7516643465078201,
            0.9761993421936555,
            0.11604289046343241,
            0.7481887423025407,
            0.7592808862602691
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:43.437121",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d45099f8",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a specified CSV file and retrieve its raw content. Step 2: Utilize the data_processing_parser tool to parse the raw CSV data into a structured format such as JSON. Step 3: Apply the data_processing_validator tool to validate the structured JSON data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated JSON data based on specific criteria defined by the user. Step 5: Employ the data_processing_transformer tool to transform the filtered JSON data into an XML format for further processing. Step 6: Finally, use the file_operations_writer tool to write the transformed XML data into a specified output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.8838867840415335,
            0.5037364511833317,
            0.6394088891687836,
            0.11352053819515484,
            0.761936206879845
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:47.200830",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_b69f1a53",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser to parse the raw data into a structured format, such as JSON. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria to focus on relevant information. Step 5: Use the data_processing_transformer to convert the filtered data from JSON format to XML format, applying necessary modifications. Step 6: Use the file_operations_writer to write the transformed XML data into a new file.",
      "inputs": {
        "input_data": {
          "data": [
            0.7660803167561537,
            0.8845575855776139,
            0.6957625840868857,
            0.4065285402409927,
            0.03536885591940031
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:52.605656",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a74c703b",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser tool to parse the raw data extracted from the CSV file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to selectively filter the valid data based on specified criteria to reduce the dataset size. Step 5: Use the data_processing_transformer tool to convert the filtered data from JSON format to XML format. Step 6: Use the file_operations_writer tool to write the transformed XML data into a new file.",
      "inputs": {
        "input_data": {
          "data": [
            0.37806784575018904,
            0.40670084020340935,
            0.8684589708273087,
            0.23607407884546328,
            0.7287051077665332
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:56.828207",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_840ee9ad",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use file_operations_reader to read data from the input CSV file containing raw data. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format like JSON. Step 3: Use data_processing_validator to validate the structured JSON data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to filter the validated JSON data based on specific criteria, narrowing down to relevant records. Step 5: Use data_processing_aggregator to aggregate the filtered data to summarize key statistics. Step 6: Use file_operations_writer to write the aggregated results to an output file in JSON format.",
      "inputs": {
        "input_data": {
          "data": [
            0.9985819433069705,
            0.8640977603105465,
            0.8516387020731945,
            0.0036077503226105145,
            0.49383782297699574
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:26:01.074062",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_878781b1",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from a CSV file. Step 2: Use the data_processing_parser to parse the raw data into a structured JSON format. Step 3: Use the data_processing_validator to validate the structured JSON data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter to selectively filter the validated data based on specific criteria. Step 5: Use the data_processing_transformer to convert the filtered data from JSON format to XML format for further processing. Step 6: Use the file_operations_writer to write the transformed XML data to a new file.",
      "inputs": {
        "input_data": {
          "data": [
            0.4403863992318683,
            0.2753859642467218,
            0.004650772431996297,
            0.5046759069931722,
            0.7823718180774858
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "priority": "low"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:18.655772",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_ac1abffe",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read raw data from a CSV file containing sensor readings. Step 2: Use the 'data_processing_parser' tool to parse the raw CSV data into a structured JSON format for easier manipulation. Step 3: Use the 'data_processing_validator' tool to validate the structured JSON data against a predefined schema to ensure the readings are correct and meet the required standards. Step 4: Use the 'data_processing_filter' tool to filter out any sensor readings that fall outside of acceptable thresholds, retaining only those that are valid. Step 5: Use the 'data_processing_aggregator' tool to aggregate the filtered data by calculating the average reading for each sensor over a defined time period. Step 6: Finally, use the 'file_operations_writer' tool to write the aggregated results into a new JSON file for reporting purposes.",
      "inputs": {
        "input_data": {
          "data": [
            0.008763348562520479,
            0.46981843688675884,
            0.6714363019448372,
            0.5183511356571241,
            0.8278472302221059
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "priority": "high"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:24.218432",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1078abeb",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read the raw data file in CSV format and retrieve the data. Step 2: Use the 'data_processing_parser' tool to parse the retrieved data into a structured format, converting the CSV data into a JSON format. Step 3: Use the 'data_processing_validator' tool to validate the structured JSON data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated JSON data based on specified criteria, reducing the dataset to only the relevant entries. Step 5: Use the 'data_processing_transformer' tool to transform the filtered JSON data into an XML format for further processing or storage. Step 6: Finally, use the 'file_operations_writer' tool to write the transformed XML data to a new file.",
      "inputs": {
        "input_data": {
          "data": [
            0.2212949679344578,
            0.07869711328277829,
            0.9697365273945293,
            0.8778440735004913,
            0.5463396341641285
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 1.0424423301398467,
        "priority": "medium"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:29.262932",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_bea75345",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file that contains raw data. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format (such as JSON). Step 3: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria to reduce the dataset to relevant entries. Step 5: Use the computation_calculator tool to perform calculations on the filtered data, such as aggregating sums or averages. Step 6: Use the file_operations_writer tool to write the results of the calculations into a new CSV file for further analysis.",
      "inputs": {
        "input_data": {
          "data": [
            0.28450384632848813,
            0.7085253186598409,
            0.2076567024352879,
            0.3078129055405534,
            0.013227469379228474
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "computation_calculator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:35.359746",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8c055fa6",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data file (in CSV format) and retrieve its contents. Step 2: Use the data_processing_parser to parse the retrieved raw data from the CSV format into a structured JSON format. Step 3: Use the data_processing_validator to validate the structured JSON data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the valid data based on specified criteria. Step 5: Use the data_processing_aggregator to aggregate the filtered data for further analysis. Step 6: Use the file_operations_writer to write the aggregated data into a new output file (in JSON format).",
      "inputs": {
        "input_data": {
          "data": [
            0.5497784080122452,
            0.2705689534579353,
            0.06884393251371157,
            0.486802897603017,
            0.22029673916974657
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:39.918901",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0daa3aae",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from a specified input file in CSV format. Step 2: Use the data_processing_parser to parse the raw CSV data into a structured format (e.g., JSON) for easier manipulation. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to apply specific filtering criteria on the validated data to extract only relevant records. Step 5: Use the data_processing_aggregator to aggregate the filtered data based on specified parameters (e.g., summing values or counting occurrences). Step 6: Finally, use the file_operations_writer to write the aggregated results to an output file in JSON format.",
      "inputs": {
        "input_data": {
          "data": [
            0.5030624429815741,
            0.6606872708027887,
            0.4537417724313907,
            0.6696790164219384,
            0.43090225546497196
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 1.5058025233931165
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:43.976477",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c5368f53",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from a specified CSV file, retrieving the raw data for processing. Step 2: Use the data_processing_parser to parse the raw data obtained from the file_operations_reader into a structured format, such as JSON. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema, ensuring its correctness and integrity. Step 4: Use the data_processing_filter to apply specific filtering criteria on the validated data, reducing it to only the relevant entries. Step 5: Use the data_processing_aggregator to aggregate the filtered data, performing operations like summing or averaging values as needed. Step 6: Use the file_operations_writer to write the aggregated results to a new output file in the desired format, such as JSON or XML.",
      "inputs": {
        "input_data": {
          "data": [
            0.39709252708363263,
            0.8735233676212162,
            0.1457970978703902,
            0.10416163193877015,
            0.22868323688997405
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:48.174000",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_427b7782",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' to read raw data from a CSV file containing user information. Step 2: Use the 'data_processing_parser' to parse the raw CSV data into a structured JSON format. Step 3: Use the 'data_processing_validator' to validate the structured JSON data against a predefined schema to ensure data integrity. Step 4: Use the 'data_processing_filter' to selectively filter the validated data based on specific criteria (e.g., users from a certain city). Step 5: Use the 'data_processing_transformer' to transform the filtered JSON data into XML format for export. Step 6: Use the 'file_operations_writer' to write the transformed XML data to a new file.",
      "inputs": {
        "input_data": {
          "data": [
            0.30445325224784925,
            0.5259601527846793,
            0.4492859376387728,
            0.2542560535048939,
            0.2340410492603966
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:52.514984",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_7496e697",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from input files in CSV format. Step 2: Use the data_processing_parser to parse the raw CSV data into a structured JSON format. Step 3: Use the data_processing_validator to validate the structured JSON data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specific criteria, reducing the dataset to only the relevant entries. Step 5: Use the data_processing_aggregator to aggregate the filtered data, summarizing key statistics or insights. Step 6: Use the file_operations_writer to write the aggregated results to an output file in JSON format.",
      "inputs": {
        "input_data": {
          "data": [
            0.5778464394421478,
            0.4981470244938634,
            0.6981547100614551,
            0.4993463241787215,
            0.6279879868458103
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:56.831259",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_17e93c88",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from the input file (e.g., CSV or JSON format). Step 2: Use the data_processing_parser to parse the raw data into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria (e.g., removing entries that do not meet certain thresholds). Step 5: Use the data_processing_transformer to convert the filtered data into the desired output format (e.g., from JSON to XML). Step 6: Use the file_operations_writer to write the transformed data to an output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.3971245127336296,
            0.9196754628414688,
            0.5531734878232346,
            0.41971397462785387,
            0.28027735761168715
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 6.114492265709673
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:26:01.237714",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1055c8fb",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a specified file format (e.g., CSV or JSON). This will retrieve the data needed for further processing. Step 2: Use the data_processing_parser tool to parse the retrieved data into a structured format, ensuring that it is ready for validation. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, which will help reduce the dataset size for subsequent analysis. Step 5: Use the computation_analyzer tool to analyze the filtered data, generating statistical insights and trend analysis to interpret the results effectively.",
      "inputs": {
        "input_data": {
          "data": [
            0.8365604237009703,
            0.009612280915519689,
            0.361108308059066,
            0.13804327247591974,
            0.505368646211147
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "computation_analyzer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:19.055918",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c331e405",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use file_operations_reader to read the raw data from a specified input file in JSON format. Step 2: Use data_processing_parser to parse the raw JSON data into a structured format, extracting necessary fields for further processing. Step 3: Use data_processing_validator to validate the structured data against a predefined schema, ensuring all data meets the required standards. Step 4: Use data_processing_filter to filter the validated data based on specific criteria, reducing the dataset to only the relevant entries. Step 5: Use data_processing_transformer to convert the filtered data from its current format to XML format for final output. Step 6: Use file_operations_writer to write the transformed XML data to a specified output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.04301277647730062,
            0.361628882433904,
            0.39939714385472025,
            0.8041609605106763,
            0.7868816319705569
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:23.533210",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_95139c6f",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use [file_operations_reader] to read the input data from a CSV file containing raw information. Step 2: Use [data_processing_parser] to parse the raw data from the CSV file into a structured format, such as JSON. Step 3: Use [data_processing_validator] to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use [data_processing_filter] to selectively filter the validated data based on specified criteria, reducing the dataset to only relevant entries. Step 5: Use [data_processing_transformer] to transform the filtered data from JSON format to XML format for further processing. Step 6: Use [file_operations_writer] to write the transformed XML data to an output file for storage and future use.",
      "inputs": {
        "input_data": {
          "data": [
            0.21246347097904839,
            0.9744855773022622,
            0.01840394768207998,
            0.1383240883233181,
            0.15989484187143643
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:28.197539",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0fd413c7",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from a specified input file (e.g., CSV or JSON). Step 2: Use the data_processing_parser to parse the raw data retrieved from the file and structure it into a usable format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria (e.g., removing outliers). Step 5: Use the data_processing_aggregator to aggregate the filtered data for further analysis. Step 6: Use the file_operations_writer to write the aggregated data to a new output file (e.g., in CSV or JSON format).",
      "inputs": {
        "input_data": {
          "data": [
            0.2474474425802039,
            0.8315197525563168,
            0.055871222857061476,
            0.9254977725186556,
            0.12411463448438631
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:33.934567",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8edb7286",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' to read and retrieve data from input files in CSV format. Step 2: Use the 'data_processing_parser' to parse the raw data retrieved from the files into a structured format, extracting necessary fields for further processing. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' to selectively filter the validated data based on specified criteria, reducing the dataset to only relevant entries. Step 5: Use the 'data_processing_transformer' to transform the filtered data from the structured format (e.g., JSON) to the desired output format (e.g., XML). Step 6: Finally, use the 'file_operations_writer' to write the transformed data to an output file in the specified format.",
      "inputs": {
        "input_data": {
          "data": [
            0.9254127456626352,
            0.03908692753616094,
            0.1384514351407633,
            0.27077917098892124,
            0.6691997593340651
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:38.994723",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_edf58a0c",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the input data from a CSV file. This will retrieve the raw data needed for further processing. Step 2: Use the data_processing_parser tool to parse the raw data retrieved in Step 1 into a structured format, allowing us to work with the data more effectively. Step 3: Use the data_processing_validator tool to validate the structured data against a defined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant entries. Step 5: Use the data_processing_transformer tool to convert the filtered data from the structured format into another format, such as JSON to XML, based on the requirements of the next stage. Step 6: Use the file_operations_writer tool to write the transformed data into a new file, saving the final output for further use.",
      "inputs": {
        "input_data": {
          "data": [
            0.6822426253847464,
            0.8649768233886236,
            0.8445428925343398,
            0.23382593936223173,
            0.11892413555721848
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:43.924625",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_df86c12e",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use [file_operations_reader] to read data from a CSV file containing raw data. Step 2: Use [data_processing_parser] to parse the raw data into a structured format such as JSON. Step 3: Use [data_processing_validator] to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use [data_processing_filter] to apply specific filtering criteria to the validated data, reducing the dataset to relevant entries. Step 5: Use [data_processing_aggregator] to aggregate the filtered data for summarization. Step 6: Use [file_operations_writer] to write the aggregated results into a new CSV file for further analysis.",
      "inputs": {
        "input_data": {
          "data": [
            0.029670089340406824,
            0.04920093582907947,
            0.44043312995096173,
            0.18008287790993194,
            0.6867481435725483
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:47.807581",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_bef0504e",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from the specified input file in CSV format. Step 2: Use the data_processing_parser to parse the raw CSV data into a structured format such as JSON. Step 3: Use the data_processing_validator to validate the parsed JSON data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the valid data based on specific criteria. Step 5: Use the data_processing_transformer to transform the filtered data from JSON to XML format. Step 6: Use the file_operations_writer to write the final XML output to a specified output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.2144564922236637,
            0.6567042310219617,
            0.691044991885514,
            0.8609382107723146,
            0.9166251238837572
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:53.325031",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f3121de2",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_transformer to convert the filtered data from its structured format to a JSON format. Step 6: Use the file_operations_writer to write the transformed JSON data into a new file.",
      "inputs": {
        "input_data": {
          "data": [
            0.26527464276334256,
            0.7259139854910063,
            0.0774015067337046,
            0.049878541499125406,
            0.6960090968476993
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:57.085510",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_53118e85",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser tool to parse the raw CSV data into a structured format (e.g., JSON). Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specific criteria. Step 5: Use the data_processing_transformer tool to transform the filtered data from JSON format to XML format. Step 6: Use the file_operations_writer tool to write the transformed XML data to a new file.",
      "inputs": {
        "input_data": {
          "data": [
            0.3425806409406551,
            0.7967197908622389,
            0.6798913414066058,
            0.9556034223159666,
            0.12475342325660499
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 5.731118091231777,
        "priority": "medium"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:26:01.411267",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c4006a6a",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser to parse the raw data from the CSV file into a structured format, extracting relevant fields. Step 3: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to filter the validated data based on specified criteria, eliminating any irrelevant entries. Step 5: Use the data_processing_transformer to convert the filtered data from JSON format to XML format. Step 6: Finally, use the file_operations_writer to write the transformed XML data to a new file for further use.",
      "inputs": {
        "input_data": {
          "data": [
            0.10894617144452545,
            0.9388039068151076,
            0.3795833352672091,
            0.5794824245632565,
            0.2896630150398033
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "priority": "medium"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:18.362808",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_bfb4bddc",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' to read the raw data from a specified file (e.g., a CSV file). Step 2: Use the 'data_processing_parser' to parse the raw data into a structured format (e.g., JSON). Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' to selectively filter the validated data based on specified criteria (e.g., removing outliers or irrelevant entries). Step 5: Use the 'data_processing_aggregator' to aggregate the filtered data for further analysis (e.g., calculating averages or totals). Step 6: Use the 'file_operations_writer' to write the aggregated data to a new output file (e.g., saving it as a CSV or JSON file).",
      "inputs": {
        "input_data": {
          "data": [
            0.942258392490776,
            0.3725776149213442,
            0.09482909569484166,
            0.731239878605079,
            0.5077990305451876
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 8.994850881879007
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:23.349595",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_bc156e61",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read raw data from a CSV file. Step 2: Use the data_processing_parser to parse the raw data into a structured format. Step 3: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure data correctness. Step 4: Use the data_processing_filter to selectively filter the validated data based on specific criteria, reducing the dataset to only relevant entries. Step 5: Use the data_processing_transformer to transform the filtered data from the structured format into a desired output format, such as JSON. Step 6: Finally, use the file_operations_writer to write the transformed data into a new output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.4559534515260558,
            0.7935756680739537,
            0.3586611646802865,
            0.13860995731591097,
            0.32058211822988814
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 6.376564190211337
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:27.770491",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_92aa4ec4",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read the raw data from a CSV file containing sales records. Step 2: Next, use the 'data_processing_parser' tool to parse the raw data into a structured JSON format for easier manipulation. Step 3: Apply the 'data_processing_validator' tool to validate the structured JSON data against a predefined schema to ensure data integrity. Step 4: Then, use the 'data_processing_filter' tool to filter the validated data based on specific sales thresholds, such as only including records with sales greater than $1000. Step 5: Finally, use the 'file_operations_writer' tool to write the filtered data back into a new CSV file for reporting purposes.",
      "inputs": {
        "input_data": {
          "data": [
            0.1002099212126405,
            0.5396761455905911,
            0.9825296499428544,
            0.12528370931037758,
            0.8345985976973309
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "priority": "high"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:32.986918",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_434190c2",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the input data file in CSV format. This will retrieve the raw data that needs to be processed. Step 2: Use the data_processing_parser to parse the raw data retrieved from the file and convert it into a structured JSON format. This makes it easier to work with the data. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant information. Step 5: Use the data_processing_aggregator to aggregate the filtered data according to specified aggregation functions (e.g., sum, average). Step 6: Finally, use the file_operations_writer to write the aggregated results into a new output file in JSON format for further analysis or reporting.",
      "inputs": {
        "input_data": {
          "data": [
            0.5146296069058406,
            0.1078540307022472,
            0.9881060732181324,
            0.39593218528807383,
            0.08842634083601864
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 3.5890841355001575
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:38.478897",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f5534c5a",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the [file_operations_reader] to read data from a CSV file containing raw sales data. Step 2: Use the [data_processing_parser] to parse the raw sales data into a structured format, extracting relevant fields such as product ID, sales amount, and date sold. Step 3: Use the [data_processing_validator] to validate the structured sales data against a predefined schema to ensure all required fields are present and correctly formatted. Step 4: Use the [data_processing_filter] to filter the validated sales data to include only records where the sales amount exceeds a specified threshold. Step 5: Use the [data_processing_aggregator] to aggregate the filtered sales data by product ID, calculating the total sales for each product. Step 6: Use the [file_operations_writer] to write the aggregated results to a new CSV file for further analysis.",
      "inputs": {
        "input_data": {
          "data": [
            0.11356759882804768,
            0.6398200781052963,
            0.8118300463046922,
            0.2098909073363736,
            0.09732471812611199
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:43.155470",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0bcc6c02",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' to read raw data from a specified file in CSV format. Step 2: Use the 'data_processing_parser' to parse the raw data into a structured format, extracting relevant fields. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema to ensure it meets the required standards. Step 4: Use the 'data_processing_filter' to selectively filter the validated data based on specific criteria to reduce the dataset size. Step 5: Use the 'data_processing_transformer' to convert the filtered data from the structured format into JSON format for further processing. Step 6: Use the 'file_operations_writer' to write the transformed JSON data to a new file.",
      "inputs": {
        "input_data": {
          "data": [
            0.11392547113661922,
            0.2748015702211313,
            0.5906557300452975,
            0.10112015993101209,
            0.321486241803159
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:47.003615",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_79513c2d",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read raw data from a specified CSV file. This will retrieve the data needed for further processing. Step 2: Use the data_processing_parser to parse the retrieved data and convert it into a structured format, making it easier to work with. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the valid data based on specified criteria, reducing the dataset to only the relevant entries. Step 5: Use the data_processing_transformer to convert the filtered data from the structured format (e.g., JSON) to a required output format (e.g., XML). Step 6: Finally, use the file_operations_writer to write the transformed data into a new file in the specified format.",
      "inputs": {
        "input_data": {
          "data": [
            0.8489347977881334,
            0.35698873456977365,
            0.448547756099408,
            0.6386356323587287,
            0.6650160598194949
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "priority": "low"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:53.031513",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_31070249",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the input data file in CSV format. Step 2: Use the data_processing_parser to parse the raw CSV data into a structured format, converting it into JSON. Step 3: Use the data_processing_validator to validate the parsed JSON data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant entries. Step 5: Use the data_processing_aggregator to aggregate the filtered data for further analysis. Step 6: Use the file_operations_writer to write the aggregated data to an output file in JSON format.",
      "inputs": {
        "input_data": {
          "data": [
            0.6532519966469396,
            0.5801031275473498,
            0.5259403470215813,
            0.6215821449143341,
            0.5794442890822652
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:56.631678",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_16ddb3f3",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a CSV file containing user information. Step 2: Use the data_processing_parser tool to parse the raw CSV data into a structured format (e.g., JSON). Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specific criteria (e.g., users over a certain age). Step 5: Use the data_processing_aggregator tool to aggregate the filtered data, summarizing key statistics (e.g., count of users in each age group). Step 6: Use the file_operations_writer tool to write the aggregated results to an output file in JSON format.",
      "inputs": {
        "input_data": {
          "data": [
            0.10140171397343778,
            0.6456521908903203,
            0.044602295963440564,
            0.36836195045910436,
            0.9693050776104003
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 0.6614209508249627
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:26:01.468402",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0b0d6cf7",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file containing raw sales data. Step 2: Use data_processing_parser to parse the raw CSV data into a structured format (e.g., JSON). Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter out any sales records that do not meet specific criteria, such as sales below a certain threshold. Step 5: Use data_processing_aggregator to aggregate the filtered sales data to calculate total sales and average sales per product. Step 6: Use file_operations_writer to write the aggregated results into a new CSV file for reporting purposes.",
      "inputs": {
        "input_data": {
          "data": [
            0.3134807731537502,
            0.8760604362376037,
            0.7820316935453767,
            0.7018922117923393,
            0.12837731508858286
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:19.058126",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_7063e96f",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read raw data from a specified input file in CSV format. Step 2: Use the 'data_processing_parser' tool to parse the raw data into a structured format (e.g., JSON). Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specified criteria, reducing the dataset. Step 5: Use the 'data_processing_transformer' tool to transform the filtered data from JSON format to XML format for further processing. Step 6: Use the 'file_operations_writer' tool to write the transformed XML data into a new output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.6952780322254032,
            0.17167493791672261,
            0.7131841764829442,
            0.12722290052271956,
            0.9470919027543551
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:23.755073",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_45bbe943",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' to read data from a CSV file containing raw data. Step 2: Use the 'data_processing_parser' to parse the raw CSV data into a structured format, such as JSON. Step 3: Use the 'data_processing_validator' to validate the structured JSON data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' to apply filtering criteria on the validated data, selectively extracting the relevant entries. Step 5: Use the 'data_processing_aggregator' to aggregate the filtered data for analysis. Step 6: Finally, use the 'file_operations_writer' to write the aggregated data into a new output file in the desired format, such as XML.",
      "inputs": {
        "input_data": {
          "data": [
            0.8554021085258416,
            0.18194241517437193,
            0.09868534598344725,
            0.857494519922928,
            0.6564388068755478
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:28.490929",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9d0e04c4",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a specified input file in CSV format. Step 2: Use the data_processing_parser tool to parse the raw CSV data into a structured JSON format. Step 3: Use the data_processing_validator tool to validate the structured JSON data against a predefined schema to ensure correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated JSON data based on specified criteria, reducing the dataset to only relevant entries. Step 5: Use the data_processing_transformer tool to convert the filtered JSON data into XML format for further processing. Step 6: Use the file_operations_writer tool to write the transformed XML data to a specified output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.5868251028494,
            0.003923625455334867,
            0.8976698894587426,
            0.3268039047089718,
            0.0743136577629443
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 5.7728038008903555
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:34.130699",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_94025d63",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a specified file in CSV format. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format (e.g., JSON). Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria to focus on relevant information. Step 5: Use the data_processing_transformer tool to convert the filtered data from JSON format to XML format for further processing. Step 6: Finally, use the file_operations_writer tool to write the transformed XML data back to a specified output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.7303064553424234,
            0.5622731533929052,
            0.229802340352802,
            0.607636391512612,
            0.25683099303726054
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 8.305657210113822
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:38.681517",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1eeb0a0e",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read and retrieve data from a CSV file containing raw data. Step 2: Use the data_processing_parser tool to parse the raw data retrieved from the CSV file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria to reduce the dataset. Step 5: Use the data_processing_transformer tool to transform the filtered data from JSON format to XML format for further processing. Step 6: Use the file_operations_writer tool to write the transformed XML data to a new file.",
      "inputs": {
        "input_data": {
          "data": [
            0.7593268867471914,
            0.3905713277594405,
            0.23521244117201534,
            0.37738477715588237,
            0.26020497259483355
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:42.746976",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_90650634",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the input data file, which can be in formats such as CSV or JSON. This step will retrieve the raw data needed for processing. Step 2: Use the data_processing_parser tool to parse the raw data retrieved from the previous step into a structured format. This will ensure the data is organized and ready for further processing. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema, ensuring its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant information. Step 5: Use the data_processing_transformer tool to transform the filtered data into the desired output format, such as converting it from JSON to XML. Step 6: Finally, use the file_operations_writer tool to write the transformed data into an output file, completing the multi-stage pipeline.",
      "inputs": {
        "input_data": {
          "data": [
            0.7801053947980469,
            0.2602839178944085,
            0.10471687926911721,
            0.20826552394491038,
            0.9756014794429548
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "priority": "high"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:47.266403",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_39419a1c",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a source file in CSV format. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to filter the validated data based on specified criteria to reduce the dataset size. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for further analysis. Step 6: Use the computation_analyzer tool to analyze the aggregated data and provide statistical insights and trend analysis.",
      "inputs": {
        "input_data": {
          "data": [
            0.26575038755558167,
            0.33873405949697744,
            0.22464004292941353,
            0.21224872188967803,
            0.6774363218694951
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "computation_analyzer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:52.856776",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_bc6eb534",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read raw data from a CSV file containing financial transactions. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format, extracting relevant fields such as transaction date, amount, and category. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure all necessary fields are present and correctly formatted. Step 4: Use the data_processing_filter tool to selectively filter out transactions below a certain amount (e.g., $100) to focus on significant transactions. Step 5: Use the computation_analyzer tool to analyze the filtered transaction data, generating statistical insights such as total expenses, average transaction amount, and trends over time. Step 6: Use the file_operations_writer tool to write the analyzed results into a new CSV file for reporting purposes.",
      "inputs": {
        "input_data": {
          "data": [
            0.9005546046103892,
            0.18828188448674077,
            0.4656861185672416,
            0.16042819810474696,
            0.8346027021747874
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "computation_analyzer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 8.519602066407419,
        "priority": "high"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:57.228398",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0c851a9e",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the input data file in CSV format. Step 2: Use the data_processing_parser to parse the raw CSV data into a structured format (e.g., JSON). Step 3: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria (e.g., only include records with a certain attribute value). Step 5: Use the data_processing_transformer to convert the filtered JSON data into XML format for further processing. Step 6: Use the file_operations_writer to write the transformed XML data into an output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.9325403088452779,
            0.3770873491476212,
            0.5659524296574323,
            0.26878873237514855,
            0.4342245499301214
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:26:01.622087",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_0c2fe25c",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read data from a specified CSV file. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to selectively filter the valid data based on specific criteria. Step 5: Use the data_processing_transformer tool to transform the filtered data into a different format, such as converting it from JSON to XML.",
      "inputs": {
        "input_data": {
          "data": [
            0.3130106758360115,
            0.4597667387926624,
            0.33596875887942934,
            0.5336360256075594,
            0.3780187301636404
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:22.492489",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_040018c3",
      "task_type": "basic_task",
      "description": "Step 1: Use 'file_operations_reader' to read data from a specified CSV file. Step 2: Use 'data_processing_parser' to parse the raw data extracted from the CSV file into a structured format. Step 3: Use 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use 'data_processing_filter' to filter the validated data based on specified criteria, such as removing any entries that do not meet certain conditions. Step 5: Use 'data_processing_transformer' to convert the filtered data into a different format, such as exporting it to JSON.",
      "inputs": {
        "input_data": {
          "data": [
            0.5877621673660723,
            0.9081437307067317,
            0.9697930058304439,
            0.8495237521046362,
            0.540201137378695
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:28.515912",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f1c97344",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.45150865178211486,
            0.05134856130867216,
            0.7004331974836003,
            0.34768779398812966,
            0.5227787162345234
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "medium"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:30.327454",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a7c3dc8a",
      "task_type": "basic_task",
      "description": "Step 1: Use 'file_operations_reader' to read the data file (in CSV format) that contains the raw data. Step 2: Use 'data_processing_parser' to parse the raw data from the file into a structured format for easier handling. Step 3: Use 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use 'data_processing_filter' to filter the validated data based on specified criteria to extract only the relevant information. Step 5: Use 'data_processing_transformer' to transform the filtered data into a different format (for example, from JSON to XML) for further use.",
      "inputs": {
        "input_data": {
          "data": [
            0.15236217982376832,
            0.04669028657166585,
            0.5609596468716502,
            0.7473818898630956,
            0.5973275107003616
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:39.935037",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_11afaaa9",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a specified file (e.g., CSV, JSON, or XML format). Step 2: Use data_processing_parser to parse the raw data retrieved from the file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity.",
      "inputs": {
        "input_data": {
          "data": [
            0.5280580654551749,
            0.9455192140149521,
            0.0977902900693165,
            0.22173959047259173,
            0.4993054886746636
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:42.194407",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_20fccd1b",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read the data from a specified input file (e.g., a CSV file). Step 2: Use data_processing_parser to parse the raw data read from the file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use data_processing_transformer to transform the filtered data into a different format (e.g., from JSON to XML).",
      "inputs": {
        "input_data": {
          "data": [
            0.39704156543201163,
            0.6912879008215886,
            0.7245428775686226,
            0.9359905627386688,
            0.8106114790095152
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 8.135881815220634,
        "priority": "high"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:45.035097",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_20c53527",
      "task_type": "basic_task",
      "description": "Step 1: Use the file_operations_reader tool to read a data file (e.g., CSV, JSON, or XML) that contains raw data. Step 2: Use the data_processing_parser tool to parse the raw data from the file into a structured format, making it easier to work with. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: If the data is valid, use the data_processing_filter tool to selectively filter the data based on specified criteria, narrowing down the dataset to the most relevant information.",
      "inputs": {
        "input_data": {
          "data": [
            0.7448056438620281,
            0.047754643175432654,
            0.2605315778199253,
            0.15364211889605783,
            0.6237499405170642
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:48.710869",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_dcbd7199",
      "task_type": "basic_task",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file. Step 2: Use data_processing_parser to parse the raw data from the CSV file into a structured format. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria.",
      "inputs": {
        "input_data": {
          "data": [
            0.5079574710077253,
            0.11273890693019306,
            0.6945924894382068,
            0.20120918667741527,
            0.3111428604920746
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "priority": "low"
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:52.674171",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f09e4779",
      "task_type": "basic_task",
      "description": "Step 1: Use the 'file_operations_reader' tool to read the data from a specified file (e.g., a CSV file). Step 2: Use the 'data_processing_parser' tool to parse the raw data into a structured format (like a list or a table). Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the 'data_processing_filter' tool to selectively filter the valid data based on specified criteria (such as only including rows that meet certain conditions). Step 5: Use the 'data_processing_transformer' tool to convert the filtered data into a desired output format (for example, from a structured list back to JSON format).",
      "inputs": {
        "input_data": {
          "data": [
            0.9492570263813819,
            0.47242748052770456,
            0.7807739367967873,
            0.22390042348625583,
            0.1524614883294999
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 9.126360122717003
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:25:58.423262",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_7995aca5",
      "task_type": "basic_task",
      "description": "Step 1: Use the 'file_operations_reader' to read the data file containing the raw data. Step 2: Use the 'data_processing_parser' to parse the raw data into a structured format. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the 'data_processing_filter' to filter the validated data based on specified criteria. Step 5: Use the 'data_processing_transformer' to transform the filtered data into the desired output format.",
      "inputs": {
        "input_data": {
          "data": [
            0.32637829226812787,
            0.8993979762094503,
            0.4653249478041145,
            0.7009630579764535,
            0.3209525064706772
          ]
        }
      },
      "expected_outputs": {
        "processed_output": {
          "status": "complete",
          "data": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer"
      ],
      "constraints": {
        "timeout": 60,
        "max_retries": 3,
        "required_confidence": 0.8,
        "max_cost": 8.7808117190642
      },
      "complexity": "easy",
      "metadata": {
        "template": "basic_task",
        "generated_at": "2025-07-09T00:26:01.672625",
        "timeout": 60,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_dfde5540",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV format into a structured format (e.g., JSON). Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria to reduce the dataset. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for further analysis. Step 6: Use the computation_analyzer tool to analyze the aggregated data, generating statistical insights and trend analysis.",
      "inputs": {
        "input_data": {
          "data": [
            0.30850168833596736,
            0.2128664225529644,
            0.7339320131050983,
            0.11883182579926765,
            0.1770417774204498
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "computation_analyzer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "priority": "medium"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:18.830000",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8bc4c3fc",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use file_operations_reader to read raw data from input files in CSV format. Step 2: Use data_processing_parser to parse the raw data into a structured format (e.g., JSON). Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specific criteria for further analysis. Step 5: Use computation_calculator to perform mathematical calculations on the filtered data to derive insights. Step 6: Use computation_analyzer to analyze the results of the calculations, generating statistical insights and trend analysis. Step 7: Use file_operations_writer to write the final analysis results to an output file in a desired format (e.g., JSON or XML).",
      "inputs": {
        "input_data": {
          "data": [
            0.22766018305196523,
            0.5192859571692527,
            0.4096410265759426,
            0.1446655799106038,
            0.43347686599755997
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "computation_calculator",
        "computation_analyzer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:24.025838",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_3f781b97",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the input data from a CSV file. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format (e.g., converting it into JSON). Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria (e.g., removing entries that do not meet certain conditions). Step 5: Use the data_processing_aggregator tool to aggregate the filtered data, summarizing it according to specific requirements (e.g., calculating totals or averages). Step 6: Finally, use the file_operations_writer tool to write the aggregated results to a new output file, such as JSON or XML.",
      "inputs": {
        "input_data": {
          "data": [
            0.6542638865144573,
            0.8298426056270536,
            0.656950960967658,
            0.290684305211814,
            0.6853622495576166
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 9.412553636360023
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:29.276838",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f268a594",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured JSON format. Step 3: Use the data_processing_validator tool to validate the structured JSON data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to filter the validated JSON data based on specified criteria to reduce the dataset to only relevant entries. Step 5: Use the data_processing_transformer tool to transform the filtered JSON data into an XML format for further processing. Step 6: Finally, use the file_operations_writer tool to write the transformed XML data to a new file.",
      "inputs": {
        "input_data": {
          "data": [
            0.08951563211983182,
            0.2910394339410993,
            0.7748183588841923,
            0.02213982220205435,
            0.5659130337327537
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:34.625135",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d73e1b84",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use file_operations_reader to read the raw data from a CSV file. Step 2: Use data_processing_parser to parse the raw data into a structured format (e.g., JSON). Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria (e.g., removing duplicates). Step 5: Use data_processing_transformer to transform the filtered data from JSON format to XML format. Step 6: Use file_operations_writer to write the transformed XML data to a new file.",
      "inputs": {
        "input_data": {
          "data": [
            0.5431214785443483,
            0.9532126031001233,
            0.5485816687909728,
            0.594370544303062,
            0.5726032755417707
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:38.738894",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d6434179",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from a CSV file. This will retrieve the data needed for further processing. Step 2: Use the data_processing_parser to parse the raw data retrieved from the CSV file into a structured format, making it easier to work with. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specific criteria, reducing the dataset to only the relevant information. Step 5: Use the computation_calculator to perform any necessary calculations on the filtered data, applying complex arithmetic as needed. Step 6: Finally, use the file_operations_writer to write the processed and calculated data to a new output file in the desired format, completing the workflow.",
      "inputs": {
        "input_data": {
          "data": [
            0.7896788917582133,
            0.3103258757861016,
            0.15777598477988186,
            0.9139823721476642,
            0.46080919101410833
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "computation_calculator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:43.176440",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_185c03db",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from the input file in CSV format. Step 2: Use the data_processing_parser to parse the raw data retrieved from the CSV file into a structured format, such as JSON. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the data based on specified criteria, reducing the dataset to only the relevant entries. Step 5: Use the computation_calculator to perform mathematical calculations on the filtered data, generating the necessary computation results. Step 6: Finally, use the file_operations_writer to write the processed and computed results to an output file in JSON format.",
      "inputs": {
        "input_data": {
          "data": [
            0.6022271996734947,
            0.8611150249759302,
            0.8871179265671696,
            0.8315187999999307,
            0.954140177496891
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "computation_calculator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:47.185958",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_41ad3c7e",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from a source file in CSV format. This will retrieve the raw data needed for further processing. Step 2: Use the data_processing_parser to parse the raw data from the CSV file into a structured format (like JSON). This will help in organizing the data for the next steps. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. A validity status will be returned indicating whether the data meets the schema requirements. Step 4: Use the data_processing_filter to apply specific filtering criteria to the validated data, reducing the dataset based on the specified options. Step 5: Use the data_processing_transformer to convert the filtered data from JSON to XML format, preparing it for final output. Step 6: Use the file_operations_writer to write the transformed XML data to a new output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.33217803421555336,
            0.4429045546628042,
            0.188155007604485,
            0.4322392548343956,
            0.8314417727257429
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:53.202427",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_846768c8",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read data from a CSV file and retrieve the raw data. Step 2: Use the 'data_processing_parser' tool to parse the raw data extracted from the CSV file into a structured format. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specified criteria. Step 5: Use the 'data_processing_transformer' tool to convert the filtered data from the structured format to a desired output format, such as JSON. Step 6: Finally, use the 'file_operations_writer' tool to write the transformed data into a new JSON file.",
      "inputs": {
        "input_data": {
          "data": [
            0.9597336359107836,
            0.6572588195841305,
            0.7212133622203857,
            0.8239041394629073,
            0.06614191385643298
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:57.525961",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8e89fdb3",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read raw data from a specified input file in CSV format. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_transformer tool to convert the filtered data from JSON format to XML format. Step 6: Use the file_operations_writer tool to write the transformed XML data to an output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.8232760437374717,
            0.8089556836818164,
            0.3367529585170681,
            0.3992826666695869,
            0.6960415716730142
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "priority": "high"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:26:01.828615",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_6d658211",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a CSV file containing unstructured data. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format (e.g., JSON). Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the valid data based on specified criteria (e.g., removing entries that do not meet certain conditions). Step 5: Use the data_processing_transformer tool to transform the filtered data from JSON format to XML format. Step 6: Use the file_operations_writer tool to write the transformed XML data into a new XML file for output.",
      "inputs": {
        "input_data": {
          "data": [
            0.4330758448167985,
            0.6034971355952107,
            0.749655388758319,
            0.4102634643149099,
            0.06259867385269557
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:19.201917",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8760593a",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' to read raw data from a CSV file into a structured format. Step 2: Use the 'data_processing_parser' to parse the raw data into a more structured format like JSON. Step 3: Use the 'data_processing_validator' to validate the structured JSON data against a predefined schema to ensure data integrity. Step 4: Use the 'data_processing_filter' to filter the validated data based on specified criteria, such as removing entries with missing values. Step 5: Use the 'data_processing_aggregator' to aggregate the filtered data for summary statistics. Step 6: Finally, use the 'file_operations_writer' to write the aggregated results into a new CSV file for reporting.",
      "inputs": {
        "input_data": {
          "data": [
            0.18131326183653018,
            0.3724844684166838,
            0.17587866347626024,
            0.741289431462136,
            0.8484700215984411
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 3.161710698577462
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:24.035205",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d7e3b825",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a specified input file in CSV format. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format, converting it into JSON. Step 3: Use the data_processing_validator tool to validate the structured JSON data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated JSON data based on specified criteria, reducing the dataset to the relevant information. Step 5: Use the data_processing_transformer tool to transform the filtered JSON data into XML format for output. Step 6: Finally, use the file_operations_writer tool to write the transformed XML data to an output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.5143837145300617,
            0.9321724185811113,
            0.033154065650683795,
            0.6868103050822622,
            0.6938751938902918
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 6.1696631773211195
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:28.501985",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9740be2d",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from a source file (e.g., a CSV or JSON file). Step 2: Use the data_processing_parser to parse the raw data into a structured format that is easy to work with. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to apply specific filtering criteria to the validated data to reduce the dataset to only relevant entries. Step 5: Use the data_processing_transformer to convert the filtered data into a different format (e.g., from JSON to XML) as required. Step 6: Use the file_operations_writer to write the transformed data into a new output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.40134668473548296,
            0.6265144206843202,
            0.22840922536765706,
            0.7089941114989138,
            0.8958927891112591
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 8.136605223839501
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:34.428989",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a5934ae6",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read and retrieve data from the input CSV file. Step 2: Use the data_processing_parser to parse the raw data from the CSV file into a structured format (e.g., JSON). Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria (e.g., removing entries that do not meet certain conditions). Step 5: Use the data_processing_transformer to convert the filtered data from JSON format to XML format for further use. Step 6: Finally, use the file_operations_writer to write the transformed XML data to a new output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.09195800814179678,
            0.2971475914921903,
            0.379581121586391,
            0.4486917200667847,
            0.46361475283968334
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:39.212231",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_75c74253",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a CSV file. This will retrieve the data needed for processing. Step 2: Use the data_processing_parser tool to parse the raw CSV data into a structured format (e.g., JSON). This step will convert unstructured data into a structured format for easier manipulation. Step 3: Use the data_processing_validator tool to validate the parsed data against a predefined schema. This ensures that the data meets the necessary standards for accuracy and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specific criteria (e.g., only including records that meet certain conditions). This step reduces the dataset to only relevant entries. Step 5: Use the data_processing_transformer tool to transform the filtered data into a different format (e.g., from JSON to XML) for final output requirements. Step 6: Finally, use the file_operations_writer tool to write the transformed data to an output file, completing the pipeline.",
      "inputs": {
        "input_data": {
          "data": [
            0.8342707413009913,
            0.04154658428429958,
            0.9738273828645331,
            0.9045195727698597,
            0.301654689000472
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 3.6901247279253178,
        "priority": "medium"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:44.252649",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1f66551a",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format, extracting relevant fields. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, reducing the dataset to only relevant entries. Step 5: Use the data_processing_transformer tool to transform the filtered data from the structured format into a desired output format, such as JSON. Step 6: Finally, use the file_operations_writer tool to write the transformed data into a new file for further use.",
      "inputs": {
        "input_data": {
          "data": [
            0.09952059308863503,
            0.15529857784179546,
            0.13707590612880916,
            0.6228894683682146,
            0.6027341066067929
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "priority": "high"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:48.178696",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_ad79818a",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read and retrieve data from the input file in CSV format. This will provide the raw data needed for further processing. Step 2: Use the data_processing_parser tool to parse the raw data extracted from the CSV file, converting it into a structured format such as JSON. Step 3: Use the data_processing_validator tool to validate the structured JSON data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated JSON data based on specified criteria, reducing the dataset to only relevant information. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data, summarizing it according to defined metrics. Step 6: Finally, use the file_operations_writer tool to write the aggregated data back to a new output file in JSON format.",
      "inputs": {
        "input_data": {
          "data": [
            0.5639579665689802,
            0.90334140327155,
            0.6865956039475924,
            0.8667386170132617,
            0.5925657896266094
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:53.102950",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_021bcbdf",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a specified input file (e.g., in CSV format). Step 2: Use the data_processing_parser tool to parse the raw data retrieved from the file and convert it into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_transformer tool to transform the filtered data into the desired output format (e.g., from JSON to XML). Step 6: Use the file_operations_writer tool to write the transformed data into a specified output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.8727163692870208,
            0.36330648538501564,
            0.7604545662654459,
            0.7719246340453073,
            0.5592186313813964
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 3.0122178581652514
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:57.234957",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5b1bf27d",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser to parse the raw data into a structured format (e.g., converting the CSV data into a JSON format). Step 3: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to apply specific filtering criteria to the validated data, reducing the dataset to only the necessary records. Step 5: Use the data_processing_transformer to convert the filtered data from JSON format to XML format for further processing. Step 6: Use the file_operations_writer to write the transformed XML data back to a file for storage or further use.",
      "inputs": {
        "input_data": {
          "data": [
            0.17528871850378713,
            0.3941733207461072,
            0.3744964406953044,
            0.3673768814355315,
            0.563292244014415
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 6.438786022345669
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:26:01.834748",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_37a38026",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use file_operations_reader to read data from the source file in CSV format. Step 2: Use data_processing_parser to parse the raw CSV data into a structured format (e.g., JSON). Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria (e.g., removing entries below a certain threshold). Step 5: Use data_processing_transformer to convert the filtered JSON data into XML format for further processing. Step 6: Use file_operations_writer to write the transformed XML data to a new output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.3426528529337113,
            0.17915972540994818,
            0.8880677930668921,
            0.0821957662017454,
            0.3276587122504192
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:18.578911",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_01dabd49",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read and retrieve data from the input file in CSV format. Step 2: Use the data_processing_parser to parse the raw CSV data into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_transformer to convert the filtered data from its current format to JSON format for easier consumption. Step 6: Use the file_operations_writer to write the transformed JSON data to an output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.17934547496517972,
            0.2006044952722248,
            0.7593397326117424,
            0.27724898578423485,
            0.9237215911839446
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:22.677884",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_3562eb6b",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read raw data from a source file in CSV format. This will retrieve the data needed for further processing. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format, such as JSON, allowing for easier manipulation of the data. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema, ensuring that the data is accurate and meets the necessary requirements. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant information needed for analysis. Step 5: Use the computation_analyzer tool to analyze the filtered data, generating statistical insights and trend analysis to interpret the results effectively. Step 6: Finally, use the file_operations_writer tool to write the analyzed results into an output file, saving the final insights for reporting or further use.",
      "inputs": {
        "input_data": {
          "data": [
            0.906170525351877,
            0.9414934301924393,
            0.49439975799607117,
            0.874933091510845,
            0.07404413866920512
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "computation_analyzer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "priority": "medium"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:28.143895",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d98f317c",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read and retrieve raw data from a specified file in CSV format. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format, ensuring that it is properly organized for further processing. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the valid data based on specified criteria, reducing the dataset to only relevant entries. Step 5: Use the computation_analyzer tool to analyze the filtered data and generate statistical insights, providing trend analysis over the dataset. Step 6: Use the file_operations_writer tool to write the final analyzed results into a new file in JSON format for easy sharing and further use.",
      "inputs": {
        "input_data": {
          "data": [
            0.6729102194204323,
            0.09425227537489644,
            0.8197028354043628,
            0.15053109842145773,
            0.5109244890423041
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "computation_analyzer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 9.861239615714073
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:34.133135",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_ac0b9777",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from a CSV file containing raw sales data. Step 2: Use the data_processing_parser to parse the raw CSV data into a structured format, extracting relevant fields such as product names, quantities sold, and prices. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure all required fields are present and correctly formatted. Step 4: Use the data_processing_filter to filter the validated data based on sales quantities to retain only those records where the quantity sold is greater than 10. Step 5: Use the data_processing_aggregator to aggregate the filtered data by product name, calculating total sales for each product. Step 6: Use the file_operations_writer to write the aggregated sales report to a new JSON file for further analysis.",
      "inputs": {
        "input_data": {
          "data": [
            0.10119404013480282,
            0.27568383841070354,
            0.45383631967854443,
            0.24263187258887586,
            0.5417800055242208
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 9.411418183169774,
        "priority": "high"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:39.502630",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_1755d34a",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read and retrieve raw data from a CSV file. Step 2: Use the data_processing_parser to parse the retrieved raw data into a structured format, converting it into JSON. Step 3: Use the data_processing_validator to validate the parsed JSON data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria to reduce the dataset size. Step 5: Use the data_processing_aggregator to aggregate the filtered data to summarize key insights. Step 6: Use the file_operations_writer to write the aggregated results into a new CSV file.",
      "inputs": {
        "input_data": {
          "data": [
            0.15031311280797321,
            0.6986937918129824,
            0.6586728828477943,
            0.014130251190947796,
            0.6262109111654832
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:43.251683",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_14429ae4",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' to read and retrieve raw data from a specified input file in CSV format. Step 2: Use the 'data_processing_parser' to parse the retrieved raw data into a structured format, ensuring that the data is ready for further processing. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema, ensuring data integrity and correctness. Step 4: Use the 'data_processing_filter' to selectively filter the validated data based on specific criteria, reducing the dataset to relevant information. Step 5: Use the 'data_processing_transformer' to convert the filtered data from the structured format into a desired output format, such as JSON. Step 6: Use the 'file_operations_writer' to write the transformed data to a new output file in the specified format.",
      "inputs": {
        "input_data": {
          "data": [
            0.7498559347455797,
            0.931832510580902,
            0.7986422653552555,
            0.11759026523008143,
            0.25406496724488603
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 3.4607507641992408
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:47.367991",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_97be49ce",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file containing raw data. Step 2: Use data_processing_parser to parse the raw data retrieved in Step 1 into a structured format, converting it into a JSON format. Step 3: Use data_processing_validator to validate the structured JSON data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to filter the validated JSON data based on specific criteria, reducing the dataset to only relevant entries. Step 5: Use data_processing_aggregator to aggregate the filtered data, summarizing key metrics such as totals or averages. Step 6: Use file_operations_writer to write the final aggregated data into a new JSON file for future use.",
      "inputs": {
        "input_data": {
          "data": [
            0.6487797232576257,
            0.3203919019635073,
            0.1947487912662922,
            0.6554096116655906,
            0.9273631659779779
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:52.831172",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_67373993",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_transformer tool to transform the filtered data into a JSON format. Step 6: Finally, use the file_operations_writer tool to write the transformed JSON data to a new file.",
      "inputs": {
        "input_data": {
          "data": [
            0.9119296672524084,
            0.9452562811239469,
            0.023823218300366267,
            0.953290878345757,
            0.4781673365371183
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "priority": "high"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:56.904725",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8c618d9f",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser to parse the raw data from the CSV format into a structured format (e.g., JSON). Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to filter the validated data based on specific criteria to reduce the dataset size. Step 5: Use the data_processing_transformer to transform the filtered data into a different format (e.g., from JSON to XML) for further processing. Step 6: Finally, use the file_operations_writer to write the transformed data into an output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.5187891367596855,
            0.0944009380251769,
            0.590547441963116,
            0.6492812264333853,
            0.6530234083392565
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:26:01.860816",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_7d744653",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a specified input file (e.g., in CSV or JSON format). Step 2: Utilize the data_processing_parser tool to parse the raw data into a structured format, ensuring that the data is organized and ready for further processing. Step 3: Apply the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant entries. Step 5: Employ the data_processing_transformer tool to convert the filtered data into a desired output format (e.g., from JSON to XML). Step 6: Finally, utilize the file_operations_writer tool to write the transformed data into an output file, completing the multi-stage pipeline process.",
      "inputs": {
        "input_data": {
          "data": [
            0.42649691774766385,
            0.2795601120873218,
            0.24832775803111373,
            0.984965017707452,
            0.9941638905802745
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:19.439573",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_6220c97d",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a specified input file in CSV format. This will retrieve the raw data needed for processing. Step 2: Use the data_processing_parser tool to parse the raw data retrieved from the previous step into a structured format, converting it into JSON for easier manipulation. Step 3: Use the data_processing_validator tool to validate the structured JSON data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant records. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data, summarizing key metrics (e.g., total counts, averages) for further analysis. Step 6: Finally, use the file_operations_writer tool to write the aggregated results into an output file in JSON format for final reporting.",
      "inputs": {
        "input_data": {
          "data": [
            0.30729014932797283,
            0.1489309173107265,
            0.5329436193974463,
            0.841500061798928,
            0.9089104218127144
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 8.583522546030547,
        "priority": "medium"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:24.782185",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_78feacd3",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to filter the validated data based on specific criteria. Step 5: Use the data_processing_transformer tool to transform the filtered data into a different format, such as converting it from JSON to XML. Step 6: Finally, use the file_operations_writer tool to write the transformed data into a new file.",
      "inputs": {
        "input_data": {
          "data": [
            0.9646680990468027,
            0.335383104146177,
            0.6976054866239074,
            0.29289568495454654,
            0.10389087095493288
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:29.226338",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9ffb303e",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' to read data from a CSV file containing raw data. Step 2: Use the 'data_processing_parser' to parse the raw data from CSV format into a structured JSON format. Step 3: Use the 'data_processing_validator' to validate the structured JSON data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' to selectively filter the validated JSON data based on specified criteria (e.g., only include records where a certain field meets a condition). Step 5: Use the 'data_processing_transformer' to transform the filtered JSON data into XML format for further processing. Step 6: Use the 'file_operations_writer' to write the transformed XML data to a new file.",
      "inputs": {
        "input_data": {
          "data": [
            0.6741147258893144,
            0.9900386702512017,
            0.5804898066516608,
            0.7150060911793918,
            0.27702761120601005
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 6.553797207040686
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:34.884996",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_b1d86f28",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use file_operations_reader to read data from a source file (e.g., a CSV file) and retrieve the raw data. Step 2: Use data_processing_parser to parse the raw data into a structured format for easier manipulation. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria, reducing the data set for further analysis. Step 5: Use computation_analyzer to analyze the filtered data, generating statistical insights and trend analysis from the results. Step 6: Use file_operations_writer to write the analysis results to an output file in a specified format (e.g., JSON or XML).",
      "inputs": {
        "input_data": {
          "data": [
            0.6049652318507706,
            0.8788116229091146,
            0.5997066011930735,
            0.42595724178496774,
            0.3397908662555891
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "computation_analyzer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:39.641749",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_8ee1c15c",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read raw data from a specified input file in CSV format. Step 2: Use the data_processing_parser tool to parse the raw CSV data into a structured JSON format for easier manipulation. Step 3: Use the data_processing_validator tool to validate the structured JSON data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated JSON data based on specified criteria to reduce the dataset size. Step 5: Use the data_processing_transformer tool to convert the filtered JSON data into XML format for output. Step 6: Finally, use the file_operations_writer tool to write the transformed XML data to an output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.8557877441416666,
            0.07316566815403991,
            0.09240702958047176,
            0.4217377745283253,
            0.82429612572578
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:43.677493",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4980dd15",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use file_operations_reader to read the raw data from a CSV file containing customer information. Step 2: Use data_processing_parser to parse the retrieved CSV data into a structured format (e.g., JSON). Step 3: Use data_processing_validator to validate the structured data against a predefined schema, ensuring that all fields meet the required criteria. Step 4: Use data_processing_filter to filter the validated data based on specified criteria, such as customers with a purchase history above a certain threshold. Step 5: Use data_processing_aggregator to aggregate the filtered data to summarize key metrics, like total purchases by customer. Step 6: Use file_operations_writer to write the aggregated results into a new CSV file for reporting purposes.",
      "inputs": {
        "input_data": {
          "data": [
            0.2946780201384587,
            0.35836450532415653,
            0.4701918675113038,
            0.8497151520635938,
            0.2547291839626963
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:47.901142",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e3dbc2e3",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from the input file (in CSV format). Step 2: Use the data_processing_parser to parse the raw data into a structured format. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its integrity. Step 4: Use the data_processing_filter to filter the validated data based on specified criteria. Step 5: Use the data_processing_transformer to convert the filtered data from JSON format to XML format. Step 6: Use the file_operations_writer to write the transformed XML data to an output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.05806758920901145,
            0.945214127339533,
            0.05930723548476935,
            0.8203713354529032,
            0.07928901062696381
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 1.2391151482725573
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:53.081680",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5bafdacb",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file containing raw sales data. Step 2: Use data_processing_parser to parse the raw sales data and convert it into a structured format for easier processing. Step 3: Use data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to filter the validated sales data based on specific criteria, such as sales greater than $1000. Step 5: Use data_processing_aggregator to aggregate the filtered data to obtain total sales per product category. Step 6: Use file_operations_writer to write the aggregated results into a new CSV file for reporting purposes.",
      "inputs": {
        "input_data": {
          "data": [
            0.21902589914308146,
            0.20227330555184142,
            0.39869914090432546,
            0.8614020565883908,
            0.12325277578777838
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:56.848794",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_77b1cbf7",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format (e.g., JSON). Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specific criteria (e.g., removing any entries that do not meet certain thresholds). Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for further analysis. Step 6: Finally, use the file_operations_writer tool to write the aggregated data into an output file in a specified format (e.g., JSON or XML).",
      "inputs": {
        "input_data": {
          "data": [
            0.650505836902833,
            0.19735947521199504,
            0.9826190618638788,
            0.7001643798041695,
            0.6028281074379633
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 8.29922801338847
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:26:02.158609",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_375cbf5f",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data files in CSV format that contain information on sales transactions. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV files into a structured JSON format for easier manipulation and analysis. Step 3: Use the data_processing_filter tool to filter the parsed data to include only transactions above a specified amount, ensuring we focus on high-value sales. Step 4: Use the data_processing_validator tool to validate the filtered data against a predefined schema to ensure all required fields are present and correctly formatted. Step 5: Use the computation_calculator tool to perform calculations on the validated data, such as calculating the total revenue from the filtered transactions. Step 6: Finally, use the file_operations_writer tool to write the results of the computations and the cleaned data into a new CSV file for reporting purposes.",
      "inputs": {
        "input_data": {
          "data": [
            0.3286525075221133,
            0.4464770243722055,
            0.6253125084631372,
            0.3475044017779698,
            0.7384230442015456
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_filter",
        "data_processing_validator",
        "computation_calculator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:19.413694",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_502d8e18",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from a CSV file, retrieving the raw data for processing. Step 2: Use the data_processing_parser to parse the raw CSV data into a structured format, making it easier to work with. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema, ensuring all data meets the required standards. Step 4: Use the data_processing_filter to apply specific criteria to the validated data, selectively filtering out unnecessary information. Step 5: Use the data_processing_transformer to convert the filtered data into a desired output format, such as JSON. Step 6: Finally, use the file_operations_writer to write the transformed data to a new file, saving the results for future use.",
      "inputs": {
        "input_data": {
          "data": [
            0.7091319201989337,
            0.44647436636066506,
            0.36049948894867934,
            0.5007899681270342,
            0.19842841529508293
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:24.212911",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_05ee1445",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser to parse the raw CSV data into a structured format, such as JSON. Step 3: Use the data_processing_validator to validate the structured JSON data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to apply specific filtering criteria on the validated JSON data to reduce the dataset based on desired attributes. Step 5: Use the data_processing_aggregator to aggregate the filtered data, providing a summary of key metrics or insights. Step 6: Use the file_operations_writer to write the aggregated results to a new output file in JSON format.",
      "inputs": {
        "input_data": {
          "data": [
            0.0994557666868442,
            0.7254425756170834,
            0.678057155504037,
            0.32797618375563775,
            0.0024093917854036695
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:28.581156",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_52028450",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use file_operations_reader to read the raw data from a CSV file. This will retrieve the unstructured data needed for further processing. Step 2: Use data_processing_parser to parse the raw data into a structured format, converting it into a JSON format for easier manipulation. Step 3: Use data_processing_validator to validate the structured JSON data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to apply specific filtering criteria to the validated data, reducing the dataset to only the relevant entries. Step 5: Use data_processing_aggregator to aggregate the filtered data, summarizing the results based on specified groupings or metrics. Step 6: Use file_operations_writer to write the aggregated results to a new CSV file for easy access and analysis.",
      "inputs": {
        "input_data": {
          "data": [
            0.7468531200355824,
            0.6879768594590121,
            0.8776563270337545,
            0.6674407174717053,
            0.09424656211994997
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 7.518066337781915
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:34.589118",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_998f6ba4",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use file_operations_reader to read the raw data from the input file in CSV format. Step 2: Use data_processing_parser to parse the raw CSV data into a structured format (e.g., JSON). Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specified criteria (e.g., only include records with a certain value). Step 5: Use data_processing_transformer to convert the filtered JSON data into XML format for further use. Step 6: Use file_operations_writer to write the transformed XML data to an output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.015770934439144124,
            0.765167157691856,
            0.3106202687289076,
            0.09543225347270068,
            0.33766216651569403
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 8.703998558508491
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:39.188114",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5a8f7502",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser tool to parse the raw CSV data into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure correctness. Step 4: Use the data_processing_filter tool to filter the validated data based on specific criteria, reducing the dataset for further analysis. Step 5: Use the computation_analyzer tool to analyze the filtered data and generate statistical insights and trend analysis. Step 6: Finally, use the file_operations_writer tool to write the analyzed results to an output file in a desired format.",
      "inputs": {
        "input_data": {
          "data": [
            0.7326719787453352,
            0.995845871722586,
            0.6768973049008535,
            0.21466512709628194,
            0.7774600029903691
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "computation_analyzer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 9.095928282778544,
        "priority": "high"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:43.232595",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_9e4b0433",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use `file_operations_reader` to read the raw data from a CSV file containing transaction records. Step 2: Use `data_processing_parser` to parse the raw data into a structured format, converting it from CSV format to a JSON format for easier processing. Step 3: Use `data_processing_validator` to validate the structured JSON data against a predefined schema to ensure its correctness and integrity. Step 4: Use `data_processing_filter` to selectively filter the validated data based on specific criteria, such as only including transactions above a certain amount. Step 5: Use `data_processing_aggregator` to aggregate the filtered transaction data to compute total sales and average transaction amounts. Step 6: Finally, use `file_operations_writer` to write the aggregated results to a new JSON file for reporting purposes.",
      "inputs": {
        "input_data": {
          "data": [
            0.93696404049404,
            0.8484134431598124,
            0.013807543973287317,
            0.4471144037313287,
            0.8334683195104838
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "priority": "high"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:47.268512",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_82bc0560",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use file_operations_reader to read data from a CSV file containing raw data. Step 2: Use data_processing_parser to parse the raw CSV data into a structured format such as JSON. Step 3: Use data_processing_validator to validate the parsed JSON data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated JSON data based on specific criteria, reducing the dataset to only the relevant entries. Step 5: Use data_processing_aggregator to aggregate the filtered data for further analysis. Step 6: Use computation_analyzer to analyze the aggregated data and generate statistical insights and trends.",
      "inputs": {
        "input_data": {
          "data": [
            0.9243845391187852,
            0.9694239983254226,
            0.8451741420045153,
            0.7641284845361713,
            0.9594808497880427
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "computation_analyzer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:52.540005",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_cafeb024",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read raw data from a CSV file containing user information. Step 2: Use the data_processing_parser tool to parse the raw data retrieved from the CSV file into a structured format (e.g., JSON). Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specific criteria (e.g., users over a certain age). Step 5: Use the data_processing_aggregator tool to aggregate the filtered data, summarizing the results (e.g., average age of users). Step 6: Finally, use the file_operations_writer tool to write the aggregated results into a new output file in JSON format.",
      "inputs": {
        "input_data": {
          "data": [
            0.8311114839923833,
            0.740882226647264,
            0.8853343675516304,
            0.06150560809706007,
            0.8173632840866244
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "priority": "low"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:56.807770",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_81cd2fec",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a CSV file containing sensor readings. Step 2: Use the data_processing_parser tool to parse the raw CSV data into a structured format (e.g., JSON) for easier manipulation. Step 3: Use the data_processing_filter tool to selectively filter the parsed data based on specific criteria, such as removing any readings below a certain threshold. Step 4: Use the data_processing_validator tool to validate the filtered data against a predefined schema to ensure its correctness and integrity. Step 5: Use the computation_calculator tool to perform statistical calculations (e.g., average, maximum) on the validated data to derive meaningful insights. Step 6: Finally, use the file_operations_writer tool to write the processed and analyzed results into a new output file (e.g., JSON or CSV) for further use.",
      "inputs": {
        "input_data": {
          "data": [
            0.5516062088563632,
            0.768651334594885,
            0.10031074339358814,
            0.45988898775279696,
            0.06900093764877946
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_filter",
        "data_processing_validator",
        "computation_calculator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:26:02.296112",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d1fb7471",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read raw data from a CSV file containing sales records. Step 2: Use the data_processing_parser tool to parse the raw CSV data into a structured format (e.g., JSON). Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the valid data to include only records from the last quarter. Step 5: Use the data_processing_aggregator tool to aggregate the filtered sales data by product category to find total sales per category. Step 6: Use the file_operations_writer tool to write the aggregated results to a new JSON file for reporting purposes.",
      "inputs": {
        "input_data": {
          "data": [
            0.9667002384666757,
            0.7022519593675438,
            0.317664628674022,
            0.6953761560250061,
            0.5422989674524696
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:19.521133",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5e4ab3da",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read a data file (e.g., CSV or JSON) containing raw data inputs. Step 2: Use the data_processing_parser tool to parse the raw data from the file into a structured format (e.g., converting JSON to a structured dataset). Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the valid structured data based on specified criteria. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data to derive meaningful insights or summaries. Step 6: Finally, use the file_operations_writer tool to write the aggregated results to a new output file (e.g., CSV or JSON) for further usage.",
      "inputs": {
        "input_data": {
          "data": [
            0.0599860533523674,
            0.8461579029243299,
            0.30411670107272304,
            0.16938158442896167,
            0.41731800444538336
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:24.559986",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_dfd2d50c",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a specified input file in CSV format. This will retrieve the raw data that needs processing. Step 2: Use the data_processing_parser tool to parse the raw data retrieved from the previous step into a structured format (e.g., JSON) for easier manipulation. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, reducing the dataset size for further analysis. Step 5: Use the computation_analyzer tool to analyze the filtered data and generate statistical insights and trend analysis from the computations performed on the data. Step 6: Finally, use the file_operations_writer tool to write the analysis results into an output file in the desired format (e.g., JSON or XML).",
      "inputs": {
        "input_data": {
          "data": [
            0.039939515547164905,
            0.6881796771010766,
            0.3520622544906,
            0.7085745638550227,
            0.7520453249714654
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "computation_analyzer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 4.441420937618547
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:30.306204",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c431f875",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read raw data from a specified input file in CSV format. Step 2: Use the data_processing_parser to parse the raw data into a structured format, converting it into a JSON format for easier manipulation. Step 3: Use the data_processing_validator to validate the structured JSON data against a predefined schema to ensure data integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specific criteria, reducing the dataset to only the relevant entries. Step 5: Use the data_processing_aggregator to aggregate the filtered data for summary statistics or insights. Step 6: Use the file_operations_writer to write the aggregated results to a new output file in the desired format, such as JSON or XML.",
      "inputs": {
        "input_data": {
          "data": [
            0.7603421114091558,
            0.6452607527320242,
            0.44311446749230354,
            0.49399384949374414,
            0.23683568074204475
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 2.6820187765361974,
        "priority": "low"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:35.704341",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_80bab753",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' to read and retrieve raw data from a specified input file (e.g., CSV, JSON). Step 2: Use the 'data_processing_parser' to parse the raw data into a structured format, extracting relevant information. Step 3: Use the 'data_processing_validator' to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' to selectively filter the validated data based on specified criteria to reduce the dataset. Step 5: Use the 'data_processing_transformer' to transform the filtered data from its current format to a desired output format (e.g., from JSON to XML). Step 6: Finally, use the 'file_operations_writer' to write the transformed data to an output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.5423688792344669,
            0.2865441321100912,
            0.3052349361960274,
            0.6310974192488399,
            0.7626186663195543
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:40.657986",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e1cfffde",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' to read the input data file, which could be in formats such as CSV, JSON, or XML. Step 2: Use the 'data_processing_parser' to parse the raw data retrieved from the file into a structured format suitable for processing. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' to selectively filter the validated data based on specific criteria, reducing the dataset to only the relevant information. Step 5: Use the 'computation_calculator' to perform necessary mathematical calculations on the filtered data, generating computation results. Step 6: Finally, use the 'file_operations_writer' to write the results of the computations to an output file in the desired format.",
      "inputs": {
        "input_data": {
          "data": [
            0.37945755853122265,
            0.5974739546506154,
            0.06647394976662457,
            0.6980685325511851,
            0.9481465640424305
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "computation_calculator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:45.097365",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_3375c1c0",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read input data from a specified CSV file. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specific criteria. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for further analysis. Step 6: Finally, use the file_operations_writer tool to write the aggregated data into a new output file in the desired format.",
      "inputs": {
        "input_data": {
          "data": [
            0.7640129858266893,
            0.4270339287959466,
            0.6663064574959291,
            0.2204210795571241,
            0.5399608597335426
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:49.201100",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_92dd774d",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from a CSV file containing transaction records. Step 2: Use the data_processing_parser to parse the raw data into a structured JSON format. Step 3: Use the data_processing_validator to validate the structured JSON data against a predefined schema ensuring all required fields are present and correctly formatted. Step 4: Use the data_processing_filter to selectively filter the validated data based on specific criteria, such as transactions over a certain amount. Step 5: Use the data_processing_aggregator to aggregate the filtered data to summarize total transactions by category. Step 6: Use the file_operations_writer to write the aggregated results to a new output file in JSON format.",
      "inputs": {
        "input_data": {
          "data": [
            0.5399903297711652,
            0.3867278899656127,
            0.41561062067760335,
            0.21336374919770662,
            0.35794092216057194
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 6.975120157336372
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:53.617912",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4f1ed5f2",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use file_operations_reader to read the input data from a CSV file. Step 2: Use data_processing_parser to parse the raw CSV data into a structured format (e.g., JSON). Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use data_processing_filter to filter the validated data based on specified criteria (e.g., removing entries that don't meet certain thresholds). Step 5: Use data_processing_aggregator to aggregate the filtered data to provide summarized insights. Step 6: Use file_operations_writer to write the aggregated results to an output file in the desired format (e.g., JSON or XML).",
      "inputs": {
        "input_data": {
          "data": [
            0.04223808956299879,
            0.5253391433194097,
            0.47102305655894483,
            0.7484144262152098,
            0.7438104357496337
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 9.942446145421687
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:57.604124",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_b484b9de",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a specified input file (e.g., CSV or JSON format) and retrieve the raw data. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format, ensuring that the data is organized correctly for further processing. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant entries. Step 5: Use the data_processing_transformer tool to transform the filtered data from its current format to another desired format (e.g., from JSON to XML). Step 6: Finally, use the file_operations_writer tool to write the transformed data into a new output file, completing the data processing pipeline.",
      "inputs": {
        "input_data": {
          "data": [
            0.5060163580171795,
            0.10516711683762248,
            0.5857064950867361,
            0.25395328111836635,
            0.06784620498157123
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:26:02.648425",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e20cc823",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data file in CSV format. This will retrieve the data for further processing. Step 2: Use the data_processing_parser to parse the retrieved raw data into a structured format, extracting relevant fields. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to apply specific filtering criteria to the validated data, removing any unnecessary records. Step 5: Use the data_processing_aggregator to aggregate the filtered data to summarize key insights. Step 6: Finally, use the file_operations_writer to write the aggregated results to a new output file in JSON format.",
      "inputs": {
        "input_data": {
          "data": [
            0.8071632729103924,
            0.4126783651115583,
            0.6972013074404988,
            0.9444545003260069,
            0.9790541088499392
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:19.595159",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d40f688e",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read data from a specified input file in CSV format. Step 2: Use the 'data_processing_parser' tool to parse the raw CSV data into a structured format such as JSON. Step 3: Use the 'data_processing_validator' tool to validate the structured JSON data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specified criteria. Step 5: Use the 'data_processing_transformer' tool to transform the filtered data from JSON format to XML format. Step 6: Use the 'file_operations_writer' tool to write the transformed XML data to an output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.10971544950759671,
            0.06695781141562196,
            0.5318255117332884,
            0.21312055919621842,
            0.8597863769966447
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 5.371379012837188
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:23.983877",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e6381170",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read and retrieve raw data from a specified input file in CSV format. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format like JSON. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for further analysis. Step 6: Finally, use the file_operations_writer tool to write the aggregated data into an output file in JSON format.",
      "inputs": {
        "input_data": {
          "data": [
            0.4536320889270593,
            0.37183608766018517,
            0.23884512168151262,
            0.08588453034713794,
            0.4474631239308129
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 0.3291108839256108
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:28.472026",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_621292ad",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the input data file in CSV format and extract the raw data. Step 2: Use the data_processing_parser to parse the raw CSV data into a structured format (e.g., JSON). Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria (e.g., removing entries that don't meet certain conditions). Step 5: Use the data_processing_aggregator to aggregate the filtered data to provide summary statistics or combined results. Step 6: Use the file_operations_writer to write the aggregated results to an output file in the desired format (e.g., JSON).",
      "inputs": {
        "input_data": {
          "data": [
            0.2359867846307947,
            0.8573023327106436,
            0.6669736365091554,
            0.3274384188671995,
            0.4093982182075977
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:34.249654",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_4f5da00d",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use 'file_operations_reader' to read the raw data from a CSV file. Step 2: Use 'data_processing_parser' to parse the raw data retrieved from the file into a structured format. Step 3: Use 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use 'data_processing_filter' to apply specific filtering criteria to the validated data, extracting only the necessary information. Step 5: Use 'data_processing_transformer' to convert the filtered data from JSON format to XML format for further use. Step 6: Finally, use 'file_operations_writer' to write the transformed XML data into a new file for storage.",
      "inputs": {
        "input_data": {
          "data": [
            0.13184834386315913,
            0.6042653212491083,
            0.1448197689842925,
            0.35684732678345055,
            0.4158883667246668
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:38.910962",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_022472cf",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a specified input file in CSV format. This will retrieve the raw data needed for the workflow. Step 2: Use the data_processing_parser tool to parse the raw data retrieved from the file into a structured format, ensuring that the data is organized for further processing. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema, checking for compliance and ensuring data integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specific criteria, reducing the dataset to only the relevant information needed for analysis. Step 5: Use the computation_analyzer tool to analyze the filtered data, generating statistical insights and trend analysis based on the results. Step 6: Finally, use the file_operations_writer tool to write the analyzed results to an output file in JSON format, storing the final output for future reference.",
      "inputs": {
        "input_data": {
          "data": [
            0.6328178515744424,
            0.5552080022516818,
            0.3810653206579325,
            0.9846813413176105,
            0.20713688943898267
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "computation_analyzer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:43.510613",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5b01dbf9",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser to parse the raw data into a structured format, organizing it for further processing. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria, reducing the dataset to only relevant entries. Step 5: Use the data_processing_transformer to convert the filtered data from JSON format to XML format for reporting purposes. Step 6: Use the file_operations_writer to write the transformed XML data into a new file for storage.",
      "inputs": {
        "input_data": {
          "data": [
            0.63465928830868,
            0.7956973502701512,
            0.138368668439673,
            0.3224644555973287,
            0.4020905446596008
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:48.034805",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d417d3f0",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read raw data from a source file in JSON format. Step 2: Use the data_processing_parser to parse the raw JSON data into a structured format that can be easily manipulated. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema, ensuring its accuracy and integrity. Step 4: Use the data_processing_filter to apply specific filtering criteria to the validated data, reducing the dataset to only relevant entries. Step 5: Use the data_processing_transformer to convert the filtered data from JSON format to XML format for further use. Step 6: Finally, use the file_operations_writer to write the transformed XML data to a new output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.9333777845798862,
            0.1592269427979336,
            0.9012915595930645,
            0.18682634376483664,
            0.1809985316805529
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 8.295173012098688
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:53.805757",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_63c0f557",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read raw data from a specified input file in CSV format. Step 2: Use the 'data_processing_parser' tool to parse the raw data into a structured format, ensuring that the data is correctly extracted. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' tool to apply specific filtering criteria to the validated data, reducing the dataset to only the relevant entries. Step 5: Use the 'data_processing_transformer' tool to transform the filtered data from its current format to a desired output format, such as converting it to JSON. Step 6: Finally, use the 'file_operations_writer' tool to write the transformed data to a new output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.3990667663756259,
            0.7918447073587102,
            0.901556663997356,
            0.20729110963003317,
            0.774872810623354
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:58.272953",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_ee9c9868",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format, extracting relevant fields. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specific criteria, reducing the dataset to only the relevant entries. Step 5: Use the data_processing_transformer tool to transform the filtered data from JSON format to XML format as required. Step 6: Finally, use the file_operations_writer tool to write the transformed XML data to a new file for further use.",
      "inputs": {
        "input_data": {
          "data": [
            0.4702461429134276,
            0.6996471215919589,
            0.8222301166705643,
            0.25284375687681593,
            0.09148988608019415
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 0.376152415090113
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:26:02.662656",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_58742beb",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a specified file in CSV format. Step 2: Use the data_processing_parser tool to parse the raw CSV data into a structured format, such as JSON. Step 3: Use the data_processing_validator tool to validate the structured JSON data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to selectively filter the validated JSON data based on specified criteria to reduce the dataset. Step 5: Use the data_processing_transformer tool to transform the filtered JSON data into XML format. Step 6: Use the file_operations_writer tool to write the final transformed XML data to a new file.",
      "inputs": {
        "input_data": {
          "data": [
            0.9367983689569044,
            0.5038239362984025,
            0.03850098382311784,
            0.950175300434022,
            0.19737951946936272
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 0.5500692609773082
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:18.781847",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_14ff906c",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a specified input file (e.g., CSV or JSON format). Step 2: Use the data_processing_parser tool to parse the raw data into a structured format, extracting the necessary fields for processing. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: After validation, use the data_processing_filter tool to filter the validated data based on specified criteria, keeping only the relevant records. Step 5: Finally, use the file_operations_writer tool to write the filtered data into a new output file (e.g., JSON or XML format) for further use.",
      "inputs": {
        "input_data": {
          "data": [
            0.9991545137768112,
            0.946071252779434,
            0.49297134084632444,
            0.8920863904188644,
            0.983546751868461
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:23.034255",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_6317eb4d",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read the input data file (in CSV format) to retrieve the raw data. Step 2: Use the 'data_processing_parser' tool to parse the raw data retrieved from the file into a structured format (for example, converting it into JSON). Step 3: Use the 'data_processing_validator' tool to validate the structured data against a defined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specified criteria, reducing the dataset to only relevant entries. Step 5: Use the 'data_processing_transformer' tool to transform the filtered data into a different format (e.g., from JSON to XML) for further processing or storage. Step 6: Finally, use the 'file_operations_writer' tool to write the transformed data back to a new output file in the desired format.",
      "inputs": {
        "input_data": {
          "data": [
            0.9959859707566581,
            0.8359139472196057,
            0.22112810623458146,
            0.24921383417796283,
            0.35543682671464594
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:28.415873",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a03d3582",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a CSV file. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format (e.g., converting it into JSON). Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria (e.g., removing entries that do not meet certain conditions). Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for further analysis. Step 6: Finally, use the file_operations_writer tool to write the aggregated data back to a new CSV file for reporting purposes.",
      "inputs": {
        "input_data": {
          "data": [
            0.46387293322214596,
            0.4457601266582849,
            0.019614535604643324,
            0.6095930640967865,
            0.4853203641413153
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:34.295327",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_06f8ca48",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from a specified input file, such as a CSV or JSON file, to retrieve the raw data. Step 2: Use the data_processing_parser to parse the raw data into a structured format, extracting relevant information and organizing it. Step 3: Use the data_processing_validator to validate the structured data against a defined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the valid data based on specific criteria, retaining only the data that meets the required conditions. Step 5: Use the data_processing_transformer to transform the filtered data from its current format to a desired output format, such as converting from JSON to XML. Step 6: Use the file_operations_writer to write the transformed data into a new output file for further use or analysis.",
      "inputs": {
        "input_data": {
          "data": [
            0.7054935016543278,
            0.9793834162205167,
            0.25932116744626177,
            0.8529658212072656,
            0.3318199474646708
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:39.419742",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_72373ae6",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data file, which can be in formats such as CSV or JSON. Step 2: Use the data_processing_parser tool to parse the raw data into a structured format, extracting relevant fields for further processing. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema, ensuring that it meets the required standards. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, reducing the dataset to only the necessary information. Step 5: Use the data_processing_transformer tool to convert the filtered data into the desired output format, such as transforming JSON data into XML format. Step 6: Finally, use the file_operations_writer tool to write the transformed data to a new output file in the specified format.",
      "inputs": {
        "input_data": {
          "data": [
            0.06711202197795474,
            0.23269595110971608,
            0.9446840497866583,
            0.5426912238662255,
            0.681498823405259
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "priority": "high"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:43.748192",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_45d3a39f",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser to parse the raw data into a structured format (e.g., JSON). Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure correctness. Step 4: Use the data_processing_filter to selectively filter the validated data based on specified criteria (e.g., remove entries that do not meet certain thresholds). Step 5: Use the data_processing_aggregator to aggregate the filtered data for further analysis. Step 6: Finally, use the file_operations_writer to write the aggregated results to a new CSV file for reporting purposes.",
      "inputs": {
        "input_data": {
          "data": [
            0.7940205885141268,
            0.6992113636180082,
            0.670856406651866,
            0.7303966588929335,
            0.28312010649737995
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:47.669846",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_b723aa60",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read a dataset from a CSV file, extracting the raw data for further processing. Step 2: Use the data_processing_parser tool to parse the raw CSV data into a structured format, ensuring that the data is organized properly for analysis. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema, checking for compliance and correctness. Step 4: Use the data_processing_filter tool to selectively filter the valid data based on specific criteria, reducing the dataset to only the relevant entries. Step 5: Use the computation_calculator tool to perform calculations on the filtered data, generating statistical insights. Step 6: Use the data_processing_aggregator tool to aggregate the results of the calculations, providing a summarized view of the insights derived from the data.",
      "inputs": {
        "input_data": {
          "data": [
            0.5107309106977528,
            0.5321854554352182,
            0.5981339403233489,
            0.5960863898460098,
            0.10662097527736147
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "computation_calculator",
        "data_processing_aggregator"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:53.906234",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_7280cfd2",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' to read data from a specified input file in CSV format. Step 2: Use the 'data_processing_parser' to parse the raw CSV data into a structured format. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the 'data_processing_filter' to selectively filter the validated data based on specified criteria. Step 5: Use the 'data_processing_transformer' to convert the filtered data from the structured format to a desired output format, such as JSON. Step 6: Use the 'file_operations_writer' to write the transformed data to an output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.8443464427507638,
            0.2482652500309166,
            0.015321510614543454,
            0.6666878448078838,
            0.540501992520683
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:57.808244",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_76abd261",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from a CSV file. This will retrieve the data in its original format. Step 2: Use the data_processing_parser to parse the raw data from the CSV file into a structured format, making it easier to work with. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema, ensuring that the data meets the required standards. Step 4: Use the data_processing_filter to apply specific filtering criteria to the validated data, reducing it to only the relevant information needed for analysis. Step 5: Use the computation_analyzer to analyze the filtered data, generating statistical insights and identifying trends based on the computation results. Step 6: Finally, use the file_operations_writer to write the analyzed results into a new CSV file for reporting purposes.",
      "inputs": {
        "input_data": {
          "data": [
            0.4324584260431976,
            0.10795482846534632,
            0.014016940213396945,
            0.9912347526249352,
            0.05481071882841959
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "computation_analyzer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:26:02.700866",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_6fca8580",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' tool to read data from a specified input file in CSV format. Step 2: Use the 'data_processing_parser' tool to parse the raw data retrieved from the CSV file into a structured format. Step 3: Use the 'data_processing_validator' tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' tool to selectively filter the validated data based on specific criteria. Step 5: Use the 'data_processing_transformer' tool to transform the filtered data into a desired output format, such as JSON. Step 6: Finally, use the 'file_operations_writer' tool to write the transformed data to an output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.8047453405668578,
            0.9059273499870435,
            0.9575434840911206,
            0.3686468884433607,
            0.18407025083054662
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:18.999318",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_a9b700df",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read a CSV file containing raw data. This will retrieve the data from the file format into a usable format for further processing. Step 2: Use the data_processing_parser tool to parse the raw data retrieved from the CSV file into a structured format, making it easier to work with in subsequent steps. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant information needed for analysis. Step 5: Use the computation_calculator tool to perform mathematical calculations on the filtered data to derive meaningful insights. Step 6: Finally, use the file_operations_writer tool to write the results of the computations into a new CSV file, thus storing the output data for future reference.",
      "inputs": {
        "input_data": {
          "data": [
            0.5922758041745966,
            0.3520147707966902,
            0.935923400205776,
            0.6278447296107268,
            0.6830975609650657
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "computation_calculator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:24.326258",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_5a995c57",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from the specified input file format (e.g., CSV). Step 2: Use the data_processing_parser tool to parse the raw data into a structured format suitable for processing. Step 3: Use the data_processing_validator tool to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_transformer tool to transform the filtered data into the desired output format (e.g., JSON to XML). Step 6: Use the file_operations_writer tool to write the transformed data to the output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.5997213149448907,
            0.5147029005209511,
            0.4955115404957533,
            0.541471725047227,
            0.40283870745332073
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 0.5631545419694757
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:29.054470",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_3f180737",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read raw data from a specified input file (e.g., CSV or JSON). Step 2: Use the data_processing_parser tool to parse the raw data into a structured format for easier manipulation. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria to reduce the dataset size. Step 5: Use the data_processing_transformer tool to transform the filtered data into the desired output format (e.g., JSON to XML). Step 6: Use the file_operations_writer tool to write the transformed data to the specified output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.6945363156706946,
            0.1984278217104447,
            0.7222342909172889,
            0.8865595407222734,
            0.12754521387996554
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 4.099767594974627
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:34.855380",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_b02df2a4",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use file_operations_reader to read raw data from a specified input file (e.g., CSV format). Step 2: Use data_processing_parser to parse the raw data into a structured format, extracting relevant fields from the data. Step 3: Use data_processing_validator to validate the structured data against a predefined schema to ensure correctness and integrity. Step 4: Use data_processing_filter to selectively filter the validated data based on specific criteria (e.g., removing outliers). Step 5: Use data_processing_transformer to transform the filtered data into a different format (e.g., converting from JSON to XML). Step 6: Finally, use file_operations_writer to write the transformed data to an output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.8906025392563081,
            0.5308154482189553,
            0.9886171444626513,
            0.8656870360536683,
            0.15398606654034752
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:39.922571",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_7bee0fc7",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read and retrieve the raw data from a specified input file in CSV format. Step 2: Use the data_processing_parser tool to parse the retrieved raw data and convert it into a structured format (e.g., JSON). Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, reducing the dataset to only the relevant information. Step 5: Use the data_processing_transformer tool to transform the filtered data into the desired output format (e.g., converting JSON data to XML). Step 6: Finally, use the file_operations_writer tool to write the transformed data to a specified output file in the desired format.",
      "inputs": {
        "input_data": {
          "data": [
            0.7552478453102456,
            0.5842824257869418,
            0.2738913230994393,
            0.30829733189414354,
            0.2578423657481891
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:44.245123",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_e0996d6d",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file and retrieve its contents. Step 2: Use the data_processing_parser tool to parse the raw data extracted from the CSV file into a structured format (e.g., JSON). Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria (e.g., removing entries that do not meet certain conditions). Step 5: Use the data_processing_transformer tool to convert the filtered data from JSON format to XML format for further use. Step 6: Finally, use the file_operations_writer tool to write the transformed XML data into a new file for storage.",
      "inputs": {
        "input_data": {
          "data": [
            0.1409971026789506,
            0.07343646168132556,
            0.9176802032612472,
            0.12381432585271024,
            0.332906396902221
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:49.377351",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_de9bd261",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from a raw CSV file containing user information. Step 2: Use the data_processing_parser to parse the raw CSV data into a structured format (e.g., converting it into a JSON object). Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure the data's correctness and integrity. Step 4: Use the data_processing_filter to filter the validated data based on specified criteria (e.g., filtering users by age or location). Step 5: Use the data_processing_aggregator to aggregate the filtered data based on certain parameters (e.g., counting the number of users per location). Step 6: Use the file_operations_writer to write the aggregated results to a new output file in JSON format.",
      "inputs": {
        "input_data": {
          "data": [
            0.48498247871749556,
            0.46628518750154213,
            0.4915351195991269,
            0.011182911765409087,
            0.3680285094043333
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 9.255195645917436,
        "priority": "high"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:53.922688",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_f15651c0",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read data from a CSV file containing raw sales data. Step 2: Use the data_processing_parser to parse the raw sales data into a structured format. Step 3: Use the data_processing_validator to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to selectively filter the validated data based on specific criteria, such as sales above a certain threshold. Step 5: Use the data_processing_aggregator to aggregate the filtered sales data to calculate total sales per region. Step 6: Use the file_operations_writer to write the aggregated sales data into a new JSON file for reporting.",
      "inputs": {
        "input_data": {
          "data": [
            0.7897145933074599,
            0.29044752254024986,
            0.7199327720543818,
            0.5776951330796547,
            0.06247683275193139
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:57.783544",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_c5fbfe7e",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the raw data from a CSV file. This will retrieve the necessary data for processing. Step 2: Use the data_processing_parser tool to parse the retrieved data into a structured format, ensuring it is ready for further analysis. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema, ensuring the data's correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the valid data based on specific criteria, reducing the dataset to only the relevant information. Step 5: Use the computation_calculator tool to perform calculations on the filtered data, generating computed results. Step 6: Finally, use the file_operations_writer tool to write the computed results into a new CSV file, completing the data processing pipeline.",
      "inputs": {
        "input_data": {
          "data": [
            0.09844628681454115,
            0.27891537744887707,
            0.4076170365970684,
            0.8561310089042214,
            0.9644057849593
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "computation_calculator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:26:02.912736",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_d0316493",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read data from a CSV file containing raw data. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV into a structured format. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria. Step 5: Use the data_processing_transformer tool to transform the filtered data from the structured format into a desired output format, such as JSON. Step 6: Use the file_operations_writer tool to write the transformed data into a new file in the specified output format.",
      "inputs": {
        "input_data": {
          "data": [
            0.5223116638089276,
            0.8653896015968566,
            0.5384735547671707,
            0.3742065059027807,
            0.4535903168481016
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "priority": "high"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:18.560596",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_eb43c4fa",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data from a CSV file containing user information. Step 2: Use the data_processing_parser to parse the raw CSV data into a structured format (e.g., JSON) for easier manipulation. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure all required fields are present and correctly formatted. Step 4: Use the data_processing_filter to filter the validated data based on specific criteria, such as users from a certain location or age group. Step 5: Use the data_processing_aggregator to aggregate the filtered data to summarize important metrics, such as the total number of users or average age. Step 6: Finally, use the file_operations_writer to write the aggregated results to a new CSV file for reporting purposes.",
      "inputs": {
        "input_data": {
          "data": [
            0.2725962079252888,
            0.0885738760392818,
            0.9961971207988579,
            0.043572232894160456,
            0.9499765566854239
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 3.5997961107171443
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:23.496194",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_714a01bb",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use file_operations_reader to read raw data from a CSV file containing sales records. Step 2: Use data_processing_parser to parse the raw data into a structured JSON format for easier manipulation. Step 3: Use data_processing_validator to validate the structured JSON data against a predefined schema to ensure correctness and integrity. Step 4: Use data_processing_filter to filter the validated data based on specific criteria, such as sales above a certain threshold. Step 5: Use data_processing_aggregator to aggregate the filtered data, summarizing total sales by product category. Step 6: Use file_operations_writer to write the aggregated results into a new CSV file for reporting purposes.",
      "inputs": {
        "input_data": {
          "data": [
            0.5870109116021328,
            0.9684015276114459,
            0.9877946891461968,
            0.5922023617966619,
            0.8062129413070658
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:27.830544",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_7eaa1e00",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data file, retrieving the contents in a structured format. Step 2: Use the data_processing_parser to parse the raw data into a more organized structure, ensuring all relevant information is extracted. Step 3: Use the data_processing_validator to validate the parsed data against a predefined schema, confirming its correctness and integrity. Step 4: Use the data_processing_filter to apply specific filtering criteria to the validated data, reducing the dataset to only the most relevant entries. Step 5: Use the data_processing_transformer to convert the filtered data from JSON format to XML format for further processing. Step 6: Use the file_operations_writer to write the transformed XML data to a new output file.",
      "inputs": {
        "input_data": {
          "data": [
            0.27806781908529066,
            0.06608725142944383,
            0.9090508152704356,
            0.7865514933487963,
            0.11194562966544441
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "priority": "low"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:33.743003",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_3ff52402",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use [file_operations_reader] to read data from a CSV file containing raw data. This will allow us to retrieve the necessary information for processing. Step 2: Use [data_processing_parser] to parse the raw data extracted from the CSV file into a structured format, making it easier to work with. Step 3: Use [data_processing_validator] to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use [data_processing_filter] to selectively filter the validated data based on specific criteria, reducing the dataset to only the relevant entries. Step 5: Use [data_processing_transformer] to transform the filtered data into a desired format, such as converting it from JSON to XML. Step 6: Use [file_operations_writer] to write the transformed data into a new file, saving the final output for further use.",
      "inputs": {
        "input_data": {
          "data": [
            0.12711198990439387,
            0.43331553623934227,
            0.3551306021035602,
            0.604009493958476,
            0.8097120169658446
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 0.1965052149586733
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:39.061023",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_ea31a519",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read raw data from a CSV file. This will retrieve the data needed for processing. Step 2: Use the data_processing_parser tool to parse the retrieved raw data into a structured format, extracting key information for further analysis. Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema, ensuring its correctness and integrity. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria, reducing the dataset for more focused analysis. Step 5: Use the computation_calculator tool to perform mathematical calculations on the filtered data, generating results that can be analyzed. Step 6: Finally, use the computation_analyzer tool to analyze the computation results, providing statistical insights and trend analysis.",
      "inputs": {
        "input_data": {
          "data": [
            0.699199924693612,
            0.24953819263970511,
            0.5655796941272483,
            0.6936658744370833,
            0.8957778453980005
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "computation_calculator",
        "computation_analyzer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "priority": "high"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:43.545650",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_ad0e8b9b",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader to read the raw data file in CSV format. This step will retrieve the data from the file for further processing. Step 2: Use the data_processing_parser to parse the raw data retrieved from the file into a structured format, making it easier to work with. Step 3: Use the data_processing_validator to validate the structured data against a predefined schema to ensure its correctness and integrity. Step 4: Use the data_processing_filter to apply specific filtering criteria to the validated data, reducing the dataset to only the relevant records. Step 5: Use the data_processing_transformer to transform the filtered data from the structured format into JSON format for final output. Step 6: Use the file_operations_writer to write the transformed data into a new JSON file.",
      "inputs": {
        "input_data": {
          "data": [
            0.7981119548821874,
            0.3165622687078895,
            0.6129949161234679,
            0.989839156238602,
            0.3742809821202179
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "priority": "high"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:47.617095",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_72ce5f58",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' to read data from a CSV file containing raw data. Step 2: Use the 'data_processing_parser' to parse the raw data into a structured format, extracting relevant fields. Step 3: Use the 'data_processing_validator' to validate the parsed data against a predefined schema to ensure its correctness and integrity. Step 4: Use the 'data_processing_filter' to selectively filter the validated data based on specific criteria, narrowing down the dataset to only include the necessary records. Step 5: Use the 'data_processing_transformer' to convert the filtered data from JSON format to XML format, preparing it for further use. Step 6: Finally, use the 'file_operations_writer' to write the transformed XML data to a new file.",
      "inputs": {
        "input_data": {
          "data": [
            0.9182879591149984,
            0.9423130754592398,
            0.9767381170831995,
            0.7670961213718165,
            0.6823961536005415
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 3.590663032371211
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:53.679751",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_caa566b6",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the 'file_operations_reader' to read data from a specified input file in CSV format. This will retrieve the raw data needed for processing. Step 2: Use the 'data_processing_parser' to parse the raw data retrieved in the previous step into a structured format, making it easier to work with. Step 3: Use the 'data_processing_validator' to validate the structured data against a predefined schema, ensuring it meets the required standards for correctness and integrity. Step 4: Use the 'data_processing_filter' to selectively filter the validated data based on specified criteria, which helps in reducing the dataset to only the relevant information. Step 5: Use the 'data_processing_transformer' to transform the filtered data from the structured format into the desired output format, such as converting it from JSON to XML. Step 6: Finally, use the 'file_operations_writer' to write the transformed data to an output file in the specified format, ensuring that the results are saved for further use.",
      "inputs": {
        "input_data": {
          "data": [
            0.3691785323083864,
            0.47319772702833673,
            0.10386828778725343,
            0.3247129601683263,
            0.5703100020928382
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_transformer",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "priority": "low"
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:25:58.782630",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    },
    {
      "instance_id": "task_6139920d",
      "task_type": "multi_stage_pipeline",
      "description": "Step 1: Use the file_operations_reader tool to read the input data from a CSV file. Step 2: Use the data_processing_parser tool to parse the raw data from the CSV file into a structured format (e.g., JSON). Step 3: Use the data_processing_validator tool to validate the structured data against a predefined schema to ensure its correctness. Step 4: Use the data_processing_filter tool to selectively filter the validated data based on specified criteria (e.g., removing entries with null values). Step 5: Use the data_processing_aggregator tool to aggregate the filtered data for further analysis (e.g., summarizing totals). Step 6: Use the file_operations_writer tool to write the aggregated results to an output JSON file.",
      "inputs": {
        "input_data": {
          "data": [
            0.48008480991222247,
            0.01706310693818136,
            0.5936723748959769,
            8.439841684337868e-05,
            0.08847927413178036
          ]
        },
        "pipeline_config": {
          "stages": 3,
          "parallel": false
        }
      },
      "expected_outputs": {
        "pipeline_result": {
          "stages_completed": 3,
          "final_output": {}
        }
      },
      "required_tools": [
        "file_operations_reader",
        "data_processing_parser",
        "data_processing_validator",
        "data_processing_filter",
        "data_processing_aggregator",
        "file_operations_writer"
      ],
      "constraints": {
        "timeout": 600,
        "max_retries": 1,
        "required_confidence": 0.95,
        "max_cost": 6.378604484528213
      },
      "complexity": "hard",
      "metadata": {
        "template": "multi_stage_pipeline",
        "generated_at": "2025-07-09T00:26:03.725271",
        "timeout": 600,
        "semantic_generation": true,
        "llm_generated": true
      }
    }
  ],
  "metadata": {
    "generated_at": "2025-07-09T00:26:04.061399",
    "num_tasks": 1000,
    "parallel_generation": true,
    "tool_registry_path": "mcp_generated_library/tool_registry_consolidated.json",
    "llm_enhanced": true,
    "task_distribution": {
      "basic_task": 0.2,
      "simple_task": 0.2,
      "data_pipeline": 0.2,
      "api_integration": 0.2,
      "multi_stage_pipeline": 0.2
    },
    "generation_time": 51.98427391052246
  }
}