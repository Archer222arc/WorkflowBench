# ç›¸åŒä½ç½®çš„ä¿®å¤ä»£ç 
# ä¿®æ”¹çš„è¡Œç”¨æ³¨é‡Šæ ‡æ³¨ï¼š# <- ä¿®æ”¹äº†è¿™ä¸€è¡Œ

#!/usr/bin/env python3
"""
Unified Training Manager - Refactored for Better Structure
==========================================================
é‡æ„åçš„ç‰ˆæœ¬ï¼Œè§£å†³äº†ç±»èŒè´£ä¸æ¸…å’Œæ–¹æ³•ç¼ºå¤±çš„é—®é¢˜
"""

import os
import sys
import json
import logging
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.distributions import Categorical  # <- æ–°å¢è¿™ä¸€è¡Œï¼šå¯¼å…¥Categorical
from torch.optim.lr_scheduler import ReduceLROnPlateau
import numpy as np
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any, Union
from collections import defaultdict, deque, OrderedDict
from datetime import datetime
import random
import uuid
import re
import asyncio
from concurrent.futures import ThreadPoolExecutor, as_completed
import concurrent.futures
import faiss
import time
import threading
import math

from torch.distributed.fsdp import FullyShardedDataParallel as FSDP


from workflow_reasoning_generator import WorkflowReasoningGenerator
from tool_capability_manager import ToolCapabilityManager  # <- ä¿®æ”¹äº†è¿™ä¸€è¡Œï¼šæ·»åŠ å¯¼å…¥
from interactive_executor import ToolExecutionResult
from mcp_embedding_manager import MCPEmbeddingManager, SearchResult


# Configure logging
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
log_filename = f"logs/debug__unified_training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
file_handler = logging.FileHandler(log_filename)
file_handler.setLevel(logging.INFO)

# Device configuration
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
logger.info(f"Using device: {device}")

# Import ScoringThresholds for type annotation  # <- ä¿®æ”¹äº†è¿™ä¸€è¡Œ
from workflow_quality_test_flawed import ScoringThresholds  # <- ä¿®æ”¹äº†è¿™ä¸€è¡Œ


# æ–‡ä»¶ï¼šunified_training_manager.py
# ä½ç½®ï¼šEmbeddingDistillationCacheç±»

class EmbeddingDistillationCache:
    """åŸºäºè¯­ä¹‰embeddingçš„teacherè’¸é¦ç¼“å­˜ç³»ç»Ÿ"""
    
    def __init__(self, cache_dir: str = ".distillation_cache", 
                 max_size: int = 100000,
                 similarity_threshold: float = 0.95):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.max_size = max_size
        self.similarity_threshold = similarity_threshold
        
        # åˆå§‹åŒ–MCPEmbeddingManageræ¥è·å–embeddings
        from mcp_embedding_manager import get_embedding_manager
        self.embedding_manager = get_embedding_manager()
            
        # ç¼“å­˜æ–‡ä»¶è·¯å¾„
        self.cache_file = self.cache_dir / "teacher_cache_v2.pkl"
        self.index_file = self.cache_dir / "teacher_index.faiss"
        
        # ç¼“å­˜æ•°æ®ç»“æ„
        self.cache_entries = OrderedDict()  # key -> distribution
        self.cache_keys = []  # ä¿å­˜åŸå§‹keyç”¨äºæŸ¥æ‰¾
        self.embeddings = []  # å¯¹åº”çš„embeddingå‘é‡
        
        # FAISSç´¢å¼•
        self.index = None
        self.embedding_dim = None  # åŠ¨æ€ç¡®å®šç»´åº¦
        
        # åŠ è½½å·²æœ‰ç¼“å­˜
        self._load_cache()
        
    def _load_cache(self):
        """ä»ç£ç›˜åŠ è½½ç¼“å­˜å’Œç´¢å¼•"""
        if self.cache_file.exists():
            try:
                with open(self.cache_file, 'rb') as f:
                    saved_data = pickle.load(f)
                    self.cache_entries = saved_data.get('entries', OrderedDict())
                    self.cache_keys = saved_data.get('keys', [])
                    self.embeddings = saved_data.get('embeddings', [])
                    self.embedding_dim = saved_data.get('embedding_dim', None)
                print(f"[EmbeddingDistillationCache] Loaded {len(self.cache_entries)} cached entries")
                print(f"[EmbeddingDistillationCache] Embedding dimension: {self.embedding_dim}")
            except Exception as e:
                print(f"[EmbeddingDistillationCache] Failed to load cache: {e}")
                self.cache_entries = OrderedDict()
                self.cache_keys = []
                self.embeddings = []
                self.embedding_dim = None
        
        # é‡å»ºFAISSç´¢å¼•
        if self.embeddings and self.embedding_dim:
            self._rebuild_index()
    
    def _rebuild_index(self):
        """é‡å»ºFAISSç´¢å¼•"""
        if not self.embeddings or not self.embedding_dim:
            return
            
        try:
            # åˆ›å»ºFAISSç´¢å¼•
            self.index = faiss.IndexFlatIP(self.embedding_dim)  # ä½¿ç”¨å†…ç§¯ç›¸ä¼¼åº¦
            
            # æ£€æŸ¥æ‰€æœ‰embeddingsçš„ç»´åº¦
            valid_embeddings = []
            valid_indices = []
            
            for i, emb in enumerate(self.embeddings):
                emb_array = np.array(emb) if not isinstance(emb, np.ndarray) else emb
                if emb_array.shape[0] == self.embedding_dim:
                    valid_embeddings.append(emb_array)
                    valid_indices.append(i)
                else:
                    print(f"[EmbeddingDistillationCache] WARNING: Embedding {i} has wrong dimension {emb_array.shape[0]}, expected {self.embedding_dim}")
            
            if valid_embeddings:
                # åªæ·»åŠ æœ‰æ•ˆçš„embeddings
                embeddings_matrix = np.vstack(valid_embeddings).astype('float32')
                faiss.normalize_L2(embeddings_matrix)  # å½’ä¸€åŒ–ä»¥ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦
                self.index.add(embeddings_matrix)
                
                # å¦‚æœæœ‰æ— æ•ˆçš„embeddingsï¼Œæ¸…ç†å®ƒä»¬
                if len(valid_indices) < len(self.embeddings):
                    print(f"[EmbeddingDistillationCache] Cleaning up {len(self.embeddings) - len(valid_indices)} invalid embeddings")
                    # é‡å»ºæ•°æ®ç»“æ„ï¼Œåªä¿ç•™æœ‰æ•ˆçš„
                    new_embeddings = []
                    new_keys = []
                    new_entries = OrderedDict()
                    
                    for idx in valid_indices:
                        new_embeddings.append(self.embeddings[idx])
                        key = self.cache_keys[idx]
                        new_keys.append(key)
                        new_entries[key] = self.cache_entries[key]
                    
                    self.embeddings = new_embeddings
                    self.cache_keys = new_keys
                    self.cache_entries = new_entries
                
                print(f"[EmbeddingDistillationCache] Rebuilt index with {len(valid_embeddings)} entries")
            else:
                print(f"[EmbeddingDistillationCache] ERROR: No valid embeddings found!")
                self.index = None
                
        except Exception as e:
            print(f"[EmbeddingDistillationCache] ERROR rebuilding index: {e}")
            self.index = None
    
    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜åˆ°ç£ç›˜"""
        try:
            save_data = {
                'entries': self.cache_entries,
                'keys': self.cache_keys,
                'embeddings': self.embeddings,
                'embedding_dim': self.embedding_dim  # ä¿å­˜ç»´åº¦ä¿¡æ¯
            }
            with open(self.cache_file, 'wb') as f:
                pickle.dump(save_data, f)
            print(f"[EmbeddingDistillationCache] Saved {len(self.cache_entries)} entries to disk")
        except Exception as e:
            print(f"[EmbeddingDistillationCache] Failed to save cache: {e}")
    
    def _get_embedding(self, text: str) -> np.ndarray:
        """è·å–æ–‡æœ¬çš„embeddingï¼ˆä½¿ç”¨MCPEmbeddingManagerçš„ç¼“å­˜ï¼‰"""
        # ç›´æ¥ä½¿ç”¨embedding managerçš„ç¼“å­˜æœºåˆ¶
        embedding = self.embedding_manager._get_embedding(text)
        
        # ç¬¬ä¸€æ¬¡è·å–embeddingæ—¶è®¾ç½®ç»´åº¦
        if self.embedding_dim is None:
            self.embedding_dim = embedding.shape[0]
            print(f"[EmbeddingDistillationCache] Set embedding dimension to {self.embedding_dim}")
        
        return embedding
    
    def get(self, state_desc: str, tool_list: List[str], 
            rag_context: Optional[Dict] = None) -> Optional[Dict[str, float]]:
        """è·å–ç¼“å­˜çš„åˆ†å¸ƒï¼ˆåŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦ï¼‰"""
        if not self.index or len(self.cache_entries) == 0:
            return None
        
        # åˆ›å»ºæŸ¥è¯¢key
        query_key = self._create_cache_key(state_desc, tool_list, rag_context)
        
        # è·å–æŸ¥è¯¢embedding
        query_embedding = self._get_embedding(query_key)
        
        # æ£€æŸ¥ç»´åº¦
        if query_embedding.shape[0] != self.embedding_dim:
            print(f"[EmbeddingDistillationCache] WARNING: Query embedding dimension {query_embedding.shape[0]} != expected {self.embedding_dim}")
            return None
        
        query_embedding = query_embedding.reshape(1, -1).astype('float32')
        faiss.normalize_L2(query_embedding)
        
        # æœç´¢æœ€ç›¸ä¼¼çš„ç¼“å­˜é¡¹
        distances, indices = self.index.search(query_embedding, 1)
        
        if distances[0][0] >= self.similarity_threshold:
            # æ‰¾åˆ°è¶³å¤Ÿç›¸ä¼¼çš„ç¼“å­˜é¡¹
            cache_idx = indices[0][0]
            original_key = self.cache_keys[cache_idx]
            
            # æ›´æ–°LRUé¡ºåº
            self.cache_entries.move_to_end(original_key)
            
            print(f"[EmbeddingDistillationCache] Cache hit with similarity {distances[0][0]:.3f}")
            return self.cache_entries[original_key]
        
        return None
    
    def put(self, state_desc: str, tool_list: List[str], 
            distribution: Dict[str, float], rag_context: Optional[Dict] = None):
        """å­˜å‚¨åˆ†å¸ƒåˆ°ç¼“å­˜"""
        # åˆ›å»ºkey
        cache_key = self._create_cache_key(state_desc, tool_list, rag_context)
        
        # è·å–embedding
        embedding = self._get_embedding(cache_key)
        
        # æ£€æŸ¥ç»´åº¦
        if self.embedding_dim and embedding.shape[0] != self.embedding_dim:
            print(f"[EmbeddingDistillationCache] ERROR: Embedding dimension mismatch: {embedding.shape[0]} != {self.embedding_dim}")
            return
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ é™¤æ—§æ¡ç›®
        if len(self.cache_entries) >= self.max_size:
            # åˆ é™¤æœ€æ—§çš„æ¡ç›®
            oldest_key = next(iter(self.cache_entries))
            self.cache_entries.pop(oldest_key)
            
            # æ‰¾åˆ°å¹¶åˆ é™¤å¯¹åº”çš„ç´¢å¼•
            old_idx = self.cache_keys.index(oldest_key)
            self.cache_keys.pop(old_idx)
            self.embeddings.pop(old_idx)
            
            # éœ€è¦é‡å»ºç´¢å¼•
            self._rebuild_index()
        
        # æ·»åŠ æ–°æ¡ç›®
        self.cache_entries[cache_key] = distribution
        self.cache_keys.append(cache_key)
        self.embeddings.append(embedding)
        
        # æ›´æ–°FAISSç´¢å¼•
        if self.index is None:
            self._rebuild_index()
        else:
            # æ·»åŠ å•ä¸ªå‘é‡åˆ°ç´¢å¼•
            embedding_matrix = embedding.reshape(1, -1).astype('float32')
            faiss.normalize_L2(embedding_matrix)
            self.index.add(embedding_matrix)
        
        # å®šæœŸä¿å­˜
        if len(self.cache_entries) % 100 == 0:
            self._save_cache()
            
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        self.cache_entries = OrderedDict()
        self.cache_keys = []
        self.embeddings = []
        self.index = None
        self.embedding_dim = None
        print("[EmbeddingDistillationCache] Cache cleared")
    
    def _create_cache_key(self, state_desc: str, tool_list: List[str], 
                         rag_context: Optional[Dict] = None) -> str:
        """åˆ›å»ºç”¨äºç”Ÿæˆembeddingçš„æ–‡æœ¬æè¿°"""
        # è¿‡æ»¤Noneå€¼å’Œç‰¹æ®ŠåŠ¨ä½œ
        valid_tools = []
        for tool in tool_list:
            if tool is not None and not tool.startswith('action_'):
                valid_tools.append(tool)
        
        # æ„å»ºæè¿°æ–‡æœ¬
        key_parts = [
            f"Task: {state_desc}",
            f"Available tools: {', '.join(sorted(valid_tools)) if valid_tools else 'none'}"
        ]
        
        # æ·»åŠ RAGä¸Šä¸‹æ–‡ä¿¡æ¯
        if rag_context:
            rag_info = []
            for operation, results in rag_context.items():
                if results and len(results) > 0:
                    top_result = results[0]
                    if hasattr(top_result, 'tool_name'):
                        rag_info.append(f"{operation}:{top_result.tool_name}")
            if rag_info:
                key_parts.append(f"RAG context: {', '.join(rag_info)}")
        
        return " | ".join(key_parts)


class ActorCriticNetwork(nn.Module):
    """è¶…çº§å¢å¼ºçš„Actor-Criticç½‘ç»œï¼ŒåŒ…å«æœ€æ–°çš„æ·±åº¦å­¦ä¹ æŠ€æœ¯ã€RAGæ”¯æŒå’Œrequired_toolsè¾“å…¥"""
    
    def __init__(self, state_dim: int, action_dim: int, config: Dict[str, Any] = None):
        super().__init__()
        
        print(f"[ActorCriticNetwork.__init__] Debug - state_dim={state_dim}, action_dim={action_dim}, config type={type(config)}")
        
        # æ£€æŸ¥action_dimæ˜¯å¦ä¸ºNone
        if action_dim is None:
            print("[ActorCriticNetwork] ERROR: action_dim is None!")
            raise ValueError("action_dim cannot be None. Please check model loading logic.")
        
        # å…¼å®¹æ€§å¤„ç†ï¼šæ£€æŸ¥ç¬¬ä¸‰ä¸ªå‚æ•°æ˜¯å¦ä¸ºé…ç½®å­—å…¸
        if config is None:
            print("[ActorCriticNetwork] Warning: config is None, using default configuration")
            config = {}
        elif isinstance(config, int):
            # å‘åå…¼å®¹ï¼šå¦‚æœä¼ å…¥çš„æ˜¯æ•´æ•°ï¼Œå°†å…¶ä½œä¸ºhidden_dim
            print(f"[ActorCriticNetwork] Warning: received int {config} instead of dict, treating as hidden_dim")
            config = {'hidden_dim': config}
        elif not isinstance(config, dict):
            print(f"[ActorCriticNetwork] Error: config is neither dict nor int, type={type(config)}")
            print(f"[ActorCriticNetwork] config value: {config}")
            # ä½¿ç”¨é»˜è®¤é…ç½®
            config = {}
        
        self.state_dim = state_dim
        self.action_dim = action_dim
        self.expected_state_dim = state_dim

        # ä»é…ç½®ä¸­è·å–å‚æ•°ï¼Œç¡®ä¿ç±»å‹æ­£ç¡®
        self.hidden_dim = int(config.get('hidden_dim', 1024))
        self.num_layers = int(config.get('num_layers', 6))
        self.num_heads = int(config.get('num_heads', 16))
        self.dropout = float(config.get('dropout', 0.1))
        self.use_pre_norm = bool(config.get('use_pre_norm', True))
        self.use_rag_enhancement = bool(config.get('use_rag_enhancement', True))
        self.rag_dim = int(config.get('rag_dim', 64))
        # ç§»é™¤å¯¹use_tools_inputçš„ä¾èµ–ï¼Œå§‹ç»ˆåˆ›å»ºtools_projection
        self.num_tools = int(config.get('num_tools', action_dim))
        self.tools_dim = int(config.get('tools_dim', 64))

        print(self.num_tools, "number of tools \n\n\n")
        
        # éªŒè¯hidden_dimçš„å€¼
        if self.hidden_dim <= 0:
            print(f"[ActorCriticNetwork] Error: invalid hidden_dim={self.hidden_dim}, using default 1024")
            self.hidden_dim = 1024
        
        print(f"[ActorCriticNetwork] Initialized with hidden_dim={self.hidden_dim}, num_layers={self.num_layers}")
        
        # è¾“å…¥æŠ•å½±
        self.input_projection = nn.Sequential(
            nn.Linear(state_dim, self.hidden_dim),
            nn.LayerNorm(self.hidden_dim),
            nn.GELU(),
            nn.Dropout(self.dropout)
        )

        self.dynamic_adapter = None

        
        # RAGæŠ•å½±å±‚ï¼ˆæ¡ä»¶åˆ›å»ºï¼‰
        self.rag_projection = nn.Sequential(
            nn.Linear(self.rag_dim, self.hidden_dim),
            nn.LayerNorm(self.hidden_dim),
            nn.GELU(),
            nn.Dropout(self.dropout)
        )
        print(f"[ActorCriticNetwork] RAG projection layer created with dim={self.rag_dim}")
        
        # ToolsæŠ•å½±å±‚ - å§‹ç»ˆåˆ›å»ºï¼ˆå…³é”®ä¿®æ”¹ï¼šç§»é™¤æ¡ä»¶åˆ¤æ–­ï¼‰
        self.tools_projection = nn.Sequential(
            nn.Linear(self.tools_dim, self.hidden_dim),
            nn.LayerNorm(self.hidden_dim),
            nn.GELU(),
            nn.Dropout(self.dropout)
        )
        print(f"[ActorCriticNetwork] Tools projection layer ALWAYS created with dim={self.tools_dim}")
        
        # Transformerç¼–ç å™¨å±‚
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=self.hidden_dim,
            nhead=self.num_heads,
            dim_feedforward=self.hidden_dim * 4,
            dropout=self.dropout,
            activation='gelu',
            norm_first=self.use_pre_norm,
            batch_first=True
        )
        
        self.transformer_encoder = nn.TransformerEncoder(
            encoder_layer,
            num_layers=self.num_layers,
            norm=nn.LayerNorm(self.hidden_dim) if self.use_pre_norm else None
        )
        
        # ä½ç½®ç¼–ç ï¼ˆå¯å­¦ä¹ ï¼‰
        self.pos_encoding = nn.Parameter(torch.randn(1, 128, self.hidden_dim))
        
        # Actorå’ŒCriticå¤´éƒ¨ï¼ˆä½¿ç”¨æ›´æ·±çš„ç½‘ç»œï¼‰
        self.actor_head = nn.Sequential(
            nn.Linear(self.hidden_dim, self.hidden_dim // 2),
            nn.LayerNorm(self.hidden_dim // 2),
            nn.GELU(),
            nn.Dropout(self.dropout),
            nn.Linear(self.hidden_dim // 2, self.hidden_dim // 4),
            nn.LayerNorm(self.hidden_dim // 4),
            nn.GELU(),
            nn.Linear(self.hidden_dim // 4, action_dim)
        )
        
        self.critic_head = nn.Sequential(
            nn.Linear(self.hidden_dim, self.hidden_dim // 2),
            nn.LayerNorm(self.hidden_dim // 2),
            nn.GELU(),
            nn.Dropout(self.dropout),
            nn.Linear(self.hidden_dim // 2, self.hidden_dim // 4),
            nn.LayerNorm(self.hidden_dim // 4),
            nn.GELU(),
            nn.Linear(self.hidden_dim // 4, 1)
        )
        
        # è¾…åŠ©ä»»åŠ¡å¤´éƒ¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if config.get('use_auxiliary_tasks', False):
            # ä¸‹ä¸€æ­¥å·¥å…·é¢„æµ‹
            self.next_tool_predictor = nn.Linear(self.hidden_dim, action_dim)
            
            # è¿›åº¦é¢„æµ‹
            self.progress_predictor = nn.Linear(self.hidden_dim, 1)
            
            # é”™è¯¯é¢„æµ‹
            self.error_predictor = nn.Linear(self.hidden_dim, 2)  # äºŒåˆ†ç±»ï¼šæ˜¯å¦ä¼šå‡ºé”™
        
        # å¥½å¥‡å¿ƒæ¨¡å—ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if config.get('use_curiosity', False):
            # å‰å‘æ¨¡å‹ï¼šé¢„æµ‹ä¸‹ä¸€ä¸ªçŠ¶æ€
            self.forward_model = nn.Sequential(
                nn.Linear(state_dim + action_dim, self.hidden_dim),
                nn.ReLU(),
                nn.Linear(self.hidden_dim, state_dim)
            )
            
            # é€†å‘æ¨¡å‹ï¼šä»çŠ¶æ€é¢„æµ‹åŠ¨ä½œ
            self.inverse_model = nn.Sequential(
                nn.Linear(state_dim * 2, self.hidden_dim),
                nn.ReLU(),
                nn.Linear(self.hidden_dim, action_dim)
            )
        
        # å¤šä»»åŠ¡é€‚é…å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if config.get('use_task_adapters', False):
            self.task_adapters = nn.ModuleDict()
            for task_type in ['data_pipeline', 'api_integration', 'basic_task', 'multi_stage']:
                self.task_adapters[task_type] = nn.Sequential(
                    nn.Linear(self.hidden_dim, self.hidden_dim // 2),
                    nn.ReLU(),
                    nn.Linear(self.hidden_dim // 2, self.hidden_dim // 4)
                )
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
        
        # è°±å½’ä¸€åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if config.get('spectral_norm', False):
            self._apply_spectral_norm()
        
        print("[ActorCriticNetwork] Initialization complete with tools_projection always created")
    def _init_weights(self):
        """æ”¹è¿›çš„æƒé‡åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.orthogonal_(m.weight, gain=np.sqrt(2))
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.LayerNorm):
                nn.init.constant_(m.weight, 1)
                nn.init.constant_(m.bias, 0)
            elif isinstance(m, nn.TransformerEncoderLayer):
                # Transformerå±‚çš„ç‰¹æ®Šåˆå§‹åŒ–
                for p in m.parameters():
                    if p.dim() > 1:
                        nn.init.xavier_uniform_(p)
    
    def _apply_spectral_norm(self):
        """åº”ç”¨è°±å½’ä¸€åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Linear) and m.out_features > 1:
                nn.utils.spectral_norm(m)


    def forward(self, state, rag_context=None, required_tools=None, return_features=False):
        """å¢å¼ºçš„å‰å‘ä¼ æ’­ï¼Œæ”¯æŒRAGä¸Šä¸‹æ–‡å’Œrequired_toolsè¾“å…¥ - ä¿®å¤FSDPå…¼å®¹æ€§"""
        
        # ç»´åº¦æ£€æŸ¥å’Œè‡ªåŠ¨ä¿®å¤ - å¤„ç†1Dè¾“å…¥çš„æƒ…å†µ
        if state.dim() == 1:
            logger.debug(f"[PPORAGNetwork.forward] WARNING: Received 1D state tensor with shape {state.shape}, adding batch dimension")
            state = state.unsqueeze(0)  # æ·»åŠ  batch ç»´åº¦
        elif state.dim() > 3:
            logger.debug(f"[PPORAGNetwork.forward] ERROR: State tensor has too many dimensions: {state.shape}")
            raise ValueError(f"State tensor must be 1D, 2D or 3D, got {state.dim()}D tensor")
        
        logger.debug(f"[PPORAGNetwork.forward] Input state shape: {state.shape}, expected state_dim: {self.expected_state_dim}")
        
        # è·å–è¾“å…¥çš„æ•°æ®ç±»å‹å’Œè®¾å¤‡ï¼Œç”¨äºä¿æŒä¸€è‡´æ€§
        target_dtype = state.dtype
        target_device = state.device
        logger.debug(f"[PPORAGNetwork.forward] Target dtype: {target_dtype}, device: {target_device}")
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šæ£€æŸ¥æ˜¯å¦åœ¨FSDPç¯å¢ƒä¸‹
        # æ–¹æ³•1ï¼šæ£€æŸ¥æ¨¡å‹æ˜¯å¦è¢«FSDPåŒ…è£…
        if isinstance(self, FSDP):
            logger.debug(f"[PPORAGNetwork.forward] Model is wrapped with FSDP")
            # åœ¨FSDPç¯å¢ƒä¸‹ï¼Œç¡®ä¿ä½¿ç”¨æ­£ç¡®çš„è®¾å¤‡
            if hasattr(self, 'compute_device'):
                target_device = self.compute_device
                logger.debug(f"[PPORAGNetwork.forward] Using FSDP compute_device: {target_device}")
                state = state.to(target_device)
        
        # æ–¹æ³•2ï¼šæ£€æŸ¥ç¬¬ä¸€ä¸ªå‚æ•°çš„è®¾å¤‡ï¼Œç¡®ä¿ä¸€è‡´æ€§
        # è·å–æ¨¡å‹å‚æ•°çš„è®¾å¤‡ï¼ˆé€šè¿‡ç¬¬ä¸€ä¸ªå‚æ•°ï¼‰
        try:
            first_param = next(self.parameters())
            param_device = first_param.device
            if state.device != param_device:
                logger.debug(f"[PPORAGNetwork.forward] Moving state from {state.device} to model device {param_device}")
                state = state.to(param_device)
                target_device = param_device
        except StopIteration:
            # æ²¡æœ‰å‚æ•°çš„æƒ…å†µï¼ˆä¸å¤ªå¯èƒ½ï¼‰
            logger.debug(f"[PPORAGNetwork.forward] Warning: Model has no parameters")
        
        # æ£€æŸ¥è¾“å…¥ç»´åº¦
        actual_state_dim = state.shape[-1]
        if actual_state_dim != self.expected_state_dim:
            logger.debug(f"[PPORAGNetwork.forward] WARNING: State dimension mismatch! Expected {self.expected_state_dim}, got {actual_state_dim}")
            
            # åŠ¨æ€åˆ›å»ºé€‚é…å±‚
            if self.dynamic_adapter is None or self.dynamic_adapter.in_features != actual_state_dim:
                logger.debug(f"[PPORAGNetwork.forward] Creating dynamic adapter: {actual_state_dim} -> {self.expected_state_dim}")
                self.dynamic_adapter = nn.Linear(actual_state_dim, self.expected_state_dim).to(target_device, dtype=target_dtype)
                # åˆå§‹åŒ–ä¸ºè¿‘ä¼¼æ’ç­‰æ˜ å°„
                with torch.no_grad():
                    if actual_state_dim <= self.expected_state_dim:
                        # å¦‚æœå®é™…ç»´åº¦æ›´å°ï¼Œå¡«å……é›¶
                        self.dynamic_adapter.weight.zero_()
                        self.dynamic_adapter.weight[:actual_state_dim, :actual_state_dim] = torch.eye(actual_state_dim).to(target_device, dtype=target_dtype)
                    else:
                        # å¦‚æœå®é™…ç»´åº¦æ›´å¤§ï¼Œæˆªæ–­
                        self.dynamic_adapter.weight.zero_()
                        self.dynamic_adapter.weight[:, :self.expected_state_dim] = torch.eye(self.expected_state_dim, actual_state_dim).T.to(target_device, dtype=target_dtype)
                    self.dynamic_adapter.bias.zero_()
            
            # ç¡®ä¿adapteråœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            if self.dynamic_adapter.weight.device != target_device:
                self.dynamic_adapter = self.dynamic_adapter.to(target_device)
            
            # ä½¿ç”¨é€‚é…å±‚è½¬æ¢è¾“å…¥
            state = self.dynamic_adapter(state)
            logger.debug(f"[PPORAGNetwork.forward] After adaptation: {state.shape}")
            
            # é‡è¦ï¼šç¡®ä¿ç»è¿‡adapteråä»ç„¶ä¿æŒæ­£ç¡®çš„ç»´åº¦
            if state.dim() == 1:
                logger.debug(f"[PPORAGNetwork.forward] ERROR: State became 1D after adapter, this should not happen!")
                logger.debug(f"[PPORAGNetwork.forward] dynamic_adapter info: in_features={self.dynamic_adapter.in_features}, out_features={self.dynamic_adapter.out_features}")
                logger.debug(f"[PPORAGNetwork.forward] Fixing by adding batch dimension...")
                state = state.unsqueeze(0)
            elif state.dim() != 2 and state.dim() != 3:
                logger.debug(f"[PPORAGNetwork.forward] ERROR: Unexpected state dimension after adapter: {state.dim()}")
                raise ValueError(f"State tensor after adapter must be 2D or 3D, got {state.dim()}D tensor with shape {state.shape}")
        
        # æœ€ç»ˆç»´åº¦æ£€æŸ¥ - åœ¨è°ƒç”¨input_projectionä¹‹å‰
        if state.dim() == 1:
            logger.debug(f"[PPORAGNetwork.forward] CRITICAL: State is still 1D before input_projection!")
            state = state.unsqueeze(0)
        
        logger.debug(f"[PPORAGNetwork.forward] Final state shape before projection: {state.shape}")
        logger.debug(f"[PPORAGNetwork.forward] State device: {state.device}, dtype: {state.dtype}")
        
        # ğŸ”§ å…³é”®ä¿®å¤ï¼šç¡®ä¿æ•°æ®ç±»å‹å…¼å®¹æ€§
        # åœ¨å¤šGPUç¯å¢ƒä¸‹ï¼Œä¿æŒfloat32ä»¥é¿å…ç²¾åº¦é—®é¢˜
        if state.dtype not in [torch.float32, torch.float16]:
            logger.debug(f"[PPORAGNetwork.forward] Converting state from {state.dtype} to float32")
            state = state.to(torch.float32)
            target_dtype = torch.float32
        
        # è¾“å…¥æŠ•å½± - è¿™é‡Œæ˜¯å‡ºé”™çš„åœ°æ–¹
        logger.debug(f"[PPORAGNetwork.forward] Calling input_projection with state shape: {state.shape}, device: {state.device}, dtype: {state.dtype}")
        
        # ç¡®ä¿stateåœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Šï¼ˆæœ€åä¸€æ¬¡æ£€æŸ¥ï¼‰
        state = state.to(target_device)
        
        x = self.input_projection(state)
        logger.debug(f"[PPORAGNetwork.forward] After input_projection: shape={x.shape}, device={x.device}, dtype={x.dtype}")
        
        # å‡†å¤‡åºåˆ—å…ƒç´ åˆ—è¡¨
        sequence_elements = [x.unsqueeze(1) if x.dim() == 2 else x]
        
        # RAGä¸Šä¸‹æ–‡èåˆï¼ˆå¦‚æœæä¾›ï¼‰
        if rag_context is not None and hasattr(self, 'rag_projection'):
            # ç¡®ä¿rag_contextå…·æœ‰æ­£ç¡®çš„ç»´åº¦
            if rag_context.dim() == 1:
                logger.debug(f"[PPORAGNetwork.forward] RAG context is 1D, adding batch dimension")
                rag_context = rag_context.unsqueeze(0)
            
            # ç¡®ä¿rag_contextå…·æœ‰æ­£ç¡®çš„æ•°æ®ç±»å‹å’Œè®¾å¤‡
            rag_context = rag_context.to(dtype=target_dtype, device=target_device)
            rag_features = self.rag_projection(rag_context)
            if rag_features.dim() == 2:
                rag_features = rag_features.unsqueeze(1)
            sequence_elements.append(rag_features)
            logger.debug(f"Added RAG features to sequence")
        
        # æ–°å¢ï¼šRequired toolsèåˆï¼ˆå¦‚æœæä¾›ï¼‰
        if required_tools is not None and hasattr(self, 'tools_projection'):
            # ç¡®ä¿required_toolså…·æœ‰æ­£ç¡®çš„ç»´åº¦
            if required_tools.dim() == 1:
                logger.debug(f"[PPORAGNetwork.forward] Required tools is 1D, adding batch dimension")
                required_tools = required_tools.unsqueeze(0)
            
            # ç¡®ä¿required_toolså…·æœ‰æ­£ç¡®çš„æ•°æ®ç±»å‹å’Œè®¾å¤‡
            required_tools = required_tools.to(dtype=target_dtype, device=target_device)
            tools_features = self.tools_projection(required_tools)
            if tools_features.dim() == 2:
                tools_features = tools_features.unsqueeze(1)
            sequence_elements.append(tools_features)
            logger.debug(f"Added tools features to sequence")
        
        # åˆå¹¶æ‰€æœ‰åºåˆ—å…ƒç´ 
        x = torch.cat(sequence_elements, dim=1)
        
        # æ·»åŠ ä½ç½®ç¼–ç  - ç¡®ä¿æ•°æ®ç±»å‹åŒ¹é…
        seq_len = x.size(1)
        if seq_len > self.pos_encoding.size(1):
            # åŠ¨æ€æ‰©å±•ä½ç½®ç¼–ç ï¼Œä½¿ç”¨æ­£ç¡®çš„dtypeå’Œdevice
            additional_pos = torch.randn(1, seq_len - self.pos_encoding.size(1), self.hidden_dim, 
                                    dtype=target_dtype, device=target_device)
            # ç¡®ä¿åŸå§‹ä½ç½®ç¼–ç ä¹Ÿå…·æœ‰æ­£ç¡®çš„æ•°æ®ç±»å‹
            self.pos_encoding.data = self.pos_encoding.data.to(dtype=target_dtype, device=target_device)
            self.pos_encoding = nn.Parameter(torch.cat([self.pos_encoding, additional_pos], dim=1))
            logger.debug(f"[PPORAGNetwork.forward] Expanded pos_encoding to seq_len={seq_len}")
        
        # ç¡®ä¿ä½ç½®ç¼–ç å…·æœ‰æ­£ç¡®çš„æ•°æ®ç±»å‹å’Œè®¾å¤‡
        pos_encoding_slice = self.pos_encoding[:, :seq_len, :].to(dtype=target_dtype, device=target_device)
        x = x + pos_encoding_slice
        
        # Transformerç¼–ç 
        logger.debug(f"[PPORAGNetwork.forward] Before transformer: x shape={x.shape}, dtype={x.dtype}, device={x.device}")
        features = self.transformer_encoder(x)
        logger.debug(f"[PPORAGNetwork.forward] After transformer: features shape={features.shape}, dtype={features.dtype}, device={features.device}")
        
        # å…¨å±€æ± åŒ–ï¼ˆå–å¹³å‡ï¼‰
        global_features = features.mean(dim=1)
        
        # è®¡ç®—è¾“å‡º
        policy_logits = self.actor_head(global_features)
        value = self.critic_head(global_features)
        
        logger.debug(f"[PPORAGNetwork.forward] Final output - policy_logits shape: {policy_logits.shape}, value shape: {value.shape}")
        logger.debug(f"[PPORAGNetwork.forward] Output device: {policy_logits.device}, dtype: {policy_logits.dtype}")
        
        if return_features:
            return policy_logits, value, global_features
        else:
            return policy_logits, value

            
    def get_action_and_value(self, state, action=None, rag_context=None, required_tools=None):
        """è·å–åŠ¨ä½œåˆ†å¸ƒã€é‡‡æ ·åŠ¨ä½œã€å¯¹æ•°æ¦‚ç‡å’Œä»·å€¼"""
        logits, value = self(state, rag_context, required_tools)
        
        # åˆ›å»ºåˆ†ç±»åˆ†å¸ƒ
        probs = F.softmax(logits, dim=-1)
        dist = torch.distributions.Categorical(probs)
        
        if action is None:
            action = dist.sample()
        
        return action, dist.log_prob(action), dist.entropy(), value
    
    def load_from_old_version(self, old_state_dict):
        """ä»æ—§ç‰ˆæœ¬æ¨¡å‹åŠ è½½æƒé‡çš„å…¼å®¹æ€§æ–¹æ³•"""
        new_state_dict = self.state_dict()
        
        # å¤åˆ¶åŒ¹é…çš„é”®
        for key in old_state_dict:
            if key in new_state_dict and old_state_dict[key].shape == new_state_dict[key].shape:
                new_state_dict[key] = old_state_dict[key]
                print(f"[INFO] Loaded weights for {key}")
            else:
                print(f"[WARNING] Skipping {key} (not found or shape mismatch)")
        
        # åŠ è½½æ›´æ–°åçš„çŠ¶æ€å­—å…¸
        self.load_state_dict(new_state_dict, strict=False)
        print("[INFO] Loaded weights from old version with compatibility mapping")



def encode_required_tools_embedding(required_tools: List[str], embedding_manager=None, target_dim: int = None) -> np.ndarray:
    """
    å°†required_toolsåˆ—è¡¨ç¼–ç ä¸ºå›ºå®šå¤§å°çš„embeddingå‘é‡
    ä½¿ç”¨ä¸RAGç›¸åŒçš„ç¼–ç æ–¹å¼ä¿æŒä¸€è‡´æ€§
    
    Args:
        required_tools: éœ€è¦çš„å·¥å…·åç§°åˆ—è¡¨
        embedding_manager: embeddingç®¡ç†å™¨å®ä¾‹
        target_dim: ç›®æ ‡ç»´åº¦ï¼Œå¦‚æœä¸ºNoneåˆ™ä»embedding_manageræ¨æ–­
        
    Returns:
        å›ºå®šå¤§å°çš„embeddingå‘é‡
    """
    # ç¡®å®šç›®æ ‡ç»´åº¦
    if target_dim is not None:
        embedding_dim = target_dim
    else:
        # é»˜è®¤ä½¿ç”¨64ç»´ä»¥ä¿æŒå‘åå…¼å®¹
        embedding_dim = 64
        print(f"[WARNING] encode_required_tools_embedding: target_dim not specified, using default {embedding_dim}")
    
    print(f"[DEBUG] encode_required_tools_embedding using target dim={embedding_dim}")
    
    tools_embedding = np.zeros(embedding_dim)
    
    if not required_tools:
        return tools_embedding
    
    # å¦‚æœæœ‰embedding_managerï¼Œä½¿ç”¨çœŸå®çš„å·¥å…·embeddings
    if embedding_manager and hasattr(embedding_manager, 'tool_embeddings'):
        all_embeddings = []
        all_scores = []
        
        for tool_name in required_tools:
            if tool_name in embedding_manager.tool_embeddings:
                tool_emb = embedding_manager.tool_embeddings[tool_name]
                # ä½¿ç”¨combined_embeddingï¼ˆç»¼åˆäº†æè¿°ã€å‚æ•°ã€åŠŸèƒ½ï¼‰
                if hasattr(tool_emb, 'combined_embedding'):
                    all_embeddings.append(tool_emb.combined_embedding)
                    all_scores.append(1.0)  # required toolsæƒé‡æœ€é«˜
        
        if all_embeddings:
            # åŠ æƒå¹³å‡
            all_embeddings = np.array(all_embeddings)
            all_scores = np.array(all_scores)
            all_scores = all_scores / all_scores.sum()
            
            # è®¡ç®—åŠ æƒembedding
            high_dim_embedding = np.zeros(all_embeddings.shape[1])
            for i, (emb, score) in enumerate(zip(all_embeddings, all_scores)):
                high_dim_embedding += emb * score
            
            # é™ç»´åˆ°ç›®æ ‡ç»´åº¦
            if high_dim_embedding.shape[0] > embedding_dim:
                # ä½¿ç”¨PCAæˆ–ç®€å•çš„æŠ•å½±æ¥é™ç»´
                # è¿™é‡Œä½¿ç”¨ç®€å•çš„åˆ†æ®µå¹³å‡æ–¹æ³•
                chunk_size = high_dim_embedding.shape[0] // embedding_dim
                for i in range(embedding_dim):
                    start_idx = i * chunk_size
                    end_idx = start_idx + chunk_size if i < embedding_dim - 1 else high_dim_embedding.shape[0]
                    tools_embedding[i] = high_dim_embedding[start_idx:end_idx].mean()
            else:
                # å¦‚æœåŸå§‹ç»´åº¦å°äºç›®æ ‡ç»´åº¦ï¼Œç›´æ¥å¤åˆ¶å¹¶å¡«å……é›¶
                tools_embedding[:high_dim_embedding.shape[0]] = high_dim_embedding
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°embeddingï¼Œä½¿ç”¨ç®€å•ç¼–ç 
            for i, tool in enumerate(required_tools[:10]):  # æœ€å¤š10ä¸ªå·¥å…·
                # åœ¨embeddingç©ºé—´ä¸­åˆ†æ•£ç¼–ç 
                index = hash(tool) % embedding_dim
                tools_embedding[index] = 1.0
                # æ·»åŠ ä¸€äº›ç›¸é‚»ä½ç½®çš„æ¿€æ´»
                for offset in [-1, 1]:
                    neighbor_idx = (index + offset) % embedding_dim
                    tools_embedding[neighbor_idx] = 0.5
    else:
        # Fallbackï¼šç®€å•çš„one-hoté£æ ¼ç¼–ç 
        for i, tool in enumerate(required_tools[:10]):
            # ä½¿ç”¨å“ˆå¸Œå‡½æ•°å°†å·¥å…·åæ˜ å°„åˆ°embeddingç©ºé—´
            index = hash(tool) % embedding_dim
            tools_embedding[index] = 1.0
    
    # å½’ä¸€åŒ–
    norm = np.linalg.norm(tools_embedding)
    if norm > 0:
        tools_embedding = tools_embedding / norm
    
    return tools_embedding

# ===========================
# Base Trainer Interface
# ===========================

# æ–‡ä»¶ï¼šunified_training_manager.py
# ç±»ï¼šBaseTrainerï¼ˆè¿›ä¸€æ­¥å¢å¼ºç‰ˆæœ¬ï¼‰
# ä½ç½®ï¼šçº¦310-500è¡Œ

from abc import ABC, abstractmethod

# æ–‡ä»¶ï¼šunified_training_manager.py
# ä½ç½®ï¼šBaseTrainerç±»çš„__init__æ–¹æ³•ï¼ˆçº¦310-400è¡Œï¼‰
# æ³¨æ„ï¼šå®Œæ•´çš„ä¿®å¤ç‰ˆæœ¬ï¼Œæ·»åŠ äº†eval_modeåˆå§‹åŒ–

class BaseTrainer(ABC):
    """Enhanced abstract base class for all training algorithms"""
    
    def __init__(self, env: 'MDPEnvironment', config: Dict[str, Any]):
        self.env = env
        self.config = config
        
        # å¢å¼ºçš„è®¾å¤‡é€‰æ‹©é€»è¾‘
        print(f"[BaseTrainer.__init__] Initializing device selection")
        print(f"[BaseTrainer.__init__] Config device: {config.get('device', 'not specified')}")
        print(f"[BaseTrainer.__init__] CUDA available: {torch.cuda.is_available()}")
        
        # ä¼˜å…ˆä½¿ç”¨é…ç½®ä¸­çš„è®¾å¤‡è®¾ç½®
        if 'device' in config and config['device']:
            if config['device'] == 'cuda' and not torch.cuda.is_available():
                print(f"[BaseTrainer.__init__] ERROR: CUDA requested but not available!")
                raise RuntimeError("CUDA device requested but not available on this system")
            self.device = torch.device(config['device'])
        else:
            # è‡ªåŠ¨é€‰æ‹©è®¾å¤‡
            self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
            
        print(f"[BaseTrainer.__init__] Selected device: {self.device}")
        
        # å¦‚æœä½¿ç”¨GPUï¼Œæ‰“å°è¯¦ç»†ä¿¡æ¯
        if self.device.type == 'cuda':
            print(f"[BaseTrainer.__init__] GPU Device: {torch.cuda.get_device_name()}")
            print(f"[BaseTrainer.__init__] GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
            
        # Training parameters
        self.gamma = config.get('gamma', 0.99)
        self.learning_rate = config.get('learning_rate', 3e-4)
        
        # Training state
        self.training_steps = 0
        self.total_timesteps = 0
        
        # å…³é”®ä¿®å¤ï¼šæ·»åŠ eval_modeåˆå§‹åŒ–
        self.eval_mode = False
        print(f"[BaseTrainer.__init__] Initialized eval_mode = {self.eval_mode}")
        
    @abstractmethod
    def select_action(self, state: np.ndarray, valid_actions: Optional[List[int]] = None) -> int:
        """Select an action given the current state"""
        pass
    
    @abstractmethod
    def train_step(self) -> float:
        """Perform one training step and return loss"""
        pass
    
    @abstractmethod
    def update_exploration(self):
        """Update exploration parameters (e.g., epsilon for DQN)"""
        pass
    
    @abstractmethod  # <- æ–°å¢ï¼šç»Ÿä¸€çš„ç»éªŒå­˜å‚¨æ¥å£
    def store_experience(self, state: np.ndarray, action: int, reward: float, 
                        next_state: np.ndarray, done: bool, **kwargs) -> None:
        """Store experience/transition - unified interface for all algorithms"""
        pass
    
    @abstractmethod  # <- æ–°å¢ï¼šåˆ¤æ–­æ˜¯å¦åº”è¯¥è®­ç»ƒ
    def should_train(self) -> bool:
        """Check if training should be performed"""
        pass
    
    @abstractmethod  # <- æ–°å¢ï¼šepisodeç»“æŸæ—¶çš„æ¸…ç†
    def on_episode_end(self) -> None:
        """Called when an episode ends - for algorithm-specific cleanup"""
        pass
    
    def step_completed(self) -> None:  # <- æ–°å¢ï¼šæ¯æ­¥å®Œæˆåè°ƒç”¨
        """Called after each environment step"""
        self.total_timesteps += 1
    
    def set_eval_mode(self, eval_mode: bool):
        """Set evaluation mode for the trainer"""
        self.eval_mode = eval_mode
        
    def apply_action_mask(self, action_values: torch.Tensor, 
                         valid_actions: Optional[List[int]]) -> torch.Tensor:
        """Apply action masking to action values/probabilities"""
        if valid_actions is not None:
            mask = torch.full_like(action_values, float('-inf'))
            mask[valid_actions] = 0
            return action_values + mask
        return action_values
    
    @staticmethod
    def stabilize_logits(logits: torch.Tensor, 
                        clamp_min: float = -10.0, 
                        clamp_max: float = 10.0) -> torch.Tensor:
        """Stabilize logits for numerical stability"""
        if torch.isnan(logits).any():
            logger.warning("NaN detected in logits, replacing with zeros")
            logits = torch.nan_to_num(logits, nan=0.0)
        
        logits = torch.clamp(logits, min=clamp_min, max=clamp_max)
        logits = logits - logits.max(dim=-1, keepdim=True)[0]
        
        return logits
    
    def save_checkpoint_base(self, path: str, 
                            state_dicts: Dict[str, Any],
                            additional_data: Dict[str, Any] = None) -> None:
        """Base checkpoint saving functionality"""
        checkpoint = {
            'algorithm': self.__class__.__name__.replace('Trainer', '').lower(),
            'training_steps': self.training_steps,
            'total_timesteps': self.total_timesteps,  # <- æ–°å¢
            'config': self.config,
            'timestamp': datetime.now().isoformat()
        }
        
        checkpoint.update(state_dicts)
        
        if additional_data:
            checkpoint.update(additional_data)
        
        torch.save(checkpoint, path)
        logger.info(f"Checkpoint saved to {path}")
    
    def load_checkpoint_base(self, path: str) -> Dict[str, Any]:
        """Base checkpoint loading functionality"""
        checkpoint = torch.load(path, map_location=self.device, weights_only=False)
        
        self.training_steps = checkpoint.get('training_steps', 0)
        self.total_timesteps = checkpoint.get('total_timesteps', 0)  # <- æ–°å¢
        
        if 'config' in checkpoint:
            self.config.update(checkpoint['config'])
        
        return checkpoint
    
    @abstractmethod
    def save_checkpoint(self, path: str, additional_data: Dict[str, Any] = None):
        """Save model checkpoint - must be implemented by subclasses"""
        pass
    
    @abstractmethod
    def load_checkpoint(self, path: str) -> Dict[str, Any]:
        """Load model checkpoint - must be implemented by subclasses"""
        pass
    
    def get_device(self) -> torch.device:
        """Get the device being used"""
        return self.device
    
    def get_training_info(self) -> Dict[str, Any]:  # <- æ–°å¢ï¼šè·å–è®­ç»ƒä¿¡æ¯
        """Get current training information for logging"""
        return {
            'training_steps': self.training_steps,
            'total_timesteps': self.total_timesteps
        }


# æ–‡ä»¶ï¼šunified_training_manager.py
# åœ¨DQNTrainerç±»åæ·»åŠ ï¼ˆçº¦ç¬¬350è¡Œï¼‰

# ===========================
# PPO Trainer
# ===========================


class RolloutBuffer:
    """Base buffer for storing trajectories in PPO with RAG support and returns normalization"""
    
    def __init__(self, gamma: float = 0.99, normalize_returns: bool = True):
        """Initialize rollout buffer with discount factor
        
        Args:
            gamma: Discount factor for computing returns
            normalize_returns: Whether to normalize returns
        """
        self.states = []
        self.actions = []
        self.rewards = []
        self.values = []
        self.log_probs = []
        self.dones = []
        self.rag_embeddings = []
        self.old_values = []  # å­˜å‚¨æ—§çš„valueé¢„æµ‹ç”¨äºclipping
        self.gamma = gamma
        self.normalize_returns = normalize_returns
        print(f"[RolloutBuffer.__init__] gamma={gamma}, normalize_returns={normalize_returns}")
    
    def add(self, state, action, reward, value, log_prob, done, rag_embedding=None, **kwargs):
        """Add transition with optional RAG embedding"""
        self.states.append(state)
        self.actions.append(action)
        self.rewards.append(reward)
        self.values.append(value)
        self.log_probs.append(log_prob)
        self.dones.append(done)
        
        # å­˜å‚¨æ—§çš„valueç”¨äºåç»­çš„value clipping
        self.old_values.append(value)
        
        # å­˜å‚¨RAG embeddingï¼Œå¦‚æœæ²¡æœ‰åˆ™å­˜å‚¨é›¶å‘é‡
        if rag_embedding is not None:
            self.rag_embeddings.append(rag_embedding)
        else:
            self.rag_embeddings.append(np.zeros(64))  # é»˜è®¤64ç»´
    
    def get(self):
        """Get all data and compute advantages"""
        if not self.states:
            return None
        
        states = torch.FloatTensor(np.array(self.states)).to(device)
        actions = torch.LongTensor(self.actions).to(device)
        rewards = torch.FloatTensor(self.rewards).to(device)
        values = torch.FloatTensor(self.values).to(device)
        log_probs = torch.FloatTensor(self.log_probs).to(device)
        dones = torch.FloatTensor(self.dones).to(device)
        rag_embeddings = torch.FloatTensor(np.array(self.rag_embeddings)).to(device)
        old_values = torch.FloatTensor(self.old_values).to(device)
        

        # Compute returns and advantages
        returns = self._compute_returns(rewards, values, dones)
        
        # æ ‡å‡†åŒ–returnsä»¥ç¨³å®šè®­ç»ƒ
        if self.normalize_returns and len(returns) > 1:
            returns_mean = returns.mean()
            returns_std = returns.std()
            if returns_std > 1e-8:
                returns = (returns - returns_mean) / (returns_std + 1e-8)
                # ç¼©æ”¾åˆ°åˆç†èŒƒå›´
                returns = returns * 10.0  # å°†æ ‡å‡†åŒ–åçš„returnsç¼©æ”¾åˆ°[-10, 10]èŒƒå›´
                print(f"[RolloutBuffer.get] Returns normalized: mean={returns_mean:.2f}, std={returns_std:.2f}")
            else:
                print(f"[RolloutBuffer.get] Returns std too small, skipping normalization")
        
        advantages = returns - values
        
        return states, actions, log_probs, returns, advantages, rag_embeddings, old_values
    
    def clear(self):
        """Clear all buffers"""
        self.states = []
        self.actions = []
        self.rewards = []
        self.values = []
        self.log_probs = []
        self.dones = []
        self.rag_embeddings = []
        self.old_values = []
    

    def _compute_returns(self, rewards: torch.Tensor, values: torch.Tensor, dones: torch.Tensor) -> torch.Tensor:
        """Compute discounted returns with reward scaling
        
        Args:
            rewards: Tensor of rewards
            values: Tensor of value estimates
            dones: Tensor of done flags
            
        Returns:
            Tensor of discounted returns
        """
        print(f"[RolloutBuffer._compute_returns] Computing returns with gamma={self.gamma}")
        
        # æ£€æŸ¥ç©ºå¼ é‡æƒ…å†µ
        if len(rewards) == 0:
            print("[RolloutBuffer._compute_returns] ERROR: Empty rewards tensor!")
            print("[RolloutBuffer._compute_returns] This indicates no data was collected.")
            print("[RolloutBuffer._compute_returns] Check episode collection and buffer operations.")
            # ç›´æ¥æŠ¥é”™ï¼Œä¸è¦é™é»˜å¤„ç†
            # raise ValueError("Cannot compute returns on empty rewards tensor. No experience data collected.")
        
        # å¯¹å¥–åŠ±è¿›è¡Œç¼©æ”¾ä»¥é˜²æ­¢returnsè¿‡å¤§
        reward_scale = 0.1  # å°†å¥–åŠ±ç¼©å°10å€
        scaled_rewards = rewards * reward_scale
        
        returns = torch.zeros_like(scaled_rewards)
        running_return = 0
        
        # ä»åå‘å‰è®¡ç®—æŠ˜æ‰£å›æŠ¥
        for t in reversed(range(len(scaled_rewards))):
            if dones[t]:
                running_return = 0
            running_return = scaled_rewards[t] + self.gamma * running_return
            returns[t] = running_return
        
        # æ‰“å°è¯Šæ–­ä¿¡æ¯
        print(f"[RolloutBuffer._compute_returns] Raw rewards range: [{rewards.min().item():.2f}, {rewards.max().item():.2f}]")
        print(f"[RolloutBuffer._compute_returns] Scaled returns range: [{returns.min().item():.2f}, {returns.max().item():.2f}]")
            
        return returns
    
    
class TaskAwareRolloutBuffer(RolloutBuffer):  # <- ä¿®æ”¹äº†è¿™ä¸€è¡Œï¼šç»§æ‰¿è‡ªRolloutBuffer
    """Task-aware buffer for PPO with experience replay capability"""
    
    def __init__(self, capacity_per_task: int = 10000, min_episodes_per_task: int = 10):
        super().__init__()  # <- æ–°å¢ï¼šè°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°
        
        # ä»»åŠ¡ç‰¹å®šçš„ç»éªŒå­˜å‚¨
        self.task_buffers = {}  # task_type -> list of episodes
        self.capacity_per_task = capacity_per_task
        self.min_episodes_per_task = min_episodes_per_task
        
        # å½“å‰episodeçš„ä¸´æ—¶å­˜å‚¨ï¼ˆæ‰©å±•çˆ¶ç±»åŠŸèƒ½ï¼‰
        self.current_episode = {
            'states': [],
            'actions': [],
            'rewards': [],
            'values': [],
            'log_probs': [],
            'dones': [],
            'task_type': None,
            'rag_contexts': [],  
            'action_infos': []   
        }
        
        # å…¨å±€ç»Ÿè®¡
        self.task_counts = defaultdict(int)
        self.total_episodes = 0
        
    def add(self, state, action, reward, value, log_prob, done, 
            rag_embedding=None, action_info=None):  # ä¿®æ”¹ï¼šç»Ÿä¸€ä½¿ç”¨rag_embeddingå‚æ•°å
        """æ·»åŠ å•æ­¥ç»éªŒï¼ˆæ‰©å±•çˆ¶ç±»åŠŸèƒ½ï¼‰"""
        print(f"[TaskAwareRolloutBuffer.add] Storing experience with rag_embedding shape: {rag_embedding.shape if rag_embedding is not None else None}")
        
        # è°ƒç”¨çˆ¶ç±»çš„addæ–¹æ³•å¤„ç†åŸºç¡€æ•°æ®ï¼Œä¼ é€’æ‰€æœ‰å‚æ•°
        super().add(state, action, reward, value, log_prob, done, 
                   rag_embedding=rag_embedding)  # ä¿®æ”¹ï¼šæ­£ç¡®ä¼ é€’rag_embeddingå‚æ•°
        
        # å¤„ç†æ‰©å±•æ•°æ®
        self.current_episode['states'].append(state)
        self.current_episode['actions'].append(action)
        self.current_episode['rewards'].append(reward)
        self.current_episode['values'].append(value)
        self.current_episode['log_probs'].append(log_prob)
        self.current_episode['dones'].append(done)
        
        # ä¿®æ”¹ï¼šå°†rag_embeddingå­˜å‚¨ä¸ºrag_contextsï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
        # å¦‚æœaction_infoä¸­åŒ…å«rag_contextä¿¡æ¯ï¼Œä¼˜å…ˆä½¿ç”¨
        if action_info and 'rag_context' in action_info:
            self.current_episode['rag_contexts'].append(action_info['rag_context'])
        else:
            # å¦åˆ™å­˜å‚¨rag_embeddingä½œä¸ºcontext
            self.current_episode['rag_contexts'].append(rag_embedding)
            
        self.current_episode['action_infos'].append(action_info)
        
        # è°ƒè¯•ä¿¡æ¯
        if done:
            print(f"[TaskAwareRolloutBuffer.add] Episode complete. Total steps: {len(self.current_episode['states'])}")
    
    def set_task_type(self, task_type: str):
        """è®¾ç½®å½“å‰episodeçš„ä»»åŠ¡ç±»å‹"""
        self.current_episode['task_type'] = task_type
    
    def store_episode(self):
        """å­˜å‚¨å®Œæ•´çš„episodeåˆ°ä»»åŠ¡bufferï¼Œæ·»åŠ æ•°æ®éªŒè¯"""
        task_type = self.current_episode['task_type'] or 'default'
        
        # éªŒè¯episodeæ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
        if not self.current_episode['states'] or not self.current_episode['rewards']:
            print(f"[TaskAwareRolloutBuffer.store_episode] WARNING: Empty episode for task {task_type}")
            print(f"  States: {len(self.current_episode['states'])}")
            print(f"  Rewards: {len(self.current_episode['rewards'])}")
            print(f"  Actions: {len(self.current_episode['actions'])}")
            # æ¸…ç©ºå½“å‰episodeå¹¶è¿”å›ï¼Œä¸å­˜å‚¨ç©ºepisode
            self.current_episode = {
                'states': [],
                'actions': [],
                'rewards': [],
                'values': [],
                'log_probs': [],
                'dones': [],
                'task_type': None,
                'rag_contexts': [],
                'action_infos': []
            }
            return
        
        # éªŒè¯æ•°æ®é•¿åº¦ä¸€è‡´æ€§
        expected_length = len(self.current_episode['states'])
        for key in ['actions', 'rewards', 'values', 'log_probs', 'dones']:
            if len(self.current_episode[key]) != expected_length:
                print(f"[TaskAwareRolloutBuffer.store_episode] ERROR: Inconsistent data lengths!")
                print(f"  Expected: {expected_length}, {key}: {len(self.current_episode[key])}")
                # ä¸å­˜å‚¨ä¸ä¸€è‡´çš„æ•°æ®
                return
        
        if task_type not in self.task_buffers:
            self.task_buffers[task_type] = deque(maxlen=self.capacity_per_task)
        
        # æ·±æ‹·è´å½“å‰episodeæ•°æ®
        episode_copy = {
            k: list(v) if isinstance(v, list) else v
            for k, v in self.current_episode.items()
        }
        
        # è®¡ç®—episodeæ€»å¥–åŠ±
        episode_copy['total_reward'] = sum(self.current_episode['rewards'])
        
        self.task_buffers[task_type].append(episode_copy)
        self.task_counts[task_type] += 1
        self.total_episodes += 1
        
        print(f"[TaskAwareRolloutBuffer.store_episode] Stored episode for {task_type}")
        print(f"  Total reward: {episode_copy['total_reward']:.2f}")
        print(f"  Episode length: {expected_length}")
    
    def get(self, current_task_type=None, mix_ratio=0.7):
        """è·å–è®­ç»ƒæ•°æ®ï¼Œæ”¯æŒä»»åŠ¡æ„ŸçŸ¥çš„é‡‡æ ·ç­–ç•¥"""
        # å¦‚æœæ²¡æœ‰task bufferï¼Œä½¿ç”¨çˆ¶ç±»çš„getæ–¹æ³•  # <- æ–°å¢
        if not self.task_buffers:
            return super().get()
            
        episodes_to_process = []
        
        if current_task_type and current_task_type in self.task_buffers:
            # æ··åˆé‡‡æ ·ç­–ç•¥
            current_task_episodes = int(mix_ratio * self.min_episodes_per_task)
            other_episodes = self.min_episodes_per_task - current_task_episodes
            
            # ä»å½“å‰ä»»åŠ¡é‡‡æ ·
            current_buffer = list(self.task_buffers[current_task_type])
            if current_buffer:
                sorted_episodes = sorted(current_buffer, 
                                       key=lambda ep: abs(ep.get('total_reward', 0) - 5.0))
                n_current = min(current_task_episodes, len(sorted_episodes))
                episodes_to_process.extend(sorted_episodes[:n_current])
            
            # ä»å…¶ä»–ä»»åŠ¡é‡‡æ ·
            other_tasks = [t for t in self.task_buffers if t != current_task_type]
            if other_tasks and other_episodes > 0:
                task_avg_rewards = {}
                for task in other_tasks:
                    task_buffer = self.task_buffers[task]
                    if task_buffer:
                        avg_reward = np.mean([ep.get('total_reward', 0) for ep in task_buffer])
                        task_avg_rewards[task] = avg_reward
                
                # æŒ‰å¹³å‡å¥–åŠ±æ’åºï¼Œä¼˜å…ˆé‡‡æ ·å›°éš¾ä»»åŠ¡
                sorted_tasks = sorted(other_tasks, key=lambda t: task_avg_rewards.get(t, 0))
                
                episodes_per_task = max(1, other_episodes // len(other_tasks))
                for task in sorted_tasks:
                    task_buffer = list(self.task_buffers[task])
                    if task_buffer:
                        # åŒæ ·ä¼˜å…ˆé€‰æ‹©ä¸­ç­‰å¥–åŠ±çš„episodes
                        sorted_task_episodes = sorted(task_buffer,
                                                    key=lambda ep: abs(ep.get('total_reward', 0) - 5.0))
                        n_select = min(episodes_per_task, len(sorted_task_episodes))
                        episodes_to_process.extend(sorted_task_episodes[:n_select])
        else:
            # å‡è¡¡é‡‡æ ·æ‰€æœ‰ä»»åŠ¡
            for task_type, buffer in self.task_buffers.items():
                if buffer:
                    task_buffer = list(buffer)
                    sampled = np.random.choice(
                        len(task_buffer),
                        min(self.min_episodes_per_task, len(task_buffer)),
                        replace=False
                    )
                    episodes_to_process.extend([task_buffer[i] for i in sampled])
        
        # åˆå¹¶æ‰€æœ‰episodesçš„æ•°æ®
        if not episodes_to_process:  # <- æ–°å¢ï¼šç©ºæ•°æ®æ£€æŸ¥
            return None
        return self._merge_episodes(episodes_to_process)

    def _merge_episodes(self, episodes):
        """åˆå¹¶å¤šä¸ªepisodesçš„æ•°æ®ï¼Œè¿‡æ»¤ç©ºepisode"""
        all_states = []
        all_actions = []
        all_log_probs = []
        all_returns = []
        all_advantages = []
        all_rag_embeddings = []
        all_old_values = []
        
        # è¿‡æ»¤å¹¶å¤„ç†æœ‰æ•ˆçš„episodes
        valid_episodes = 0
        skipped_episodes = 0
        
        for episode in episodes:
            # æ£€æŸ¥episodeæ˜¯å¦æœ‰æ•ˆ
            if not episode.get('states') or not episode.get('rewards'):
                print(f"[TaskAwareRolloutBuffer._merge_episodes] Skipping empty episode")
                skipped_episodes += 1
                continue
            
            try:
                # _process_episode_data ç°åœ¨è¿”å›7ä¸ªå€¼ï¼ˆåŒ…å«old_valuesï¼‰
                states, actions, log_probs, returns, advantages, rag_embeddings, old_values = self._process_episode_data(episode)
                all_states.append(states)
                all_actions.append(actions)
                all_log_probs.append(log_probs)
                all_returns.append(returns)
                all_advantages.append(advantages)
                all_rag_embeddings.append(rag_embeddings)
                all_old_values.append(old_values)
                valid_episodes += 1
            except ValueError as e:
                print(f"[TaskAwareRolloutBuffer._merge_episodes] Error processing episode: {e}")
                skipped_episodes += 1
                continue
        
        print(f"[TaskAwareRolloutBuffer._merge_episodes] Processed {valid_episodes} valid episodes, skipped {skipped_episodes}")
        
        # è¿æ¥æ‰€æœ‰æ•°æ®
        if not all_states:
            print(f"[TaskAwareRolloutBuffer._merge_episodes] ERROR: No valid episodes to merge!")
            return None
            
        return (
            torch.cat(all_states),
            torch.cat(all_actions),
            torch.cat(all_log_probs),
            torch.cat(all_returns),
            torch.cat(all_advantages),
            torch.cat(all_rag_embeddings),
            torch.cat(all_old_values)
        )

    def _process_episode_data(self, episode):
        """å¤„ç†å•ä¸ªepisodeæ•°æ®ï¼Œæ·»åŠ ç©ºæ•°æ®æ£€æŸ¥"""
        # é¦–å…ˆæ£€æŸ¥episodeæ˜¯å¦æœ‰æ•ˆ
        if not episode.get('states') or not episode.get('rewards'):
            print(f"[TaskAwareRolloutBuffer._process_episode_data] WARNING: Empty episode data")
            print(f"  States: {len(episode.get('states', []))}")
            print(f"  Rewards: {len(episode.get('rewards', []))}")
            # è¿”å›Noneæˆ–æŠ›å‡ºæ›´æ˜ç¡®çš„é”™è¯¯
            raise ValueError(f"Empty episode data: states={len(episode.get('states', []))}, rewards={len(episode.get('rewards', []))}")
        
        states = torch.FloatTensor(np.array(episode['states'])).to(device)
        actions = torch.LongTensor(episode['actions']).to(device)
        rewards = torch.FloatTensor(episode['rewards']).to(device)
        values = torch.FloatTensor(episode['values']).to(device)
        log_probs = torch.FloatTensor(episode['log_probs']).to(device)
        dones = torch.FloatTensor(episode['dones']).to(device)
        
        # å†æ¬¡éªŒè¯rewardsä¸ä¸ºç©º
        if len(rewards) == 0:
            print(f"[TaskAwareRolloutBuffer._process_episode_data] ERROR: Empty rewards tensor after conversion")
            raise ValueError("Empty rewards tensor after conversion")
        
        # å¤„ç†RAG contexts/embeddings
        rag_contexts = episode.get('rag_contexts', [])
        if rag_contexts:
            # å¤„ç†å­˜å‚¨çš„rag_contexts
            rag_embeddings_list = []
            for rag_data in rag_contexts:
                if isinstance(rag_data, np.ndarray):
                    # å·²ç»æ˜¯embedding
                    rag_embeddings_list.append(rag_data)
                elif rag_data is None:
                    # ä½¿ç”¨é›¶å‘é‡
                    rag_embeddings_list.append(np.zeros(64))
                else:
                    # å…¶ä»–æƒ…å†µä¹Ÿä½¿ç”¨é›¶å‘é‡
                    rag_embeddings_list.append(np.zeros(64))
            rag_embeddings = torch.FloatTensor(np.array(rag_embeddings_list)).to(device)
        else:
            # å¦‚æœæ²¡æœ‰rag_contextsï¼Œåˆ›å»ºä¸statesç›¸åŒé•¿åº¦çš„é›¶å‘é‡
            print(f"[TaskAwareRolloutBuffer._process_episode_data] No rag_contexts found, using zero embeddings")
            rag_embeddings = torch.zeros(len(states), 64).to(device)
        
        # è®¡ç®—returns - ä½¿ç”¨çˆ¶ç±»å®ä¾‹çš„_compute_returnsæ–¹æ³•
        # åˆ›å»ºä¸´æ—¶çš„RolloutBufferå®ä¾‹æ¥è®¿é—®gammaå’Œ_compute_returnsæ–¹æ³•
        temp_buffer = RolloutBuffer(gamma=self.gamma if hasattr(self, 'gamma') else 0.99)
        returns = temp_buffer._compute_returns(rewards, values, dones)
        
        # è®¡ç®—advantages
        advantages = returns - values
        
        # è·å–old_values
        old_values = values.clone()
        
        # æ‰“å°è°ƒè¯•ä¿¡æ¯
        print(f"[TaskAwareRolloutBuffer._process_episode_data] Processed episode successfully")
        print(f"  Episode length: {len(states)}")
        print(f"  Rewards range: [{rewards.min().item():.2f}, {rewards.max().item():.2f}]")
        print(f"  Returns range: [{returns.min().item():.2f}, {returns.max().item():.2f}]")
        print(f"  Advantages range: [{advantages.min().item():.2f}, {advantages.max().item():.2f}]")
        
        return states, actions, log_probs, returns, advantages, rag_embeddings, old_values
            
    def clear(self):
        """æ¸…ç©ºå½“å‰episode buffer"""
        super().clear()  # <- æ–°å¢ï¼šè°ƒç”¨çˆ¶ç±»çš„clear
        
        # æ¸…ç©ºæ‰©å±•æ•°æ®
        self.current_episode = {
            'states': [],
            'actions': [],
            'rewards': [],
            'values': [],
            'log_probs': [],
            'dones': [],
            'task_type': None,
            'rag_contexts': [],
            'action_infos': []
        }
    
    def get_task_statistics(self):
        """è·å–ä»»åŠ¡åˆ†å¸ƒç»Ÿè®¡"""
        return {
            task_type: {
                'episodes': len(buffer),
                'total_collected': self.task_counts[task_type]
            }
            for task_type, buffer in self.task_buffers.items()
        }


class PPOTrainer(BaseTrainer):
    """PPO trainer implementation with enhanced GPT-4o-mini distillation"""
    
    def __init__(self, env: 'MDPEnvironment', config: Dict[str, Any]):
        super().__init__(env, config)
        
        # PPOåŸºæœ¬å‚æ•° - å¿…é¡»åœ¨ä½¿ç”¨å‰åˆå§‹åŒ–
        self.gamma = config.get('gamma', 0.99)
        self.gae_lambda = config.get('gae_lambda', 0.95)
        self.n_steps = config.get('n_steps', 2048)
        self.n_epochs = config.get('n_epochs', 10)
        self.batch_size = config.get('batch_size', 64)
        self.clip_range = config.get('clip_range', 0.2)
        self.clip_range_vf = config.get('clip_range_vf', None)
        self.ent_coef = config.get('ent_coef', 0.01)
        self.vf_coef = config.get('vf_coef', 0.5)
        self.max_grad_norm = config.get('max_grad_norm', 0.5)
        
        # Network
        state_dim = env.get_state_dim()
        action_dim = env.num_actions
        
        # åˆ›å»ºç½‘ç»œé…ç½®
        network_config = {
            'hidden_dim': config.get('hidden_dim', 256),
            'num_layers': config.get('num_layers', 3),
            'num_heads': config.get('num_heads', 4),
            'dropout': config.get('dropout', 0.1),
            'use_pre_norm': config.get('use_pre_norm', True),
            'use_auxiliary_tasks': config.get('use_auxiliary_tasks', False),
            'use_curiosity': config.get('use_curiosity', False),
            'spectral_norm': config.get('spectral_norm', False),
            'rag_dim': config.get('rag_dim', 64),
            'use_mac_optimization': config.get('use_mac_optimization', False),
            'use_rag_enhancement': config.get('use_rag_enhancement', True),
            'use_tools_input': config.get('use_tools_input', True),
            'tools_dim': config.get('tools_dim', 64),
            'num_tools': config.get('num_tools', action_dim)
        }
        
        print(f"[PPOTrainer.__init__] Creating ActorCriticNetwork with config: use_rag_enhancement={network_config['use_rag_enhancement']}")
        
        # åˆ›å»ºActorCriticNetworkå®ä¾‹
        self.network = ActorCriticNetwork(state_dim, action_dim, network_config)
        self.network.to(self.device)
        
        try:
            from apex.optimizers import FusedAdam
            self.optimizer = FusedAdam(
                self.network.parameters(),
                lr=self.learning_rate,
                betas=(0.9, 0.999),
                eps=1e-8,
                weight_decay=self.config.get('weight_decay', 0.0)
            )
            logger.info("âœ“ Using FusedAdam optimizer (faster on V100)")
        except ImportError:
            # å¦‚æœæ²¡æœ‰å®‰è£… apexï¼Œé€€å›åˆ°æ ‡å‡† Adam
            self.optimizer = torch.optim.Adam(
                self.network.parameters(),
                lr=self.learning_rate,
                betas=(0.9, 0.999),
                eps=1e-8
            )
            logger.warning("FusedAdam not available, using standard Adam")   

        def create_scheduler(optimizer, config):
            warmup_steps = config.get('lr_warmup_steps', 100)
            total_steps = config.get('total_training_steps', 10000)
            
            def lr_lambda(step):
                if step < warmup_steps:
                    # Warm-up é˜¶æ®µï¼šçº¿æ€§å¢é•¿
                    return float(step) / float(max(1, warmup_steps))
                else:
                    # Cosine é€€ç«é˜¶æ®µ
                    progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))
                    return 0.5 * (1.0 + math.cos(math.pi * progress))
            
            return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

        # ä½¿ç”¨
        self.lr_scheduler = create_scheduler(self.optimizer, self.config)

            
        # PPO specific parameters
        use_task_aware_buffer = config.get('use_task_aware_buffer', True)
        if use_task_aware_buffer:
            self.rollout_buffer = TaskAwareRolloutBuffer(
                capacity_per_task=config.get('buffer_capacity_per_task', 100),
                min_episodes_per_task=config.get('min_episodes_per_task', 5)
            )
        else:
            self.rollout_buffer = RolloutBuffer(gamma=self.gamma)
            
        # Tracking
        self.current_learning_rate = config.get('learning_rate', 3e-4)
        self.current_task_type = None
        
        # å¢å¼ºçš„Teacher guidanceå‚æ•°
        self.use_teacher_guidance = config.get('use_teacher_guidance', True)
        self.teacher_guidance_prob = config.get('teacher_guidance_start_prob', 0.5)
        self.teacher_guidance_decay = config.get('teacher_guidance_decay', 0.995)
        self.teacher_guidance_min_prob = config.get('teacher_guidance_min_prob', 0.05)
        
        # GPT-4o-miniè’¸é¦å‚æ•°
        self.use_distillation = config.get('use_distillation', True)
        self.distillation_weight = config.get('distillation_weight', 0.1)
        self.distillation_temperature = config.get('distillation_temperature', 2.0)
        self.teacher_model_name = config.get('teacher_model_name', 'gpt-4o-mini')
        
        # å¹¶å‘å‚æ•°
        self.distillation_batch_size = config.get('distillation_batch_size', 10)
        self.max_concurrent_requests = config.get('max_concurrent_requests', 100)
        self.request_timeout = config.get('request_timeout', 10)
        
        # Soft guidanceå‚æ•°
        self.use_soft_guidance = config.get('use_soft_guidance', True)
        self.guidance_temperature = config.get('guidance_temperature', 0.5)
        self.guidance_blend_factor = config.get('guidance_blend_factor', 0.7)
        
        # è‡ªé€‚åº”guidance
        self.adaptive_guidance = config.get('adaptive_guidance', True)
        self.task_difficulty_scores = {}
        self.model_confidence_threshold = 0.8
        self.llm_client = None
        
        # Episode-level guidance
        self.episode_guidance_mode = config.get('episode_guidance_mode', True)
        self.current_episode_workflow = None
        self.workflow_cache = {}
        self.max_cache_size = 100
        
        # åˆå§‹åŒ–è’¸é¦ç¼“å­˜
        self.distillation_cache = EmbeddingDistillationCache(
            cache_dir=config.get('cache_dir', '.distillation_cache'),
            max_size=config.get('cache_size', 10000),
            similarity_threshold=config.get('cache_similarity_threshold', 0.9)
        )
        
        self.cache_lock = threading.RLock()  # ä½¿ç”¨å¯é‡å…¥é”

        # åˆå§‹åŒ–å·¥å…·ä¿¡æ¯
        self._initialize_tool_info()
        
        # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
        if self.use_teacher_guidance or self.use_distillation:
            self._init_llm_client()
            print(f"[PPOTrainer] Teacher guidance enabled with model: {self.teacher_model_name}")


    def _initialize_tool_info(self):
        """åˆå§‹åŒ–å·¥å…·ä¿¡æ¯ä¾›teacherä½¿ç”¨"""
        self.tool_info = {}
        self.tool_embeddings = {}
        self.tool_categories = {}
        self.tool_dependencies = {}
        
        # ä»ç¯å¢ƒè·å–å·¥å…·ä¿¡æ¯
        if hasattr(self.env, 'action_space'):
            for i, action in enumerate(self.env.action_space):
                # è·å–å·¥å…·åç§°ï¼Œå¤„ç†ç‰¹æ®ŠåŠ¨ä½œç±»å‹
                tool_name = None
                if hasattr(action, 'tool_name'):
                    tool_name = action.tool_name
                elif hasattr(action, 'action_type'):
                    # ç‰¹æ®ŠåŠ¨ä½œç±»å‹ä½¿ç”¨action_typeä½œä¸ºåç§°
                    tool_name = str(action.action_type.value) if hasattr(action.action_type, 'value') else str(action.action_type)
                
                if tool_name is None:
                    # å¦‚æœè¿˜æ˜¯Noneï¼Œä½¿ç”¨ç´¢å¼•ä½œä¸ºåç§°
                    tool_name = f"action_{i}"
                    print(f"[PPOTrainer] WARNING: Action at index {i} has no identifiable name, using '{tool_name}'")
                
                self.tool_info[i] = {
                    'name': tool_name,
                    'index': i,
                    'description': getattr(action, 'description', ''),
                    'category': getattr(action, 'category', 'general'),
                    'parameters': getattr(action, 'parameters', []),
                    'returns': getattr(action, 'returns', []),
                    'dependencies': getattr(action, 'dependencies', [])
                }
                
                # æŒ‰ç±»åˆ«åˆ†ç»„
                category = self.tool_info[i]['category']
                if category not in self.tool_categories:
                    self.tool_categories[category] = []
                self.tool_categories[category].append(i)
                
                # è®°å½•ä¾èµ–å…³ç³»
                if self.tool_info[i]['dependencies']:
                    self.tool_dependencies[tool_name] = self.tool_info[i]['dependencies']
        
        print(f"[PPOTrainer] Initialized {len(self.tool_info)} tools")
    
        # æ‰“å°ç‰¹æ®ŠåŠ¨ä½œä¿¡æ¯
        special_actions = [(i, info) for i, info in self.tool_info.items() 
                        if info['name'].startswith('action_') or 
                        info['name'] in ['NO_OP', 'CREATE_CHECKPOINT', 'RECOVER_ERROR', 'RESTORE_CHECKPOINT']]
        if special_actions:
            print(f"[PPOTrainer] Special actions found: {[info['name'] for _, info in special_actions]}")
            
        # å¦‚æœæœ‰embedding managerï¼Œè·å–å·¥å…·embeddings
        if hasattr(self.env, 'mdp') and hasattr(self.env.mdp, 'embedding_manager'):
            embedding_manager = self.env.mdp.embedding_manager
            for i, tool_info in self.tool_info.items():
                tool_name = tool_info['name']
                if hasattr(embedding_manager, 'tool_embeddings') and tool_name in embedding_manager.tool_embeddings:
                    tool_embedding = embedding_manager.tool_embeddings[tool_name]
                    if hasattr(tool_embedding, 'combined_embedding'):
                        self.tool_embeddings[i] = tool_embedding.combined_embedding
        else:
            raise ValueError("No embedding manager")
        
        print(f"[PPOTrainer] Initialized {len(self.tool_info)} tools with {len(self.tool_embeddings)} embeddings")
        print(f"[PPOTrainer] Tool categories: {list(self.tool_categories.keys())}")


            
    def store_experience(self, state: np.ndarray, action: int, reward: float,  # <- æ–°å¢ï¼šå®ç°ç»Ÿä¸€æ¥å£
                        next_state: np.ndarray, done: bool, **kwargs) -> None:
        """Store experience using store_transition"""
        # PPOä¸éœ€è¦next_stateï¼Œç›´æ¥è°ƒç”¨åŸæœ‰çš„store_transition
        self.store_transition(state, action, reward, done)
    
    def should_train(self) -> bool:  # <- æ–°å¢ï¼šåˆ¤æ–­æ˜¯å¦åº”è¯¥è®­ç»ƒ
        """PPO trains every n_steps"""
        return self.total_timesteps > 0 and self.total_timesteps % self.n_steps == 0
    
    def on_episode_end(self) -> None:  # <- æ–°å¢ï¼šepisodeç»“æŸå¤„ç†
        """PPO needs to handle episode end for TaskAwareRolloutBuffer"""
        if isinstance(self.rollout_buffer, TaskAwareRolloutBuffer):
            self.rollout_buffer.store_episode()
    
    def _init_llm_client(self):
        """Initialize LLM client for teacher guidance"""
        from api_client_manager import get_api_client, get_model_name
        self.llm_client = get_api_client()
        self.model_name = get_model_name()
        logger.info(f"LLM client initialized for teacher guidance using model: {self.model_name}")

    
    def set_eval_mode(self, eval_mode: bool):  # <- æ–°å¢ï¼šé‡å†™åŸºç±»æ–¹æ³•
        """Set evaluation mode - disable teacher guidance"""
        super().set_eval_mode(eval_mode)
        if eval_mode:
            self.stored_teacher_prob = self.teacher_guidance_prob
            self.teacher_guidance_prob = 0.0
        else:
            if hasattr(self, 'stored_teacher_prob'):
                self.teacher_guidance_prob = self.stored_teacher_prob


    def select_action(self, state: np.ndarray, valid_actions: Optional[List[int]] = None) -> int:
        """Select action using policy network with RAG enhancement and required_tools"""
        # Episode-level workflow guidance
        
        # if (self.use_teacher_guidance and 
        #     self.episode_guidance_mode and 
        #     not self.eval_mode and
        #     hasattr(self.env, 'episode_steps') and 
        #     self.env.episode_steps == 0):
        #     self._get_episode_workflow_guidance()
        
        # Get teacher action if using guidance
        teacher_action = None
        teacher_flag = random.random()
        self.use_teacher_guidance = False
        if self.use_teacher_guidance and not self.eval_mode and teacher_flag < self.teacher_guidance_prob:
            print("ä½¿ç”¨æ•™å¸ˆæŒ‡å¯¼æ¨¡å¼")
            if self.episode_guidance_mode and self.current_episode_workflow:
                teacher_action = self._get_next_workflow_action(valid_actions)
            if teacher_action is None:
                teacher_action = self._get_teacher_action(state, valid_actions)
        else:
            print(self.teacher_guidance_prob)
            # raise ValueError("No teacher guidance, the flag is ", teacher_flag)
            
        
        # å‡†å¤‡RAG embedding
        rag_embedding = None
        if hasattr(self.env, 'last_rag_context') and self.env.last_rag_context:
            rag_embedding = self.env._encode_rag_embedding(self.env.last_rag_context)
            rag_tensor = torch.FloatTensor(rag_embedding).unsqueeze(0).to(self.device)
        else:
            rag_tensor = torch.zeros(1, self.config.get('rag_dim', 64)).to(self.device)
        
        # å‡†å¤‡required_tools embedding
        required_tools_tensor = None
        if hasattr(self.env, 'current_task_info') and self.env.current_task_info:
            required_tools = self.env.current_task_info.get('required_tools', [])
            if required_tools and hasattr(self.network, 'tools_projection'):
                # è·å–embedding_manager
                embedding_manager = None
                if hasattr(self.env, 'mdp') and hasattr(self.env.mdp, 'embedding_manager'):
                    embedding_manager = self.env.mdp.embedding_manager
                
                # è·å–ç½‘ç»œæœŸæœ›çš„toolsç»´åº¦
                target_tools_dim = getattr(self.network, 'tools_dim', 64)
                
                # ä½¿ç”¨embeddingæ–¹æ³•ç¼–ç required_tools
                required_tools_embedding = encode_required_tools_embedding(
                    required_tools,
                    embedding_manager,
                    target_dim=target_tools_dim  # ä¼ å…¥ç›®æ ‡ç»´åº¦
                )
                required_tools_tensor = torch.FloatTensor(required_tools_embedding).unsqueeze(0).to(self.device)
                logger.debug(f" Encoded {len(required_tools)} required tools with target dim={target_tools_dim}")
        else:
            logger.debug(f" No required tools found, using zero vector")
            if hasattr(self.network, 'tools_projection'):
                tools_dim = getattr(self.network, 'tools_dim', 64)
                required_tools_tensor = torch.zeros(1, tools_dim).to(self.device)
        
        # å¦‚æœç½‘ç»œæ”¯æŒtoolsè¾“å…¥ä½†æ²¡æœ‰required_toolsï¼Œåˆ›å»ºé›¶å‘é‡
        if required_tools_tensor is None and hasattr(self.network, 'tools_projection'):
            raise ValueError("Network supports tools input but no required_tools provided")
            logger.debug(f" No required tools provided, using zero vector for tools input")
            required_tools_tensor = torch.zeros(1, getattr(self.network, 'tools_dim', 64)).to(self.device)
        
        # å¦‚æœç½‘ç»œæ”¯æŒtoolsè¾“å…¥ä½†æ²¡æœ‰required_toolsï¼Œåˆ›å»ºé›¶å‘é‡

        # Get policy action with RAG and required_tools
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)
            
            # æ ¹æ®ç½‘ç»œèƒ½åŠ›è°ƒç”¨forward
            if hasattr(self.network, 'tools_projection') and required_tools_tensor is not None:
                # æ–°ç‰ˆç½‘ç»œï¼Œæ”¯æŒä¸‰ä¸ªè¾“å…¥
                logits, value = self.network(state_tensor, rag_tensor, required_tools_tensor)
                logger.debug(f" Using network with RAG and tools embedding input")
            elif hasattr(self.network, 'rag_projection'):
                # ä¸­ç‰ˆç½‘ç»œï¼Œåªæ”¯æŒRAG
                logits, value = self.network(state_tensor, rag_tensor)
                logger.debug(f" Using network with RAG input only")
            else:
                # æ—§ç‰ˆç½‘ç»œï¼Œåªæ”¯æŒstate
                logits, value = self.network(state_tensor)
                print("[WARNING] Network doesn't support RAG or tools input")
            
            # ä½¿ç”¨åŸºç±»çš„æ•°å€¼ç¨³å®šåŒ–
            if hasattr(self, '_stabilize_logits'):
                logits = self._stabilize_logits(logits)
            
            # Apply action masking if provided
            if valid_actions is not None:
                mask = torch.ones_like(logits[0]) * float('-inf')
                mask[valid_actions] = 0
                logits = logits + mask.unsqueeze(0)
            
            # åˆ›å»ºåŠ¨ä½œåˆ†å¸ƒ
            probs = F.softmax(logits, dim=-1)
            dist = torch.distributions.Categorical(probs)
            
            # é‡‡æ ·æˆ–ä½¿ç”¨æ•™å¸ˆåŠ¨ä½œ
            if teacher_action is not None:
                action = teacher_action
            else:
                # æ™®é€šçš„åŠ¨ä½œé€‰æ‹©é€»è¾‘
                with torch.no_grad():
                    # è°ƒç”¨ç½‘ç»œå‰å‘ä¼ æ’­
                    if hasattr(self.network, 'tools_projection'):
                        logits, value = self.network(state_tensor, rag_tensor, required_tools_tensor)
                    elif hasattr(self.network, 'rag_projection'):
                        logits, value = self.network(state_tensor, rag_tensor)
                    else:
                        logits, value = self.network(state_tensor)
                    
                    # åº”ç”¨åŠ¨ä½œæ©ç å¹¶é€‰æ‹©åŠ¨ä½œ
                    if valid_actions is not None:
                        mask = torch.zeros_like(logits[0])
                        mask[valid_actions] = 1
                        masked_logits = logits[0] + (mask - 1) * 1e8
                        probs = torch.softmax(masked_logits, dim=0)
                    else:
                        probs = torch.softmax(logits[0], dim=0)
                    
                    dist = torch.distributions.Categorical(probs)
                    action = dist.sample().item()
                    log_prob = dist.log_prob(torch.tensor(action, device=self.device))

                    
                    # å­˜å‚¨valueå’Œlog_probä¾›åç»­ä½¿ç”¨
                    self.last_value = value.item() if value.dim() == 0 else value[0].item()
                    self.last_log_prob = log_prob.item()
                    self.last_rag_embedding = rag_embedding
                
            return action  # åªè¿”å›é€‰æ‹©çš„åŠ¨ä½œ


    def _get_episode_workflow_guidance(self):
        """è·å–æ•´ä¸ªepisodeçš„workflowæŒ‡å¯¼"""
        if not self.llm_client or not hasattr(self.env, 'current_task'):
            print("[_get_episode_workflow_guidance] Missing llm_client or current_task")
            return
        
        current_task = self.env.current_task
        # ä½¿ç”¨getattrå®‰å…¨è®¿é—®ï¼Œä¼˜å…ˆä½¿ç”¨description
        task_description = getattr(current_task, 'description', getattr(current_task, 'task_objective', 'Unknown task'))
        task_key = f"{current_task.task_type}_{task_description[:50]}"
        
        # æ£€æŸ¥ç¼“å­˜
        if task_key in self.workflow_cache:
            self.current_episode_workflow = self.workflow_cache[task_key].copy()
            self.env.workflow_position = 0
            print(f"[_get_episode_workflow_guidance] Using cached workflow for {task_key}")
            return
        
        # æ„å»ºæ›´å‹å¥½çš„promptä»¥é¿å…è§¦å‘å†…å®¹è¿‡æ»¤
        prompt = f"""I need help planning a workflow for a data processing task. Here are the details:

    Task Category: {current_task.task_type}
    Task Goal: {task_description}

    I have these tools available to use:
    {', '.join(list(self.env.tool_registry.keys()))}

    Could you suggest a good sequence of tools to accomplish this task? Please provide your recommendation as a structured list in JSON format.

    For example, if I were processing CSV data, a good sequence might be:
    ["data_reader", "validator", "transformer", "csv_writer"]

    Please consider:
    - Which tools naturally follow each other
    - Data flow between tools
    - The specific requirements of this task type

    Your suggested tool sequence:"""
        
        # å¤„ç†APIè°ƒç”¨
        workflow = []
        content = ""
        
        # APIè°ƒç”¨
        response = self.llm_client.chat.completions.create(
            model=self.model_name,
            messages=[
                {"role": "system", "content": "You are a helpful assistant specializing in data processing workflows. You help users plan efficient tool execution sequences."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.3,
            max_completion_tokens=200
        )
        
        # è§£æå“åº”
        content = response.choices[0].message.content.strip()
        
        # æ·»åŠ è°ƒè¯•æ—¥å¿—
        print(f"[_get_episode_workflow_guidance] LLM response received, length: {len(content)}")
        
        # éªŒè¯å“åº”å†…å®¹
        if not content:
            print("[_get_episode_workflow_guidance] Empty response from LLM")
            workflow = []
        else:
            # æŸ¥æ‰¾JSONæ•°ç»„æ¨¡å¼ - åŒ¹é… [...] æ ¼å¼
            json_pattern = r'\[\s*(?:"[^"]+"\s*(?:,\s*"[^"]+"\s*)*)?\]'
            json_match = re.search(json_pattern, content)
            
            if json_match:
                json_str = json_match.group(0)
                # å†æ¬¡éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆJSON
                if json_str.startswith('[') and json_str.endswith(']'):
                    # è§£æJSON
                    print(f"[_get_episode_workflow_guidance] Parsing JSON: {json_str[:100]}...")
                    workflow = json.loads(json_str)
                    logger.debug(f"Extracted workflow: {workflow}")
                else:
                    logger.warning(f"Invalid JSON format in LLM response: {json_str}")
                    print(f"[_get_episode_workflow_guidance] Invalid JSON format")
                    workflow = []
            else:
                logger.warning(f"No JSON array found in LLM response: {content[:200]}")
                print(f"[_get_episode_workflow_guidance] No JSON array found")
                workflow = []
        
        # ç¡®ä¿workflowæ˜¯åˆ—è¡¨
        if not isinstance(workflow, list):
            logger.warning(f"Workflow is not a list: {type(workflow)}")
            print(f"[_get_episode_workflow_guidance] Workflow is not a list, resetting")
            workflow = []
        
        # éªŒè¯å·¥å…·åç§°
        valid_tools = set(self.env.tool_registry.keys())
        validated_workflow = []
        for tool in workflow:
            if isinstance(tool, str) and tool in valid_tools:
                validated_workflow.append(tool)
            else:
                logger.debug(f"Skipping invalid tool: {tool}")
                print(f"[_get_episode_workflow_guidance] Skipping invalid tool: {tool}")
        
        workflow = validated_workflow
        
        # ç¼“å­˜ç»“æœ
        if len(self.workflow_cache) >= self.max_cache_size:
            # ç§»é™¤æœ€æ—§çš„æ¡ç›®
            self.workflow_cache.pop(next(iter(self.workflow_cache)))
        
        self.workflow_cache[task_key] = workflow
        self.current_episode_workflow = workflow.copy()
        self.env.workflow_position = 0
        
        logger.debug(f"Generated workflow guidance: {workflow}")
        print(f"[_get_episode_workflow_guidance] Final workflow: {workflow}")
    
    def _get_next_workflow_action(self, valid_actions: Optional[List[int]]) -> Optional[int]:
        """æ ¹æ®workflowå»ºè®®è·å–ä¸‹ä¸€ä¸ªåŠ¨ä½œ"""
        if not self.current_episode_workflow or not hasattr(self.env, 'workflow_position'):
            print("[_get_next_workflow_action] No workflow or position")
            return None
        
        # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰å»ºè®®çš„å·¥å…·
        if self.env.workflow_position >= len(self.current_episode_workflow):
            print("[_get_next_workflow_action] Workflow position exceeded")
            return None
        
        # è·å–ä¸‹ä¸€ä¸ªå»ºè®®çš„å·¥å…·
        suggested_tool = self.current_episode_workflow[self.env.workflow_position]
        print(f"[_get_next_workflow_action] Suggesting tool: {suggested_tool}")
        
        # æŸ¥æ‰¾å¯¹åº”çš„åŠ¨ä½œ
        for idx, action in enumerate(self.env.action_space):
            if (hasattr(action, 'tool_name') and 
                action.tool_name == suggested_tool and
                action.action_type.value == 'invoke_tool'):
                
                # éªŒè¯åŠ¨ä½œæœ‰æ•ˆæ€§
                if valid_actions is None or idx in valid_actions:
                    self.env.workflow_position += 1  # ç§»åŠ¨åˆ°ä¸‹ä¸€ä¸ªå·¥å…·
                    print(f"[_get_next_workflow_action] Found action {idx} for tool {suggested_tool}")
                    return idx
        
        # å¦‚æœå»ºè®®çš„å·¥å…·ä¸å¯ç”¨ï¼Œè·³è¿‡
        print(f"[_get_next_workflow_action] Tool {suggested_tool} not available, skipping")
        self.env.workflow_position += 1
        return self._get_next_workflow_action(valid_actions)  # é€’å½’å°è¯•ä¸‹ä¸€ä¸ª


    def _get_teacher_action(self, state: np.ndarray, valid_actions: Optional[List[int]] = None) -> Optional[int]:
        """è·å–RAGå¢å¼ºçš„LLMæ•™å¸ˆåŠ¨ä½œå»ºè®®ï¼ŒåŒ…å«å·¥å…·ä¿¡æ¯"""
        if not self.llm_client or not hasattr(self.env, 'current_state'):
            return None
        
        # æ„å»ºå½“å‰çŠ¶æ€çš„æè¿°
        current_state = self.env.current_state
        task_desc = current_state.task_objective if hasattr(current_state, 'task_objective') else "Complete the task"
        
        # è·å–å·²æ‰§è¡Œçš„å·¥å…·
        executed_tools = []
        if hasattr(current_state, 'execution_sequence'):
            for item in current_state.execution_sequence:
                if isinstance(item, str):
                    executed_tools.append(item)
                elif hasattr(item, 'tool_name'):
                    executed_tools.append(item.tool_name)
        
        # è·å–å½“å‰ä»»åŠ¡ç±»å‹
        task_type = getattr(current_state, 'task_type', 'general')
        
        # è·å–required_toolsä¿¡æ¯
        required_tools = []
        remaining_required_tools = []
        if hasattr(self.env, 'current_task_info') and self.env.current_task_info:
            required_tools = self.env.current_task_info.get('required_tools', [])
            if required_tools:
                executed_set = set(executed_tools)
                remaining_required_tools = [tool for tool in required_tools if tool not in executed_set]
        
        # åˆå§‹åŒ–RAGå¢å¼ºéƒ¨åˆ†
        rag_enhancement = ""
        
        # 1. è·å–è¯­ä¹‰ç›¸ä¼¼çš„å·¥å…·ï¼ˆä½¿ç”¨RAGï¼‰
        if hasattr(self.env, 'last_rag_context') and self.env.last_rag_context:
            rag_enhancement += "\n## Semantically Related Tools (from RAG):\n"
            for operation, results in self.env.last_rag_context.items():
                if results:
                    top_tools = []
                    for result in results[:3]:
                        if hasattr(result, 'tool_name') and hasattr(result, 'score'):
                            # æ‰¾åˆ°å·¥å…·çš„ç´¢å¼•
                            tool_idx = None
                            for idx, info in self.tool_info.items():
                                if info['name'] == result.tool_name:
                                    tool_idx = idx
                                    break
                            if tool_idx is not None:
                                top_tools.append(f"{tool_idx}:{result.tool_name} (score:{result.score:.3f})")
                    if top_tools:
                        rag_enhancement += f"- {operation}: {', '.join(top_tools)}\n"
        
        # 2. æ·»åŠ å·¥å…·ç±»åˆ«ä¿¡æ¯
        relevant_categories = set()
        if task_type == 'data_pipeline':
            relevant_categories = {'data', 'transform', 'io'}
        elif task_type == 'api_integration':
            relevant_categories = {'api', 'network', 'integration'}
        elif task_type == 'basic_task':
            relevant_categories = {'file', 'io', 'process', 'basic', 'simple'}
        
        # 3. æ„å»ºå¯ç”¨å·¥å…·çš„è¯¦ç»†æè¿°
        tool_descriptions = []
        for i in (valid_actions or range(len(self.tool_info))):
            if i in self.tool_info:
                info = self.tool_info[i]
                # æ£€æŸ¥ä¾èµ–å…³ç³»
                deps_met = True
                if info['dependencies']:
                    deps_met = all(dep in executed_tools for dep in info['dependencies'])
                
                # è®¡ç®—ä¼˜å…ˆçº§åˆ†æ•°
                priority = 0
                if info['name'] in remaining_required_tools:
                    priority = 10  # æœ€é«˜ä¼˜å…ˆçº§ç»™required tools
                elif info['category'] in relevant_categories:
                    priority = 5
                if not deps_met:
                    priority = -5  # ä¾èµ–æœªæ»¡è¶³çš„é™ä½ä¼˜å…ˆçº§
                
                desc = f"{i}: {info['name']} [{info['category']}]"
                if info['description']:
                    desc += f" - {info['description'][:50]}..."
                if info['dependencies']:
                    desc += f" (requires: {', '.join(info['dependencies'])})"
                if priority > 0:
                    desc += f" [Priority: {priority}]"
                
                tool_descriptions.append((priority, desc))
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        tool_descriptions.sort(key=lambda x: -x[0])
        tool_desc_text = "\n".join([desc for _, desc in tool_descriptions])
        
        # æ„å»ºå¢å¼ºçš„prompt
        prompt = f"""You are an expert workflow optimizer with complete knowledge of available tools.

        Current Task: {task_desc}
        Task Type: {task_type}
        
        Executed Tools: {executed_tools}
        Required Tools: {required_tools}
        Remaining Required: {remaining_required_tools}
        
        {rag_enhancement}
        
        Available Tools (with priorities):
        {tool_desc_text}
        
        Tool Categories Available: {', '.join(self.tool_categories.keys())}
        
        Consider:
        1. **PRIORITY: Execute remaining required tools first** - these are mandatory
        2. Tool dependencies - some tools require others to be executed first
        3. Logical workflow order based on data flow
        4. Semantic similarity from RAG results
        5. Task type alignment with tool categories
        
        Return ONLY the action index number. Prioritize required tools."""

        # è°ƒç”¨LLM
        try:
            response = self.llm_client.chat.completions.create(
                model=self.teacher_model_name,
                messages=[
                    {"role": "system", "content": "You are an expert in workflow optimization. You have complete knowledge of all available tools and their relationships."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.3,
                max_tokens=10
            )
            
            # è§£æå“åº”
            content = response.choices[0].message.content.strip()
            match = re.search(r'\d+', content)
            if not match:
                logger.debug("No valid action index found in teacher response")
                return None
            
            action_idx = int(match.group())
            
            # éªŒè¯åŠ¨ä½œæœ‰æ•ˆæ€§
            if valid_actions and action_idx not in valid_actions:
                logger.debug(f"Teacher suggested invalid action {action_idx}")
                return None
            if action_idx >= len(self.env.action_space):
                logger.debug(f"Teacher suggested out-of-range action {action_idx}")
                return None
            
            logger.debug(f"Teacher selected action {action_idx} ({self.tool_info.get(action_idx, {}).get('name', 'unknown')})")
            return action_idx
            
        except Exception as e:
            logger.error(f"Teacher guidance failed: {e}")
            return None


    def _get_single_teacher_action(self, request: Dict) -> int:
        """è·å–å•ä¸ªçŠ¶æ€çš„teacheræ¨èåŠ¨ä½œï¼Œåˆ©ç”¨embeddingç¼“å­˜"""
        task_desc = request['task_desc']
        tool_list = request['tool_list']
        executed_tools = request.get('executed_tools', [])
        required_tools = request.get('required_tools', [])
        rag_context = request.get('rag_context', None)

        with self.cache_lock:
            # æ£€æŸ¥ç¼“å­˜
            cached_action = self.distillation_cache.get(task_desc, tool_list, rag_context)
            if cached_action is not None:
                if isinstance(cached_action, dict) and len(cached_action) == 1:
                    action_idx = int(list(cached_action.keys())[0])
                    print(f"[PPOTrainer._get_single_teacher_action] Cache hit! Returning action {action_idx}")
                    return action_idx
                elif isinstance(cached_action, (int, float)):
                    print(f"[PPOTrainer._get_single_teacher_action] Cache hit! Returning action {int(cached_action)}")
                    return int(cached_action)
        
        # ä½¿ç”¨EmbeddingDistillationCacheè¿›è¡Œè¯­ä¹‰ç›¸ä¼¼åº¦æœç´¢
        # æ³¨æ„ï¼šç¼“å­˜ä¸­å­˜å‚¨çš„æ˜¯åŠ¨ä½œç´¢å¼•è€Œéåˆ†å¸ƒ
        cached_action = self.distillation_cache.get(task_desc, tool_list, rag_context)
        if cached_action is not None:
            # ç¼“å­˜å‘½ä¸­ï¼Œç›´æ¥è¿”å›åŠ¨ä½œ
            if isinstance(cached_action, dict) and len(cached_action) == 1:
                # å¦‚æœç¼“å­˜çš„æ˜¯å•åŠ¨ä½œçš„dictæ ¼å¼ {"action_idx": 1.0}
                action_idx = int(list(cached_action.keys())[0])
                print(f"[PPOTrainer._get_single_teacher_action] Cache hit! Returning action {action_idx}")
                return action_idx
            elif isinstance(cached_action, (int, float)):
                # å¦‚æœç›´æ¥ç¼“å­˜çš„æ˜¯åŠ¨ä½œç´¢å¼•
                print(f"[PPOTrainer._get_single_teacher_action] Cache hit! Returning action {int(cached_action)}")
                return int(cached_action)
        
        # æ„å»ºprompt - ä¿æŒç®€æ´
        tools_info = []
        for i, tool_name in enumerate(tool_list):
            status = "âœ“" if tool_name in executed_tools else "â—‹"
            required = " [REQUIRED]" if tool_name in required_tools else ""
            tools_info.append(f"{i}: {tool_name} {status}{required}")
        
        # RAGä¿¡æ¯ - å……åˆ†åˆ©ç”¨è¯­ä¹‰æœç´¢ç»“æœ
        rag_info = ""
        rag_top_suggestions = {}  # è®°å½•æ¯ä¸ªæ“ä½œçš„æœ€ä½³å·¥å…·
        
        if rag_context:
            rag_suggestions = []
            for operation, results in rag_context.items():
                if results:
                    # è·å–å¾—åˆ†æœ€é«˜çš„å·¥å…·
                    top_tools = []
                    for r in results[:3]:
                        if hasattr(r, 'tool_name') and hasattr(r, 'score'):
                            top_tools.append((r.tool_name, r.score))
                            # è®°å½•æ¯ä¸ªæ“ä½œçš„æœ€ä½³å·¥å…·
                            if operation not in rag_top_suggestions or r.score > rag_top_suggestions[operation][1]:
                                # æ‰¾åˆ°å·¥å…·ç´¢å¼•
                                try:
                                    tool_idx = tool_list.index(r.tool_name)
                                    rag_top_suggestions[operation] = (tool_idx, r.score)
                                except ValueError:
                                    pass
                    
                    if top_tools:
                        tool_info_str = ', '.join([f"{name}({score:.2f})" for name, score in top_tools])
                        rag_suggestions.append(f"{operation}: {tool_info_str}")
            
            if rag_suggestions:
                rag_info = f"\n\nRAG Semantic Suggestions:\n" + "\n".join(rag_suggestions)
        
        # æ‰¾å‡ºå‰©ä½™çš„å¿…éœ€å·¥å…·
        remaining_required = [t for t in required_tools if t not in executed_tools]
        
        # å¦‚æœæœ‰æ˜ç¡®çš„RAGå»ºè®®ä¸”æ˜¯å¿…éœ€å·¥å…·ï¼Œå¯ä»¥ç›´æ¥ä½¿ç”¨
        if remaining_required and rag_top_suggestions:
            for req_tool in remaining_required:
                try:
                    req_idx = tool_list.index(req_tool)
                    # æ£€æŸ¥è¿™ä¸ªå¿…éœ€å·¥å…·æ˜¯å¦åœ¨RAGå»ºè®®ä¸­
                    for op, (suggested_idx, score) in rag_top_suggestions.items():
                        if suggested_idx == req_idx and score > 0.8:  # é«˜ç½®ä¿¡åº¦
                            print(f"[PPOTrainer._get_single_teacher_action] High-confidence RAG suggestion for required tool: {req_tool} (score: {score})")
                            # ç¼“å­˜è¿™ä¸ªå†³ç­–
                            self.distillation_cache.put(task_desc, tool_list, {str(req_idx): 1.0}, rag_context)
                            return req_idx
                except ValueError:
                    pass
        
        prompt = f"""Task: {task_desc}

    Available Tools:
    {chr(10).join(tools_info)}

    Executed: {executed_tools}
    Remaining Required: {remaining_required}{rag_info}

    Based on:
    1. Required tools that MUST be executed
    2. Logical workflow dependencies
    3. RAG semantic suggestions above

    Select the BEST next tool index (0-{len(tool_list)-1}). Return ONLY the number!!!!! Don't return anything else!!!!!"""

        print(f"[PPOTrainer._get_single_teacher_action] Making API call for task: {task_desc[:50]}...")
        
        # APIè°ƒç”¨
        response = self.llm_client.chat.completions.create(
            model="gpt-4o-mini",
            # model="o4-mini",
            messages=[
                {"role": "system", "content": "You are a workflow optimization expert. Analyze the semantic suggestions and required tools to select the optimal next action. Return only a number."},
                {"role": "user", "content": prompt}
            ],
            max_completion_tokens=50,
            timeout=self.request_timeout
        )
        
        content = response.choices[0].message.content.strip()
        print(f"[PPOTrainer._get_single_teacher_action] Raw response: {content}")
        
        # æå–æ•°å­—
        import re
        match = re.search(r'\d+', content)
        if not match:
            print(f"[PPOTrainer._get_single_teacher_action] ERROR: No number found in response")
            raise ValueError(f"Invalid action response: {content}")
        
        action_idx = int(match.group())
        
        # éªŒè¯åŠ¨ä½œæœ‰æ•ˆæ€§
        if action_idx < 0 or action_idx >= self.env.num_actions:
            print(f"[PPOTrainer._get_single_teacher_action] ERROR: Action {action_idx} out of range")
            raise ValueError(f"Action index {action_idx} out of range")
        
        # ç¼“å­˜ç»“æœ - ä½¿ç”¨EmbeddingDistillationCacheçš„è¯­ä¹‰ç¼“å­˜
        # å­˜å‚¨ä¸ºå•åŠ¨ä½œçš„åˆ†å¸ƒæ ¼å¼ä»¥å…¼å®¹ç°æœ‰ç¼“å­˜ç»“æ„
        with self.cache_lock:
            self.distillation_cache.put(task_desc, tool_list, {str(action_idx): 1.0}, rag_context)
        
        print(f"[PPOTrainer._get_single_teacher_action] Selected action: {action_idx} ({tool_list[action_idx] if action_idx < len(tool_list) else 'unknown'})")
        return action_idx


    def _get_teacher_distributions_batch(self, states: np.ndarray, 
                                        valid_actions_list: Optional[List[List[int]]] = None) -> torch.Tensor:
        """æ‰¹é‡è·å–teacheråˆ†å¸ƒï¼Œé€šè¿‡æ”¶é›†å•ä¸ªåŠ¨ä½œæ„å»ºï¼Œå……åˆ†åˆ©ç”¨RAG"""
        batch_size = min(len(states), self.distillation_batch_size)
        
        print(f"[PPOTrainer._get_teacher_distributions_batch] Processing {batch_size} states with RAG enhancement")
        
        # å‡†å¤‡æ‰€æœ‰è¯·æ±‚å‚æ•°
        requests = []
        for i in range(batch_size):
            state = states[i]
            
            # è·å–çŠ¶æ€ä¿¡æ¯
            task_desc = "Complete the task"
            executed_tools = []
            required_tools = []
            tool_list = list(self.env.tool_registry.keys())
            
            # ä»current_stateè·å–ä¿¡æ¯
            if hasattr(self.env, 'current_state'):
                current_state = self.env.current_state
                if hasattr(current_state, 'task_objective'):
                    task_desc = current_state.task_objective
                
                # è·å–å·²æ‰§è¡Œçš„å·¥å…·
                if hasattr(current_state, 'execution_sequence'):
                    for item in current_state.execution_sequence:
                        if isinstance(item, str):
                            executed_tools.append(item)
                        elif hasattr(item, 'tool_name'):
                            executed_tools.append(item.tool_name)
                
                # è·å–å¿…éœ€çš„å·¥å…·
                if hasattr(self.env, 'current_task_info') and self.env.current_task_info:
                    required_tools = self.env.current_task_info.get('required_tools', [])
            
            # è·å–RAG context - è¿™æ˜¯å…³é”®ï¼
            rag_context = None
            if hasattr(self.env, 'last_rag_context'):
                rag_context = self.env.last_rag_context
                print(f"[PPOTrainer._get_teacher_distributions_batch] State {i} has RAG context with {len(rag_context)} operations")
            elif hasattr(current_state, 'rag_search_results'):
                # å¤‡ç”¨ï¼šä»stateä¸­è·å–
                rag_context = current_state.rag_search_results
            
            requests.append({
                'task_desc': task_desc,
                'tool_list': tool_list,
                'executed_tools': executed_tools,
                'required_tools': required_tools,
                'rag_context': rag_context  # ä¼ é€’RAGä¸Šä¸‹æ–‡ï¼
            })
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘æ‰§è¡Œ
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_concurrent_requests) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_request = {
                executor.submit(self._get_single_teacher_action, req): i 
                for i, req in enumerate(requests)
            }
            
            # æ”¶é›†ç»“æœ - ä¿æŒé¡ºåº
            results = [None] * len(requests)
            
            for future in concurrent.futures.as_completed(future_to_request):
                request_idx = future_to_request[future]
                try:
                    action = future.result(timeout=self.request_timeout)
                    results[request_idx] = action
                    print(f"[PPOTrainer._get_teacher_distributions_batch] Got action {action} for request {request_idx}")
                except Exception as e:
                    print(f"[PPOTrainer._get_teacher_distributions_batch] ERROR for request {request_idx}: {e}")
                    raise
        
        # å°†åŠ¨ä½œè½¬æ¢ä¸ºåˆ†å¸ƒ
        print(f"[PPOTrainer._get_teacher_distributions_batch] Converting {len(results)} actions to distributions")
        
        teacher_distributions = []
        for i, action in enumerate(results):
            if action is None:
                print(f"[PPOTrainer._get_teacher_distributions_batch] ERROR: Missing action for index {i}")
                raise ValueError(f"Failed to get teacher action for state {i}")
            
            # åˆ›å»ºåˆ†å¸ƒ - å¯ä»¥æ˜¯one-hotæˆ–smoothed
            distribution = torch.zeros(self.env.num_actions)
            
            # æ–¹æ¡ˆ1ï¼šOne-hotåˆ†å¸ƒ
            # distribution[action] = 1.0
            
            # æ–¹æ¡ˆ2ï¼šSmoothedåˆ†å¸ƒï¼ˆæ¨èï¼‰
            smoothing_epsilon = 0.1
            distribution.fill_(smoothing_epsilon / self.env.num_actions)
            distribution[action] = 1.0 - smoothing_epsilon + smoothing_epsilon / self.env.num_actions
            
            # å¦‚æœæœ‰RAG contextï¼Œå¯ä»¥æ ¹æ®è¯­ä¹‰ç›¸ä¼¼åº¦è¿›ä¸€æ­¥è°ƒæ•´åˆ†å¸ƒ
            if i < len(requests) and requests[i]['rag_context']:
                rag_context = requests[i]['rag_context']
                tool_list = requests[i]['tool_list']
                
                # ä¸ºè¯­ä¹‰ç›¸å…³çš„å·¥å…·åˆ†é…å°‘é‡æ¦‚ç‡
                for operation, results in rag_context.items():
                    for result in results[:3]:  # top-3
                        if hasattr(result, 'tool_name') and hasattr(result, 'score'):
                            try:
                                tool_idx = tool_list.index(result.tool_name)
                                if tool_idx != action:  # ä¸æ˜¯ä¸»è¦åŠ¨ä½œ
                                    # æ ¹æ®è¯­ä¹‰å¾—åˆ†åˆ†é…å°æ¦‚ç‡
                                    bonus_prob = result.score * 0.05  # æœ€å¤š5%çš„é¢å¤–æ¦‚ç‡
                                    distribution[tool_idx] += bonus_prob
                            except ValueError:
                                pass
                
                # é‡æ–°å½’ä¸€åŒ–
                distribution = distribution / distribution.sum()
            
            teacher_distributions.append(distribution)
        
        print(f"[PPOTrainer._get_teacher_distributions_batch] Created {len(teacher_distributions)} distributions with RAG enhancement")
        return torch.stack(teacher_distributions)
    
    def store_transition(self, state: np.ndarray, action: int, reward: float, done: bool):
        """Store transition in rollout buffer with RAG embeddingï¼Œæ·»åŠ æ•°æ®éªŒè¯"""
        # éªŒè¯è¾“å…¥æ•°æ®
        if state is None or (isinstance(state, np.ndarray) and state.size == 0):
            print(f"[PPOTrainer.store_transition] ERROR: Invalid state")
            return
        
        if not hasattr(self, 'last_value') or not hasattr(self, 'last_log_prob'):
            print(f"[PPOTrainer.store_transition] ERROR: Missing last_value or last_log_prob")
            return
        
        # è®¾ç½®ä»»åŠ¡ç±»å‹ï¼ˆå¦‚æœæ˜¯TaskAwareRolloutBufferï¼‰
        if isinstance(self.rollout_buffer, TaskAwareRolloutBuffer) and hasattr(self.env, 'current_task'):
            if self.env.episode_steps == 0:
                task_type = getattr(self.env.current_task, 'task_type', 'default')
                self.rollout_buffer.set_task_type(task_type)
                self.current_task_type = task_type
        
        # è·å–RAG embedding
        rag_embedding = None
        if hasattr(self, 'last_rag_embedding'):
            rag_embedding = self.last_rag_embedding
        elif hasattr(self.env, 'last_rag_context'):
            # å¦‚æœæ²¡æœ‰é¢„è®¡ç®—çš„embeddingï¼Œç°åœºè®¡ç®—
            rag_embedding = self.env._encode_rag_embedding(self.env.last_rag_context)
        
        # è·å–åŠ¨ä½œä¿¡æ¯
        action_info = None
        if hasattr(self.env, 'action_space') and action < len(self.env.action_space):
            action_obj = self.env.action_space[action]
            if hasattr(action_obj, '__dict__'):
                action_info = {
                    'tool_name': getattr(action_obj, 'tool_name', None),
                    'semantic_score': getattr(action_obj, 'semantic_score', 0.0),
                    'search_source': getattr(action_obj, 'search_source', 'rule_based'),
                    'confidence': getattr(action_obj, 'confidence', 1.0)
                }
        
        # Store the transition with RAG embedding
        self.rollout_buffer.add(
            state=state,
            action=action,
            reward=reward,
            value=self.last_value,
            log_prob=self.last_log_prob,
            done=done,
            rag_embedding=rag_embedding,
            action_info=action_info
        )
        
        # å¦‚æœepisodeç»“æŸä¸”ä½¿ç”¨TaskAwareRolloutBufferï¼Œå­˜å‚¨æ•´ä¸ªepisode
        if done and isinstance(self.rollout_buffer, TaskAwareRolloutBuffer):
            # éªŒè¯episodeæœ‰æ•°æ®å†å­˜å‚¨
            if len(self.rollout_buffer.current_episode['states']) > 0:
                self.rollout_buffer.store_episode()
            else:
                print(f"[PPOTrainer.store_transition] WARNING: Episode ended with no data")


    def train_step(self) -> float:
        """å¢å¼ºçš„PPOè®­ç»ƒæ­¥éª¤ï¼ŒåŒ…å«RAGå¢å¼ºçš„GPT-4o-miniè’¸é¦"""
        # Get data from rollout buffer
        if isinstance(self.rollout_buffer, TaskAwareRolloutBuffer):
            rollout_data = self.rollout_buffer.get(
                current_task_type=self.current_task_type,
                mix_ratio=self.config.get('task_mix_ratio', 0.7)
            )
        else:
            rollout_data = self.rollout_buffer.get()
        
        if not rollout_data:
            print("[PPOTrainer.train_step] No rollout data available")
            return 0.0
        
        # è§£åŒ…æ•°æ®ï¼ŒåŒ…å«RAG embeddingså’Œold_values
        if len(rollout_data) == 7:
            states, actions, old_log_probs, returns, advantages, rag_embeddings, old_values = rollout_data
        else:
            states, actions, old_log_probs, returns, advantages, rag_embeddings = rollout_data
            old_values = None
        
        # æ£€æŸ¥æ•°æ®ç±»å‹å’Œè®¾å¤‡
        if isinstance(states, torch.Tensor):
            # æ•°æ®å·²ç»æ˜¯tensorï¼Œç¡®ä¿åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
            states = states.to(self.device)
            actions = actions.to(self.device) if isinstance(actions, torch.Tensor) else torch.LongTensor(actions).to(self.device)
            old_log_probs = old_log_probs.to(self.device) if isinstance(old_log_probs, torch.Tensor) else torch.FloatTensor(old_log_probs).to(self.device)
            returns = returns.to(self.device) if isinstance(returns, torch.Tensor) else torch.FloatTensor(returns).to(self.device)
            advantages = advantages.to(self.device) if isinstance(advantages, torch.Tensor) else torch.FloatTensor(advantages).to(self.device)
            if rag_embeddings is not None:
                rag_embeddings = rag_embeddings.to(self.device) if isinstance(rag_embeddings, torch.Tensor) else torch.FloatTensor(rag_embeddings).to(self.device)
            if old_values is not None:
                old_values = old_values.to(self.device) if isinstance(old_values, torch.Tensor) else torch.FloatTensor(old_values).to(self.device)
        else:
            # æ•°æ®æ˜¯numpy arrayï¼Œéœ€è¦è½¬æ¢
            states = torch.FloatTensor(states).to(self.device)
            actions = torch.LongTensor(actions).to(self.device)
            old_log_probs = torch.FloatTensor(old_log_probs).to(self.device)
            returns = torch.FloatTensor(returns).to(self.device)
            advantages = torch.FloatTensor(advantages).to(self.device)
            if rag_embeddings is not None:
                rag_embeddings = torch.FloatTensor(rag_embeddings).to(self.device)
            if old_values is not None:
                old_values = torch.FloatTensor(old_values).to(self.device)
        
        # éªŒè¯æ•°æ®é•¿åº¦
        if len(states) < self.batch_size:
            print(f"[PPOTrainer.train_step] Insufficient data: {len(states)} < {self.batch_size}")
            return 0.0
        
        # Normalize advantages
        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)
        
        # æ”¶é›†teacheråˆ†å¸ƒï¼ˆä¿®å¤ï¼šç‹¬ç«‹çš„è’¸é¦æ§åˆ¶ï¼‰
        teacher_distributions = None
        if self.use_distillation:
            # ä½¿ç”¨ç‹¬ç«‹çš„è’¸é¦æ¦‚ç‡ï¼Œä¸ä¾èµ–teacher_guidance
            distillation_rate = self.config.get('distillation_sample_rate', 1.0)  # é»˜è®¤æ€»æ˜¯è’¸é¦
            
            # æ¯ä¸ªepochçš„è’¸é¦æ¦‚ç‡å¯ä»¥åŠ¨æ€è°ƒæ•´
            if hasattr(self, 'training_episodes'):
                # å‰æœŸå¤šè’¸é¦ï¼ŒåæœŸé€æ¸å‡å°‘
                warmup_episodes = self.config.get('distillation_warmup_episodes', 5000)
                if self.training_episodes < warmup_episodes:
                    distillation_rate = 1.0  # å‰æœŸ100%è’¸é¦
                else:
                    # åæœŸçº¿æ€§è¡°å‡åˆ°æœ€å°å€¼
                    decay_episodes = self.config.get('distillation_decay_episodes', 20000)
                    min_rate = self.config.get('distillation_min_rate', 0.3)
                    decay_progress = min(1.0, (self.training_episodes - warmup_episodes) / decay_episodes)
                    distillation_rate = 1.0 - decay_progress * (1.0 - min_rate)
            
            if random.random() < distillation_rate:
                print(f"[PPOTrainer] Collecting teacher distributions (rate={distillation_rate:.2f}, weight={self.distillation_weight})")
                # ç¡®ä¿statesæ˜¯numpy arrayä¼ é€’ç»™æ‰¹é‡æ–¹æ³•
                states_numpy = states.cpu().numpy() if isinstance(states, torch.Tensor) else states
                teacher_distributions = self._get_teacher_distributions_batch(states_numpy)
                teacher_distributions = teacher_distributions.to(self.device)
                print(f"[PPOTrainer] Collected {len(teacher_distributions)} teacher distributions")
        
        # è¿½è¸ªç»Ÿè®¡
        total_loss = 0
        num_updates = 0
        early_stop = False
        kl_divergences = []
        policy_losses = []
        value_losses = []
        distillation_losses = []
        
        # Training epochs
        for epoch in range(self.n_epochs):
            if early_stop:
                break
                
            # Shuffle indices
            indices = np.arange(len(states))
            np.random.shuffle(indices)
            
            # Mini-batch training
            for start_idx in range(0, len(states), self.batch_size):
                batch_indices = indices[start_idx:start_idx + self.batch_size]
                
                # å‡†å¤‡batchæ•°æ®
                batch_states = states[batch_indices]
                batch_actions = actions[batch_indices]
                batch_old_log_probs = old_log_probs[batch_indices]
                batch_returns = returns[batch_indices]
                batch_advantages = advantages[batch_indices]
                
                # RAG embeddings
                if rag_embeddings is not None:
                    batch_rag = rag_embeddings[batch_indices]
                else:
                    batch_rag = torch.zeros(len(batch_indices), self.config.get('rag_dim', 64)).to(self.device)
                
                # Teacher distributions for distillation
                batch_teacher_dists = None
                if teacher_distributions is not None and start_idx < len(teacher_distributions):
                    end_idx = min(start_idx + self.batch_size, len(teacher_distributions))
                    if end_idx > start_idx:
                        batch_teacher_dists = teacher_distributions[start_idx:end_idx]
                
                # Forward pass
                logits, values = self.network(batch_states, rag_context=batch_rag)
                
                # Calculate log probs and entropy
                dist = torch.distributions.Categorical(logits=logits)
                log_probs = dist.log_prob(batch_actions)
                entropy = dist.entropy().mean()
                
                # Policy loss with clipping
                ratio = torch.exp(log_probs - batch_old_log_probs)
                policy_loss_1 = -batch_advantages * ratio
                policy_loss_2 = -batch_advantages * torch.clamp(
                    ratio, 
                    1 - self.clip_range, 
                    1 + self.clip_range
                )
                policy_loss = torch.max(policy_loss_1, policy_loss_2).mean()
                
                # Value loss
                values_pred = values.squeeze(-1)
                if old_values is not None:
                    batch_old_values = old_values[batch_indices]
                    values_clipped = batch_old_values + torch.clamp(
                        values_pred - batch_old_values,
                        -self.clip_range_vf if self.clip_range_vf else float('inf'),
                        self.clip_range_vf if self.clip_range_vf else float('inf')
                    )
                    value_loss_1 = F.mse_loss(values_pred, batch_returns)
                    value_loss_2 = F.mse_loss(values_clipped, batch_returns)
                    value_loss = torch.max(value_loss_1, value_loss_2)
                else:
                    value_loss = F.mse_loss(values_pred, batch_returns)
                
                # Distillation lossï¼ˆå¦‚æœæœ‰teacheråˆ†å¸ƒï¼‰
                distillation_loss = torch.tensor(0.0).to(self.device)
                if batch_teacher_dists is not None and batch_teacher_dists.shape[0] == batch_states.shape[0]:
                    # è®¡ç®—studentçš„log probabilities
                    student_log_probs = F.log_softmax(logits / self.distillation_temperature, dim=-1)
                    # è®¡ç®—teacherçš„probabilities
                    teacher_probs = batch_teacher_dists.to(self.device)
                    # KLæ•£åº¦æŸå¤±
                    distillation_loss = F.kl_div(
                        student_log_probs,
                        teacher_probs,
                        reduction='batchmean'
                    ) * (self.distillation_temperature ** 2)
                    distillation_losses.append(distillation_loss.item())
                
                # Total loss
                loss = (policy_loss + 
                    self.vf_coef * value_loss - 
                    self.ent_coef * entropy +
                    self.distillation_weight * distillation_loss)
                
                # Backward pass
                self.optimizer.zero_grad()
                loss.backward()
                
                # Gradient clipping
                torch.nn.utils.clip_grad_norm_(self.network.parameters(), self.max_grad_norm)
                
                # Optimizer step
                self.optimizer.step()
                
                # æ›´æ–°å­¦ä¹ ç‡
                if hasattr(self, 'lr_scheduler'):
                    self.lr_scheduler.step()
                    
                    # è®°å½•å½“å‰å­¦ä¹ ç‡
                    current_lr = self.optimizer.param_groups[0]['lr']
                    if self.training_steps % 100 == 0:
                        logger.debug(f"Current learning rate: {current_lr:.6f}")
                
                self.training_steps += 1
                
                # Track metrics
                with torch.no_grad():
                    kl_div = (batch_old_log_probs - log_probs).mean().item()
                    kl_divergences.append(kl_div)
                    
                    # Early stopping check
                    if kl_div > 1.5 * 0.01:
                        print(f"[PPOTrainer] Early stopping at epoch {epoch} due to high KL divergence: {kl_div:.4f}")
                        early_stop = True
                        break
                
                total_loss += loss.item()
                policy_losses.append(policy_loss.item())
                value_losses.append(value_loss.item())
                num_updates += 1
                
                # è®°å½•è¯¦ç»†ç»Ÿè®¡
                if num_updates % 10 == 0:
                    log_msg = f"[PPOTrainer] Epoch {epoch}, Update {num_updates}: " \
                            f"loss={loss.item():.4f}, policy_loss={policy_loss.item():.4f}, " \
                            f"value_loss={value_loss.item():.4f}, entropy={entropy.item():.4f}"
                    if distillation_loss.item() > 0:
                        log_msg += f", distill_loss={distillation_loss.item():.4f}"
                    print(log_msg)
        
        # Clear rollout buffer
        self.rollout_buffer.clear()
        self.training_steps += 1
        
        # æ›´æ–°teacher guidanceæ¦‚ç‡ï¼ˆå¦‚æœä½¿ç”¨ï¼‰
        if self.use_teacher_guidance:
            self.teacher_guidance_prob = max(
                self.teacher_guidance_min_prob,
                self.teacher_guidance_prob * self.teacher_guidance_decay
            )
        
        # æ›´æ–°ç»Ÿè®¡
        if kl_divergences:
            avg_kl = np.mean(kl_divergences)
            logger.info(f"Average KL divergence: {avg_kl:.4f}")
        
        if distillation_losses:
            avg_distill_loss = np.mean(distillation_losses)
            logger.info(f"Average distillation loss: {avg_distill_loss:.4f}")
            logger.info(f"Distillation applied to {len(distillation_losses)}/{num_updates} batches")
        
        # ä¿å­˜ç¼“å­˜
        if hasattr(self, 'distillation_cache'):
            self.distillation_cache._save_cache()
        
        # è¿”å›å¹³å‡æŸå¤±
        if num_updates > 0:
            avg_loss = total_loss / num_updates
            print(f"[PPOTrainer.train_step] Training completed: {num_updates} updates, avg_loss={avg_loss:.4f}")
            return avg_loss
        else:
            print("[PPOTrainer.train_step] No updates performed")
            return 0.0
        
    def update_exploration(self):
        """PPO doesn't use explicit exploration parameters, but update teacher guidance"""
        if self.use_teacher_guidance:
            self.teacher_guidance_prob = max(
                self.teacher_guidance_min_prob,
                self.teacher_guidance_prob * self.teacher_guidance_decay
            )
    
    def save_checkpoint(self, path: str, additional_data: Dict[str, Any] = None):
        """Save PPO checkpoint"""
        state_dicts = {
            'network_state_dict': self.network.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict()
        }
        
        # ä¿å­˜ä»»åŠ¡åˆ†å¸ƒç»Ÿè®¡
        extra_data = {}
        if isinstance(self.rollout_buffer, TaskAwareRolloutBuffer):
            extra_data['task_statistics'] = self.rollout_buffer.get_task_statistics()
            extra_data['task_counts'] = dict(self.rollout_buffer.task_counts)
        
        if additional_data:
            extra_data.update(additional_data)
        
        self.save_checkpoint_base(path, state_dicts, extra_data)
    

    def load_checkpoint(self, path: str) -> Dict[str, Any]:
        """Load model checkpoint and return additional data"""
        checkpoint = self.load_checkpoint_base(path)
        
        # ä½¿ç”¨strict=FalseåŠ è½½ï¼Œå¤„ç†dynamic_adapterçš„æƒ…å†µ
        missing_keys, unexpected_keys = self.network.load_state_dict(
            checkpoint['network_state_dict'], 
            strict=False
        )
        
        # è®°å½•åŠ è½½ä¿¡æ¯ä»¥ä¾¿è°ƒè¯•
        if missing_keys:
            print(f"[PPOTrainer.load_checkpoint] Missing keys in state_dict: {missing_keys}")
        if unexpected_keys:
            print(f"[PPOTrainer.load_checkpoint] Unexpected keys in state_dict: {unexpected_keys}")
            # ç‰¹åˆ«å¤„ç†dynamic_adapter
            dynamic_adapter_keys = [k for k in unexpected_keys if 'dynamic_adapter' in k]
            if dynamic_adapter_keys:
                print(f"[PPOTrainer.load_checkpoint] Dynamic adapter will be recreated on first forward pass")
        
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        return checkpoint
# ===========================
# Import Required Components
# ===========================

def import_mdp_components():
    from generalized_mdp_framework import (
        GeneralizedMDP, GeneralizedMDPState, GeneralizedAction,
        ActionType, ToolExecutionStatus, ErrorCategory,
        DataFlowState, TaskDomain, TaskComplexity,
        TaskFeatures, ToolCapability, load_tool_capabilities
    )
    return True, locals()

def import_phase2_scoring():
    """Import Phase 2 scoring components"""
    logger.debug(f" Importing Phase 2 scoring components...")
    from workflow_quality_test_flawed import StableScorer, SimplifiedScoringConfig
    logger.debug(f" Phase 2 scoring components imported successfully")
    return True, StableScorer, SimplifiedScoringConfig


def import_workflow_generator():
    """Import workflow generator for enforcement"""
    logger.debug(f" Importing workflow generator...")
    # Avoid circular import by importing here
    import mdp_workflow_generator
    logger.debug(f" Workflow generator imported successfully")
    return True, mdp_workflow_generator.MDPWorkflowGenerator


# Import components
MDP_AVAILABLE, mdp_components = import_mdp_components()
if MDP_AVAILABLE:
    globals().update(mdp_components)

PHASE2_AVAILABLE, StableScorer, SimplifiedScoringConfig = import_phase2_scoring()
WORKFLOW_AVAILABLE, MDPWorkflowGenerator = import_workflow_generator()


# ===========================
# Task Loading and Management
# ===========================

class TaskManager:
    """Manages task loading and organization"""
    
    def __init__(self, task_path: Optional[str] = None, task_types: Optional[List[str]] = None):
        self.task_path = task_path or "mcp_generated_library/task_library_all_difficulties.json"
        self.tasks = []
        self.tasks_by_type = defaultdict(list)
        self.tasks_by_complexity = defaultdict(list)
        self.tasks_by_difficulty = defaultdict(list)  # æ–°å¢ï¼šæŒ‰éš¾åº¦çº§åˆ«ç»„ç»‡
        self.target_task_types = task_types
        self._load_tasks()
    
    def _load_tasks(self):
        """Load tasks from file or create samples"""
        task_paths = [
            # Path("mcp_generated_library/task_library_enhanced_v3.json"),  # ä¼˜å…ˆä½¿ç”¨å¢å¼ºç‰ˆ
            Path("mcp_generated_library/task_library_all_difficulties.json")
            # Path(self.task_path),
            # Path("mcp_generated_library/task_library.json"),
            # Path("task_library.json"),
        ]
        
        for path in task_paths:
            if path.exists():
                logger.debug(f" Attempting to load tasks from {path}")
                with open(path, 'r') as f:
                    data = json.load(f)
                
                # Handle different formats
                if isinstance(data, dict):
                    if 'tasks' in data:
                        self._process_task_list(data['tasks'])
                    else:
                        self._process_task_list(list(data.values()))
                elif isinstance(data, list):
                    self._process_task_list(data)
                else:
                    print(f"[ERROR] Unknown data format in {path}: {type(data)}")
                    raise ValueError(f"Unknown task data format: {type(data)}")
                
                if self.tasks:
                    # Filter by task types if specified
                    if self.target_task_types:
                        logger.debug(f" Filtering tasks by types: {self.target_task_types}")
                        self._filter_by_types()
                    logger.info(f"Loaded {len(self.tasks)} tasks from {path}")
                    self._organize_tasks()
                    return
                else:
                    print(f"[WARNING] No tasks loaded from {path}")
        
        logger.warning("No task file found, creating sample tasks")
        self._create_sample_tasks()
    
    def _process_task_list(self, tasks_data: List[Any]):
        """Process a list of task data"""
        for i, task_data in enumerate(tasks_data):
            if isinstance(task_data, dict):
                try:
                    task = self._create_task_object(task_data)
                    if task:
                        self.tasks.append(task)
                except Exception as e:
                    logger.error(f"Failed to create task {i}: {e}")
                    # Try to print task data for debugging
                    logger.debug(f"Task data: {json.dumps(task_data, indent=2, default=str)[:200]}...")
            elif isinstance(task_data, str):
                logger.warning(f"Skipping string task data: {task_data[:50]}...")
            else:
                logger.warning(f"Skipping unknown task data type: {type(task_data)}")
    

    def _create_task_object(self, data: dict):
        """Create a task object from dictionary data"""
        class Task:
            def __init__(self, data):
                # Handle different field names
                self.id = data.get('id', data.get('instance_id', f"task_{uuid.uuid4().hex[:8]}"))
                self.instance_id = self.id
                self.task_type = data.get('task_type', 'unknown')
                self.description = data.get('description', '')
                self.complexity = data.get('complexity', 'medium')
                
                # æ·»åŠ difficulty_levelå±æ€§è¯»å–
                self.difficulty_level = data.get('difficulty_level', 'medium')
                
                # Handle input data
                self.test_input = data.get('test_input', data.get('inputs', {}))
                self.inputs = self.test_input
                
                # Handle output data
                self.expected_output = data.get('expected_output', data.get('expected_outputs', {}))
                self.expected_outputs = self.expected_output
                
                # Handle tools
                self.required_tools = data.get('required_tools', [])
                
                # Metadata
                self.metadata = data.get('metadata', {})
                
                # Additional fields
                self.semantic_features = data.get('semantic_features', {})
        
        return Task(data)
    
    def _organize_tasks(self):
        """Organize tasks by type, complexity and difficulty level"""
        self.tasks_by_type.clear()
        self.tasks_by_complexity.clear()
        self.tasks_by_difficulty.clear()  # æ–°å¢ï¼šæ¸…ç©ºéš¾åº¦å­—å…¸
        
        for task in self.tasks:
            if hasattr(task, 'task_type'):
                self.tasks_by_type[task.task_type].append(task)
            if hasattr(task, 'complexity'):
                self.tasks_by_complexity[task.complexity].append(task)
            if hasattr(task, 'difficulty_level'):  # æ–°å¢ï¼šæŒ‰difficulty_levelç»„ç»‡
                self.tasks_by_difficulty[task.difficulty_level].append(task)
        
        logger.info(f"Organized tasks: {len(self.tasks_by_type)} types, "
                   f"{len(self.tasks_by_complexity)} complexity levels, "
                   f"{len(self.tasks_by_difficulty)} difficulty levels")
        
        # æ‰“å°éš¾åº¦çº§åˆ«ç»Ÿè®¡
        for difficulty, tasks in self.tasks_by_difficulty.items():
            print(f"[TaskManager] Difficulty level '{difficulty}': {len(tasks)} tasks")



    def _filter_by_types(self):
        """Filter tasks by target task types"""
        if not self.target_task_types:
            return
        
        # è®°å½•åŸå§‹æ•°é‡
        original_count = len(self.tasks)
        original_type_counts = {}
        for task in self.tasks:
            if hasattr(task, 'task_type'):
                task_type = task.task_type
                original_type_counts[task_type] = original_type_counts.get(task_type, 0) + 1
        
        # ç­›é€‰ä»»åŠ¡
        filtered_tasks = []
        for task in self.tasks:
            if hasattr(task, 'task_type') and task.task_type in self.target_task_types:
                filtered_tasks.append(task)
        
        self.tasks = filtered_tasks
        
        # è®°å½•ç­›é€‰åçš„æ•°é‡
        filtered_type_counts = {}
        for task in self.tasks:
            if hasattr(task, 'task_type'):
                task_type = task.task_type
                filtered_type_counts[task_type] = filtered_type_counts.get(task_type, 0) + 1
        
        # æ—¥å¿—è¾“å‡º
        logger.info(f"Task filtering results:")
        logger.info(f"  Total: {original_count} -> {len(self.tasks)}")
        for task_type in self.target_task_types:
            original = original_type_counts.get(task_type, 0)
            filtered = filtered_type_counts.get(task_type, 0)
            logger.info(f"  {task_type}: {original} -> {filtered}")
        
        # è­¦å‘Šå¦‚æœæŸäº›ç±»å‹æ²¡æœ‰ä»»åŠ¡
        for task_type in self.target_task_types:
            if filtered_type_counts.get(task_type, 0) == 0:
                logger.warning(f"No tasks found for type '{task_type}' in the task library!")
        
        # å¦‚æœç­›é€‰åæ²¡æœ‰ä»»ä½•ä»»åŠ¡ï¼ŒæŠ¥é”™
        if not self.tasks:
            raise ValueError(f"No tasks found for types {self.target_task_types} in the task library!")
    
    def _create_sample_tasks(self):
        """Create sample tasks if no file is found"""
        sample_tasks = [
            {
                'id': 'sample_task_1',
                'task_type': 'simple_task',
                'description': 'Process a simple data transformation',
                'complexity': 'easy',
                'test_input': {'data': [1, 2, 3, 4, 5]},
                'expected_output': {'result': [2, 4, 6, 8, 10]},
                'required_tools': ['reader', 'transformer', 'writer']
            },
            {
                'id': 'sample_task_2',
                'task_type': 'data_pipeline',
                'description': 'Build a complete data processing pipeline',
                'complexity': 'medium',
                'test_input': {'source': 'data.csv', 'format': 'csv'},
                'expected_output': {'processed': True, 'output_format': 'json'},
                'required_tools': ['reader', 'parser', 'validator', 'transformer', 'writer']
            },
            {
                'id': 'sample_task_3',
                'task_type': 'api_integration',
                'description': 'Integrate multiple APIs with validation',
                'complexity': 'hard',
                'test_input': {'endpoints': ['api1', 'api2'], 'auth': 'token'},
                'expected_output': {'integrated': True, 'validated': True},
                'required_tools': ['authenticator', 'fetcher', 'validator', 'aggregator', 'poster']
            },
            {
                'id': 'sample_task_4',
                'task_type': 'workflow_automation',
                'description': 'Automate a multi-stage workflow',
                'complexity': 'hard',
                'test_input': {'stages': 5, 'data': [100, 200, 300]},
                'expected_output': {'completed_stages': 5},
                'required_tools': ['reader', 'parser', 'transformer', 'aggregator', 'writer']
            }
        ]
        
        for i, task_data in enumerate(sample_tasks):
            logger.debug(f" Creating sample task {i+1}/{len(sample_tasks)}: {task_data['id']}")
            task = self._create_task_object(task_data)
            if task:
                self.tasks.append(task)
                logger.info(f"Created sample task: {task.id}")
            else:
                # ç›´æ¥æŠ›å‡ºé”™è¯¯ï¼Œä¸éšè—é—®é¢˜
                raise ValueError(f"Failed to create sample task from data: {task_data}")
        
        logger.info(f"Created {len(self.tasks)} sample tasks")
        self._organize_tasks()
    
    def get_task(self, task_type: Optional[str] = None, 
                 complexity: Optional[str] = None,
                 difficulty_level: Optional[str] = None) -> Any:  # æ–°å¢å‚æ•°
        """Get a task based on criteria"""
        candidates = self.tasks
        
        # ä¼˜å…ˆæŒ‰difficulty_levelç­›é€‰ï¼ˆç”¨äºcurriculum learningï¼‰
        if difficulty_level and difficulty_level in self.tasks_by_difficulty:
            candidates = self.tasks_by_difficulty[difficulty_level]
            print(f"[TaskManager] Filtering by difficulty_level '{difficulty_level}': {len(candidates)} candidates")
            
            # å¦‚æœåŒæ—¶æŒ‡å®šäº†task_typeï¼Œè¿›ä¸€æ­¥ç­›é€‰
            if task_type:
                candidates = [t for t in candidates if hasattr(t, 'task_type') and t.task_type == task_type]
                print(f"[TaskManager] Further filtering by task_type '{task_type}': {len(candidates)} candidates")
        elif task_type and task_type in self.tasks_by_type:
            candidates = self.tasks_by_type[task_type]
        elif complexity and complexity in self.tasks_by_complexity:
            candidates = self.tasks_by_complexity[complexity]
        
        if candidates:
            selected = random.choice(candidates)
            print(f"[TaskManager] Selected task: {getattr(selected, 'id', 'unknown')}, "
                  f"difficulty: {getattr(selected, 'difficulty_level', 'unknown')}")
            return selected
        elif self.tasks:
            print(f"[TaskManager] WARNING: No candidates found, selecting from all {len(self.tasks)} tasks")
            return random.choice(self.tasks)
        else:
            print(f"[TaskManager] ERROR: No tasks available!")
            return None



# ===========================
# MDP Environment Wrapper
# ===========================

# ç›¸åŒä½ç½®çš„ä¿®å¤ä»£ç 
# ä¿®æ”¹çš„è¡Œç”¨æ³¨é‡Šæ ‡æ³¨ï¼š# <- ä¿®æ”¹äº†è¿™ä¸€è¡Œ

# æ–‡ä»¶ï¼šunified_training_manager.py
# ä½ç½®ï¼šç¬¬2360-2450è¡Œ
# å®Œæ•´çš„MDPEnvironment.__init__å‡½æ•°ï¼ˆworkflowå’Œscoreråˆå§‹åŒ–éƒ¨åˆ†ï¼‰

class MDPEnvironment:
    """Environment wrapper for MDP with Phase 2/3 integration"""

    def __init__(self, mdp: GeneralizedMDP, task_manager, 
                use_task_aware_state: bool = True,
                enforce_workflow: bool = False,
                use_phase2_scoring: bool = True,
                normalize_rewards: bool = True,  # æ·»åŠ è¿™ä¸ªå‚æ•°
                trainer=None):  # æ·»åŠ  trainer å‚æ•°
        """Initialize MDP environment"""
        self.mdp = mdp
        self.task_manager = task_manager
        self.use_task_aware_state = use_task_aware_state
        self.enforce_workflow = enforce_workflow
        self.use_phase2_scoring = use_phase2_scoring
        self.normalize_rewards = normalize_rewards  # ä¿å­˜å½’ä¸€åŒ–é…ç½®


        self.current_success_rate = 0.0
        self.success_rate_threshold = 0.3  # é»˜è®¤é˜ˆå€¼
        
        # State representation
        # self.state_dim = mdp.state_dim
        
        # Current episode state
        self.current_task = None
        self.current_state = None
        self.episode_steps = 0
        
        # è¯„ä¼°æ¨¡å¼æ ‡å¿—
        self.is_evaluation_mode = False
        
        # Action space
        self.action_space = self._build_action_space()
        self.num_actions = len(self.action_space)
        self.num_tools = len(mdp.tool_capabilities)
        
        # Create tool registry for verifier
        self.tool_registry = mdp.tool_capabilities
        
        # åˆ›å»ºå·¥å…·ç´¢å¼•æ˜ å°„ï¼ˆç”¨äºåºåˆ—ç¼–ç ï¼‰
        self.tool_names = sorted(list(mdp.tool_capabilities.keys()))
        self.tool_to_idx = {tool: idx for idx, tool in enumerate(self.tool_names)}
    
        # Workflow enforcement
        self.workflow_generator = None
        self.current_workflow = None
        self.workflow_step = 0
        
        if self.enforce_workflow and MDPWorkflowGenerator:
            print("[DEBUG] Initializing MDPWorkflowGenerator for workflow enforcement")
            self.workflow_generator = MDPWorkflowGenerator(
                model_path=None,  # ä¸åŠ è½½æ¨¡å‹æ–‡ä»¶
                tools_path="./mcp_generated_library/tool_registry_consolidated.json"
            )
            
            # å¦‚æœæä¾›äº† trainerï¼Œå…±äº«å…¶ç½‘ç»œ
            if trainer and hasattr(trainer, 'network'):
                print("[DEBUG] Sharing trainer's network with workflow_generator")
                self.workflow_generator.set_network_reference(
                    trainer.network,
                    algorithm='ppo'
                )
            elif trainer and hasattr(trainer, 'q_network'):
                print("[DEBUG] Sharing trainer's Q-network with workflow_generator")
                self.workflow_generator.set_network_reference(
                    trainer.q_network,
                    algorithm='dqn'
                )
            else:
                print("[WARNING] No trainer network available for workflow_generator")
            
            logger.info("Workflow enforcement enabled with shared network")

        
        self.scorer = None
        if self.use_phase2_scoring and StableScorer:
            # Initialize embedding manager first
            embedding_manager = None
            logger.debug(f" Initializing embedding manager for Phase 2 scoring")
            from mcp_embedding_manager import MCPEmbeddingManager
            embedding_manager = MCPEmbeddingManager()
            
            # Try to load existing index
            index_path = Path(".mcp_embedding_cache/tool_index.pkl")
            embedding_manager.load_index(index_path)
            print(f"[INFO] Loaded tool embeddings for {len(embedding_manager.tool_embeddings)} tools")
            logger.info(f"Loaded embedding manager with {len(embedding_manager.tool_embeddings)} tools")

            
            # Create verifier with embedding manager
            verifier = None
            if PHASE2_AVAILABLE:
                try:
                    from workflow_quality_test_flawed import ToolCallVerifier
                    verifier = ToolCallVerifier(self.tool_registry, embedding_manager=embedding_manager)
                    logger.info("Created ToolCallVerifier for Phase 2 scoring")
                    print(f"[VERIFIER] Created with {len(self.tool_registry)} tools and embedding_manager={'enabled' if embedding_manager else 'disabled'}")
                except Exception as e:
                    logger.warning(f"Failed to create verifier: {e}")
                    print(f"[WARNING] Failed to create verifier: {e}")
            
            # Create StableScorer with both verifier and embedding_manager
            self.scorer = StableScorer(
                SimplifiedScoringConfig(), 
                verifier=verifier,
                embedding_manager=embedding_manager
            )
            logger.info(f"Phase 2 stable scoring enabled with embedding_manager={'yes' if embedding_manager else 'no'}")
        
        # Curriculum learning
        self.curriculum_stage = 0
        

        # æ–°å¢ï¼šå¥–åŠ±å½’ä¸€åŒ–ç»Ÿè®¡
        self.reward_history = []  # ä¿å­˜æœ€è¿‘çš„å¥–åŠ±å†å²
        self.reward_history_size = 1000  # å†å²çª—å£å¤§å°
        self.reward_mean = 0.0
        self.reward_std = 1.0
        self.min_reward_std = 1.0  # æœ€å°æ ‡å‡†å·®ï¼Œé¿å…é™¤é›¶
        self.normalize_rewards = True  # æ˜¯å¦å¯ç”¨å½’ä¸€åŒ–
        
        # å¥–åŠ±ç»Ÿè®¡æ›´æ–°è®¡æ•°
        self.reward_update_count = 0
        self.reward_update_frequency = 10  # æ¯10æ­¥æ›´æ–°ä¸€æ¬¡ç»Ÿè®¡
        
        
        # Task instance tracking
        self.current_task_info = {}


        logger.info(f"Environment initialized:")
        logger.info(f"  Tools: {self.num_tools}")
        logger.info(f"  Actions: {self.num_actions}")
        logger.info(f"  Task-aware: {self.use_task_aware_state}")
        logger.info(f"  Workflow enforcement: {self.enforce_workflow}")
        logger.info(f"  Phase 2 scoring: {self.use_phase2_scoring}")


    def set_current_success_rate(self, success_rate: float, threshold: Optional[float] = None):
        """è®¾ç½®å½“å‰è®­ç»ƒæˆåŠŸç‡ï¼Œç”¨äºå¥–åŠ±è®¡ç®—
        
        Args:
            success_rate: å½“å‰çš„ä»»åŠ¡æˆåŠŸç‡ï¼ˆ0-1ï¼‰
            threshold: æ¨¡å¼åˆ‡æ¢é˜ˆå€¼
                - success_rate < threshold: è¦†ç›–ç‡æ¨¡å¼ï¼ˆåªè¦æ±‚æ‰§è¡Œæ‰€æœ‰required_toolsï¼‰
                - success_rate >= threshold: é¡ºåºæ¨¡å¼ï¼ˆè¦æ±‚æŒ‰æ­£ç¡®é¡ºåºæ‰§è¡Œï¼‰
        """
        self.current_success_rate = success_rate
        if threshold is not None:
            self.success_rate_threshold = threshold
        
        mode = "coverage" if success_rate < self.success_rate_threshold else "sequence"
        print(f"[ENV] Reward mode: {mode} (success_rate={success_rate:.2%} {'<' if success_rate < self.success_rate_threshold else '>='} threshold={self.success_rate_threshold:.2%})")
    

    def _build_action_space(self) -> List[GeneralizedAction]:
        """Build action space from available actions"""
        actions = []
        
        # Add tool invocation actions
        for tool_name in sorted(self.mdp.tool_capabilities.keys()):
            actions.append(GeneralizedAction(
                action_type=ActionType.INVOKE_TOOL,
                tool_name=tool_name
            ))
        
        # Add non-tool actions
        actions.append(GeneralizedAction(ActionType.NO_OP))
        actions.append(GeneralizedAction(ActionType.VALIDATE_OUTPUT))
        actions.append(GeneralizedAction(ActionType.RETRY_TOOL))
        actions.append(GeneralizedAction(ActionType.RECOVER_ERROR))
        
        return actions

    def _decode_action(self, action_idx: int) -> Tuple[ActionType, Optional[str]]:
        """Decode action index to action type and tool name
        
        Args:
            action_idx: Integer action index
            
        Returns:
            Tuple of (ActionType, tool_name or None)
        """
        if action_idx < 0 or action_idx >= len(self.action_space):
            # é»˜è®¤è¿”å›NO_OP
            return ActionType.NO_OP, None
        
        action = self.action_space[action_idx]
        return action.action_type, getattr(action, 'tool_name', None)

    def ensure_required_tools_in_state(self, state: GeneralizedMDPState, task):
        """ç¡®ä¿çŠ¶æ€çš„metadataä¸­åŒ…å«required_toolsä¿¡æ¯"""
        
        # åˆå§‹åŒ–metadataï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if not hasattr(state, 'metadata'):
            state.metadata = {}
        
        # æ„å»ºtask_instanceä¿¡æ¯
        task_instance = {
            'task_type': getattr(task, 'task_type', 'unknown'),
            'description': getattr(task, 'description', ''),
            'required_tools': getattr(task, 'required_tools', []),
            'instance_id': getattr(task, 'id', f"task_{datetime.now().strftime('%Y%m%d_%H%M%S')}"),
            'complexity': getattr(task, 'complexity', 'medium')
        }
        
        # å­˜å‚¨åœ¨metadataä¸­
        state.metadata['task_instance'] = task_instance
        
        # åŒæ—¶ä¸ºæ¯ä¸ªrequired_toolåˆ›å»ºæœŸæœ›çš„é‡Œç¨‹ç¢‘
        if task_instance['required_tools']:
            for i, tool in enumerate(task_instance['required_tools']):
                # æ·»åŠ é¡ºåºç›¸å…³çš„é‡Œç¨‹ç¢‘
                state.expected_milestones.add(f"required_tool_{i}_{tool}_completed")
        
        logger.debug(f" Task setup - Type: {task_instance['task_type']}, "
            f"Required tools: {task_instance['required_tools']}")
        
        return state



    def reset(self, task: Optional[Any] = None, 
            task_type: Optional[str] = None,
            curriculum_stage: Optional[int] = None) -> np.ndarray:
        """Reset environment with curriculum support"""
        
        # æ›´æ–°MDPçš„è¯¾ç¨‹é˜¶æ®µ
        if curriculum_stage is not None and hasattr(self.mdp, 'set_curriculum_stage'):
            self.mdp.set_curriculum_stage(curriculum_stage)
            print(f"[ENV] Reset with curriculum stage {curriculum_stage}")
        
        # Select task based on curriculum
        if task is None:
            # æ ¹æ®è¯¾ç¨‹é˜¶æ®µé€‰æ‹©åˆé€‚éš¾åº¦çš„ä»»åŠ¡
            # ä½¿ç”¨curriculum_stageå‚æ•°ï¼Œè®©_select_taskå†…éƒ¨å¤„ç†complexityæ˜ å°„
            self.current_task = self._select_task(task_type=task_type, curriculum_stage=curriculum_stage)
        else:
            self.current_task = task
        
        # å…¶ä½™reseté€»è¾‘ä¿æŒä¸å˜...
        self.current_task_info = {
            'id': getattr(self.current_task, 'id', 'unknown'),
            'task_type': getattr(self.current_task, 'task_type', 'unknown'),
            'description': getattr(self.current_task, 'description', ''),
            'required_tools': getattr(self.current_task, 'required_tools', []),
            'complexity': getattr(self.current_task, 'complexity', 'medium'),
            'metadata': getattr(self.current_task, 'metadata', {})
        }
        
        # Create initial state
        if self.use_task_aware_state and hasattr(globals(), 'TaskAwareMDPState'):
            self.current_state = globals()['TaskAwareMDPState'](
                task_id=self.current_task.id,
                task_type=self.current_task.task_type,
                task_objective=self.current_task.description
            )
            self.current_state.task_features = self._extract_task_features(self.current_task)
        else:
            self.current_state = GeneralizedMDPState(
                task_id=self.current_task.id,
                task_type=self.current_task.task_type,
                task_objective=self.current_task.description
            )

        self.current_state.metadata['current_success_rate'] = self.current_success_rate
        self.current_state.metadata['success_rate_threshold'] = self.success_rate_threshold
        self.current_state.metadata['last_action'] = None  # åˆå§‹åŒ–ä¸Šä¸€ä¸ªåŠ¨ä½œ


        print(f"[ENV] State metadata updated - Mode: {'coverage' if self.current_success_rate < self.success_rate_threshold else 'sequence'}")
        print(f"[ENV] Required tools: {self.current_state.metadata.get('required_tools', [])}")

        # ç¡®ä¿required_toolsä¿¡æ¯åœ¨çŠ¶æ€ä¸­
        self.current_state = self.ensure_required_tools_in_state(
            self.current_state, 
            self.current_task
        )
        
        self._reset_workflow()
        self.episode_steps = 0
        
        return self._encode_state()


    def _select_task(self, task_type: Optional[str] = None,
                    curriculum_stage: Optional[int] = None) -> Any:
        """Select task based on curriculum and type"""
        print(f"[TASK_SELECT] Selecting task for stage {curriculum_stage}")
        
        # æ”¹è¿›çš„è¯¾ç¨‹éš¾åº¦æ˜ å°„ - æ›´å¹³æ»‘çš„éš¾åº¦é€’è¿›
        if curriculum_stage == 0:
            # Stage 0: åªé€‰æ‹© very_easy éš¾åº¦çš„ä»»åŠ¡
            difficulty_level = 'very_easy'
            print(f"[TASK_SELECT] Stage 0 - Using very_easy tasks only")
        elif curriculum_stage == 1:
            # Stage 1: éšæœºé€‰æ‹© very_easy æˆ– easy
            import random
            difficulty_level = random.choice(['very_easy', 'easy'])
            print(f"[TASK_SELECT] Stage 1 - Selected {difficulty_level} difficulty")
        elif curriculum_stage == 2:
            # Stage 2: ä¸»è¦é€‰æ‹© easyï¼Œå¶å°”é€‰æ‹© medium
            import random
            difficulty_level = random.choices(['easy', 'medium'], weights=[0.7, 0.3])[0]
            print(f"[TASK_SELECT] Stage 2 - Selected {difficulty_level} difficulty")
        elif curriculum_stage == 3:
            # Stage 3: å¹³è¡¡é€‰æ‹© medium å’Œ hard
            import random
            difficulty_level = random.choices(['medium', 'hard'], weights=[0.6, 0.4])[0]
            print(f"[TASK_SELECT] Stage 3 - Selected {difficulty_level} difficulty")
        else:
            # Stage 4+: æ‰€æœ‰éš¾åº¦
            difficulty_level = None
            print(f"[TASK_SELECT] Stage 4+ - Using all difficulties")
        
        # ä½¿ç”¨ TaskManager è·å–ä»»åŠ¡ - ä¿®æ”¹ï¼šä½¿ç”¨difficulty_levelè€Œä¸æ˜¯complexity
        task = self.task_manager.get_task(task_type=task_type, difficulty_level=difficulty_level)
        
        # éªŒè¯ä»»åŠ¡éš¾åº¦
        if task and hasattr(task, 'difficulty_level'):
            actual_difficulty = getattr(task, 'difficulty_level', 'unknown')
            print(f"[TASK_SELECT] Got task with difficulty: {actual_difficulty}")
            
            # ç¡®ä¿ä»»åŠ¡éš¾åº¦åŒ¹é…é¢„æœŸ
            if difficulty_level and actual_difficulty != difficulty_level:
                print(f"[TASK_SELECT] WARNING: Expected {difficulty_level}, got {actual_difficulty}")
        
        else:
            raise ValueError("No difficulty_level")
        
        return task
    
    def _extract_task_features(self, task) -> 'TaskFeatures':
        """Extract semantic features from task"""
        features = TaskFeatures()
        
        # Extract from description
        desc_lower = task.description.lower() if hasattr(task, 'description') else ''
        
        features.has_input_requirement = any(kw in desc_lower for kw in ['read', 'load', 'fetch', 'input'])
        features.has_output_requirement = any(kw in desc_lower for kw in ['write', 'save', 'export', 'output'])
        features.requires_validation = 'validat' in desc_lower
        features.requires_transformation = any(kw in desc_lower for kw in ['transform', 'convert', 'process'])
        features.requires_aggregation = any(kw in desc_lower for kw in ['aggregat', 'combin', 'merg'])
        
        # Estimate complexity
        if hasattr(task, 'complexity'):
            comp_map = {'easy': TaskComplexity.SIMPLE, 'medium': TaskComplexity.MODERATE, 'hard': TaskComplexity.COMPLEX}
            features.complexity = comp_map.get(task.complexity, TaskComplexity.MODERATE)
        
        # Estimate steps
        if hasattr(task, 'required_tools'):
            features.estimated_steps = len(task.required_tools)
        
        # Determine domain
        if 'api' in desc_lower:
            features.domain = TaskDomain.API_INTEGRATION
        elif 'file' in desc_lower:
            features.domain = TaskDomain.FILE_OPERATIONS
        elif 'data' in desc_lower:
            features.domain = TaskDomain.DATA_PROCESSING
        
        return features
    


# æ–‡ä»¶ï¼šunified_training_manager.py
# ä½ç½®ï¼šç¬¬2500-2550è¡Œ
# å®Œæ•´çš„_reset_workflowå‡½æ•°

    def _reset_workflow(self):
        """Reset workflow for enforcement"""
        # æ·»åŠ ä¸¥æ ¼çš„Noneæ£€æŸ¥
        logger.debug(f" _reset_workflow called, workflow_generator = {self.workflow_generator}")
        logger.debug(f" enforce_workflow = {self.enforce_workflow}")
        
        # å¦‚æœworkflow_generatorä¸ºNoneï¼Œç›´æ¥è¿”å›
        if self.workflow_generator is None:
            logger.debug(f" workflow_generator is None, skipping workflow reset")
            self.current_workflow = None
            self.workflow_step = 0
            return
        
        # æ£€æŸ¥current_taskæ˜¯å¦ä¸ºNone
        if self.current_task is None:
            logger.error("current_task is None in _reset_workflow")
            print("[ERROR] No task selected - task library may be empty or path incorrect")
            print("[ERROR] Please check if task_library_all_difficulties.json exists")
            self.current_workflow = None
            self.workflow_step = 0
            # æä¾›æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
            raise RuntimeError(
                "No task available. Possible causes:\n"
                "1. Task library file not found at: mcp_generated_library/task_library_all_difficulties.json\n"
                "2. Task library is empty\n"
                "3. No tasks match the requested criteria\n"
                "Please ensure the task library file exists and contains valid tasks."
            )
        
        # æ£€æŸ¥current_taskæ˜¯å¦æœ‰å¿…è¦çš„å±æ€§
        if not hasattr(self.current_task, 'task_type'):
            logger.error(f"current_task missing task_type attribute: {self.current_task}")
            print(f"[ERROR] Invalid task object: {self.current_task}")
            self.current_workflow = None
            self.workflow_step = 0
            raise RuntimeError(f"Invalid task object - missing task_type attribute: {self.current_task}")
        
        # åªæœ‰åœ¨workflow_generatorå­˜åœ¨æ—¶æ‰æ‰§è¡Œworkflowç”Ÿæˆ
        # åˆ›å»ºtask_instanceå­—å…¸ï¼ŒåŒ…å«å®Œæ•´çš„ä»»åŠ¡ä¿¡æ¯
        task_instance = {
            'task_type': self.current_task.task_type,
            'description': getattr(self.current_task, 'description', ''),
            'required_tools': getattr(self.current_task, 'required_tools', []),
            'id': getattr(self.current_task, 'id', 'unknown'),
            'complexity': getattr(self.current_task, 'complexity', 'medium'),
            'inputs': getattr(self.current_task, 'inputs', {}),
            'expected_outputs': getattr(self.current_task, 'expected_outputs', {})
        }
        
        logger.debug(f" Generating workflow for task_type: {self.current_task.task_type}")
        
        # ä½¿ç”¨task_instanceç”Ÿæˆworkflowï¼Œå¯ç”¨instance-dependent + RAG
        workflow = self.workflow_generator.generate_workflow(
            self.current_task.task_type,
            task_instance=task_instance
        )
        self.current_workflow = workflow.get('optimal_sequence', [])
        self.workflow_step = 0
        logger.info(f"Generated instance-aware workflow: {self.current_workflow}")
        logger.debug(f" Successfully generated workflow: {self.current_workflow}")

    

    def _encode_state(self) -> np.ndarray:
        """Encode state as vector with Phase 3 support"""
        state = self.current_state
        encoded = []
        
        # Tool states (one-hot encoding)
        for tool_name in sorted(self.mdp.tool_capabilities.keys()):
            if hasattr(state, 'tool_states') and tool_name in state.tool_states:
                status = state.tool_states[tool_name]
                # Handle status encoding
                if hasattr(ToolExecutionStatus, '__members__'):
                    status_list = list(ToolExecutionStatus)
                    status_idx = status_list.index(status) if status in status_list else 0
                else:
                    status_idx = 0
            else:
                status_idx = 0
            
            # One-hot encode (11 possible states)
            one_hot = [0.0] * 11
            one_hot[status_idx] = 1.0
            encoded.extend(one_hot)
        
        # Progress features (10 dimensions)
        progress_features = [
            getattr(state, 'overall_progress', 0.0),
            float(getattr(state, 'workflow_step', 0)) / 50.0,
            float(getattr(state, 'consecutive_errors', 0)) / 10.0,
            float(getattr(state, 'total_errors', 0)) / 20.0,
            float(len(getattr(state, 'execution_sequence', []))) / 20.0,
            float(len(getattr(state, 'milestones_achieved', set()))) / 10.0,
            float(getattr(state, 'current_stage', 0)) / 5.0,
            float(getattr(state, 'recovery_count', 0)) / 5.0,
            getattr(state, 'confidence_score', 1.0),
            min(1.0, getattr(state, 'time_elapsed', 0.0) / 100.0)
        ]
        encoded.extend(progress_features)
        
        # Task-aware features (if available)
        if self.use_task_aware_state and hasattr(state, 'task_features'):
            task_vector = state.task_features.to_vector()
            encoded.extend(task_vector)
        
        # Semantic features (if available)
        if hasattr(state, 'semantic_milestones'):
            semantic_features = [
                float('data_loaded' in state.milestones_achieved),
                float('data_validated' in state.milestones_achieved),
                float('data_transformed' in state.milestones_achieved),
                float('data_exported' in state.milestones_achieved),
                float(getattr(state, 'data_flow_state', DataFlowState.EMPTY) == DataFlowState.VALIDATED),
                float(getattr(state, 'data_flow_state', DataFlowState.EMPTY) == DataFlowState.TRANSFORMED),
                float(len(getattr(state, 'validations_performed', []))) / 5.0,
                float(len(getattr(state, 'semantic_milestones', []))) / 10.0,
                getattr(state, 'subtask_progress', {}).get('input_acquired', 0.0),
                getattr(state, 'subtask_progress', {}).get('validated', 0.0)
            ]
            encoded.extend(semantic_features)
        
        # åºåˆ—æ„ŸçŸ¥ç‰¹å¾ - åŒ…å«å·¥å…·æ‰§è¡Œä½ç½®å’Œé¡ºåºä¿¡æ¯  # <- æ–°å¢äº†è¿™éƒ¨åˆ†
        sequence_features = self._encode_sequence_features(state)  # <- æ–°å¢äº†è¿™ä¸€è¡Œ
        encoded.extend(sequence_features)  # <- æ–°å¢äº†è¿™ä¸€è¡Œ
        
        return np.array(encoded, dtype=np.float32)


    def _encode_sequence_features(self, state) -> List[float]:
        """ç¼–ç åºåˆ—ç›¸å…³ç‰¹å¾ï¼ŒåŸºäºæ•°æ®æµå’Œè¯­ä¹‰è¿›å±•è€Œérequired_tools"""
        features = []
        
        # 1. æ•°æ®æµè¿›å±•ç‰¹å¾ï¼ˆä¸ä¾èµ–required_toolsï¼‰
        data_flow_progression = [
            float(state.data_flow_state == DataFlowState.EMPTY),
            float(state.data_flow_state == DataFlowState.INITIALIZED),
            float(state.data_flow_state == DataFlowState.PARTIAL),  # <- ä¿®æ”¹äº†è¿™ä¸€è¡Œï¼šç”¨PARTIALæ›¿ä»£åŸæ¥é”™è¯¯çš„ä½ç½®
            float(state.data_flow_state == DataFlowState.TRANSFORMED),
            float(state.data_flow_state == DataFlowState.VALIDATED)  # <- ä¿®æ”¹äº†è¿™ä¸€è¡Œï¼šVALIDATEDæ˜¯æœ€ç»ˆçŠ¶æ€
        ]
        features.extend(data_flow_progression)  # 5ç»´
        
        # 2. è¯­ä¹‰æ“ä½œè¦†ç›–åº¦ï¼ˆå·²å®Œæˆçš„æ“ä½œç±»å‹ï¼‰
        semantic_coverage = {
            'read': False,
            'validate': False,
            'transform': False,
            'aggregate': False,
            'write': False
        }
        
        # æ£€æŸ¥å·²æ‰§è¡Œå·¥å…·çš„è¯­ä¹‰æ“ä½œ
        for tool in state.execution_sequence:
            if tool in self.mdp.tool_capabilities:
                capability = self.mdp.tool_capabilities[tool]
                for op in capability.semantic_operations:
                    for key in semantic_coverage:
                        if key in op.lower():
                            semantic_coverage[key] = True
        
        features.extend([float(v) for v in semantic_coverage.values()])  # 5ç»´
        
        # 3. å·¥å…·ç±»åˆ«è½¬æ¢æ¨¡å¼ï¼ˆæœ€è¿‘çš„ç±»åˆ«è½¬æ¢ï¼‰
        tool_category_sequence = []
        for tool in state.execution_sequence[-3:]:  # æœ€è¿‘3ä¸ªå·¥å…·
            if tool in self.mdp.tool_capabilities:
                # è·å–å·¥å…·çš„ä¸»è¦ç±»åˆ«
                category = self._get_tool_category(tool)
                tool_category_sequence.append(category)
        
        # ç¼–ç ç±»åˆ«è½¬æ¢ï¼ˆå¦‚ï¼šread->transform->writeï¼‰
        category_transitions = self._encode_category_transitions(tool_category_sequence)
        features.extend(category_transitions)  # 3ç»´
        
        # 4. æ‰§è¡Œå¯†åº¦å’Œæ•ˆç‡ç‰¹å¾
        if len(state.execution_sequence) > 0:
            # æˆåŠŸç‡
            success_rate = len([t for t in state.execution_sequence 
                            if state.tool_states.get(t) == ToolExecutionStatus.SUCCESS]) / len(state.execution_sequence)
            # å·¥å…·å¤šæ ·æ€§
            diversity = len(set(state.execution_sequence)) / len(state.execution_sequence)
        else:
            success_rate = 0.0
            diversity = 0.0
        
        features.extend([success_rate, diversity])  # 2ç»´
        
        return features  # æ€»å…±15ç»´


    def _get_tool_category(self, tool_name: str) -> float:
        """è·å–å·¥å…·çš„è¯­ä¹‰ç±»åˆ«ç¼–ç  - RAGå¢å¼ºç‰ˆæœ¬"""
        # é¦–å…ˆå°è¯•ä½¿ç”¨tool_capability_managerï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        capability = self.mdp.tool_capabilities.get(tool_name)
        # è·å–ç±»åˆ«åç§° - é€šè¿‡self.mdpè®¿é—®tool_capability_manager
        category = self.mdp.tool_capability_manager.get_category(capability)  # <- ä¿®æ”¹äº†è¿™ä¸€è¡Œï¼šæ·»åŠ self.mdp.
        # è·å–ç±»åˆ«ç¼–ç 
        category_encoding = self.mdp.tool_capability_manager.get_category_encoding(category)  # <- ä¿®æ”¹äº†è¿™ä¸€è¡Œï¼šæ·»åŠ self.mdp.
        
        # å¦‚æœæœ‰åµŒå…¥ç®¡ç†å™¨ï¼Œä½¿ç”¨RAGå¢å¼º
        if self.mdp.embedding_manager:  # <- ä¿®æ”¹äº†è¿™ä¸€è¡Œï¼šæ·»åŠ æ¡ä»¶æ£€æŸ¥å’Œself.mdp.
            # logger.debug(f" Using RAG enhancement for {tool_name}")
            # æ„å»ºæœç´¢æŸ¥è¯¢ï¼ŒåŒ…å«å·¥å…·åå’Œè¯­ä¹‰æ“ä½œ
            search_query = f"{tool_name} {' '.join(capability.semantic_operations)}"
            
            # æœç´¢è¯­ä¹‰ç›¸ä¼¼çš„å·¥å…·
            search_results = self.mdp.embedding_manager.search(  # <- ä¿®æ”¹äº†è¿™ä¸€è¡Œï¼šæ·»åŠ self.mdp.
                query=search_query,
                k=10,
                return_scores=True
            )
            
            # åˆ†ææœç´¢ç»“æœä¸­çš„ç±»åˆ«åˆ†å¸ƒ
            category_scores = {}
            total_score = 0.0
            
            for result in search_results:
                if result.tool_name in self.mdp.tool_capabilities:
                    result_capability = self.mdp.tool_capabilities[result.tool_name]
                    result_category = self.mdp.tool_capability_manager.get_category(result_capability)  # <- ä¿®æ”¹äº†è¿™ä¸€è¡Œï¼šæ·»åŠ self.mdp.
                    result_encoding = self.mdp.tool_capability_manager.get_category_encoding(result_category)  # <- ä¿®æ”¹äº†è¿™ä¸€è¡Œï¼šæ·»åŠ self.mdp.
                    
                    # ä½¿ç”¨ç›¸ä¼¼åº¦åˆ†æ•°åŠ æƒ
                    if result_encoding not in category_scores:
                        category_scores[result_encoding] = 0.0
                    category_scores[result_encoding] += result.score
                    total_score += result.score
            
            # å¦‚æœæœ‰æœ‰æ•ˆçš„æœç´¢ç»“æœï¼Œä½¿ç”¨åŠ æƒå¹³å‡
            if total_score > 0:
                rag_encoding = sum(encoding * score / total_score 
                                for encoding, score in category_scores.items())
                # æ··åˆåŸå§‹ç¼–ç å’ŒRAGç¼–ç 
                final_encoding = category_encoding * 0.6 + rag_encoding * 0.4
                # logger.debug(f" RAG-enhanced encoding for {tool_name}: {final_encoding:.3f}")
                return final_encoding
                
        return category_encoding

    def _encode_category_transitions(self, categories: List[float]) -> List[float]:
        """ç¼–ç ç±»åˆ«è½¬æ¢æ¨¡å¼"""
        if len(categories) == 0:
            return [0.0, 0.0, 0.0]
        elif len(categories) == 1:
            return [categories[0], 0.0, 0.0]
        elif len(categories) == 2:
            return [categories[0], categories[1], categories[1] - categories[0]]
        else:
            return [
                categories[0],
                categories[-1],
                np.mean([categories[i+1] - categories[i] for i in range(len(categories)-1)])
            ]

    def _encode_task_type(self, task_type: str) -> List[float]:
        """å°†ä»»åŠ¡ç±»å‹ç¼–ç ä¸ºå‘é‡"""
        task_types = ['simple_task', 'basic_task', 'data_pipeline', 
                    'api_integration', 'multi_stage_pipeline']
        
        # One-hotç¼–ç 
        features = [0.0] * len(task_types)
        if task_type in task_types:
            idx = task_types.index(task_type)
            features[idx] = 1.0
        
        return features

    def _evaluate_execution_quality(self, state: GeneralizedMDPState) -> Dict[str, Any]:
        """è¯„ä¼°æ‰§è¡Œè´¨é‡ï¼Œç”¨äºPhase 2è¯„åˆ†
        
        Returns:
            åŒ…å«è¯„ä¼°ç»“æœçš„å­—å…¸
        """
        evaluation = {
            'success_level': 'failure',
            'required_coverage': 0.0,
            'sequence_score': 0.0,
            'retry_count': 0,
            'error_count': state.total_errors
        }
        
        # è·å–required_tools
        required_tools = []
        if hasattr(state, 'metadata') and 'required_tools' in state.metadata:
            required_tools = state.metadata['required_tools']
        elif self.task_instance:
            required_tools = self.task_instance.get('required_tools', [])
        
        if not required_tools:
            # æ²¡æœ‰required_toolsæ—¶çš„ç®€å•è¯„ä¼°
            if state.is_successful:
                evaluation['success_level'] = 'full_success'
            elif state.overall_progress > 0.5:
                evaluation['success_level'] = 'partial_success'
            return evaluation
        
        # è®¡ç®—required_toolsè¦†ç›–ç‡
        executed_required = [t for t in state.execution_sequence if t in required_tools]
        evaluation['required_coverage'] = len(executed_required) / len(required_tools) if required_tools else 0.0
        
        # è®¡ç®—åºåˆ—æ­£ç¡®æ€§
        if executed_required:
            correct_order = 0
            for i, tool in enumerate(executed_required):
                expected_index = required_tools.index(tool)
                if i == expected_index:
                    correct_order += 1
            evaluation['sequence_score'] = correct_order / len(executed_required)
        
        # ç»Ÿè®¡é‡è¯•æ¬¡æ•°
        tool_counts = {}
        for tool in state.execution_sequence:
            tool_counts[tool] = tool_counts.get(tool, 0) + 1
        
        retry_count = 0
        for tool, count in tool_counts.items():
            if count > 1:
                retry_count += count - 1
        evaluation['retry_count'] = retry_count
        
        # ç¡®å®šæˆåŠŸçº§åˆ«
        if state.is_successful and evaluation['required_coverage'] >= 0.9:
            evaluation['success_level'] = 'full_success'
        elif state.is_successful or evaluation['required_coverage'] >= 0.5:
            evaluation['success_level'] = 'partial_success'
        
        return evaluation

    def _encode_description_features(self, description: str) -> List[float]:
        """ä»ä»»åŠ¡æè¿°ä¸­æå–ç®€å•çš„è¯­ä¹‰ç‰¹å¾"""
        desc_lower = description.lower()
        
        features = [
            float('read' in desc_lower or 'extract' in desc_lower),
            float('transform' in desc_lower or 'process' in desc_lower),
            float('write' in desc_lower or 'save' in desc_lower),
            float('validate' in desc_lower or 'check' in desc_lower),
            float('sequential' in desc_lower or 'pipeline' in desc_lower)
        ]
        
        return features



    def get_state_dim(self) -> int:
        """Get state dimension"""
        # Base: tools * 11 states + 10 progress features
        base_dim = self.num_tools * 11 + 10
        
        # Add task-aware features
        if self.use_task_aware_state:
            base_dim += 20  # TaskFeatures vector dimension
        
        # Add semantic features
        base_dim += 10
        
        # æ·»åŠ åºåˆ—æ„ŸçŸ¥ç‰¹å¾  # <- æ–°å¢äº†è¿™ä¸€è¡Œ
        base_dim += 15  # åºåˆ—ç‰¹å¾ç»´åº¦  # <- æ–°å¢äº†è¿™ä¸€è¡Œ
        
        return base_dim
    
    def get_valid_actions(self) -> List[int]:
        """Get valid action indices"""
        if self.enforce_workflow and self.current_workflow:
            # Workflow enforcement
            if self.workflow_step < len(self.current_workflow):
                next_tool = self.current_workflow[self.workflow_step]
                valid_indices = []
                for i, action in enumerate(self.action_space):
                    if (action.action_type == ActionType.INVOKE_TOOL and
                        action.tool_name == next_tool):
                        valid_indices.append(i)
                if valid_indices:
                    return valid_indices
        
        # Otherwise get all semantically valid actions
        valid_actions = self.mdp.get_available_actions(self.current_state)
        valid_indices = []
        
        for i, action in enumerate(self.action_space):
            for valid_action in valid_actions:
                if (action.action_type == valid_action.action_type and
                    action.tool_name == getattr(valid_action, 'tool_name', None)):
                    valid_indices.append(i)
                    break
        
        # Always allow NO_OP
        if not valid_indices:
            for i, action in enumerate(self.action_space):
                if action.action_type == ActionType.NO_OP:
                    valid_indices.append(i)
                    break
        
        return valid_indices




    def step(self, action: int) -> Tuple[np.ndarray, float, bool, Dict[str, Any]]:
        """æ‰§è¡ŒåŠ¨ä½œå¹¶è¿”å›ä¸‹ä¸€ä¸ªçŠ¶æ€ã€å¥–åŠ±ã€å®Œæˆæ ‡å¿—å’Œé¢å¤–ä¿¡æ¯"""
        self.episode_steps += 1
        
        # Decode action
        action_type, tool_name = self._decode_action(action)
        
        # è®°å½•ä¸Šä¸€ä¸ªåŠ¨ä½œåˆ°å½“å‰çŠ¶æ€çš„metadataï¼ˆç”¨äºä¸‹ä¸€æ­¥çš„å¥–åŠ±è®¡ç®—ï¼‰
        if hasattr(self.current_state, 'metadata') and self.current_state.metadata is not None:
            self.current_state.metadata['last_action'] = action_type.value
        
        # åˆ›å»ºGeneralizedActionå¯¹è±¡
        if action_type == ActionType.INVOKE_TOOL and tool_name:
            action_obj = GeneralizedAction(
                action_type=action_type,
                tool_name=tool_name
            )
        else:
            action_obj = GeneralizedAction(action_type=action_type)
        
        # æ‰§è¡ŒçŠ¶æ€è½¬æ¢
        next_state, reward, done = self.mdp.step(self.current_state, action_obj)
        
        # ç¡®ä¿ä¸‹ä¸€ä¸ªçŠ¶æ€ä¹Ÿæœ‰å½“å‰çš„æˆåŠŸç‡ä¿¡æ¯
        if hasattr(next_state, 'metadata') and next_state.metadata is not None:
            next_state.metadata['current_success_rate'] = self.current_success_rate
            next_state.metadata['success_rate_threshold'] = self.success_rate_threshold
            # ä¿ç•™required_toolsä¿¡æ¯
            if 'required_tools' in self.current_state.metadata:
                next_state.metadata['required_tools'] = self.current_state.metadata['required_tools']
        
        # è®¡ç®—é¢å¤–çš„å¥–åŠ±æˆåˆ†
        if self.use_phase2_scoring and done:
            # Phase 2è¯„åˆ†
            required_tools = []
            # ä¿®å¤ï¼šä½¿ç”¨current_task_infoè€Œä¸æ˜¯task_instance
            if hasattr(self, 'current_task_info') and self.current_task_info:
                required_tools = self.current_task_info.get('required_tools', [])
            elif hasattr(self, 'current_task') and self.current_task:
                required_tools = getattr(self.current_task, 'required_tools', [])
            
            # è¯„ä¼°æ‰§è¡Œè´¨é‡
            evaluation_details = self._evaluate_execution_quality(next_state)
            success_level = evaluation_details.get('success_level', 'failure')
            
            # è®¡ç®—Phase 2å¥–åŠ±
            phase2_reward = 0.0
            required_coverage = evaluation_details.get('required_coverage', 0.0)
            sequence_score = evaluation_details.get('sequence_score', 0.0)
            retry_count = evaluation_details.get('retry_count', 0)
            
            if success_level == 'full_success':
                # æ»¡åˆ†åŸºç¡€
                phase2_reward = 300
                
                # æ•ˆç‡å¥–åŠ±
                steps_taken = len(next_state.execution_sequence)
                expected_steps = len(required_tools) if required_tools else 10
                efficiency_ratio = expected_steps / max(steps_taken, 1)
                efficiency_bonus = min(50, efficiency_ratio * 30)
                phase2_reward += efficiency_bonus
                
                # é¡ºåºæ­£ç¡®æ€§å¥–åŠ±
                if sequence_score >= 0.9:
                    phase2_reward += 30
                elif sequence_score >= 0.7:
                    phase2_reward += 15
                    
                # æ— é‡è¯•å¥–åŠ±
                if retry_count == 0:
                    phase2_reward += 20
                    
            elif success_level == 'partial_success':
                # éƒ¨åˆ†æˆåŠŸåŸºç¡€åˆ†
                phase2_reward = 100
                
                # æ ¹æ®å®Œæˆåº¦è°ƒæ•´
                phase2_reward += required_coverage * 100
                
                # åºåˆ—è´¨é‡å¥–åŠ±
                phase2_reward += sequence_score * 50
                
            else:  # failure
                # å¤±è´¥æƒ©ç½š
                phase2_reward = -100
                
                # æ ¹æ®è¿›åº¦ç»™äºˆéƒ¨åˆ†åˆ†æ•°
                if required_coverage > 0.5:
                    phase2_reward += required_coverage * 50
            
            # ç”¨Phase2å¥–åŠ±æ›¿æ¢åŸå§‹å¥–åŠ±
            reward = phase2_reward
            
            phase2_metrics = {
                'phase2_score': phase2_reward / 300,
                'success_level': success_level,
                'retry_count': retry_count,
                'required_coverage': required_coverage,
                'sequence_correctness': sequence_score
            }
        else:
            phase2_metrics = {}
        
        # Info dict
        info = {
            'success': next_state.is_successful,
            'progress': next_state.overall_progress,
            'tools_used': len([s for s in next_state.tool_states.values() 
                            if s == ToolExecutionStatus.SUCCESS]),
            'errors': next_state.total_errors,
            'phase2_metrics': phase2_metrics,
            'episode_steps': self.episode_steps,
            'reward': reward,  # ä¿å­˜åŸå§‹å¥–åŠ±å€¼
            # ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„å±æ€§
            'required_tools': [],
            'required_coverage': phase2_metrics.get('required_coverage', 0.0),
            'sequence_correctness': phase2_metrics.get('sequence_correctness', 0.0)
        }
        
        # æ·»åŠ required_toolsåˆ°info
        if hasattr(self, 'current_task_info') and self.current_task_info:
            info['required_tools'] = self.current_task_info.get('required_tools', [])
        elif hasattr(self, 'current_task') and self.current_task:
            info['required_tools'] = getattr(self.current_task, 'required_tools', [])
        
        # ========== å¥–åŠ±å½’ä¸€åŒ–å¤„ç†ï¼ˆä¿®å¤ç‰ˆï¼‰ ==========
        # åˆå§‹åŒ–å¥–åŠ±å†å²å’Œå½’ä¸€åŒ–è®¾ç½®
        if not hasattr(self, 'phase2_reward_history'):
            self.phase2_reward_history = []
            self.base_reward_history = []
            self.phase2_stats = {'mean': 0.0, 'std': 100.0}
            self.base_stats = {'mean': 0.0, 'std': 15.0}
            self.reward_update_count = 0
            self.reward_update_frequency = 10
            self.min_reward_std = 1.0
            # ä¿®å¤ï¼šä»ç¯å¢ƒåˆå§‹åŒ–æ—¶è·å–é…ç½®ï¼Œè€Œä¸æ˜¯ä»mdp.config
            self.normalize_rewards = True  # é»˜è®¤å¯ç”¨å½’ä¸€åŒ–
        
        # è®°å½•åŸå§‹å¥–åŠ±
        info['raw_reward'] = reward
        info['reward_mode'] = 'phase2' if (self.use_phase2_scoring and done) else 'base'
        
        # æ ¹æ®æ¨¡å¼æ›´æ–°å¯¹åº”çš„å†å²
        if self.use_phase2_scoring and done:
            self.phase2_reward_history.append(reward)
            if len(self.phase2_reward_history) > 1000:
                self.phase2_reward_history.pop(0)
            history_to_use = self.phase2_reward_history
            stats_to_use = self.phase2_stats
        else:
            self.base_reward_history.append(reward)
            if len(self.base_reward_history) > 1000:
                self.base_reward_history.pop(0)
            history_to_use = self.base_reward_history
            stats_to_use = self.base_stats
        
        # å®šæœŸæ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        self.reward_update_count += 1
        if self.reward_update_count >= self.reward_update_frequency and len(history_to_use) > 10:
            rewards_array = np.array(history_to_use)
            stats_to_use['mean'] = np.mean(rewards_array)
            stats_to_use['std'] = max(self.min_reward_std, np.std(rewards_array))
            self.reward_update_count = 0
            
            # è°ƒè¯•ä¿¡æ¯
            if self.episode_steps == 1:
                print(f"[NORMALIZE] {info['reward_mode']} mode - Mean: {stats_to_use['mean']:.2f}, "
                    f"Std: {stats_to_use['std']:.2f}, Range: [{np.min(rewards_array):.2f}, {np.max(rewards_array):.2f}]")
        
        # åº”ç”¨å½’ä¸€åŒ–
        if self.normalize_rewards and len(history_to_use) > 10:
            # ä½¿ç”¨è‡ªé€‚åº”å½’ä¸€åŒ–
            normalized_reward = (reward - stats_to_use['mean']) / stats_to_use['std']
            
            # è‡ªé€‚åº”è£å‰ª
            if len(history_to_use) > 50:
                percentile_low = np.percentile(history_to_use, 1)
                percentile_high = np.percentile(history_to_use, 99)
                
                if reward < percentile_low:
                    normalized_reward = normalized_reward * 0.5
                elif reward > percentile_high:
                    normalized_reward = normalized_reward * 0.8
            
            # æœ€ç»ˆè£å‰ª
            if self.use_phase2_scoring and done:
                clip_range = (-3.0, 3.0)
            else:
                clip_range = (-5.0, 5.0)
            
            normalized_reward = np.clip(normalized_reward, clip_range[0], clip_range[1])
            
            info['normalized_reward'] = normalized_reward
            info['reward_mean'] = stats_to_use['mean']
            info['reward_std'] = stats_to_use['std']
            
            final_reward = normalized_reward
        else:
            # æ•°æ®ä¸è¶³æ—¶çš„åˆå§‹åŒ–ç­–ç•¥
            if self.use_phase2_scoring and done:
                # Phase2æ¨¡å¼ï¼šä¸­å¿ƒ100ï¼ŒèŒƒå›´[-100, 300]
                final_reward = (reward - 100.0) / 200.0
            else:
                # åŸºç¡€æ¨¡å¼ï¼šä¸­å¿ƒ0ï¼ŒèŒƒå›´[-10, 10]
                final_reward = reward / 10.0
            
            final_reward = np.clip(final_reward, -2.0, 2.0)
            info['scaled_reward'] = final_reward
            info['scaling_mode'] = 'initial'
        
        # æ³¨æ„ï¼šåˆ é™¤äº†é”™è¯¯çš„è¦†ç›–ï¼
        # final_reward = reward  # <- è¿™è¡Œå¿…é¡»åˆ é™¤ï¼
        
        # æ›´æ–°å½“å‰çŠ¶æ€
        self.current_state = next_state
        
        # è®¡ç®—RAGä¸Šä¸‹æ–‡
        if hasattr(self, '_compute_rag_context_for_state'):
            self.last_rag_context = self._compute_rag_context_for_state(next_state)
            # é¢„è®¡ç®—embedding
            if self.mdp.embedding_manager:
                self.last_rag_embedding = self._encode_rag_embedding(self.last_rag_context)
        
        # è¿”å›å½’ä¸€åŒ–åçš„å¥–åŠ±
        return self._encode_state(), final_reward, done, info

        def _compute_rag_context_for_state(self, state: GeneralizedMDPState) -> Dict[str, List]:
            """è®¡ç®—å½“å‰çŠ¶æ€çš„RAGä¸Šä¸‹æ–‡"""
            rag_context = {}
            
            if not self.mdp.embedding_manager:
                return rag_context
            
            # åŸºäºä»»åŠ¡ç›®æ ‡å’Œå½“å‰è¿›åº¦æ„å»ºæœç´¢æŸ¥è¯¢
            task_desc = state.task_objective
            
            # 1. åŸºäºæ•´ä½“ä»»åŠ¡æè¿°çš„æœç´¢
            if task_desc:
                search_results = self.mdp.embedding_manager.search(
                    query=task_desc,
                    k=10,
                    return_scores=True
                )
                rag_context['task_description'] = search_results
            
            # 2. åŸºäºå½“å‰çŠ¶æ€çš„å·¥å…·æœç´¢
            if state.execution_sequence:
                last_tool = state.execution_sequence[-1]
                query = f"next step after {last_tool} for {task_desc}"
                search_results = self.mdp.embedding_manager.search(
                    query=query,
                    k=5,
                    return_scores=True
                )
                rag_context['next_step'] = search_results
            
            return rag_context

        def _encode_rag_embedding(self, rag_context: Dict[str, List]) -> np.ndarray:
            """å°†RAGä¸Šä¸‹æ–‡ç¼–ç ä¸ºå›ºå®šç»´åº¦çš„å‘é‡"""
            rag_dim = self.mdp.config.get('rag_dim', 64)
            
            if not rag_context:
                return np.zeros(rag_dim)
            
            # ç®€å•çš„ç¼–ç ç­–ç•¥ï¼šå¯¹æ‰€æœ‰æœç´¢ç»“æœçš„åˆ†æ•°è¿›è¡ŒåŠ æƒå¹³å‡
            all_scores = []
            for results in rag_context.values():
                for _, score in results:
                    all_scores.append(score)
            
            if not all_scores:
                return np.zeros(rag_dim)
            
            # åˆ›å»ºä¸€ä¸ªåŸºäºåˆ†æ•°åˆ†å¸ƒçš„ç‰¹å¾å‘é‡
            embedding = np.zeros(rag_dim)
            
            # ç»Ÿè®¡ç‰¹å¾
            embedding[0] = np.mean(all_scores)
            embedding[1] = np.std(all_scores)
            embedding[2] = np.max(all_scores) if all_scores else 0
            embedding[3] = np.min(all_scores) if all_scores else 0
            embedding[4] = len(all_scores)
            
            # åˆ†æ•°ç›´æ–¹å›¾
            hist, _ = np.histogram(all_scores, bins=min(10, rag_dim - 5))
            embedding[5:5+len(hist)] = hist / (len(all_scores) + 1e-8)
            
            return embedding

        def _evaluate_success_level(self, state: GeneralizedMDPState, 
                                execution_history: List[ToolExecutionResult],  # <- ä¿®æ”¹äº†è¿™ä¸€è¡Œï¼šå°† ToolExecutionEntry æ”¹ä¸º ToolExecutionResult
                                tool_calls: List[str],
                                required_tools: List[str]) -> Tuple[str, Dict]:
            """è¯„ä¼°ä»»åŠ¡å®Œæˆçš„æˆåŠŸçº§åˆ« - ä½¿ç”¨è¯­ä¹‰åˆ†æå’Œæ›´ä¸¥æ ¼çš„åˆ¤å®š"""
            evaluation_details = {
                'state_completed': state.is_completed,
                'required_tools_coverage': 0.0,
                'semantic_completion': 0.0,
                'has_output': False,
                'recovery_success': False,
                'successful_tools': 0,
                'task_coherence': 0.0,  # æ–°å¢ï¼šä»»åŠ¡è¿è´¯æ€§è¯„åˆ†
                'critical_steps_completed': False  # æ–°å¢ï¼šå…³é”®æ­¥éª¤å®Œæˆ
            }
            
            # ä½¿ç”¨é…ç½®çš„é˜ˆå€¼
            thresholds = self.task_manager.scoring_thresholds if hasattr(self.task_manager, 'scoring_thresholds') else ScoringThresholds()
            logger.debug(f" Using thresholds: partial_coverage={thresholds.partial_success_coverage}")
            
            # 1. æ£€æŸ¥required_toolsè¦†ç›–ç‡
            if required_tools:
                successful_required = sum(1 for tool in required_tools
                                        if any(h.tool_name == tool and h.success 
                                            for h in execution_history))
                evaluation_details['required_tools_coverage'] = successful_required / len(required_tools)
            else:
                evaluation_details['required_tools_coverage'] = 1.0
            
            # 2. æ£€æŸ¥è¾“å‡ºç”Ÿæˆ
            output_keywords = [
                'writer', 'export', 'save', 'output', 'post', 
                'publish', 'store', 'emit', 'notify', 'report', 
                'generate', 'filter', 'aggregator', 'compressor'
            ]
            
            for exec_result in execution_history:
                if exec_result.success:
                    tool_lower = exec_result.tool_name.lower()
                    if any(keyword in tool_lower for keyword in output_keywords):
                        evaluation_details['has_output'] = True
                        break
            
            # 3. è®¡ç®—æˆåŠŸå·¥å…·æ•°
            evaluation_details['successful_tools'] = sum(1 for r in execution_history if r.success)
            
            # 4. éƒ¨åˆ†æˆåŠŸåˆ¤å®šæ¡ä»¶
            partial_success_conditions = []
            
            # æ¡ä»¶Aï¼šå®Œæˆäº†å¤§éƒ¨åˆ†required_toolsï¼ˆ>=60%ï¼‰
            if required_tools and evaluation_details['required_tools_coverage'] >= thresholds.partial_success_coverage:
                partial_success_conditions.append(f"Completed {evaluation_details['required_tools_coverage']:.0%} of required tools")
            
            # æ¡ä»¶Bï¼šæœ‰è¾“å‡ºç”Ÿæˆ
            if evaluation_details['has_output']:
                partial_success_conditions.append("Generated output")
            
            # æ¡ä»¶Cï¼šè¾¾åˆ°äº†ç‰¹å®šä»»åŠ¡ç±»å‹çš„æœ€ä½è¦æ±‚
            task_min_requirements = thresholds.task_min_requirements
            min_required = task_min_requirements.get(state.task_type, 2)
            if evaluation_details['successful_tools'] >= min_required:
                partial_success_conditions.append(f"Met minimum tool requirement ({evaluation_details['successful_tools']}/{min_required})")
            
            # 5. åˆ¤å®šé€»è¾‘
            if state.is_completed and state.is_successful:
                # å®Œå…¨æˆåŠŸ
                return "full_success", evaluation_details
            elif len(partial_success_conditions) >= 2:
                # éƒ¨åˆ†æˆåŠŸï¼šè‡³å°‘æ»¡è¶³2ä¸ªæ¡ä»¶
                evaluation_details['success_reasons'] = partial_success_conditions
                return "partial_success", evaluation_details
            else:
                # å¤±è´¥
                evaluation_details['failure_reasons'] = [
                    f"Required tools coverage: {evaluation_details['required_tools_coverage']:.0%}",
                    f"Successful tools: {evaluation_details['successful_tools']}",
                    f"Has output: {evaluation_details['has_output']}"
                ]
                return "failure", evaluation_details
        

        def _calculate_sequence_coherence(self, tool_sequence: List[str]) -> float:
            """è®¡ç®—å·¥å…·åºåˆ—çš„è¿è´¯æ€§å¾—åˆ†"""
            if len(tool_sequence) < 2:
                return 1.0
            
            coherence_score = 0.0
            valid_transitions = 0
            
            # æ£€æŸ¥æ¯ä¸ªç›¸é‚»å·¥å…·å¯¹çš„åˆç†æ€§
            for i in range(len(tool_sequence) - 1):
                current_tool = tool_sequence[i]
                next_tool = tool_sequence[i + 1]
                
                # ä½¿ç”¨è¯­ä¹‰æœç´¢æ£€æŸ¥è½¬æ¢åˆç†æ€§
                query = f"tools that naturally follow after {current_tool}"
                results = self.task_manager.embedding_manager.search(query, k=10, return_scores=True)
                
                for result in results:
                    if result.tool_name == next_tool and result.score > 0.6:
                        valid_transitions += 1
                        break

            
            coherence_score = valid_transitions / (len(tool_sequence) - 1)
            return coherence_score

        def _is_valid_transition_by_category(self, current_tool: str, next_tool: str) -> bool:
            """åŸºäºå·¥å…·ç±»åˆ«åˆ¤æ–­è½¬æ¢æ˜¯å¦åˆç†"""
            # å®šä¹‰åˆç†çš„ç±»åˆ«è½¬æ¢
            valid_transitions = {
                'reader': ['validator', 'parser', 'transformer', 'filter'],
                'scanner': ['reader', 'validator', 'parser'],
                'validator': ['transformer', 'filter', 'aggregator'],
                'transformer': ['validator', 'writer', 'aggregator'],
                'filter': ['aggregator', 'writer', 'transformer'],
                'aggregator': ['writer', 'exporter'],
                'parser': ['transformer', 'validator', 'filter']
            }
            
            # è·å–å·¥å…·ç±»åˆ«
            current_category = None
            next_category = None
            
            for category in valid_transitions.keys():
                if category in current_tool.lower():
                    current_category = category
                if category in next_tool.lower():
                    next_category = category
            
            if current_category and next_category:
                return next_category in valid_transitions.get(current_category, [])
            
            # é»˜è®¤å…è®¸
            return True

        def _check_pipeline_stages_semantically(self, successful_tools: set) -> bool:
            """ä½¿ç”¨è¯­ä¹‰åˆ†ææ£€æŸ¥ç®¡é“é˜¶æ®µæ˜¯å¦å®Œæ•´"""
            if hasattr(self.task_manager, 'embedding_manager') and self.task_manager.embedding_manager:
                # è¯­ä¹‰åˆ†æç®¡é“é˜¶æ®µ
                pipeline_stages = [
                    "data input or reading stage",
                    "data transformation or processing stage", 
                    "data output or writing stage"
                ]
                
                stages_completed = 0
                for stage_query in pipeline_stages:
                    try:
                        results = self.task_manager.embedding_manager.search(stage_query, k=10, return_scores=True)
                        for result in results:
                            if result.tool_name in successful_tools and result.score > 0.7:
                                stages_completed += 1
                                logger.debug(f" Stage '{stage_query}' completed by {result.tool_name}")
                                break
                    except:
                        pass
                
                return stages_completed >= 3
            
            else:
                # å›é€€åˆ°ç®€å•æ£€æŸ¥
                stage_keywords = ['read', 'transform', 'write', 'parse', 'validate']
                stages_found = sum(1 for keyword in stage_keywords 
                                if any(keyword in tool.lower() for tool in successful_tools))
                return stages_found >= 3

        def _check_output_generated(self, execution_history):
            """æ£€æŸ¥æ˜¯å¦ç”Ÿæˆäº†è¾“å‡º"""
            output_keywords = [
                'writer', 'export', 'save', 'output', 'post', 
                'publish', 'store', 'emit', 'filter', 'aggregator', 'compressor'
            ]
            
            for exec_result in execution_history:
                if exec_result.success:
                    tool_lower = exec_result.tool_name.lower()
                    if any(keyword in tool_lower for keyword in output_keywords):
                        return True
            return False

# ===========================
# Replay Buffer
# ===========================

# ç›¸åŒä½ç½®çš„ä¿®å¤ä»£ç 
# ä¿®æ”¹çš„è¡Œç”¨æ³¨é‡Šæ ‡æ³¨ï¼š# <- ä¿®æ”¹äº†è¿™ä¸€è¡Œ

class ReplayBuffer:
    """Experience replay buffer for DQN training"""
    
    def __init__(self, capacity: int):
        self.capacity = capacity
        self.buffer = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done, task_type=None):  # <- ä¿®æ”¹äº†è¿™ä¸€è¡Œï¼šæ·»åŠ task_typeå‚æ•°
        """Store a transition"""
        self.buffer.append((state, action, reward, next_state, done, task_type))  # <- ä¿®æ”¹äº†è¿™ä¸€è¡Œï¼šå­˜å‚¨task_type
    
    def sample(self, batch_size: int):
        """Sample a batch of transitions"""
        batch = random.sample(self.buffer, batch_size)
        
        states = torch.FloatTensor([t[0] for t in batch]).to(device)
        actions = torch.LongTensor([t[1] for t in batch]).to(device)
        rewards = torch.FloatTensor([t[2] for t in batch]).to(device)
        next_states = torch.FloatTensor([t[3] for t in batch]).to(device)
        dones = torch.FloatTensor([t[4] for t in batch]).to(device)
        
        return states, actions, rewards, next_states, dones
    
    def __len__(self):
        return len(self.buffer)

# ===========================
# Task-Aware Memory System
# ===========================

class TaskAwareMemory:
    """Task-aware experience memory with balanced sampling"""
    
    def __init__(self, capacity_per_task: int = 10000, min_samples_per_task: int = 100):
        self.capacity_per_task = capacity_per_task
        self.min_samples_per_task = min_samples_per_task
        self.task_buffers = {}  # task_type -> ReplayBuffer
        self.task_counts = defaultdict(int)  # ç»Ÿè®¡æ¯ä¸ªä»»åŠ¡çš„ç»éªŒæ•°
        self.global_buffer = ReplayBuffer(capacity_per_task * 5)  # å…¨å±€ç¼“å†²åŒº
        
    def push(self, state, action, reward, next_state, done, task_type=None):
        """Store experience with task awareness"""
        # å­˜å‚¨åˆ°å…¨å±€ç¼“å†²åŒº
        self.global_buffer.push(state, action, reward, next_state, done, task_type)
        
        # å¦‚æœæœ‰ä»»åŠ¡ç±»å‹ï¼Œä¹Ÿå­˜å‚¨åˆ°ä»»åŠ¡ç‰¹å®šç¼“å†²åŒº
        if task_type:
            if task_type not in self.task_buffers:
                self.task_buffers[task_type] = ReplayBuffer(self.capacity_per_task)
            
            self.task_buffers[task_type].push(state, action, reward, next_state, done, task_type)
            self.task_counts[task_type] += 1
    
    def sample(self, batch_size: int, current_task_type=None, mix_ratio=0.7):
        """Sample with task-aware mixing strategy"""
        if len(self.global_buffer) < batch_size:
            return None
        
        # å¦‚æœæŒ‡å®šäº†å½“å‰ä»»åŠ¡ç±»å‹ï¼Œæ··åˆé‡‡æ ·
        if current_task_type and current_task_type in self.task_buffers:
            current_task_samples = int(batch_size * mix_ratio)
            other_samples = batch_size - current_task_samples
            
            # ä»å½“å‰ä»»åŠ¡é‡‡æ ·
            current_batch = []
            if len(self.task_buffers[current_task_type]) >= current_task_samples:
                current_batch = self._sample_from_buffer(
                    self.task_buffers[current_task_type], 
                    current_task_samples
                )
            
            # ä»å…¶ä»–ä»»åŠ¡å‡è¡¡é‡‡æ ·
            other_batch = self._sample_balanced(other_samples, exclude_task=current_task_type)
            
            # åˆå¹¶æ‰¹æ¬¡
            all_transitions = current_batch + other_batch
            
        else:
            # å‡è¡¡é‡‡æ ·æ‰€æœ‰ä»»åŠ¡
            all_transitions = self._sample_balanced(batch_size)
        
        # è½¬æ¢ä¸ºå¼ é‡
        return self._transitions_to_tensors(all_transitions)
    
    def _sample_balanced(self, num_samples, exclude_task=None):
        """Balanced sampling across task types"""
        available_tasks = [t for t in self.task_buffers.keys() if t != exclude_task]
        
        if not available_tasks:
            # ä»å…¨å±€ç¼“å†²åŒºé‡‡æ ·
            return self._sample_from_buffer(self.global_buffer, num_samples)
        
        samples_per_task = max(1, num_samples // len(available_tasks))
        remaining = num_samples - (samples_per_task * len(available_tasks))
        
        all_samples = []
        for task_type in available_tasks:
            buffer = self.task_buffers[task_type]
            if len(buffer) > 0:
                n_samples = min(samples_per_task, len(buffer))
                all_samples.extend(self._sample_from_buffer(buffer, n_samples))
        
        # å¡«å……å‰©ä½™æ ·æœ¬
        if remaining > 0 and len(all_samples) < num_samples:
            extra_samples = self._sample_from_buffer(
                self.global_buffer, 
                min(remaining, num_samples - len(all_samples))
            )
            all_samples.extend(extra_samples)
        
        random.shuffle(all_samples)
        return all_samples[:num_samples]
    
    def _sample_from_buffer(self, buffer, num_samples):
        """Sample from a specific buffer"""
        if isinstance(buffer, ReplayBuffer):
            # ä¸´æ—¶æ–¹æ³•ï¼šç›´æ¥ä»bufferçš„å†…éƒ¨dequeé‡‡æ ·
            return random.sample(buffer.buffer, min(num_samples, len(buffer.buffer)))
        return []
    
    def _transitions_to_tensors(self, transitions):
        """Convert transitions to tensors"""
        if not transitions:
            return None
            
        states = torch.FloatTensor([t[0] for t in transitions]).to(device)
        actions = torch.LongTensor([t[1] for t in transitions]).to(device)
        rewards = torch.FloatTensor([t[2] for t in transitions]).to(device)
        next_states = torch.FloatTensor([t[3] for t in transitions]).to(device)
        dones = torch.FloatTensor([t[4] for t in transitions]).to(device)
        
        return states, actions, rewards, next_states, dones
    
    def get_task_statistics(self):
        """Get memory statistics per task type"""
        stats = {}
        for task_type, buffer in self.task_buffers.items():
            stats[task_type] = {
                'count': len(buffer),
                'total_collected': self.task_counts[task_type]
            }
        return stats
    
    def __len__(self):
        return len(self.global_buffer)

# ===========================
# DQN Trainer
# ===========================


# class DQNTrainer(BaseTrainer):
    """DQN trainer implementation"""
    
    def __init__(self, env: 'MDPEnvironment', config: Dict[str, Any]):
        super().__init__(env, config)  # <- ä¿®æ”¹ï¼šè°ƒç”¨åŸºç±»æ„é€ å‡½æ•°
        
        # Network
        state_dim = env.get_state_dim()
        action_dim = env.num_actions
        hidden_dim = config.get('hidden_dim', 256)
        
        self.q_network = DuelingDQN(state_dim, action_dim, hidden_dim).to(self.device)  # <- ä¿®æ”¹ï¼šä½¿ç”¨self.device
        self.target_network = DuelingDQN(state_dim, action_dim, hidden_dim).to(self.device)  # <- ä¿®æ”¹
        self.target_network.load_state_dict(self.q_network.state_dict())
        
        # Optimizer
        self.optimizer = optim.Adam(self.q_network.parameters(), lr=config['learning_rate'])
        self.scheduler = ReduceLROnPlateau(self.optimizer, patience=10, factor=0.5)
        
        # Replay buffer - ä½¿ç”¨ä»»åŠ¡æ„ŸçŸ¥è®°å¿†ç³»ç»Ÿ
        use_task_aware_memory = config.get('use_task_aware_memory', True)
        if use_task_aware_memory:
            self.memory = TaskAwareMemory(
                capacity_per_task=config.get('memory_size', 50000) // 5,
                min_samples_per_task=100
            )
            self.replay_buffer = self.memory  # å…¼å®¹æ€§åˆ«å
        else:
            self.replay_buffer = ReplayBuffer(config['memory_size'])
        
        # Training parameters
        self.epsilon = config['epsilon_start']
        self.epsilon_decay = config['epsilon_decay']
        self.epsilon_min = config['epsilon_min']
        self.gamma = config['gamma']
        self.batch_size = config['batch_size']
        
        # Statistics
        self.target_update_counter = 0
        self.current_task_type = None
    
    def set_eval_mode(self, eval_mode: bool):  # <- ä¿®æ”¹ï¼šé‡å†™åŸºç±»æ–¹æ³•
        """Set evaluation mode - disable exploration"""
        super().set_eval_mode(eval_mode)
        if eval_mode:
            self.stored_epsilon = self.epsilon
            self.epsilon = 0.0
        else:
            if hasattr(self, 'stored_epsilon'):
                self.epsilon = self.stored_epsilon
    
    def select_action(self, state: np.ndarray, valid_actions: Optional[List[int]] = None) -> int:
        """Select action using epsilon-greedy with optional masking"""
        # Epsilon-greedy
        if not self.eval_mode and random.random() < self.epsilon:  # <- ä¿®æ”¹ï¼šæ£€æŸ¥eval_mode
            if valid_actions:
                return random.choice(valid_actions)
            else:
                return random.randint(0, self.env.num_actions - 1)
        
        # Greedy action
        with torch.no_grad():
            state_tensor = torch.FloatTensor(state).unsqueeze(0).to(self.device)  # <- ä¿®æ”¹ï¼šä½¿ç”¨self.device
            q_values = self.q_network(state_tensor).squeeze()
            
            if valid_actions:
                # ä½¿ç”¨åŸºç±»çš„åŠ¨ä½œæ©ç æ–¹æ³•  # <- ä¿®æ”¹
                q_values = self.apply_action_mask(q_values, valid_actions)
                
            return q_values.argmax().item()
    
    def store_experience(self, state: np.ndarray, action: int, reward: float,  # <- æ–°å¢ï¼šå®ç°ç»Ÿä¸€æ¥å£
                        next_state: np.ndarray, done: bool, **kwargs) -> None:
        """Store experience in replay buffer"""
        task_type = kwargs.get('task_type', None)
        
        if isinstance(self.replay_buffer, TaskAwareMemory):
            self.replay_buffer.push(state, action, reward, next_state, done, task_type)
        else:
            self.replay_buffer.push(state, action, reward, next_state, done)

    def should_train(self) -> bool:  # <- æ–°å¢ï¼šåˆ¤æ–­æ˜¯å¦åº”è¯¥è®­ç»ƒ
        """DQN trains when replay buffer has enough samples"""
        return len(self.replay_buffer) >= self.config['batch_size']
    
    def on_episode_end(self) -> None:  # <- æ–°å¢ï¼šepisodeç»“æŸå¤„ç†
        """DQN doesn't need special episode end handling"""
        pass
    
    
    def train_step(self) -> float:
        """Perform one training step"""
        if len(self.replay_buffer) < self.batch_size:
            return 0.0
        
        # Sample batch - ä½¿ç”¨ä»»åŠ¡æ„ŸçŸ¥é‡‡æ ·
        if isinstance(self.replay_buffer, TaskAwareMemory):
            batch = self.replay_buffer.sample(
                self.batch_size,
                current_task_type=self.current_task_type,
                mix_ratio=0.7  # 70%æ¥è‡ªå½“å‰ä»»åŠ¡ï¼Œ30%æ¥è‡ªå…¶ä»–ä»»åŠ¡
            )
            if batch is None:
                return 0.0
            states, actions, rewards, next_states, dones = batch
        else:
            states, actions, rewards, next_states, dones = self.replay_buffer.sample(self.batch_size)
        
        # Current Q values
        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1))
        
        # Next Q values (Double DQN)
        with torch.no_grad():
            # Select actions using online network
            next_actions = self.q_network(next_states).argmax(dim=1)
            # Evaluate using target network
            next_q_values = self.target_network(next_states).gather(1, next_actions.unsqueeze(1))
            target_q_values = rewards.unsqueeze(1) + self.gamma * next_q_values * (1 - dones.unsqueeze(1))
        
        # Loss
        loss = F.mse_loss(current_q_values, target_q_values)
        
        # Optimize
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.q_network.parameters(), 1.0)
        self.optimizer.step()
        
        # Update target network
        self.training_steps += 1
        if self.training_steps % self.config['target_update_frequency'] == 0:
            self.target_network.load_state_dict(self.q_network.state_dict())
            self.target_update_counter += 1
        
        return loss.item()
    
    def update_exploration(self):
        """Update exploration rate"""
        # æ·»åŠ åŸºäºè®­ç»ƒæ­¥æ•°çš„å¿«é€Ÿè¡°å‡
        if self.training_steps < 100:
            fast_decay_rate = 0.995
            self.epsilon = max(0.5, self.epsilon * fast_decay_rate)
        else:
            # ä¹‹åä½¿ç”¨æ­£å¸¸è¡°å‡ç‡
            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
    
    def save_checkpoint(self, path: str, additional_data: Dict[str, Any] = None):
        """Save training checkpoint"""
        state_dicts = {  # <- ä¿®æ”¹ï¼šä½¿ç”¨åŸºç±»æ–¹æ³•
            'q_network_state_dict': self.q_network.state_dict(),
            'target_network_state_dict': self.target_network.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'epsilon': self.epsilon
        }
        
        # ä¿å­˜ä»»åŠ¡æ„ŸçŸ¥è®°å¿†çš„ç»Ÿè®¡ä¿¡æ¯
        extra_data = {}
        if isinstance(self.replay_buffer, TaskAwareMemory):
            extra_data['memory_stats'] = self.replay_buffer.get_task_statistics()
        
        if additional_data:
            extra_data.update(additional_data)
        
        self.save_checkpoint_base(path, state_dicts, extra_data)  # <- ä¿®æ”¹ï¼šä½¿ç”¨åŸºç±»æ–¹æ³•
    
    def load_checkpoint(self, path: str) -> Dict[str, Any]:
        """Load training checkpoint and return additional data"""
        checkpoint = self.load_checkpoint_base(path)  # <- ä¿®æ”¹ï¼šä½¿ç”¨åŸºç±»æ–¹æ³•
        
        self.q_network.load_state_dict(checkpoint['q_network_state_dict'])
        self.target_network.load_state_dict(checkpoint['target_network_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.epsilon = checkpoint.get('epsilon', self.epsilon)
        
        return checkpoint
# ===========================
# Curriculum Learning
# ===========================

class CurriculumScheduler:
    """Manages curriculum learning progression"""
    
    def __init__(self, total_episodes: int, config: Optional[Dict[str, Any]] = None):
        self.total_episodes = total_episodes
        self.current_episode = 0
        
        # ä»configè¯»å–stage_thresholdsï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
        if config and 'curriculum_stages' in config:
            self.stage_thresholds = config['curriculum_stages']
            print(f"[CURRICULUM] Loading stage thresholds from config: {self.stage_thresholds}")
        else:
            # é»˜è®¤å€¼ï¼š5ä¸ªstages
            self.stage_thresholds = [0.1, 0.25, 0.5, 0.75]
            print(f"[CURRICULUM] Using default stage thresholds: {self.stage_thresholds}")
        
        # ä»configè¯»å–stage namesï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤å€¼
        if config and 'curriculum_stage_names' in config:
            self.stage_names = config['curriculum_stage_names']
        else:
            self.stage_names = ["Very Easy", "Easy", "Medium", "Hard", "Expert"]
        
        # éªŒè¯é…ç½®ä¸€è‡´æ€§
        num_stages = len(self.stage_thresholds) + 1
        if len(self.stage_names) < num_stages:
            # å¦‚æœåç§°ä¸å¤Ÿï¼Œè‡ªåŠ¨è¡¥å……
            for i in range(len(self.stage_names), num_stages):
                self.stage_names.append(f"Stage {i}")
        
        # æ·»åŠ è¯¦ç»†çš„è°ƒè¯•ä¿¡æ¯
        print(f"[CURRICULUM] Initialized with {total_episodes} total episodes")
        print(f"[CURRICULUM] Number of stages: {num_stages}")
        for i in range(len(self.stage_thresholds)):
            start_ep = int(total_episodes * (self.stage_thresholds[i-1] if i > 0 else 0))
            end_ep = int(total_episodes * self.stage_thresholds[i])
            print(f"[CURRICULUM] Stage {i}: episodes {start_ep}-{end_ep} "
                  f"({(self.stage_thresholds[i-1] if i > 0 else 0)*100:.0f}-{self.stage_thresholds[i]*100:.0f}%) "
                  f"- {self.stage_names[i]}")
        # æœ€åä¸€ä¸ªstage
        last_start = int(total_episodes * self.stage_thresholds[-1])
        print(f"[CURRICULUM] Stage {num_stages-1}: episodes {last_start}-{total_episodes} "
              f"({self.stage_thresholds[-1]*100:.0f}-100%) - {self.stage_names[num_stages-1]}")
        
        # æ·»åŠ è°ƒè¯•è·Ÿè¸ª
        self.stage_history = []
        self.last_printed_stage = -1
    
    
    def get_stage(self) -> int:
        """Get current curriculum stage"""
        progress = self.current_episode / self.total_episodes
        
        # æ·»åŠ è¿›åº¦æ‰“å°ï¼ˆæ¯100ä¸ªepisodeæ‰“å°ä¸€æ¬¡ï¼‰
        if self.current_episode % 100 == 0:
            print(f"[CURRICULUM] Episode {self.current_episode}/{self.total_episodes} "
                  f"(Progress: {progress:.1%})")
        
        for i, threshold in enumerate(self.stage_thresholds):
            if progress < threshold:
                return i
        
        return len(self.stage_thresholds)
    
    def get_stage_name(self) -> str:
        """Get human-readable stage name"""
        stage = self.get_stage()
        # æ·»åŠ è¾¹ç•Œæ£€æŸ¥ï¼Œç¡®ä¿ä¸ä¼šè¶Šç•Œ
        if stage < len(self.stage_names):
            name = self.stage_names[stage]
        else:
            # å¦‚æœè¶…å‡ºèŒƒå›´ï¼Œä½¿ç”¨é»˜è®¤æ ¼å¼
            name = f"Stage {stage}"
            print(f"[CURRICULUM WARNING] Stage {stage} exceeds defined names (max: {len(self.stage_names)-1})")
        
        progress = self.current_episode / self.total_episodes
        return f"{name} (Stage {stage}, Progress: {progress:.1%})"
    

    def update(self):
        """Update progress"""
        # å…ˆè·å–å½“å‰stage
        old_stage = self.get_stage()
        
        # æ›´æ–°episodeè®¡æ•°
        self.current_episode += 1
        
        # è·å–æ–°çš„stage
        new_stage = self.get_stage()
        
        # è®°å½•stageå†å²
        self.stage_history.append((self.current_episode, new_stage))
        
        # æ£€æŸ¥æ˜¯å¦è¿›å…¥æ–°é˜¶æ®µ
        if new_stage != old_stage:
            print(f"[CURRICULUM] ğŸ¯ Advancing from Stage {old_stage} to Stage {new_stage} "
                f"at episode {self.current_episode}")
            print(f"[CURRICULUM] Progress: {self.current_episode}/{self.total_episodes} = "
                f"{self.current_episode/self.total_episodes:.1%}")
            
            # æ˜¾ç¤ºæ–°stageçš„ç‰¹ç‚¹
            stage_descriptions = {
                0: "Very Easy - Minimal requirements, high tolerance",
                1: "Easy - Basic requirements, moderate tolerance", 
                2: "Medium - Standard requirements, balanced difficulty",
                3: "Hard - Strict requirements, low tolerance",
                4: "Expert - Full requirements, minimal tolerance"
            }
            if new_stage in stage_descriptions:
                print(f"[CURRICULUM] Stage {new_stage}: {stage_descriptions[new_stage]}")
        
            # æ¯500ä¸ªepisodeæ‰“å°ä¸€æ¬¡è¯¦ç»†çŠ¶æ€
            if self.current_episode % 500 == 0:
                print(f"[CURRICULUM DEBUG] Current episode: {self.current_episode}, "
                    f"Total episodes: {self.total_episodes}, "
                    f"Stage: {new_stage}, "
                    f"Progress: {self.current_episode/self.total_episodes:.1%}")

# ===========================
# Unified Training Manager (Refactored)
# ===========================

class UnifiedTrainingManager:
    def __init__(self, algorithm: str = 'ppo', 
                checkpoint_dir: str = "checkpoints",
                config_path: Optional[str] = None,
                use_task_aware_state: bool = True,
                use_phase2_scoring: bool = True,
                enforce_workflow: bool = True,
                thresholds: Optional['ScoringThresholds'] = None,
                task_types: Optional[List[str]] = None):
        
        self.algorithm = algorithm.lower()
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(exist_ok=True)
        
        # Load configuration
        self.config = self._load_config(config_path)
        
        # Task configuration
        self.use_task_aware_state = use_task_aware_state
        self.use_phase2_scoring = use_phase2_scoring
        self.enforce_workflow = enforce_workflow
        self.thresholds = thresholds
        self.task_types = task_types
        
        # Initialize components
        self.env = None
        self.trainer = None
        self.task_manager = None
        self.mdp = None
        
        # Training state
        self.training_history = defaultdict(list)
        # ç¡®ä¿å…³é”®çš„é”®è¢«åˆå§‹åŒ–
        self.training_history['rewards'] = []
        self.training_history['success'] = []
        self.training_history['lengths'] = []
        print(f"[DEBUG] Initialized training_history with keys: {list(self.training_history.keys())}")
                
        # Stage-aware best model tracking
        self.best_success_rate = 0.0  # ä¿®å¤ï¼šä»-1.0æ”¹ä¸º0.0ï¼Œé¿å…æ˜¾ç¤º-100%
        self.stage_best_success_rates = {}  # å½“å‰è®­ç»ƒçš„æ¯ä¸ªstageæœ€ä½³æˆåŠŸç‡
        self.stage_best_weighted_rates = {}  # æ–°å¢ï¼šæ¯ä¸ªstageçš„åŠ æƒæˆåŠŸç‡
        self.best_model_stages = {}  # ä¿å­˜çš„best modelçš„stageæˆåŠŸç‡
        self.best_weighted_success_rate = 0.0  # æ–°å¢ï¼šæœ€ä½³åŠ æƒæˆåŠŸç‡
        self.weighted_best_score = 0.0  # ä¿®å¤ï¼šä»-1.0æ”¹ä¸º0.0
        self.best_model_path = None
        self.current_stage = 0
        self.stage_transition_episodes = []  # è®°å½•stageè½¬æ¢çš„episode
        self.last_best_update_episode = 0  # æ–°å¢ï¼šåˆå§‹åŒ–last_best_update_episode

        
        print(f"[UnifiedTrainingManager] Initialized with stage-aware best model tracking")
        
        # Success rate evaluation weights based on stage
        self.stage_weights = {
            0: 0.1,  # Stage 0çš„æˆåŠŸç‡æƒé‡å¾ˆä½
            1: 0.3,  # Stage 1æƒé‡é€‚ä¸­
            2: 0.7,  # Stage 2æƒé‡è¾ƒé«˜
            3: 1.0,  # Stage 3+æƒé‡æœ€é«˜
        }
        
        # åˆå§‹åŒ– LLM å®¢æˆ·ç«¯ï¼ˆå¦‚æœéœ€è¦ï¼‰
        self.llm_client = None
        if self.config.get('use_teacher_guidance', False) or self.enforce_workflow:
            self.llm_client = self._get_llm_client()
            
        if self.thresholds:
            logger.info(f"Using scoring thresholds: semantic_match={self.thresholds.semantic_match_threshold}, "
                        f"partial_success={self.thresholds.partial_success_coverage}")
        
        logger.info("UnifiedTrainingManager initialized")
        logger.info(f"Algorithm: {self.algorithm}")
        logger.info(f"Task-aware state: {self.use_task_aware_state}")
        logger.info(f"Workflow enforcement: {self.enforce_workflow}")
        logger.info(f"Phase 2 scoring: {self.use_phase2_scoring}")


    def calculate_weighted_success_rate(self, episode_history: List[Dict]) -> float:
        """è®¡ç®—è€ƒè™‘æˆåŠŸçº§åˆ«çš„åŠ æƒæˆåŠŸç‡
        
        Args:
            episode_history: æœ€è¿‘è‹¥å¹²episodeçš„å†å²è®°å½•ï¼Œæ¯ä¸ªåŒ…å« 'success_level' å­—æ®µ
            
        Returns:
            åŠ æƒæˆåŠŸç‡ (0.0 - 1.0)
        """
        if not episode_history:
            return 0.0
        
        # æˆåŠŸçº§åˆ«æƒé‡å®šä¹‰
        success_level_weights = {
            'full_success': 1.0,      # å®Œå…¨æˆåŠŸ
            'partial_success': 0.0,   # éƒ¨åˆ†æˆåŠŸ
            'failure': 0.0,           # å¤±è´¥
        }
        
        weighted_sum = 0.0
        for episode_data in episode_history:
            # è·å–æˆåŠŸçº§åˆ«ï¼Œé»˜è®¤ä¸ºfailure
            success_level = episode_data.get('success_level', 'failure')
            weight = success_level_weights.get(success_level, 0.0)
            weighted_sum += weight
        
        return weighted_sum / len(episode_history)


    def update_best_model(self, episode: int, recent_success_rate: float, 
                        current_stage: int, save_callback=None, 
                        recent_episode_history: List[Dict] = None):
        """Stage-aware best model update logic using lexicographic ordering
        è‡ªé€‚åº”æ”¯æŒä»»æ„æ•°é‡çš„stagesï¼Œå¹¶è€ƒè™‘æˆåŠŸçº§åˆ«
        
        Args:
            episode: å½“å‰episodeå·
            recent_success_rate: æœ€è¿‘çš„äºŒå…ƒæˆåŠŸç‡ï¼ˆå‘åå…¼å®¹ï¼‰
            current_stage: å½“å‰è¯¾ç¨‹é˜¶æ®µ
            save_callback: ä¿å­˜æ¨¡å‹çš„å›è°ƒå‡½æ•°
            recent_episode_history: æœ€è¿‘episodeçš„è¯¦ç»†å†å²ï¼ŒåŒ…å«success_levelä¿¡æ¯
        """
        
        # æ›´æ–°å½“å‰stage
        if current_stage != self.current_stage:
            self.stage_transition_episodes.append(episode)
            print(f"[BestModel] Stage transition at episode {episode}: {self.current_stage} -> {current_stage}")
            self.current_stage = current_stage
        
        # è®¡ç®—åŠ æƒæˆåŠŸç‡ï¼ˆå¦‚æœæœ‰è¯¦ç»†å†å²ï¼‰
        if recent_episode_history:
            weighted_success_rate = self.calculate_weighted_success_rate(recent_episode_history)
            print(f"[BestModel] Binary success rate: {recent_success_rate:.2%}, "
                f"Weighted success rate: {weighted_success_rate:.2%}")
            # ä½¿ç”¨åŠ æƒæˆåŠŸç‡è¿›è¡Œæ¯”è¾ƒ
            comparison_rate = weighted_success_rate
        else:
            # å‘åå…¼å®¹ï¼šå¦‚æœæ²¡æœ‰è¯¦ç»†å†å²ï¼Œä½¿ç”¨äºŒå…ƒæˆåŠŸç‡
            comparison_rate = recent_success_rate
            weighted_success_rate = recent_success_rate
        
        # æ›´æ–°å½“å‰stageçš„æˆåŠŸç‡ï¼ˆä½¿ç”¨åŠ æƒæˆåŠŸç‡ï¼‰
        if current_stage not in self.stage_best_success_rates:
            self.stage_best_success_rates[current_stage] = 0.0
        
        # åŒæ—¶ç»´æŠ¤äºŒå…ƒå’ŒåŠ æƒæˆåŠŸç‡
        if current_stage not in self.stage_best_weighted_rates:
            self.stage_best_weighted_rates[current_stage] = 0.0  # ä¿®å¤ï¼šåªåˆå§‹åŒ–å½“å‰stageï¼Œä¸è¦é‡ç½®æ•´ä¸ªå­—å…¸
        
        # åˆ¤æ–­å½“å‰stageæ˜¯å¦æœ‰æ”¹è¿›ï¼ˆä½¿ç”¨åŠ æƒæˆåŠŸç‡ï¼‰
        stage_improved = comparison_rate > self.stage_best_success_rates[current_stage]
        if stage_improved:
            self.stage_best_success_rates[current_stage] = comparison_rate
            self.stage_best_weighted_rates[current_stage] = weighted_success_rate
            print(f"[BestModel] New best for stage {current_stage}: {comparison_rate:.2%} (weighted)")
        
        # å­—å…¸åºæ¯”è¾ƒï¼šä¼˜å…ˆæ¯”è¾ƒé«˜stage
        should_update = False
        update_reason = ""
        
        # è·å–å·²ä¿å­˜çš„best modelçš„stageä¿¡æ¯
        saved_best_stages = getattr(self, 'best_model_stages', {})
        
        # åŠ¨æ€è·å–æœ€å¤§stageæ•°
        # æ–¹æ³•1ï¼šä»curriculumè·å–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if hasattr(self, 'env') and hasattr(self.env, 'curriculum'):
            curriculum = self.env.curriculum
            if hasattr(curriculum, 'stage_thresholds'):
                max_stages = len(curriculum.stage_thresholds) + 1  # +1 å› ä¸ºè¿˜æœ‰æœ€åä¸€ä¸ªstage
            else:
                max_stages = 4  # é»˜è®¤å€¼
        else:
            # æ–¹æ³•2ï¼šä»å·²çŸ¥çš„stageä¸­æ¨æ–­
            known_stages = set(self.stage_best_success_rates.keys()) | set(saved_best_stages.keys())
            max_stages = max(known_stages, default=3) + 1  # è‡³å°‘æ”¯æŒåˆ°å½“å‰æœ€å¤§stage+1
        
        print(f"[BestModel] Using max_stages={max_stages} for comparison")
        
        # æ–°å¢ï¼šé¦–æ¬¡åˆ°è¾¾æ›´é«˜stageçš„ç‰¹æ®Šå¤„ç†
        current_max_stage = max(self.stage_best_success_rates.keys(), default=-1)
        saved_max_stage = max(saved_best_stages.keys(), default=-1)
        
        # å¦‚æœå½“å‰æ¨¡å‹åˆ°è¾¾äº†æ›´é«˜çš„stageï¼Œå¹¶ä¸”åœ¨è¯¥stageæœ‰ä»»ä½•æˆåŠŸï¼ˆå³ä½¿å¾ˆä½ï¼‰
        if current_max_stage > saved_max_stage:
            # æ£€æŸ¥æ–°stageæ˜¯å¦æœ‰ä»»ä½•æˆåŠŸ
            new_stage_rate = self.stage_best_success_rates.get(current_max_stage, 0.0)
            if new_stage_rate > 0:  # åªè¦æœ‰æˆåŠŸå°±æ›´æ–°
                should_update = True
                update_reason = f"Reached new stage {current_max_stage} with {new_stage_rate:.2%} success"
                print(f"[BestModel] Prioritizing new stage achievement!")
        
        # å¦‚æœä¸æ˜¯æ–°stageï¼Œåˆ™è¿›è¡Œæ ‡å‡†çš„å­—å…¸åºæ¯”è¾ƒ
        if not should_update:
            # ä»æœ€é«˜å¯èƒ½çš„stageå¼€å§‹æ¯”è¾ƒ
            for stage in range(max_stages - 1, -1, -1):  # åŠ¨æ€èŒƒå›´
                current_rate = self.stage_best_success_rates.get(stage, 0.0)
                saved_rate = saved_best_stages.get(stage, 0.0)
                
                if current_rate > saved_rate:
                    # å½“å‰æ¨¡å‹åœ¨è¿™ä¸ªstageä¸Šæ›´å¥½
                    should_update = True
                    update_reason = f"Better at stage {stage}: {current_rate:.2%} > {saved_rate:.2%}"
                    break
                elif current_rate < saved_rate:
                    # å½“å‰æ¨¡å‹åœ¨è¿™ä¸ªstageä¸Šæ›´å·®ï¼Œä¸æ›´æ–°
                    should_update = False
                    update_reason = f"Worse at stage {stage}: {current_rate:.2%} < {saved_rate:.2%}"
                    break
                # å¦‚æœç›¸ç­‰ï¼Œç»§ç»­æ¯”è¾ƒä¸‹ä¸€ä¸ªstage
            
            # å¦‚æœæ‰€æœ‰stageéƒ½ç›¸ç­‰ï¼Œä¿æŒç°çŠ¶
            if not update_reason:
                should_update = False
                update_reason = "All stages equal, keeping current best"
        
        # æ‰“å°æ¯”è¾ƒè¯¦æƒ…
        print(f"[BestModel] Lexicographic comparison at episode {episode}:")
        print(f"  Current stages: {dict(sorted(self.stage_best_success_rates.items()))}")
        print(f"  Saved best stages: {dict(sorted(saved_best_stages.items()))}")
        print(f"  Decision: {update_reason}")
        
        # æ›´æ–°best model
        if should_update:
            # ä¿å­˜å½“å‰çš„stageæˆåŠŸç‡ä½œä¸ºæ–°çš„best
            self.best_model_stages = self.stage_best_success_rates.copy()
            self.best_success_rate = comparison_rate
            self.best_weighted_success_rate = weighted_success_rate
            self.last_best_update_episode = episode
            
            print(f"[BestModel] âœ… Updating best model at episode {episode}")
            print(f"  Current stage: {current_stage}, Success Rate: {comparison_rate:.2%}")
            print(f"  Weighted Success Rate: {weighted_success_rate:.2%}")
            print(f"  Update reason: {update_reason}")
            
            # è°ƒç”¨ä¿å­˜å›è°ƒ
            if save_callback:
                save_callback(is_best=True)
            
            # ä¿å­˜stageä¿¡æ¯åˆ°æ–‡ä»¶
            self.best_model_path = self.checkpoint_dir / "best_model.pt"
            stage_info_path = self.checkpoint_dir / "best_model_stage_info.json"
            stage_info = {
                'episode': episode,
                'current_stage': current_stage,
                'recent_success_rate': recent_success_rate,
                'weighted_success_rate': weighted_success_rate,
                'stage_best_rates': self.stage_best_success_rates,
                'stage_best_weighted_rates': self.stage_best_weighted_rates,  # ç°åœ¨è¿™ä¸ªå­—å…¸ä¸ä¼šè¢«æ¸…ç©ºäº†
                'saved_best_stages': self.best_model_stages,
                'update_reason': update_reason,
                'max_stages_detected': max_stages,
                'timestamp': datetime.now().isoformat()
            }
            with open(stage_info_path, 'w') as f:
                json.dump(stage_info, f, indent=2)
            
            # ä¿å­˜æ›´æ–°å†å²è®°å½•ï¼ˆæ–°å¢åŠŸèƒ½ï¼‰
            history_path = self.checkpoint_dir / "best_model_update_history.json"
            history_entry = {
                'episode': episode,
                'timestamp': datetime.now().isoformat(),
                'current_stage': current_stage,
                'recent_success_rate': recent_success_rate,
                'weighted_success_rate': weighted_success_rate,
                'stage_best_rates': self.stage_best_success_rates.copy(),
                'update_reason': update_reason,
                'training_steps': self.trainer.training_steps if hasattr(self.trainer, 'training_steps') else 0
            }
            
            # åŠ è½½ç°æœ‰å†å²æˆ–åˆ›å»ºæ–°çš„
            if history_path.exists():
                with open(history_path, 'r') as f:
                    history = json.load(f)
            else:
                history = []
            
            history.append(history_entry)
            
            # ä¿å­˜æ›´æ–°åçš„å†å²
            with open(history_path, 'w') as f:
                json.dump(history, f, indent=2)
            
            print(f"[BestModel] Update history saved ({len(history)} entries)")
        else:
            print(f"[BestModel] âŒ Not updating best model")
            print(f"  Reason: {update_reason}")
        
        return should_update
    
    def _load_config(self, config_path: Optional[str]) -> Dict[str, Any]:
        """Load or create default configuration"""
        default_config = {
            # DQN parameters
            'learning_rate': 0.0003,
            'batch_size': 64,
            'memory_size': 50000,
            'gamma': 0.95,
            'epsilon_start': 0.3,
            'epsilon_decay': 0.995,
            'epsilon_min': 0.01,
            'target_update_frequency': 50,
            'hidden_dim': 256,
            
            # Training parameters
            'checkpoint_mode': 'full',
            'checkpoint_frequency': 100,
            'checkpoint_keep_recent': 3,
            'checkpoint_keep_interval': 500,
            'checkpoint_size_limit_mb': 500,
            'evaluation_frequency': 50,
            'evaluation_episodes': 10,
            'max_episode_length': 100,  # ğŸ”§ å…³é”®ä¿®æ”¹ï¼šä»30å¢åŠ åˆ°100
            
            # PPO specific (å¦‚æœä½¿ç”¨PPO)
            'n_steps': 256,
            'n_epochs': 4,
            'clip_range': 0.2,
            
            # Teacher guidance for PPO
            'use_teacher_guidance': False,
            'teacher_guidance_start_prob': 0.01,
            'teacher_guidance_decay': 0.995,
            'teacher_guidance_min_prob': 0.005,
            'episode_guidance_mode': True,
            
            # TaskAwareRolloutBufferé…ç½®
            'use_task_aware_buffer': True,
            'buffer_capacity_per_task': 100,
            'min_episodes_per_task': 10,
            'prioritize_medium_reward': True,
            
            # Curriculum learning
            'use_curriculum': True,
            
            # Action masking
            'use_action_masking': True
        }
    
        if config_path and Path(config_path).exists():
            with open(config_path, 'r') as f:
                loaded_config = json.load(f)
                default_config.update(loaded_config)
        else:
            raise ValueError("Config file doesnot exist")
        
        return default_config

    def _load_config_file(self) -> Dict[str, Any]:
        """Load configuration from config directory
        
        Returns:
            dict: Configuration dictionary or empty dict if not found
        """
        # ä½¿ç”¨ APIClientManager çš„é…ç½®
        from api_client_manager import APIClientManager
        manager = APIClientManager()
        return manager._config

    def _get_llm_client(self):
        """Initialize and return LLM client based on environment configuration
        
        Returns:
            OpenAI or AzureOpenAI client instance
        """
        from api_client_manager import get_api_client
        return get_api_client() 


    def setup_environment(self, 
                        tool_registry_path: str = "./mcp_generated_library/tool_registry_consolidated.json",
                        task_library_path: str = "mcp_generated_library/task_library_all_difficulties.json") -> bool:
        """Setup training environment"""
        logger.info("Setting up training environment...")
        
        # Load MDPWorkflowGenerator if needed
        MDPWorkflowGenerator = None
        if self.enforce_workflow:
            try:
                from mdp_workflow_generator import MDPWorkflowGenerator
            except ImportError:
                logger.warning("MDPWorkflowGenerator not available, workflow enforcement disabled")
                self.enforce_workflow = False
        
        # Load Phase 2 components
        StableScorer = None
        if self.use_phase2_scoring:
            try:
                from stable_scorer import StableScorer
                PHASE2_AVAILABLE = True
                logger.info("Phase 2 scoring components loaded")
            except ImportError:
                logger.warning("Phase 2 components not available")
                PHASE2_AVAILABLE = False
                self.use_phase2_scoring = False
        else:
            PHASE2_AVAILABLE = False
        
        if not os.path.exists(tool_registry_path):
            logger.error(f"Tool registry not found: {tool_registry_path}")
            return False
        
        # Try multiple tool registry paths
        tool_paths = [
            Path(tool_registry_path),
            Path("mcp_generated_library/tool_registry_consolidated.json"),
            Path("mcp_generated_library/tool_registry.json"),
            Path("tool_registry.json")
        ]
        
        tool_capabilities = None
        for path in tool_paths:
            if path.exists():
                tool_capabilities = load_tool_capabilities(str(path))
                logger.info(f"Loaded tools from {path}")
                break
        
        if not tool_capabilities:
            logger.error("No tool capabilities loaded!")
            return False
        
        # Create MDP
        self.mdp = GeneralizedMDP(tool_capabilities)
        
        # Load tasks
        self.task_manager = TaskManager(task_library_path, task_types=self.task_types)
        
        # If no tasks were loaded from files, ensure we have sample tasks
        if not self.task_manager.tasks:
            logger.warning("No tasks loaded from files, using sample tasks")
            self.task_manager._create_sample_tasks()
        
        if not self.task_manager.tasks:
            logger.error("Failed to create any tasks!")
            return False
        
        # Create environmentï¼ˆå…ˆä¸ä¼ å…¥trainerï¼‰
        self.env = MDPEnvironment(
            self.mdp, 
            self.task_manager,
            use_task_aware_state=self.use_task_aware_state,
            enforce_workflow=self.enforce_workflow,
            use_phase2_scoring=self.use_phase2_scoring,
            normalize_rewards=self.config.get('normalize_rewards', True)  # ä¼ å…¥å½’ä¸€åŒ–é…ç½®
        )
        
        logger.info(f"Environment created with {len(self.task_manager.tasks)} tasks")
        logger.info(f"Task types: {list(self.task_manager.tasks_by_type.keys())}")
        logger.info(f"State dimension: {self.env.get_state_dim()}")
        logger.info(f"Action space: {self.env.num_actions} actions")
        
        # åˆ›å»ºtrainer
        if not self.trainer:
            # ç°åœ¨å¯ä»¥ä»ç¯å¢ƒè·å–æ­£ç¡®çš„çŠ¶æ€ç»´åº¦
            state_dim = self.env.get_state_dim()
            action_dim = self.env.num_actions
            
            if self.algorithm == 'dqn':
                self.trainer = DQNTrainer(self.env, self.config)
            elif self.algorithm == 'ppo':
                ppo_config = self.config.copy()
                ppo_config.update({
                    'n_steps': self.config.get('n_steps', 2048),
                    'n_epochs': self.config.get('n_epochs', 10),
                    'batch_size': self.config.get('batch_size', 64),
                    'clip_range': self.config.get('clip_range', 0.2),
                    'ent_coef': self.config.get('ent_coef', 0.01),
                    'vf_coef': self.config.get('vf_coef', 0.5),
                    'gae_lambda': self.config.get('gae_lambda', 0.95),
                    'use_task_aware_buffer': self.config.get('use_task_aware_buffer', True),
                    'use_tools_input': True,
                    'use_rag_enhancement': True
                })
                
                self.trainer = PPOTrainer(self.env, ppo_config)
                
                # å¦‚æœéœ€è¦ï¼Œå¯ä»¥åœ¨è¿™é‡Œè‡ªå®šä¹‰ç½‘ç»œ
                if self.algorithm == 'ppo' and hasattr(self.trainer, 'network'):
                    network_config = {
                        'hidden_dim': ppo_config.get('hidden_dim', 256),
                        'num_layers': ppo_config.get('num_layers', 3),
                        'num_heads': ppo_config.get('num_heads', 4),
                        'dropout': ppo_config.get('dropout', 0.1),
                        'use_rag_enhancement': True,
                        'use_tools_input': True,
                        'tools_dim': 64,
                        'rag_dim': 64
                    }
                    self.trainer.network = ActorCriticNetwork(state_dim, action_dim, network_config)
                    self.trainer.network.to(self.trainer.device)
            else:
                logger.error(f"Unknown algorithm: {self.algorithm}")
                return False
        
        # === å…³é”®ä¿®å¤ï¼šåˆ›å»ºtraineråï¼Œç«‹å³å…±äº«ç½‘ç»œç»™workflow_generator ===
        if self.env.workflow_generator and self.trainer:
            print("[DEBUG] Sharing trainer network with workflow_generator after trainer creation")
            if hasattr(self.trainer, 'network'):
                self.env.workflow_generator.set_network_reference(
                    self.trainer.network,
                    algorithm='ppo'
                )
                logger.info("âœ… Shared PPO network with workflow_generator")
            elif hasattr(self.trainer, 'q_network'):
                self.env.workflow_generator.set_network_reference(
                    self.trainer.q_network,
                    algorithm='dqn'
                )
                logger.info("âœ… Shared DQN network with workflow_generator")
            else:
                logger.warning("âš ï¸ Trainer has no network to share with workflow_generator")
        

        # è®¾ç½®ç¯å¢ƒçš„trainerå¼•ç”¨ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if hasattr(self.env, 'trainer'):
            self.env.trainer = self.trainer
        
        logger.info("âœ… Environment setup complete!")
        return True


    def save_checkpoint(self, path: Path, episode: int, success_rate: float):
        """Save checkpoint with configurable detail level"""
        # ç¡®å®šcheckpointç±»å‹
        checkpoint_mode = self.config.get('checkpoint_mode', 'full')
        logger.debug(f" Saving {checkpoint_mode} checkpoint at episode {episode}")
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        print(f"[TrainingManager.save_checkpoint] Episode {episode}: success_rate={success_rate:.2%}, best_success_rate={self.best_success_rate:.2%}")
        print(f"[TrainingManager.save_checkpoint] Saving to: {path}")
        
        # åŸºç¡€ä¿¡æ¯ï¼ˆæ‰€æœ‰checkpointéƒ½åŒ…å«ï¼‰
        manager_state = {
            'episode': episode,
            'best_success_rate': self.best_success_rate,
            'algorithm': self.algorithm,
            'state_dim': self.env.get_state_dim(),
            'action_dim': self.env.num_actions,
            'timestamp': datetime.now().isoformat(),
            'checkpoint_mode': checkpoint_mode,
            'metadata': {
                'episode': episode,
                'success_rate': success_rate,
                'timestamp': datetime.now().isoformat(),
                'best_success_rate': self.best_success_rate
            }
        }
        
        # æ ‡å‡†ä¿¡æ¯ï¼ˆstandardå’Œfullæ¨¡å¼åŒ…å«ï¼‰
        if checkpoint_mode in ['standard', 'full']:
            manager_state.update({
                'config': self.config,
                'use_task_aware_state': self.use_task_aware_state,
                'enforce_workflow': self.enforce_workflow,
                'use_phase2_scoring': self.use_phase2_scoring,
            })
        
        # å®Œæ•´ä¿¡æ¯ï¼ˆä»…fullæ¨¡å¼åŒ…å«ï¼‰
        if checkpoint_mode == 'full':
            logger.debug(f" Full mode checkpoint - including training history")
            manager_state.update({
                'training_history': dict(self.training_history),
                'best_model_path': str(self.best_model_path) if self.best_model_path else None,
            })
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯best_model.ptçš„ä¿å­˜
        is_best_model_save = "best_model.pt" in str(path)
        
        # å¦‚æœæ˜¯æ™®é€šcheckpointï¼Œä½¿ç”¨åŸæœ‰é€»è¾‘
        if not is_best_model_save:
            # åŠ¨æ€è°ƒæ•´ä¿å­˜å†…å®¹ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
            if checkpoint_mode == 'lightweight':
                # è½»é‡çº§ï¼šä»…ä¿å­˜æ¨¡å‹æƒé‡
                logger.debug(f" Lightweight mode - saving minimal checkpoint")
                if self.trainer:
                    lightweight_state = {
                        'episode': episode,
                        'algorithm': self.algorithm,
                        'state_dim': self.env.get_state_dim(),
                        'action_dim': self.env.num_actions,
                        'metadata': manager_state['metadata']
                    }
                    if hasattr(self.trainer, 'q_network'):  # DQN
                        lightweight_state['q_network_state_dict'] = self.trainer.q_network.state_dict()
                    elif hasattr(self.trainer, 'network'):  # PPO  
                        lightweight_state['network_state_dict'] = self.trainer.network.state_dict()
                    torch.save(lightweight_state, path)
                logger.debug(f" Lightweight checkpoint saved: {path.stat().st_size / 1024 / 1024:.1f} MB")
            else:
                # æ ‡å‡†æˆ–å®Œæ•´ï¼šä½¿ç”¨trainerçš„save_checkpoint
                if self.trainer:
                    self.trainer.save_checkpoint(str(path), additional_data=manager_state)
                else:
                    torch.save(manager_state, path)
                logger.debug(f" {checkpoint_mode.capitalize()} checkpoint saved: {path.stat().st_size / 1024 / 1024:.1f} MB")
        else:
            # å¦‚æœæ˜¯best_model.ptï¼Œæ€»æ˜¯ä¿å­˜å®Œæ•´çŠ¶æ€
            print(f"[TrainingManager.save_checkpoint] ğŸ‰ Saving best model via update_best_model!")
            
            # ä¿å­˜best modelï¼ˆç¡®ä¿åŒ…å«å®Œæ•´çŠ¶æ€ï¼‰
            best_model_state = manager_state.copy()
            best_model_state['training_history'] = dict(self.training_history)
            best_model_state['is_best_model'] = True
            best_model_state['best_success_rate'] = self.best_success_rate  # ä½¿ç”¨å·²æ›´æ–°çš„best_success_rate
            best_model_state['best_episode'] = episode
            
            # ä½¿ç”¨fullæ¨¡å¼ä¿å­˜best model
            if self.trainer:
                self.trainer.save_checkpoint(str(path), additional_data=best_model_state)
            else:
                torch.save(best_model_state, path)
            
            logger.info(f"âœ… Best model saved with success rate: {self.best_success_rate:.2%} at episode {episode}")
        
        logger.info(f"Checkpoint saved to {path}")
        
        # å®šæœŸæ¸…ç†æ—§checkpoint
        self._cleanup_old_checkpoints()



    def _cleanup_old_checkpoints(self, keep_recent: int = 5):
        """è‡ªåŠ¨æ¸…ç†æ—§çš„checkpointæ–‡ä»¶ï¼Œä¿ç•™æœ€è¿‘çš„å‡ ä¸ªå’Œbest_model.pt"""
        try:
            # è·å–æ‰€æœ‰checkpointæ–‡ä»¶ï¼ˆä¸åŒ…æ‹¬best_model.ptå’Œfinal_model.ptï¼‰
            checkpoints = list(self.checkpoint_dir.glob("checkpoint_*.pt"))
            
            if len(checkpoints) <= keep_recent:
                return  # æ•°é‡æœªè¶…è¿‡é™åˆ¶ï¼Œæ— éœ€æ¸…ç†
            
            # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼ˆæ—§çš„åœ¨å‰ï¼‰
            checkpoints.sort(key=lambda p: p.stat().st_mtime)
            
            # è®¡ç®—éœ€è¦åˆ é™¤çš„æ•°é‡
            to_remove = checkpoints[:-keep_recent]
            
            # åˆ é™¤æ—§çš„checkpoint
            for checkpoint in to_remove:
                try:
                    checkpoint.unlink()
                    logger.info(f"Removed old checkpoint: {checkpoint.name}")
                except Exception as e:
                    logger.warning(f"Failed to remove checkpoint {checkpoint.name}: {e}")
            
            logger.info(f"Checkpoint cleanup completed. Kept {keep_recent} recent checkpoints.")
            
        except Exception as e:
            logger.warning(f"Checkpoint cleanup failed: {e}")

    def _find_latest_checkpoint(self) -> Optional[Path]:
        """Find the latest checkpoint file"""
        # First, try to find best_model.pt
        best_model_path = self.checkpoint_dir / "best_model.pt"
        if best_model_path.exists():
            logger.info("Found best_model.pt, using it for resume")
            return best_model_path
        
        # If no best model, try to find checkpoint files
        checkpoints = list(self.checkpoint_dir.glob("checkpoint_*.pt"))
        if not checkpoints:
            return None
        
        # Sort by modification time as a fallback
        checkpoints.sort(key=lambda p: p.stat().st_mtime)
        return checkpoints[-1]
    
    def _load_training_state(self, checkpoint_path: Path) -> int:
        """Load training state from checkpoint and return starting episode"""
        logger.info(f"Loading checkpoint from {checkpoint_path}")
        
        # Load checkpoint through trainer if it exists
        if self.trainer:
            checkpoint = self.trainer.load_checkpoint(str(checkpoint_path))
        else:
            checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
        
        # Restore manager state
        if 'training_history' in checkpoint:
            self.training_history = defaultdict(list, checkpoint['training_history'])
        
        if 'best_success_rate' in checkpoint:
            self.best_success_rate = checkpoint['best_success_rate']
        
        if 'best_model_path' in checkpoint:
            self.best_model_path = Path(checkpoint['best_model_path']) if checkpoint['best_model_path'] else None
        
        # Return next episode to start from
        return checkpoint.get('episode', 0) + 1
      

    def _create_initial_best_model(self):
        """åˆ›å»ºåˆå§‹çš„best_model.ptæ–‡ä»¶ï¼Œä¾›å·¥ä½œæµç”Ÿæˆå™¨ä½¿ç”¨"""
        best_model_path = self.checkpoint_dir / "best_model.pt"
        
        if not best_model_path.exists():
            print("[DEBUG] Creating initial best_model.pt for workflow generator")
            
            # ç¡®ä¿checkpointç›®å½•å­˜åœ¨
            self.checkpoint_dir.mkdir(exist_ok=True)
            
            # è·å–ç¯å¢ƒä¿¡æ¯
            state_dim = self.env.get_state_dim()
            action_dim = self.env.num_actions
            
            # åˆ›å»ºåˆå§‹checkpoint
            initial_checkpoint = {
                'episode': 0,
                'training_steps': 0,
                'algorithm': self.algorithm,
                'config': self.config,
                'state_dim': state_dim,
                'action_dim': action_dim,
                'tool_names': self.env.tool_names,
                'workflow_history': [],
                'training_history': defaultdict(list),
                'created_at': time.time(),
                'best_success_rate': 0.0,
                'stage_best_success_rates': {},
                'best_model_stages': {}
            }
            
            # æ·»åŠ ç½‘ç»œçŠ¶æ€
            if hasattr(self.trainer, 'network'):
                initial_checkpoint['network_state_dict'] = self.trainer.network.state_dict()
                initial_checkpoint['model_state_dict'] = self.trainer.network.state_dict()  # å…¼å®¹æ€§
            elif hasattr(self.trainer, 'q_network'):
                initial_checkpoint['q_network_state_dict'] = self.trainer.q_network.state_dict()
                initial_checkpoint['model_state_dict'] = self.trainer.q_network.state_dict()  # å…¼å®¹æ€§
            
            torch.save(initial_checkpoint, best_model_path)
            logger.info(f"âœ… Created initial best_model.pt at {best_model_path}")
            print(f"[DEBUG] Initial model saved with state_dim={state_dim}, action_dim={action_dim}")     

    def train(self, num_episodes: int = 1000, print_frequency: int = 50,
            resume: bool = False) -> bool:
        """Unified training loop for both DQN and PPO"""

        if not resume:
            logger.info("Starting fresh training - cleaning old checkpoints...")
            self._clean_checkpoint_directory()


        if not self.env:
            logger.error("Environment not initialized!")
            return False
        
        # Create trainer if not exists
        if not self.trainer:
            if self.algorithm == 'dqn':
                self.trainer = DQNTrainer(self.env, self.config)
            elif self.algorithm == 'ppo':
                ppo_config = self.config.copy()
                ppo_config.update({
                    'n_steps': 2048,
                    'n_epochs': 10,
                    'batch_size': 64,
                    'clip_range': 0.2,
                    'ent_coef': 0.01,
                    'vf_coef': 0.5,
                    'gae_lambda': 0.95,
                    'use_task_aware_buffer': self.config.get('use_task_aware_buffer', True),
                    'buffer_capacity_per_task': self.config.get('buffer_capacity_per_task', 100),
                    'min_episodes_per_task': self.config.get('min_episodes_per_task', 5),
                    'task_mix_ratio': self.config.get('task_mix_ratio', 0.7)
                })
                self.trainer = PPOTrainer(self.env, ppo_config)
            else:
                logger.error(f"Unknown algorithm: {self.algorithm}")
                return False
            
            logger.info(f"Using {self.algorithm.upper()} algorithm")
            
        if not resume and self.config.get('create_initial_best_model', False):
            self._create_initial_best_model()



        # Resume from checkpoint if requested
        start_episode = 0
        if resume:
            checkpoint_path = self._find_latest_checkpoint()
            if checkpoint_path:
                start_episode = self._load_training_state(checkpoint_path)
                logger.info(f"Resumed from episode {start_episode}")
            else:
                logger.warning("No checkpoint found, starting from scratch")
        
        # Initialize curriculum
        curriculum = None
        if self.config['use_curriculum']:
            curriculum = CurriculumScheduler(num_episodes, config=self.config)
            if resume and start_episode > 0:
                curriculum.current_episode = start_episode
                print(f"[CURRICULUM] Resumed training from episode {start_episode}")
        else:
            raise ValueError("No curriculum")
        
        # Training metrics
        episode_rewards = deque(maxlen=100)
        episode_success = deque(maxlen=100)
        episode_lengths = deque(maxlen=100)
        
        # Load metrics history if resuming
        if resume and 'rewards' in self.training_history:
            episode_rewards.extend(self.training_history['rewards'][-100:])
            episode_success.extend(self.training_history['success'][-100:])
            episode_lengths.extend(self.training_history['lengths'][-100:])
        
        # æ”¶é›†æˆåŠŸçš„episodesç”¨äºå­¦ä¹ 
        successful_episodes = []

        if len(episode_success) > 0:
            current_success_rate = np.mean(episode_success)
        else:
            current_success_rate = 0.0
        
        # Main training loop
        logger.info(f"Starting training from episode {start_episode} to {num_episodes}")
        
        for episode in range(start_episode, num_episodes):
            # Get curriculum stage
            curriculum_stage = curriculum.get_stage() if curriculum else None

            if hasattr(self.env, 'set_current_success_rate'):
                # æ¸è¿›å¼é˜ˆå€¼ç­–ç•¥ï¼š
                # - ä½äºé˜ˆå€¼ï¼šåªè¦æ±‚è¦†ç›–æ‰€æœ‰required_toolsï¼ˆå¿½ç•¥é¡ºåºï¼‰
                # - é«˜äºé˜ˆå€¼ï¼šå¼€å§‹å¼ºåˆ¶æ‰§è¡Œé¡ºåº
                if episode < 1000:
                    threshold = 0.1  # å‰1000è½®ï¼š10%é˜ˆå€¼ï¼Œä¸“æ³¨äºå­¦ä¹ å·¥å…·è¦†ç›–
                elif episode < 3000:
                    threshold = 0.2  # 1000-3000è½®ï¼š20%é˜ˆå€¼ï¼Œå¼€å§‹å¼•å…¥è½»å¾®é¡ºåºçº¦æŸ
                elif episode < 5000:
                    threshold = 0.3  # 3000-5000è½®ï¼š30%é˜ˆå€¼ï¼Œæ ‡å‡†é¡ºåºçº¦æŸ
                else:
                    threshold = 0.4  # 5000+è½®ï¼š40%é˜ˆå€¼ï¼Œä¸¥æ ¼é¡ºåºçº¦æŸ
                
                self.env.set_current_success_rate(current_success_rate, threshold)
                
                # æ¯1000è½®æ‰“å°ä¸€æ¬¡æ¨¡å¼è½¬æ¢ä¿¡æ¯
                if episode % 1000 == 0:
                    mode = "coverage" if current_success_rate < threshold else "sequence"
                    print(f"[TRAIN] Episode {episode}: Mode={mode}, Success rate={current_success_rate:.2%}, Threshold={threshold:.2%}")
     
            
            # Reset environment
            state = self.env.reset(curriculum_stage=curriculum_stage)
            episode_reward = 0
            done = False
            
            # æ”¶é›†episodeè½¨è¿¹ç”¨äºå­¦ä¹ 
            episode_trajectory = []
            
            while not done and self.env.episode_steps < self.config['max_episode_length']:
                # Get valid actions if using action masking
                valid_actions = self.env.get_valid_actions() if self.config['use_action_masking'] else None
                
                # Select action
                action = self.trainer.select_action(state, valid_actions)
                
                # Step
                next_state, reward, done, info = self.env.step(action)
                
                # æ”¶é›†è½¨è¿¹æ•°æ®
                if hasattr(self.env, 'current_state') and hasattr(self.env, 'action_space'):
                    action_obj = self.env.action_space[action]
                    episode_trajectory.append((
                        self.env.current_state,
                        action_obj,
                        reward,
                        self.env.current_state
                    ))
                
                # ä½¿ç”¨ç»Ÿä¸€æ¥å£å­˜å‚¨ç»éªŒ
                task_type = None
                if hasattr(self.env, 'current_task') and hasattr(self.env.current_task, 'task_type'):
                    task_type = self.env.current_task.task_type
                
                self.trainer.store_experience(state, action, reward, next_state, done, task_type=task_type)
                self.trainer.step_completed()
                
                # æ£€æŸ¥æ˜¯å¦åº”è¯¥è®­ç»ƒ
                if self.trainer.should_train():
                    loss = self.trainer.train_step()
                
                # Update
                episode_reward += reward
                state = next_state
            
            # Episodeç»“æŸå¤„ç†
            self.trainer.on_episode_end()
            
            # Episode complete
            success = info.get('success', False)
            episode_rewards.append(episode_reward)
            episode_success.append(float(success))
            episode_lengths.append(self.env.episode_steps)

            current_success_rate = np.mean(episode_success)

            
            # å®æ—¶æ›´æ–°training_history
            self.training_history['rewards'].append(episode_reward)
            self.training_history['success'].append(float(success))
            self.training_history['lengths'].append(self.env.episode_steps)
            
            # æ›´æ–°å·¥å…·å…³é”®æ€§æ•°æ®
            if hasattr(self.mdp, 'update_tool_criticality') and episode_trajectory:
                final_score = info.get('phase2_metrics', {}).get('phase2_score', 0.0)
                if final_score == 0.0:
                    final_score = 1.0 if success else 0.0
                
                episode_data = {
                    'trajectory': episode_trajectory,
                    'final_score': final_score,
                    'task_type': self.env.current_task.task_type if hasattr(self.env.current_task, 'task_type') else 'unknown',
                    'tool_failures': {}
                }
                
                self.mdp.update_tool_criticality(episode_data)
                
                if success and final_score > 0.8:
                    successful_episodes.append({
                        'trajectory': episode_trajectory,
                        'final_score': final_score,
                        'task_description': self.env.current_task.description if hasattr(self.env.current_task, 'description') else ''
                    })
            
            # Update exploration
            self.trainer.update_exploration()
            
            # Update curriculum - ä¿®å¤ï¼šä½¿ç”¨æ­£ç¡®çš„æ–¹æ³•å
            if curriculum:
                curriculum.update()  # ä½¿ç”¨updateè€Œä¸æ˜¯step
            
            # å®šæœŸæ›´æ–° MDPWorkflowGenerator çš„ç½‘ç»œï¼ˆå¦‚æœå¯ç”¨äº†å·¥ä½œæµæ‰§è¡Œï¼‰
            if self.enforce_workflow and episode % 100 == 0 and episode > 0:
                if hasattr(self.env, 'workflow_generator') and self.env.workflow_generator:
                    print(f"[Episode {episode}] Updating workflow generator network...")
                    
                    if self.algorithm == 'ppo' and hasattr(self.trainer, 'network'):
                        self.env.workflow_generator.update_network(
                            self.trainer.network.state_dict(),
                            algorithm='ppo'
                        )
                    elif self.algorithm == 'dqn' and hasattr(self.trainer, 'q_network'):
                        self.env.workflow_generator.update_network(
                            self.trainer.q_network.state_dict(),
                            algorithm='dqn'
                        )
            
            # Print progress
            if episode % print_frequency == 0:
                avg_reward = np.mean(list(episode_rewards))
                avg_success = np.mean(list(episode_success))
                avg_length = np.mean(list(episode_lengths))
                
                logger.info(f"Episode {episode}/{num_episodes}:")
                logger.info(f"  Avg Reward: {avg_reward:.2f}")
                logger.info(f"  Success Rate: {avg_success:.2%}")
                logger.info(f"  Avg Length: {avg_length:.1f}")
                
                # è·å–ç®—æ³•ç‰¹å®šçš„è®­ç»ƒä¿¡æ¯
                training_info = self.trainer.get_training_info()
                if self.algorithm == 'dqn' and hasattr(self.trainer, 'epsilon'):
                    logger.info(f"  Epsilon: {self.trainer.epsilon:.3f}")
                elif self.algorithm == 'ppo':
                    logger.info(f"  Total timesteps: {training_info['total_timesteps']}")
                
                if curriculum:
                    logger.info(f"  Curriculum: {curriculum.get_stage_name()}")
                
                # æ›´æ–°best modelï¼ˆæ–°å¢çš„ä»£ç ï¼‰
                if len(episode_success) >= 10:  # è‡³å°‘æœ‰10ä¸ªepisodeæ‰è®¡ç®—
                    window_size = min(100, len(episode_success))
                    recent_success_rate = np.mean(list(episode_success)[-window_size:])  # è¿™è¡Œæ˜¯æ­£ç¡®çš„
                    current_stage = curriculum.get_stage() if curriculum else 0
                    
                    logger.debug(f" Update best model check - Episode {episode}: recent_success_rate={recent_success_rate:.2%} from last {window_size} episodes")

                    
                    # å®šä¹‰ä¿å­˜å›è°ƒ
                    def save_best_callback(is_best=True):
                        self.save_checkpoint(
                            self.checkpoint_dir / "best_model.pt",
                            episode,
                            recent_success_rate
                        )
                    
                    # è°ƒç”¨update_best_model
                    self.update_best_model(
                        episode=episode,
                        recent_success_rate=recent_success_rate,
                        current_stage=current_stage,
                        save_callback=save_best_callback
                    )

            # Save checkpoint
            if episode % self.config['checkpoint_frequency'] == 0:
                # è®¡ç®—æœ€è¿‘100ä¸ªepisodesçš„æˆåŠŸç‡
                # ä¼˜å…ˆä½¿ç”¨ training_historyï¼Œå®ƒåº”è¯¥åŒ…å«æ‰€æœ‰å†å²æ•°æ®
                if 'success' in self.training_history and len(self.training_history['success']) > 0:
                    recent_episodes = min(100, len(self.training_history['success']))
                    recent_success_list = self.training_history['success'][-recent_episodes:]
                    current_success_rate = np.mean(recent_success_list) if recent_success_list else 0.0
                    print(f"[DEBUG] Using training_history for success rate: {current_success_rate:.2%} from {recent_episodes} episodes")
                    print(f"[DEBUG] training_history has {len(self.training_history['success'])} success records")
                elif len(episode_success) > 0:
                    # fallback: ä½¿ç”¨ episode_success dequeï¼ˆæœ€å¤š100ä¸ªæœ€è¿‘çš„episodesï¼‰
                    current_success_rate = np.mean(episode_success)
                    print(f"[DEBUG] Using episode_success deque for success rate: {current_success_rate:.2%} from {len(episode_success)} episodes")
                else:
                    current_success_rate = 0.0
                    print(f"[DEBUG] No success data available, using 0.0%")
                
                print(f"[TrainingManager.train] Checkpoint at episode {episode}")
                print(f"[TrainingManager.train] Recent success rate: {current_success_rate:.2%}")
                print(f"[TrainingManager.train] Current best: {self.best_success_rate:.2%}")
                
                checkpoint_path = self.checkpoint_dir / f"checkpoint_episode_{episode}.pt"
                self.save_checkpoint(checkpoint_path, episode, current_success_rate)


            # Evaluate
            if episode % self.config['evaluation_frequency'] == 0:
                eval_results = self.evaluate(num_episodes=self.config['evaluation_episodes'])
                logger.info(f"Evaluation at episode {episode}: {eval_results}")
                
                # Update scheduler (only for DQN)
                if self.algorithm == 'dqn' and hasattr(self.trainer, 'scheduler'):
                    self.trainer.scheduler.step(eval_results['avg_reward'])
        
        # PPO: ç¡®ä¿æœ€åçš„æ•°æ®è¢«è®­ç»ƒ
        if hasattr(self.trainer.rollout_buffer, 'states') and len(self.trainer.rollout_buffer.states) > 0:
            logger.info("Training on final rollout buffer...")
            self.trainer.train_step()
        elif hasattr(self.trainer.rollout_buffer, 'current_episode') and len(self.trainer.rollout_buffer.current_episode['states']) > 0:
            logger.info("Training on final rollout buffer...")
            self.trainer.train_step()
        
        # å­¦ä¹ å…³é”®æ¨¡å¼
        if hasattr(self.mdp, 'learn_critical_patterns_from_episodes') and successful_episodes:
            logger.info(f"Learning critical patterns from {len(successful_episodes)} successful episodes...")
            self.mdp.learn_critical_patterns_from_episodes(successful_episodes)
        
        # ä¿å­˜å­¦ä¹ åˆ°çš„å·¥å…·å…³é”®æ€§æ•°æ®
        if hasattr(self.mdp, 'save_learned_criticality'):
            criticality_path = self.checkpoint_dir / "tool_criticality.json"
            self.mdp.save_learned_criticality(str(criticality_path))
            logger.info(f"Saved tool criticality data to {criticality_path}")
        
        # Final save
        final_path = self.checkpoint_dir / "final_model.pt"
        # ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨ episode_successï¼Œå› ä¸ºå®ƒæ˜¯ deque å¯¹è±¡
        if episode_success:
            final_success_rate = np.mean(list(episode_success))  # <- ä¿®æ”¹äº†è¿™ä¸€è¡Œï¼šåˆ é™¤äº† .values()
            logger.debug(f" Final success rate calculation: {final_success_rate:.2%} from {len(episode_success)} episodes")
        else:
            final_success_rate = 0.0
            logger.debug(f" No episode_success data, using 0.0%")
        
        self.save_checkpoint(final_path, num_episodes, final_success_rate)
        
        if curriculum:
            print(f"\n[CURRICULUM FINAL] Training completed!")
            print(f"[CURRICULUM FINAL] Total episodes run: {curriculum.current_episode}")
            print(f"[CURRICULUM FINAL] Expected episodes: {curriculum.total_episodes}")
            print(f"[CURRICULUM FINAL] Final stage: {curriculum.get_stage()}")
            print(f"[CURRICULUM FINAL] Final progress: {curriculum.current_episode/curriculum.total_episodes:.1%}")
            
            # æ‰“å°stageå†å²æ‘˜è¦
            # ä¿®å¤ï¼šæ·»åŠ è¾¹ç•Œæ£€æŸ¥ï¼Œé¿å…ç´¢å¼•è¶Šç•Œ
            stage_changes = []
            for i, (ep, st) in enumerate(curriculum.stage_history):
                if ep == 1:  # ç¬¬ä¸€ä¸ªepisodeæ€»æ˜¯è®°å½•
                    stage_changes.append((ep, st))
                elif i > 0 and curriculum.stage_history[i-1][1] != st:  # æ£€æŸ¥ä¸å‰ä¸€ä¸ªè®°å½•çš„stageæ˜¯å¦ä¸åŒ
                    stage_changes.append((ep, st))
            
            print(f"[CURRICULUM FINAL] Stage transitions: {stage_changes}")

        logger.info("Training completed!")
        return True

    def _clean_checkpoint_directory(self):
        """æ¸…ç†checkpointç›®å½•ä¸­çš„æ—§æ–‡ä»¶ï¼ˆéresumeæ¨¡å¼ä¸‹ï¼‰"""
        logger.info("Cleaning checkpoint directory...")
        
        # éœ€è¦ä¿ç•™çš„æ–‡ä»¶æ¨¡å¼
        config_path = self.checkpoint_dir / "training_config.json"
        config_content = None
        if config_path.exists():
            with open(config_path, 'r') as f:
                config_content = f.read()

        # åˆ é™¤æ•´ä¸ªæ–‡ä»¶å¤¹
        removed_count = 0
        preserved_count = 0

        if self.checkpoint_dir.exists():
            # è®¡ç®—å°†è¦åˆ é™¤çš„æ–‡ä»¶æ•°ï¼ˆé™¤äº† training_config.jsonï¼‰
            for item in self.checkpoint_dir.iterdir():
                if item.name != "training_config.json":
                    removed_count += 1
            
            # åˆ é™¤æ•´ä¸ªæ–‡ä»¶å¤¹
            import shutil
            print(f"  Removing entire checkpoint directory: {self.checkpoint_dir}")
            shutil.rmtree(self.checkpoint_dir)
            
            # é‡æ–°åˆ›å»ºæ–‡ä»¶å¤¹
            self.checkpoint_dir.mkdir(parents=True, exist_ok=True)
            
            # æ¢å¤ training_config.json
            if config_content is not None:
                with open(config_path, 'w') as f:
                    f.write(config_content)
                print(f"  Preserved: training_config.json")
                preserved_count = 1
        
        # é‡ç½®è®­ç»ƒç›¸å…³çš„çŠ¶æ€
        self.best_success_rate = 0.0
        self.stage_best_success_rates = {}
        self.best_model_stages = {}
        self.weighted_best_score = 0.0
        self.best_model_path = None
        self.current_stage = 0
        self.stage_transition_episodes = []
        self.training_history = defaultdict(list)
        self.training_history['rewards'] = []
        self.training_history['success'] = []
        self.training_history['lengths'] = []
        
        logger.info(f"Checkpoint cleanup completed: removed {removed_count} files, preserved {preserved_count} files")

    def evaluate(self, num_episodes: int = 100, model_path: Optional[str] = None) -> Dict[str, float]:
        """Evaluate trained model"""
        if not self.env or not self.trainer:
            logger.error("Environment or trainer not initialized!")
            return {}
        
        if model_path:
            self.trainer.load_checkpoint(model_path)
        
        # è®¾ç½®è¯„ä¼°æ¨¡å¼ - ä½¿ç”¨trainerçš„ç»Ÿä¸€æ–¹æ³•  # <- ä¿®æ”¹
        self.env.is_evaluation_mode = True
        self.trainer.set_eval_mode(True)  # <- ä¿®æ”¹ï¼šä½¿ç”¨ç»Ÿä¸€æ–¹æ³•
        
        eval_rewards = []
        eval_success = []
        eval_lengths = []
        
        logger.info(f"Evaluating for {num_episodes} episodes...")
        
        for episode in range(num_episodes):
            state = self.env.reset()
            episode_reward = 0
            done = False
            
            while not done and self.env.episode_steps < self.config['max_episode_length']:
                valid_actions = self.env.get_valid_actions() if self.config['use_action_masking'] else None
                action = self.trainer.select_action(state, valid_actions)
                next_state, reward, done, info = self.env.step(action)
                
                episode_reward += reward
                state = next_state
            
            eval_rewards.append(episode_reward)
            eval_success.append(float(info.get('success', False)))
            eval_lengths.append(self.env.episode_steps)
        
        # æ¢å¤è®­ç»ƒæ¨¡å¼ - ä½¿ç”¨trainerçš„ç»Ÿä¸€æ–¹æ³•  # <- ä¿®æ”¹
        self.env.is_evaluation_mode = False
        self.trainer.set_eval_mode(False)  # <- ä¿®æ”¹ï¼šä½¿ç”¨ç»Ÿä¸€æ–¹æ³•
        
        # Print results
        logger.info(f"Evaluation Results:")
        logger.info(f"  Avg Reward: {np.mean(eval_rewards):.2f} Â± {np.std(eval_rewards):.2f}")
        logger.info(f"  Success Rate: {np.mean(eval_success):.2%}")
        logger.info(f"  Avg Length: {np.mean(eval_lengths):.1f} Â± {np.std(eval_lengths):.1f}")
        
        return {
            'avg_reward': np.mean(eval_rewards),
            'success_rate': np.mean(eval_success),
            'avg_length': np.mean(eval_lengths),
            'std_reward': np.std(eval_rewards),
            'std_length': np.std(eval_lengths)
        }

    def analyze_model(self) -> Dict[str, Any]:
        """Analyze trained model performance"""
        # Always try to use best_model.pt first
        best_model_path = self.checkpoint_dir / "best_model.pt"
        
        if best_model_path.exists():
            self.best_model_path = best_model_path
            logger.info(f"Using best_model.pt for analysis")
        elif not self.best_model_path:
            logger.error("No trained model to analyze!")
            return {}
        
        logger.info(f"Analyzing model from {self.best_model_path}")
        
        # Load best model
        self.trainer.load_checkpoint(str(self.best_model_path))
        
        # è®¾ç½®è¯„ä¼°æ¨¡å¼  # <- æ–°å¢äº†è¿™éƒ¨åˆ†
        if self.algorithm == 'ppo' and hasattr(self.trainer, 'eval_mode'):
            self.trainer.eval_mode = True
        elif self.algorithm == 'dqn' and hasattr(self.trainer, 'epsilon'):
            old_epsilon = self.trainer.epsilon
            self.trainer.epsilon = 0.0
        
        # Evaluate on different task types
        results = {}
        task_types = list(self.task_manager.tasks_by_type.keys())
        
        for task_type in task_types:
            logger.info(f"Evaluating on {task_type} tasks...")
            type_results = []
            
            for _ in range(10):  # 10 episodes per task type
                state = self.env.reset(task_type=task_type)
                episode_reward = 0
                done = False
                
                while not done and self.env.episode_steps < self.config['max_episode_length']:
                    valid_actions = self.env.get_valid_actions() if self.config['use_action_masking'] else None
                    action = self.trainer.select_action(state, valid_actions)
                    next_state, reward, done, info = self.env.step(action)
                    
                    episode_reward += reward
                    state = next_state
                
                type_results.append({
                    'reward': episode_reward,
                    'success': info.get('success', False),
                    'steps': self.env.episode_steps,
                    'phase2_score': info.get('phase2_metrics', {}).get('phase2_score', 0.0)
                })
            
            # Aggregate results
            results[task_type] = {
                'avg_reward': np.mean([r['reward'] for r in type_results]),
                'success_rate': np.mean([r['success'] for r in type_results]),
                'avg_steps': np.mean([r['steps'] for r in type_results]),
                'avg_phase2_score': np.mean([r['phase2_score'] for r in type_results])
            }
        
        # æ¢å¤è®­ç»ƒæ¨¡å¼  # <- æ–°å¢äº†è¿™éƒ¨åˆ†
        if self.algorithm == 'ppo' and hasattr(self.trainer, 'eval_mode'):
            self.trainer.eval_mode = False
        elif self.algorithm == 'dqn':
            self.trainer.epsilon = old_epsilon
        
        # Overall statistics
        overall_success = np.mean([r['success_rate'] for r in results.values()])
        overall_phase2 = np.mean([r['avg_phase2_score'] for r in results.values()])
        
        logger.info(f"\nAnalysis Results:")
        logger.info(f"Overall Success Rate: {overall_success:.2%}")
        logger.info(f"Overall Phase 2 Score: {overall_phase2:.3f}")
        
        for task_type, metrics in results.items():
            logger.info(f"\n{task_type}:")
            logger.info(f"  Success Rate: {metrics['success_rate']:.2%}")
            logger.info(f"  Avg Steps: {metrics['avg_steps']:.1f}")
            logger.info(f"  Phase 2 Score: {metrics['avg_phase2_score']:.3f}")
        
        return {
            'task_results': results,
            'overall_success': overall_success,
            'overall_phase2_score': overall_phase2,
            'model_path': str(self.best_model_path)
        }


# ===========================
# Utility Functions for Testing
# ===========================

def test_environment_setup():
    """Test environment setup with all features"""
    logger.info("Testing environment setup...")
    
    manager = UnifiedTrainingManager(
        use_task_aware_state=True,
        enforce_workflow=True,
        use_phase2_scoring=True
    )
    
    if manager.setup_environment():
        logger.info("âœ… Environment setup successful!")
        
        # Test reset and step
        state = manager.env.reset()
        logger.info(f"State shape: {state.shape}")
        logger.info(f"State dim: {manager.env.get_state_dim()}")
        
        # Test step
        action = 0
        next_state, reward, done, info = manager.env.step(action)
        logger.info(f"Step successful: reward={reward:.2f}")
        logger.info(f"Info: {info}")
        
        return True
    else:
        logger.error("âŒ Environment setup failed!")
        return False


def quick_train_test(episodes: int = 100):
    """Quick training test"""
    logger.info(f"Running quick training test ({episodes} episodes)...")
    
    manager = UnifiedTrainingManager(
        use_task_aware_state=True,
        enforce_workflow=False,  # Start without enforcement
        use_phase2_scoring=True
    )
    
    if manager.setup_environment():
        success = manager.train_dqn(num_episodes=episodes, print_frequency=10)
        if success:
            logger.info("âœ… Training completed!")
            # Evaluate
            results = manager.evaluate(num_episodes=10)
            logger.info(f"Evaluation: {results}")
        else:
            logger.error("âŒ Training failed!")
    else:
        logger.error("âŒ Setup failed!")


# ===========================
# Main Entry Point
# ===========================

if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Unified Training Manager")
    parser.add_argument("--test", action="store_true", help="Run quick test")
    parser.add_argument("--episodes", type=int, default=100, help="Number of episodes")
    parser.add_argument("--use-task-aware", action="store_true", default=True,
                       help="Use task-aware state representation")
    parser.add_argument("--no-task-aware", dest="use_task_aware", action="store_false")
    parser.add_argument("--enforce-workflow", action="store_true", default=False,
                       help="Enable workflow enforcement")
    parser.add_argument("--use-phase2", action="store_true", default=True,
                       help="Use Phase 2 scoring")
    parser.add_argument("--no-phase2", dest="use_phase2", action="store_false")
    parser.add_argument("--resume", defalt=False, action="store_true", help="Resume from checkpoint")
    
    args = parser.parse_args()
    
    if args.test:
        test_environment_setup()
        quick_train_test(args.episodes)
    else:
        manager = UnifiedTrainingManager(
            use_task_aware_state=args.use_task_aware,
            enforce_workflow=args.enforce_workflow,
            use_phase2_scoring=args.use_phase2
        )
        
        if manager.setup_environment():
            manager.train_dqn(num_episodes=args.episodes, resume=args.resume)
        else:
            logger.error("Failed to setup environment!")