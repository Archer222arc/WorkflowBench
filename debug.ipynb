{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b67e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 17:40:16,822 - mcp_embedding_manager - INFO - Loaded search cache with 3 entries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MCPEmbeddingManager] Initializing with unified API client manager\n",
      "[MCPEmbeddingManager] Using embedding model: text-embedding-3-large\n",
      "[MCPEmbeddingManager] Client initialized successfully\n",
      "[OperationEmbeddingIndex] Initializing with unified API client manager\n",
      "[OperationEmbeddingIndex] OpenAI client initialized with model: gpt-4o-mini\n",
      "[OperationEmbeddingIndex] Using embedding model: text-embedding-3-large\n",
      "[OperationEmbeddingIndex] Detecting actual embedding dimension...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 17:40:17,110 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:17,201 - mcp_embedding_manager - INFO - Initialized operation mappings with 142 terms\n",
      "2025-07-11 17:40:17,278 - mcp_embedding_manager - INFO - Building tool embedding index...\n",
      "2025-07-11 17:40:17,280 - mcp_embedding_manager - INFO - Creating embeddings for 30 tools...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OperationEmbeddingIndex] Detected embedding dimension: 3072\n",
      "[INFO] Loaded 6050 embeddings from persistent cache\n",
      "[OperationEmbeddingIndex] Initialized with 6050 cached embeddings\n",
      "[INFO] Loaded 14 LLM-enhanced operation definitions from cache\n",
      "[INFO] Found cached operation index at .mcp_operation_cache/operation_index.pkl\n",
      "[INFO] Loading operation index from .mcp_operation_cache/operation_index.pkl\n",
      "[INFO] Successfully loaded FAISS index with dimension 3072\n",
      "[INFO] Operation index loaded successfully from .mcp_operation_cache/operation_index.pkl\n",
      "[INFO] Loaded 14 operations with dimension 3072\n",
      "[INFO] Successfully loaded cached index\n",
      "[Cache] Loaded 4010 entries from file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]2025-07-11 17:40:17,393 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:17,480 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:17,615 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:17,704 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "  3%|‚ñé         | 1/30 [00:00<00:12,  2.38it/s]2025-07-11 17:40:17,784 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:17,871 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:17,952 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "  7%|‚ñã         | 2/30 [00:00<00:08,  3.13it/s]2025-07-11 17:40:18,028 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:18,132 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:18,206 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:18,274 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      " 10%|‚ñà         | 3/30 [00:00<00:08,  3.12it/s]2025-07-11 17:40:18,402 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:18,475 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:18,572 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:18,651 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      " 13%|‚ñà‚ñé        | 4/30 [00:01<00:08,  2.92it/s]2025-07-11 17:40:18,729 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:18,832 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:18,905 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      " 17%|‚ñà‚ñã        | 5/30 [00:01<00:07,  3.22it/s]2025-07-11 17:40:18,975 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:19,060 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:19,189 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      " 20%|‚ñà‚ñà        | 6/30 [00:01<00:07,  3.31it/s]2025-07-11 17:40:19,270 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:19,349 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:19,430 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:19,535 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      " 23%|‚ñà‚ñà‚ñé       | 7/30 [00:02<00:07,  3.16it/s]2025-07-11 17:40:19,610 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:19,724 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:19,813 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:19,917 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      " 27%|‚ñà‚ñà‚ñã       | 8/30 [00:02<00:07,  2.97it/s]2025-07-11 17:40:19,979 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:20,059 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:20,135 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:20,226 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      " 30%|‚ñà‚ñà‚ñà       | 9/30 [00:02<00:06,  3.05it/s]2025-07-11 17:40:20,290 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:20,373 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:20,460 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:20,550 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      " 33%|‚ñà‚ñà‚ñà‚ñé      | 10/30 [00:03<00:06,  3.06it/s]2025-07-11 17:40:20,628 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:20,720 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:20,797 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      " 37%|‚ñà‚ñà‚ñà‚ñã      | 11/30 [00:03<00:05,  3.30it/s]2025-07-11 17:40:20,856 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:20,970 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:21,073 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      " 40%|‚ñà‚ñà‚ñà‚ñà      | 12/30 [00:03<00:05,  3.40it/s]2025-07-11 17:40:21,230 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:21,328 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:21,445 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      " 43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 13/30 [00:04<00:05,  3.14it/s]2025-07-11 17:40:21,510 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:21,624 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:21,716 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      " 47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 14/30 [00:04<00:04,  3.29it/s]2025-07-11 17:40:21,811 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:21,891 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:21,952 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:22,014 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      " 50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 15/30 [00:04<00:04,  3.31it/s]2025-07-11 17:40:22,123 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:22,275 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:22,388 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:22,478 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      " 53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 16/30 [00:05<00:04,  2.85it/s]2025-07-11 17:40:22,599 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:22,683 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:22,782 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:22,869 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      " 57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 17/30 [00:05<00:04,  2.76it/s]2025-07-11 17:40:22,968 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:23,046 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:23,146 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      " 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 18/30 [00:05<00:04,  2.97it/s]2025-07-11 17:40:23,210 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:23,372 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:23,456 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      " 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 19/30 [00:06<00:03,  3.04it/s]2025-07-11 17:40:23,627 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:23,704 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:23,775 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 20/30 [00:06<00:03,  3.07it/s]2025-07-11 17:40:23,857 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:23,936 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:24,011 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      " 70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 21/30 [00:06<00:02,  3.34it/s]2025-07-11 17:40:24,102 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:24,204 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:24,309 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      " 73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 22/30 [00:07<00:02,  3.35it/s]2025-07-11 17:40:24,391 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:24,482 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:24,584 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      " 77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 23/30 [00:07<00:02,  3.43it/s]2025-07-11 17:40:24,675 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:24,771 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:24,884 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      " 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 24/30 [00:07<00:01,  3.40it/s]2025-07-11 17:40:24,966 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:25,041 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:25,122 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      " 83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 25/30 [00:07<00:01,  3.61it/s]2025-07-11 17:40:25,213 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:25,291 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:25,416 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      " 87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 26/30 [00:08<00:01,  3.54it/s]2025-07-11 17:40:25,491 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:25,599 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:25,694 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      " 90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 27/30 [00:08<00:00,  3.56it/s]2025-07-11 17:40:25,780 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:25,852 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:25,941 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      " 93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 28/30 [00:08<00:00,  3.69it/s]2025-07-11 17:40:26,006 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:26,096 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:26,216 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      " 97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 29/30 [00:08<00:00,  3.68it/s]2025-07-11 17:40:26,292 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:26,356 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 17:40:26,438 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 30/30 [00:09<00:00,  3.27it/s]\n",
      "2025-07-11 17:40:26,542 - mcp_embedding_manager - INFO - Search cache saved with 4010 entries\n",
      "2025-07-11 17:40:26,570 - mcp_embedding_manager - INFO - Index saved to .mcp_embedding_cache/tool_index.pkl\n",
      "2025-07-11 17:40:26,574 - mcp_embedding_manager - INFO - Index built: {'status': 'built', 'tools': 30, 'categories': 6, 'index_type': 'faiss', 'embedding_dimension': 3072}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MCPEmbeddingManager] Expected dimension: 3072\n",
      "[MCPEmbeddingManager] Actual embedding dimension: 3072\n",
      "[INFO] Saved 4010 search cache entries\n",
      "üìä Cache Statistics:\n",
      "  - Embedding cache: 100 entries (2.35 MB)\n",
      "  - Search cache: 4010 entries\n",
      "  - Total size: 6.02 MB\n",
      "üóëÔ∏è Clearing cache (keep_embeddings=True)...\n",
      "  - Cleared search cache\n",
      "  - Kept 100 embeddings\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mÂú®ÂΩìÂâçÂçïÂÖÉÊ†ºÊàñ‰∏ä‰∏Ä‰∏™ÂçïÂÖÉÊ†º‰∏≠ÊâßË°å‰ª£Á†ÅÊó∂ Kernel Â¥©Ê∫É„ÄÇ\n",
      "\u001b[1;31mËØ∑Êü•ÁúãÂçïÂÖÉÊ†º‰∏≠ÁöÑ‰ª£Á†ÅÔºå‰ª•Á°ÆÂÆöÊïÖÈöúÁöÑÂèØËÉΩÂéüÂõ†„ÄÇ\n",
      "\u001b[1;31mÂçïÂáª<a href='https://aka.ms/vscodeJupyterKernelCrash'>Ê≠§Â§Ñ</a>‰∫ÜËß£ËØ¶ÁªÜ‰ø°ÊÅØ„ÄÇ\n",
      "\u001b[1;31mÊúâÂÖ≥Êõ¥Â§öËØ¶ÁªÜ‰ø°ÊÅØÔºåËØ∑Êü•Áúã Jupyter <a href='command:jupyter.viewOutput'>log</a>„ÄÇ"
     ]
    }
   ],
   "source": [
    "# 1. Âü∫Á°ÄÈáçÂª∫Ôºà‰øùÁïôÊâÄÊúâembeddingÁºìÂ≠òÔºâ\n",
    "from mcp_embedding_manager import MCPEmbeddingManager\n",
    "from collections import OrderedDict\n",
    "\n",
    "manager = MCPEmbeddingManager()\n",
    "manager.build_index(\n",
    "    tool_registry_path=\"mcp_generated_library/tool_registry_consolidated.json\",\n",
    "    force_rebuild=True\n",
    ")\n",
    "\n",
    "# 2. Êü•ÁúãÁºìÂ≠òÁªüËÆ°\n",
    "stats = manager.get_cache_stats()\n",
    "print(f\"üìä Cache Statistics:\")\n",
    "print(f\"  - Embedding cache: {stats['embedding_cache']['entries']} entries ({stats['embedding_cache']['size_mb']:.2f} MB)\")\n",
    "print(f\"  - Search cache: {stats['search_cache']['entries']} entries\")\n",
    "print(f\"  - Total size: {stats['total_size_mb']:.2f} MB\")\n",
    "\n",
    "# 3. Ê∏ÖÁêÜÊêúÁ¥¢ÁºìÂ≠ò‰ΩÜ‰øùÁïôembedding\n",
    "manager.clear_cache(keep_embeddings=True)\n",
    "\n",
    "# 4. ÂÆåÂÖ®Ê∏ÖÁêÜÔºà‰ªÖÂú®ÂøÖË¶ÅÊó∂Ôºâ\n",
    "# manager.clear_cache(keep_embeddings=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2cf320eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CheckpointÁõÆÂΩï: checkpoints\n",
      "ÁõÆÂΩïÊòØÂê¶Â≠òÂú®: True\n",
      "\n",
      "ÊâæÂà∞ 6 ‰∏™checkpointÊñá‰ª∂:\n",
      "  - best_model.pt\n",
      "  - checkpoint_episode_0.pt\n",
      "  - checkpoint_episode_100.pt\n",
      "  - final_cpu_optimized.pt\n",
      "  - final_gpu_model.pt\n",
      "  - final_model.pt\n",
      "\n",
      "ÂàÜÊûêcheckpoint: best_model.pt\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_199909/167464570.py:38: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(target_checkpoint, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Âü∫Êú¨‰ø°ÊÅØ:\n",
      "  ‚ö†Ô∏è Ê≤°Êúâmetadata‰ø°ÊÅØ\n",
      "\n",
      "üìà ËÆ≠ÁªÉÂéÜÂè≤:\n",
      "  ÊÄªEpisodes: 200\n",
      "  ÊúÄÂêé10‰∏™episodeÊàêÂäüÁéá: 0.00%\n",
      "  ÊúÄÈ´òÊàêÂäüÁéá: 0.00%\n",
      "  ÊàêÂäüÁöÑepisodesÊï∞: 0.0\n",
      "\n",
      "  ÊúÄÂêé20‰∏™episodeÊàêÂäüÊÉÖÂÜµ:\n",
      "  ‚úó‚úó‚úó‚úó‚úó‚úó‚úó‚úó‚úó‚úó‚úó‚úó‚úó‚úó‚úó‚úó‚úó‚úó‚úó‚úó\n",
      "\n",
      "  Âπ≥ÂùáÂ•ñÂä±: -286.05\n",
      "  ÊúÄÂêé10‰∏™episodeÂπ≥ÂùáÂ•ñÂä±: -315.50\n",
      "  ÊúÄÈ´òÂ•ñÂä±: -60.00\n",
      "\n",
      "‚öôÔ∏è ËÆ≠ÁªÉÈÖçÁΩÆ:\n",
      "  ÁÆóÊ≥ï: ppo\n",
      "  Â≠¶‰π†Áéá: 0.0005\n",
      "  ÊâπÂ§ßÂ∞è: 1024\n",
      "  ÊúÄÂ§ßepisodeÈïøÂ∫¶: 100\n",
      "  ËØæÁ®ãÂ≠¶‰π†: True\n",
      "\n",
      "üì¶ ÂÖ∂‰ªñ‰ø°ÊÅØ:\n",
      "  CheckpointÂåÖÂê´ÁöÑÊâÄÊúâÈîÆ: ['algorithm', 'state_dim', 'action_dim', 'episode', 'config', 'timestamp', 'use_task_aware_state', 'enforce_workflow', 'use_phase2_scoring', 'cpu_optimized', 'network_state_dict', 'model_state_dict', 'optimizer_state_dict', 'training_steps', 'training_history', 'best_success_rate']\n",
      "\n",
      "üîç ÂèØËÉΩÁöÑÈóÆÈ¢òËØäÊñ≠:\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# ËÆæÁΩÆcheckpointË∑ØÂæÑ\n",
    "checkpoint_dir = Path(\"checkpoints\")  # Ê†πÊçÆ‰Ω†ÁöÑÂÆûÈôÖË∑ØÂæÑ‰øÆÊîπ\n",
    "print(f\"CheckpointÁõÆÂΩï: {checkpoint_dir}\")\n",
    "print(f\"ÁõÆÂΩïÊòØÂê¶Â≠òÂú®: {checkpoint_dir.exists()}\")\n",
    "\n",
    "# ÂàóÂá∫ÊâÄÊúâcheckpointÊñá‰ª∂\n",
    "if checkpoint_dir.exists():\n",
    "    checkpoints = list(checkpoint_dir.glob(\"*.pt\"))\n",
    "    print(f\"\\nÊâæÂà∞ {len(checkpoints)} ‰∏™checkpointÊñá‰ª∂:\")\n",
    "    for ckpt in sorted(checkpoints):\n",
    "        print(f\"  - {ckpt.name}\")\n",
    "else:\n",
    "    print(\"‚ùå CheckpointÁõÆÂΩï‰∏çÂ≠òÂú®!\")\n",
    "    checkpoints = []\n",
    "\n",
    "# ÂàÜÊûêÊúÄÊñ∞ÁöÑcheckpoint\n",
    "if checkpoints:\n",
    "    # ÈÄâÊã©Ë¶ÅÂàÜÊûêÁöÑcheckpoint\n",
    "    # ÂèØ‰ª•ÈÄâÊã© best_model.pt ÊàñÊúÄÊñ∞ÁöÑ checkpoint_episode_xxx.pt\n",
    "    best_model = checkpoint_dir / \"best_model.pt\"\n",
    "    # best_model = checkpoint_dir / \"checkpoint_episode_400.pt\"\n",
    "    if best_model.exists():\n",
    "        target_checkpoint = best_model\n",
    "    else:\n",
    "        # Ëé∑ÂèñÊúÄÊñ∞ÁöÑcheckpoint\n",
    "        target_checkpoint = max(checkpoints, key=lambda x: x.stat().st_mtime)\n",
    "    \n",
    "    print(f\"\\nÂàÜÊûêcheckpoint: {target_checkpoint.name}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Âä†ËΩΩcheckpoint\n",
    "    checkpoint = torch.load(target_checkpoint, map_location='cpu')\n",
    "    \n",
    "    # 1. ÊâìÂç∞Âü∫Êú¨‰ø°ÊÅØ\n",
    "    print(\"\\nüìä Âü∫Êú¨‰ø°ÊÅØ:\")\n",
    "    if 'metadata' in checkpoint:\n",
    "        metadata = checkpoint['metadata']\n",
    "        print(f\"  Episode: {metadata.get('episode', 'N/A')}\")\n",
    "        print(f\"  Success Rate: {metadata.get('success_rate', 0):.2%}\")\n",
    "        print(f\"  Timestamp: {metadata.get('timestamp', 'N/A')}\")\n",
    "        print(f\"  Best Success Rate: {metadata.get('best_success_rate', 0):.2%}\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è Ê≤°Êúâmetadata‰ø°ÊÅØ\")\n",
    "    \n",
    "    # 2. ËÆ≠ÁªÉÂéÜÂè≤\n",
    "    print(\"\\nüìà ËÆ≠ÁªÉÂéÜÂè≤:\")\n",
    "    if 'training_history' in checkpoint:\n",
    "        history = checkpoint['training_history']\n",
    "        if 'success' in history and history['success']:\n",
    "            success_rates = history['success']\n",
    "            print(f\"  ÊÄªEpisodes: {len(success_rates)}\")\n",
    "            print(f\"  ÊúÄÂêé10‰∏™episodeÊàêÂäüÁéá: {np.mean(success_rates[-10:]):.2%}\")\n",
    "            print(f\"  ÊúÄÈ´òÊàêÂäüÁéá: {max(success_rates):.2%}\")\n",
    "            print(f\"  ÊàêÂäüÁöÑepisodesÊï∞: {sum(success_rates)}\")\n",
    "            \n",
    "            # ÊâìÂç∞ÊúÄÂêéÂá†‰∏™episodeÁöÑÊàêÂäüÊÉÖÂÜµ\n",
    "            print(f\"\\n  ÊúÄÂêé20‰∏™episodeÊàêÂäüÊÉÖÂÜµ:\")\n",
    "            last_20 = success_rates[-20:] if len(success_rates) >= 20 else success_rates\n",
    "            success_str = \"\".join([\"‚úì\" if s else \"‚úó\" for s in last_20])\n",
    "            print(f\"  {success_str}\")\n",
    "        else:\n",
    "            print(\"  ‚ö†Ô∏è Ê≤°ÊúâÊàêÂäüÁéáËÆ∞ÂΩï\")\n",
    "            \n",
    "        if 'rewards' in history and history['rewards']:\n",
    "            rewards = history['rewards']\n",
    "            print(f\"\\n  Âπ≥ÂùáÂ•ñÂä±: {np.mean(rewards):.2f}\")\n",
    "            print(f\"  ÊúÄÂêé10‰∏™episodeÂπ≥ÂùáÂ•ñÂä±: {np.mean(rewards[-10:]):.2f}\")\n",
    "            print(f\"  ÊúÄÈ´òÂ•ñÂä±: {max(rewards):.2f}\")\n",
    "    else:\n",
    "        print(\"  ‚ö†Ô∏è Ê≤°ÊúâËÆ≠ÁªÉÂéÜÂè≤\")\n",
    "    \n",
    "    # 3. ÈÖçÁΩÆ‰ø°ÊÅØ\n",
    "    print(\"\\n‚öôÔ∏è ËÆ≠ÁªÉÈÖçÁΩÆ:\")\n",
    "    if 'config' in checkpoint:\n",
    "        config = checkpoint['config']\n",
    "        print(f\"  ÁÆóÊ≥ï: {config.get('algorithm', 'N/A')}\")\n",
    "        print(f\"  Â≠¶‰π†Áéá: {config.get('learning_rate', 'N/A')}\")\n",
    "        print(f\"  ÊâπÂ§ßÂ∞è: {config.get('batch_size', 'N/A')}\")\n",
    "        print(f\"  ÊúÄÂ§ßepisodeÈïøÂ∫¶: {config.get('max_episode_length', 'N/A')}\")\n",
    "        print(f\"  ËØæÁ®ãÂ≠¶‰π†: {config.get('use_curriculum', False)}\")\n",
    "    \n",
    "    # 4. È¢ùÂ§ñ‰ø°ÊÅØ\n",
    "    print(\"\\nüì¶ ÂÖ∂‰ªñ‰ø°ÊÅØ:\")\n",
    "    all_keys = list(checkpoint.keys())\n",
    "    print(f\"  CheckpointÂåÖÂê´ÁöÑÊâÄÊúâÈîÆ: {all_keys}\")\n",
    "    \n",
    "    # Ê£ÄÊü•‰ªªÂä°ÁªüËÆ°\n",
    "    if 'task_statistics' in checkpoint:\n",
    "        print(\"\\n  ‰ªªÂä°ÁªüËÆ°:\")\n",
    "        task_stats = checkpoint['task_statistics']\n",
    "        for task_type, stats in task_stats.items():\n",
    "            print(f\"    {task_type}: ÊàêÂäüÁéá {stats.get('success_rate', 0):.2%}\")\n",
    "    \n",
    "    # Ê£ÄÊü•ËØæÁ®ãÈò∂ÊÆµ\n",
    "    if 'curriculum_stage' in checkpoint:\n",
    "        print(f\"\\n  ËØæÁ®ãÈò∂ÊÆµ: Stage {checkpoint['curriculum_stage']}\")\n",
    "    \n",
    "    # Ê£ÄÊü•ÊòØÂê¶ÊúâÂ∑•ÂÖ∑ÂÖ≥ÈîÆÊÄßÊï∞ÊçÆ\n",
    "    criticality_file = checkpoint_dir / \"tool_criticality.json\"\n",
    "    if criticality_file.exists():\n",
    "        print(\"\\n  ‚úì ÊâæÂà∞tool_criticality.jsonÊñá‰ª∂\")\n",
    "        with open(criticality_file, 'r') as f:\n",
    "            criticality_data = json.load(f)\n",
    "            print(f\"    ËÆ∞ÂΩïÁöÑÂ∑•ÂÖ∑Êï∞: {len(criticality_data.get('tool_criticality', {}))}\")\n",
    "    \n",
    "    # 5. ËØäÊñ≠‰ø°ÊÅØ\n",
    "    print(\"\\nüîç ÂèØËÉΩÁöÑÈóÆÈ¢òËØäÊñ≠:\")\n",
    "    if 'metadata' in checkpoint:\n",
    "        success_rate = checkpoint['metadata'].get('success_rate', 0)\n",
    "        if success_rate == 0:\n",
    "            print(\"  ‚ùå ÊàêÂäüÁéá‰∏∫0ÔºåÂèØËÉΩÁöÑÂéüÂõ†Ôºö\")\n",
    "            print(\"    1. ‰ªªÂä°Â§™ÈöæÊàñÂ•ñÂä±ËÆæËÆ°ÊúâÈóÆÈ¢ò\")\n",
    "            print(\"    2. Êé¢Á¥¢‰∏çË∂≥ÔºåÈô∑ÂÖ•Â±ÄÈÉ®ÊúÄ‰ºò\")\n",
    "            print(\"    3. required_toolsÂÆö‰πâÂèØËÉΩÊúâÈóÆÈ¢ò\")\n",
    "            print(\"    4. ÊúÄÂ§ßÊ≠•Êï∞ÈôêÂà∂Â§™‰∏•Ê†º\")\n",
    "            print(\"    5. ËØæÁ®ãÂ≠¶‰π†ËøõÂ∫¶Â§™Âø´\")\n",
    "            \n",
    "            # Ê£ÄÊü•episodeÈïøÂ∫¶\n",
    "            if 'training_history' in checkpoint and 'lengths' in checkpoint['training_history']:\n",
    "                avg_length = np.mean(checkpoint['training_history']['lengths'])\n",
    "                max_allowed = config.get('max_episode_length', 100)\n",
    "                if avg_length > max_allowed * 0.9:\n",
    "                    print(f\"\\n    ‚ö†Ô∏è Âπ≥ÂùáepisodeÈïøÂ∫¶({avg_length:.1f})Êé•ËøëÊúÄÂ§ßÈôêÂà∂({max_allowed})!\")\n",
    "                    print(\"       ÂèØËÉΩÈúÄË¶ÅÂ¢ûÂä†max_episode_length\")\n",
    "else:\n",
    "    print(\"\\n‚ùå Ê≤°ÊúâÊâæÂà∞‰ªª‰ΩïcheckpointÊñá‰ª∂!\")\n",
    "    print(\"ËØ∑Ê£ÄÊü•:\")\n",
    "    print(\"1. checkpointË∑ØÂæÑÊòØÂê¶Ê≠£Á°Æ\")\n",
    "    print(\"2. ËÆ≠ÁªÉÊòØÂê¶Ê≠£Â∏∏‰øùÂ≠ò‰∫Ücheckpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5095732e",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "A key should be provided to invoke the endpoint",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m api_key = os.getenv(\u001b[33m\"\u001b[39m\u001b[33mAZURE_INFERENCE_CREDENTIAL\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m api_key:\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mA key should be provided to invoke the endpoint\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m client = ChatCompletionsClient(\n\u001b[32m     11\u001b[39m     endpoint=\u001b[33m'\u001b[39m\u001b[33mhttps://archer222arc.openai.azure.com/openai/deployments/gpt-4o-mini\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     12\u001b[39m     credential=AzureKeyCredential(api_key),\n\u001b[32m     13\u001b[39m \n\u001b[32m     14\u001b[39m )\n\u001b[32m     16\u001b[39m payload = {\n\u001b[32m     17\u001b[39m   \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: [\n\u001b[32m     18\u001b[39m     {\n\u001b[32m   (...)\u001b[39m\u001b[32m     34\u001b[39m   \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: []\n\u001b[32m     35\u001b[39m }\n",
      "\u001b[31mException\u001b[39m: A key should be provided to invoke the endpoint"
     ]
    }
   ],
   "source": [
    "# pip install azure-ai-inference\n",
    "import os\n",
    "from azure.ai.inference import ChatCompletionsClient\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "# api_key = os.getenv(\"AZURE_INFERENCE_CREDENTIAL\", '')\n",
    "api_key = \"9wiSC2YySp6iDFL45NIPuoJ9Ynm2CcEjPw4FDjGAeCOpRdZjdetdJQQJ99BGACYeBjFXJ3w3AAABACOGqpWV\"\n",
    "if not api_key:\n",
    "  raise Exception(\"A key should be provided to invoke the endpoint\")\n",
    "\n",
    "client = ChatCompletionsClient(\n",
    "    endpoint='https://archer222arc.openai.azure.com/openai/deployments/gpt-4o-mini',\n",
    "    credential=AzureKeyCredential(api_key),\n",
    "    \n",
    ")\n",
    "\n",
    "payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"I am going to Paris, what should I see?\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"assistant\",\n",
    "      \"content\": \"Paris, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\\n\\n1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\\n2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\\n3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\\n\\nThese are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.\"\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": \"What is so great about #1?\"\n",
    "    }\n",
    "  ],\n",
    "  \"max_tokens\": 4096,\n",
    "  \"temperature\": 1,\n",
    "  \"top_p\": 1,\n",
    "  \"stop\": []\n",
    "}\n",
    "response = client.complete(payload)\n",
    "\n",
    "print(\"Response:\", response.choices[0].message.content)\n",
    "print(\"Model:\", response.model)\n",
    "print(\"Usage:\")\n",
    "print(\"\tPrompt tokens:\", response.usage.prompt_tokens)\n",
    "print(\"\tTotal tokens:\", response.usage.total_tokens)\n",
    "print(\"\tCompletion tokens:\", response.usage.completion_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bd99a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 09:23:21,469 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/gpt-4o-mini/chat/completions?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paris is a vibrant city with a rich history, stunning architecture, and a diverse array of attractions. Here‚Äôs a list of must-see places and experiences you shouldn‚Äôt miss:\n",
      "\n",
      "### Iconic Landmarks:\n",
      "1. **Eiffel Tower** - You can't visit Paris without seeing this iconic structure. Consider going at sunset for breathtaking views.\n",
      "2. **Louvre Museum** - Home to thousands of art pieces, including the Mona Lisa and Venus de Milo; plan your visit as it can be large.\n",
      "3. **Notre-Dame Cathedral** - Though currently under restoration, the exterior and the √éle de la Cit√© are worth seeing.\n",
      "4. **Sacr√©-C≈ìur Basilica** - Located on Montmartre Hill, it offers stunning views of the city and a beautiful interior.\n",
      "5. **Arc de Triomphe** - Climb to the top for another great view of the city, and explore the Champs-√âlys√©es.\n",
      "\n",
      "### Museums and Cultural Venues:\n",
      "6. **Mus√©e d‚ÄôOrsay** - Housed in a former railway station, it features Impressionist masterpieces.\n",
      "7. **Centre Pompidou** - Known for its modern art collections and unique architecture.\n",
      "8. **Mus√©e de l'Orangerie** - Famous for Monet's Water Lilies and other Impressionist works.\n",
      "\n",
      "### Historic Areas:\n",
      "9. **Montmartre** - Explore the charming streets, artists' squares, and the vibrant bohemian atmosphere.\n",
      "10. **Le Marais** - Known for its medieval architecture, trendy shops, and art galleries.\n",
      "\n",
      "### Parks and Gardens:\n",
      "11. **Luxembourg Gardens** - A beautiful park perfect for a leisurely stroll or a picnic.\n",
      "12. **Tuileries Garden** - Located between the Louvre and Place de la Concorde, it‚Äôs great for relaxing outdoors.\n",
      "\n",
      "### Unique Experiences:\n",
      "13. **Seine River Cruise** - Consider doing a boat tour (especially at night) for unique views of the city's landmarks.\n",
      "14. **Visit a Parisian Caf√©** - Experience the culture by enjoying coffee and pastries at famous caf√©s like Caf√© de Flore or Les Deux Magots.\n",
      "15. **Explore the Catacombs** - For something different, visit the underground ossuaries filled with the remains of millions of Parisians.\n",
      "\n",
      "### Shopping and Cuisine:\n",
      "16. **Champs-√âlys√©es** - Great for shopping and people-watching.\n",
      "17. **Le Bon March√©** - One of the first department stores in Paris.\n",
      "18. **Try French Cuisine** - Dine at a traditional bistro or indulge in pastries at a local patisserie.\n",
      "\n",
      "### Day Trips:\n",
      "19. **Versailles** - A short train ride away, the Palace of Versailles is stunning with its gardens and opulent rooms.\n",
      "20. **Giverny** - The home of Claude Monet and his beautiful gardens are perfect for art lovers.\n",
      "\n",
      "Be sure to check the opening hours and any entry requirements for popular attractions, and consider booking tickets in advance where possible. Enjoy your trip to Paris!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI\n",
    "endpoint = \"https://archer222arc.openai.azure.com/\"\n",
    "model_name = \"gpt-4o-mini\"\n",
    "deployment = \"gpt-4o-mini\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_version=\"2024-12-01-preview\",\n",
    "    azure_endpoint=\"https://archer222arc.openai.azure.com/\",\n",
    "    api_key=\"9wiSC2YySp6iDFL45NIPuoJ9Ynm2CcEjPw4FDjGAeCOpRdZjdetdJQQJ99BGACYeBjFXJ3w3AAABACOGqpWV\",\n",
    ")\n",
    "response = client.chat.completions.create(\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful assistant.\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"I am going to Paris, what should I see?\",\n",
    "        }\n",
    "    ],\n",
    "    max_tokens=4096,\n",
    "    temperature=1.0,\n",
    "    top_p=1.0,\n",
    "    model=deployment\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28ff1241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ÂΩìÂâçÂ∑•‰ΩúÁõÆÂΩï: /root/WorkflowBench/scale_up\n",
      "ÂàùÂßãÂåñ Operation Embedding Index...\n",
      "[OperationEmbeddingIndex] Initializing with unified API client manager\n",
      "[OperationEmbeddingIndex] OpenAI client initialized with model: gpt-4o-mini\n",
      "[OperationEmbeddingIndex] Using embedding model: text-embedding-3-small\n",
      "[OperationEmbeddingIndex] Using embedding dimension: 1536\n",
      "[INFO] Loaded 2973 embeddings from persistent cache\n",
      "[OperationEmbeddingIndex] Initialized with 2973 cached embeddings\n",
      "[INFO] Loaded 14 LLM-enhanced operation definitions from cache\n",
      "[INFO] Loading operation index from cache\n",
      "[INFO] Operation index loaded from .mcp_operation_cache/operation_index.pkl\n",
      "\n",
      "=== Êìç‰ΩúÂÆö‰πâÁªüËÆ° ===\n",
      "ÊÄªÊìç‰ΩúÊï∞: 14\n",
      "\n",
      "ÊåâÁ±ªÂà´ÂàÜÁªÑ:\n",
      "\n",
      "AGGREGATION (1 operations):\n",
      "  - aggregate\n",
      "\n",
      "AUTOMATION (1 operations):\n",
      "  - schedule\n",
      "\n",
      "COMMUNICATION (1 operations):\n",
      "  - notify\n",
      "\n",
      "COMPUTATION (1 operations):\n",
      "  - calculate\n",
      "\n",
      "DATA TRANSFER (1 operations):\n",
      "  - stream\n",
      "\n",
      "INPUT (1 operations):\n",
      "  - read\n",
      "\n",
      "INTEGRATION (2 operations):\n",
      "  - connect\n",
      "  - expose\n",
      "\n",
      "OUTPUT (1 operations):\n",
      "  - write\n",
      "\n",
      "TRANSFORMATION (2 operations):\n",
      "  - filter\n",
      "  - transform\n",
      "\n",
      "UTILITY (2 operations):\n",
      "  - backup\n",
      "  - log\n",
      "\n",
      "VALIDATION (1 operations):\n",
      "  - validate\n",
      "\n",
      "=== ËØ¶ÁªÜÊìç‰ΩúÂÆö‰πâÁ§∫‰æã ===\n",
      "\n",
      "INPUT Á§∫‰æã - read:\n",
      "  ÊèèËø∞: Read or load data from a source\n",
      "  Âêå‰πâËØç: retrieve, extract, load, get, acquire...\n",
      "  Áõ∏ÂÖ≥Êìç‰Ωú: validate, transform...\n",
      "\n",
      "VALIDATION Á§∫‰æã - validate:\n",
      "  ÊèèËø∞: Validate or verify data correctness\n",
      "  Âêå‰πâËØç: validate, audit, check, ensure, cross-check...\n",
      "  Áõ∏ÂÖ≥Êìç‰Ωú: read, transform...\n",
      "\n",
      "TRANSFORMATION Á§∫‰æã - transform:\n",
      "  ÊèèËø∞: Transform or convert data format\n",
      "  Âêå‰πâËØç: reshape, normalize, decode, convert, modify...\n",
      "  Áõ∏ÂÖ≥Êìç‰Ωú: validate, aggregate...\n",
      "\n",
      "AGGREGATION Á§∫‰æã - aggregate:\n",
      "  ÊèèËø∞: Aggregate or combine multiple data items\n",
      "  Âêå‰πâËØç: merge, consolidate, combine, group, unify...\n",
      "  Áõ∏ÂÖ≥Êìç‰Ωú: transform, write...\n",
      "\n",
      "OUTPUT Á§∫‰æã - write:\n",
      "  ÊèèËø∞: Write or save data to a destination\n",
      "  Âêå‰πâËØç: export, write out, publish, persist, archive...\n",
      "  Áõ∏ÂÖ≥Êìç‰Ωú: aggregate, log...\n",
      "\n",
      "COMPUTATION Á§∫‰æã - calculate:\n",
      "  ÊèèËø∞: Perform calculations or computations\n",
      "  Âêå‰πâËØç: analyze, evaluate, predict, estimate, solve...\n",
      "  Áõ∏ÂÖ≥Êìç‰Ωú: transform, aggregate...\n",
      "\n",
      "INTEGRATION Á§∫‰æã - connect:\n",
      "  ÊèèËø∞: Connect to external services or APIs\n",
      "  Âêå‰πâËØç: link, authenticate, interface, establish connection, attach...\n",
      "  Áõ∏ÂÖ≥Êìç‰Ωú: read, write...\n",
      "\n",
      "UTILITY Á§∫‰æã - log:\n",
      "  ÊèèËø∞: Log information or track activities\n",
      "  Âêå‰πâËØç: log, audit, monitor, chronicle, report...\n",
      "  Áõ∏ÂÖ≥Êìç‰Ωú: connect, validate...\n",
      "\n",
      "AUTOMATION Á§∫‰æã - schedule:\n",
      "  ÊèèËø∞: Schedule operations to run at specific times or intervals\n",
      "  Âêå‰πâËØç: plan, time, set up, organize, arrange...\n",
      "\n",
      "COMMUNICATION Á§∫‰æã - notify:\n",
      "  ÊèèËø∞: Send notifications or alerts based on events or conditions\n",
      "  Âêå‰πâËØç: alert, inform, message, advise, signal...\n",
      "\n",
      "DATA TRANSFER Á§∫‰æã - stream:\n",
      "  ÊèèËø∞: Continuously transfer data in real-time from one source to another\n",
      "  Âêå‰πâËØç: broadcast, send, relay, flow, transmit...\n",
      "\n",
      "=== ÊµãËØïËØ≠‰πâÊêúÁ¥¢ ===\n",
      "\n",
      "Êü•ËØ¢: 'read data from csv file'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 10:13:39,538 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 10:13:39,670 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - read (input): 0.347\n",
      "  - parse (transformation): 0.300\n",
      "  - write (output): 0.182\n",
      "\n",
      "Êü•ËØ¢: 'validate json schema'\n",
      "  - validate (validation): 0.360\n",
      "  - parse (transformation): 0.232\n",
      "  - transform (transformation): 0.226\n",
      "\n",
      "Êü•ËØ¢: 'merge multiple datasets'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 10:13:39,834 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 10:13:39,967 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - aggregate (aggregation): 0.414\n",
      "  - integrate (integration): 0.353\n",
      "  - transform (transformation): 0.233\n",
      "\n",
      "Êü•ËØ¢: 'connect to REST API'\n",
      "  - integrate (integration): 0.244\n",
      "  - validate (validation): 0.229\n",
      "  - transform (transformation): 0.190\n",
      "\n",
      "Êü•ËØ¢: 'calculate statistical metrics'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 10:13:40,101 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 10:13:40,230 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - compute (computation): 0.361\n",
      "  - aggregate (aggregation): 0.257\n",
      "  - validate (validation): 0.199\n",
      "\n",
      "Êü•ËØ¢: 'save results to database'\n",
      "  - write (output): 0.370\n",
      "  - cache (utility): 0.301\n",
      "  - transform (transformation): 0.221\n",
      "\n",
      "Êü•ËØ¢: 'monitor system performance'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 10:13:40,356 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n",
      "2025-07-11 10:13:40,485 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-small/embeddings?api-version=2024-12-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  - integrate (integration): 0.260\n",
      "  - compute (computation): 0.254\n",
      "  - validate (validation): 0.207\n",
      "\n",
      "Êü•ËØ¢: 'parse XML documents'\n",
      "  - parse (transformation): 0.333\n",
      "  - transform (transformation): 0.251\n",
      "  - read (input): 0.225\n",
      "\n",
      "=== ÁºìÂ≠òÊñá‰ª∂Áä∂ÊÄÅ ===\n",
      "llm_operation_definitions.json: 4.77 KB\n",
      "operation_index.pkl: 196.93 KB\n",
      "embedding_cache.pkl: 35890.82 KB\n",
      "\n",
      "=== LLM Â¢ûÂº∫ÂÆö‰πâÈ™åËØÅ ===\n",
      "ÊÄªÊìç‰ΩúÊï∞: 14\n",
      "\n",
      "LLM ÁîüÊàêÁöÑÊñ∞Êìç‰Ωú (6):\n",
      "  - filter: Filter data based on specific criteria or conditions...\n",
      "  - schedule: Schedule operations to run at specific times or intervals...\n",
      "  - notify: Send notifications or alerts based on events or conditions...\n",
      "  - backup: Create a copy of data for recovery or preservation purposes...\n",
      "  - stream: Continuously transfer data in real-time from one source to a...\n",
      "  - expose: Expose data or services through an API for external access...\n",
      "\n",
      "=== Á¥¢ÂºïÁªüËÆ°‰ø°ÊÅØ ===\n",
      "{\n",
      "  \"total_operations\": 10,\n",
      "  \"categories\": {\n",
      "    \"input\": 1,\n",
      "    \"transformation\": 3,\n",
      "    \"validation\": 1,\n",
      "    \"aggregation\": 1,\n",
      "    \"computation\": 1,\n",
      "    \"output\": 1,\n",
      "    \"utility\": 1,\n",
      "    \"integration\": 1\n",
      "  },\n",
      "  \"has_faiss\": true,\n",
      "  \"has_openai\": true,\n",
      "  \"embedding_dim\": 1536\n",
      "}\n",
      "\n",
      "‰øùÂ≠òÊâÄÊúâÁºìÂ≠ò...\n",
      "[INFO] Operation index saved to .mcp_operation_cache/operation_index.pkl\n",
      "ÁºìÂ≠ò‰øùÂ≠òÂÆåÊàêÔºÅ\n",
      "\n",
      "Êìç‰ΩúÂÆö‰πâÊä•ÂëäÂ∑≤‰øùÂ≠òÂà∞: operation_definitions_report.json\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # ÂàõÂª∫ Operations Embedding ÁºìÂ≠ò\n",
    "# Ëøô‰∏™ notebook Áî®‰∫éÁîüÊàêÊñ∞ÁöÑ operations embedding ÁºìÂ≠òÊñá‰ª∂ÔºåÂåÖÊã¨ LLM Â¢ûÂº∫ÁöÑÊìç‰ΩúÂÆö‰πâ\n",
    "\n",
    "# %% ÂØºÂÖ•ÂøÖË¶ÅÁöÑÂ∫ì\n",
    "import os\n",
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "# Á°Æ‰øùÂú®Ê≠£Á°ÆÁöÑÁõÆÂΩï‰∏ãËøêË°å\n",
    "print(f\"ÂΩìÂâçÂ∑•‰ΩúÁõÆÂΩï: {os.getcwd()}\")\n",
    "\n",
    "# %% Ê∏ÖÁêÜÊóßÁºìÂ≠òÔºàÂèØÈÄâÔºâ\n",
    "def clean_old_cache(backup=True):\n",
    "    \"\"\"Ê∏ÖÁêÜÊóßÁöÑÁºìÂ≠òÊñá‰ª∂\"\"\"\n",
    "    cache_dir = Path(\".mcp_operation_cache\")\n",
    "    \n",
    "    if not cache_dir.exists():\n",
    "        print(\"ÁºìÂ≠òÁõÆÂΩï‰∏çÂ≠òÂú®ÔºåÊó†ÈúÄÊ∏ÖÁêÜ\")\n",
    "        return\n",
    "    \n",
    "    if backup:\n",
    "        # ÂàõÂª∫Â§á‰ªΩ\n",
    "        backup_dir = Path(f\".mcp_operation_cache_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}\")\n",
    "        shutil.copytree(cache_dir, backup_dir)\n",
    "        print(f\"Â∑≤Â§á‰ªΩÊóßÁºìÂ≠òÂà∞: {backup_dir}\")\n",
    "    \n",
    "    # Âà†Èô§ÁâπÂÆöÊñá‰ª∂\n",
    "    files_to_remove = [\n",
    "        \"operation_index.pkl\",\n",
    "        \"llm_operation_definitions.json\",\n",
    "        \"embedding_cache.pkl\"\n",
    "    ]\n",
    "    \n",
    "    for file_name in files_to_remove:\n",
    "        file_path = cache_dir / file_name\n",
    "        if file_path.exists():\n",
    "            file_path.unlink()\n",
    "            print(f\"Â∑≤Âà†Èô§: {file_path}\")\n",
    "    \n",
    "    print(\"ÁºìÂ≠òÊ∏ÖÁêÜÂÆåÊàê\")\n",
    "\n",
    "# ËøêË°åÊ∏ÖÁêÜÔºàÊ†πÊçÆÈúÄË¶ÅÂèñÊ∂àÊ≥®ÈáäÔºâ\n",
    "# clean_old_cache(backup=True)\n",
    "\n",
    "# %% ÂàùÂßãÂåñ Operation Embedding Index\n",
    "from operation_embedding_index import OperationEmbeddingIndex, get_operation_index\n",
    "\n",
    "print(\"ÂàùÂßãÂåñ Operation Embedding Index...\")\n",
    "# ÂàõÂª∫Êñ∞ÂÆû‰æãÔºåÂº∫Âà∂ÈáçÂª∫Á¥¢Âºï\n",
    "index = OperationEmbeddingIndex(use_cache=True)\n",
    "\n",
    "# %% Êü•ÁúãÁîüÊàêÁöÑÊìç‰ΩúÂÆö‰πâ\n",
    "print(\"\\n=== Êìç‰ΩúÂÆö‰πâÁªüËÆ° ===\")\n",
    "print(f\"ÊÄªÊìç‰ΩúÊï∞: {len(index.operation_definitions)}\")\n",
    "print(\"\\nÊåâÁ±ªÂà´ÂàÜÁªÑ:\")\n",
    "\n",
    "# ÁªüËÆ°ÂêÑÁ±ªÂà´ÁöÑÊìç‰ΩúÊï∞\n",
    "category_stats = {}\n",
    "for op_name, op_def in index.operation_definitions.items():\n",
    "    category = op_def.get('category', 'unknown')\n",
    "    if category not in category_stats:\n",
    "        category_stats[category] = []\n",
    "    category_stats[category].append(op_name)\n",
    "\n",
    "for category, operations in sorted(category_stats.items()):\n",
    "    print(f\"\\n{category.upper()} ({len(operations)} operations):\")\n",
    "    for op in sorted(operations):\n",
    "        print(f\"  - {op}\")\n",
    "\n",
    "# %% Êü•ÁúãËØ¶ÁªÜÁöÑÊìç‰ΩúÂÆö‰πâ\n",
    "print(\"\\n=== ËØ¶ÁªÜÊìç‰ΩúÂÆö‰πâÁ§∫‰æã ===\")\n",
    "\n",
    "# ÊòæÁ§∫ÊØè‰∏™Á±ªÂà´ÁöÑ‰∏Ä‰∏™Á§∫‰æã\n",
    "for category, operations in category_stats.items():\n",
    "    if operations:\n",
    "        op_name = operations[0]\n",
    "        op_def = index.operation_definitions[op_name]\n",
    "        print(f\"\\n{category.upper()} Á§∫‰æã - {op_name}:\")\n",
    "        print(f\"  ÊèèËø∞: {op_def['description']}\")\n",
    "        print(f\"  Âêå‰πâËØç: {', '.join(op_def['synonyms'][:5])}...\")\n",
    "        if 'related_operations' in op_def:\n",
    "            print(f\"  Áõ∏ÂÖ≥Êìç‰Ωú: {', '.join(op_def.get('related_operations', [])[:3])}...\")\n",
    "\n",
    "# %% ÊµãËØïËØ≠‰πâÊêúÁ¥¢ÂäüËÉΩ\n",
    "print(\"\\n=== ÊµãËØïËØ≠‰πâÊêúÁ¥¢ ===\")\n",
    "\n",
    "test_queries = [\n",
    "    \"read data from csv file\",\n",
    "    \"validate json schema\",\n",
    "    \"merge multiple datasets\",\n",
    "    \"connect to REST API\",\n",
    "    \"calculate statistical metrics\",\n",
    "    \"save results to database\",\n",
    "    \"monitor system performance\",\n",
    "    \"parse XML documents\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"\\nÊü•ËØ¢: '{query}'\")\n",
    "    results = index.search_operation(query, k=3)\n",
    "    for op_name, score in results:\n",
    "        op_def = index.operation_embeddings.get(op_name)\n",
    "        if op_def:\n",
    "            print(f\"  - {op_name} ({op_def.category}): {score:.3f}\")\n",
    "\n",
    "# %% ‰øùÂ≠òÁºìÂ≠òÁä∂ÊÄÅ\n",
    "print(\"\\n=== ÁºìÂ≠òÊñá‰ª∂Áä∂ÊÄÅ ===\")\n",
    "\n",
    "cache_dir = Path(\".mcp_operation_cache\")\n",
    "if cache_dir.exists():\n",
    "    for file_path in cache_dir.iterdir():\n",
    "        if file_path.is_file():\n",
    "            size_kb = file_path.stat().st_size / 1024\n",
    "            print(f\"{file_path.name}: {size_kb:.2f} KB\")\n",
    "\n",
    "# %% È™åËØÅ LLM Â¢ûÂº∫ÁöÑÂÆö‰πâ\n",
    "llm_def_path = Path(\".mcp_operation_cache/llm_operation_definitions.json\")\n",
    "if llm_def_path.exists():\n",
    "    with open(llm_def_path, 'r') as f:\n",
    "        llm_definitions = json.load(f)\n",
    "    \n",
    "    print(f\"\\n=== LLM Â¢ûÂº∫ÂÆö‰πâÈ™åËØÅ ===\")\n",
    "    print(f\"ÊÄªÊìç‰ΩúÊï∞: {len(llm_definitions)}\")\n",
    "    \n",
    "    # Êü•ÊâæÊñ∞Â¢ûÁöÑÊìç‰ΩúÔºà‰∏çÂú®Âü∫Á°ÄÂÆö‰πâ‰∏≠ÁöÑÔºâ\n",
    "    base_operations = {'read', 'validate', 'transform', 'aggregate', 'write', 'calculate', 'connect', 'log'}\n",
    "    new_operations = [op for op in llm_definitions.keys() if op not in base_operations]\n",
    "    \n",
    "    if new_operations:\n",
    "        print(f\"\\nLLM ÁîüÊàêÁöÑÊñ∞Êìç‰Ωú ({len(new_operations)}):\")\n",
    "        for op in new_operations[:10]:  # Âè™ÊòæÁ§∫Ââç10‰∏™\n",
    "            print(f\"  - {op}: {llm_definitions[op]['description'][:60]}...\")\n",
    "\n",
    "# %% ÂØºÂá∫ÁªüËÆ°‰ø°ÊÅØ\n",
    "stats = index.get_stats()\n",
    "print(f\"\\n=== Á¥¢ÂºïÁªüËÆ°‰ø°ÊÅØ ===\")\n",
    "print(json.dumps(stats, indent=2))\n",
    "\n",
    "# %% ÊâãÂä®Ëß¶ÂèëÁºìÂ≠ò‰øùÂ≠ò\n",
    "print(\"\\n‰øùÂ≠òÊâÄÊúâÁºìÂ≠ò...\")\n",
    "index._save_embedding_cache()\n",
    "cache_path = index._get_cache_path()\n",
    "index.save_index(cache_path)\n",
    "print(\"ÁºìÂ≠ò‰øùÂ≠òÂÆåÊàêÔºÅ\")\n",
    "\n",
    "# %% ÂàõÂª∫Êìç‰ΩúÂÆö‰πâÁöÑÂèØËØªÊä•Âëä\n",
    "report_path = Path(\"operation_definitions_report.json\")\n",
    "report = {\n",
    "    \"generated_at\": datetime.now().isoformat(),\n",
    "    \"total_operations\": len(index.operation_definitions),\n",
    "    \"categories\": category_stats,\n",
    "    \"definitions\": index.operation_definitions,\n",
    "    \"statistics\": stats\n",
    "}\n",
    "\n",
    "with open(report_path, 'w') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "print(f\"\\nÊìç‰ΩúÂÆö‰πâÊä•ÂëäÂ∑≤‰øùÂ≠òÂà∞: {report_path}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## ‰∏ã‰∏ÄÊ≠•\n",
    "# \n",
    "# 1. Ê£ÄÊü•ÁîüÊàêÁöÑÁºìÂ≠òÊñá‰ª∂ÊòØÂê¶Ê≠£Á°Æ\n",
    "# 2. ËøêË°åÂÖ∂‰ªñ‰ΩøÁî® operation index ÁöÑÊ®°ÂùóÔºåÁ°ÆËÆ§ÂÖºÂÆπÊÄß\n",
    "# 3. Â¶ÇÊûúÈúÄË¶ÅÈáçÊñ∞ÁîüÊàêÔºåÂèØ‰ª•Âà†Èô§ÁºìÂ≠òÊñá‰ª∂Âπ∂ÈáçÊñ∞ËøêË°åÊ≠§ notebook\n",
    "# \n",
    "# ### ÁºìÂ≠òÊñá‰ª∂‰ΩçÁΩÆ\n",
    "# - `.mcp_operation_cache/operation_index.pkl` - ‰∏ªÁ¥¢ÂºïÊñá‰ª∂\n",
    "# - `.mcp_operation_cache/llm_operation_definitions.json` - LLM Â¢ûÂº∫ÁöÑÂÆö‰πâ\n",
    "# - `.mcp_operation_cache/embedding_cache.pkl` - ÂµåÂÖ•ÂêëÈáèÁºìÂ≠ò"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993a260c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Ê£ÄÊü•OpenAI API‰ΩôÈ¢ùÂíå‰ΩøÁî®ÊÉÖÂÜµ\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "from openai import OpenAI\n",
    "\n",
    "def check_openai_usage():\n",
    "    \"\"\"Ê£ÄÊü•OpenAI APIÁöÑ‰ΩøÁî®ÊÉÖÂÜµ\"\"\"\n",
    "    \n",
    "    # Ëé∑ÂèñAPIÂØÜÈí•\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"ÈîôËØØÔºöÊú™ÊâæÂà∞OPENAI_API_KEYÁéØÂ¢ÉÂèòÈáè\")\n",
    "        return\n",
    "    \n",
    "    # ËÆæÁΩÆËØ∑Ê±ÇÂ§¥\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # Ëé∑ÂèñÂΩìÂâçÊó•Êúü\n",
    "    today = datetime.now()\n",
    "    start_date = (today - timedelta(days=30)).strftime(\"%Y-%m-%d\")\n",
    "    end_date = today.strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    print(f\"Ê≠£Âú®Ê£ÄÊü• {start_date} Âà∞ {end_date} ÁöÑ‰ΩøÁî®ÊÉÖÂÜµ...\\n\")\n",
    "    \n",
    "    # Ê£ÄÊü•ËÆ¢ÈòÖ‰ø°ÊÅØ\n",
    "    try:\n",
    "        # Ê≥®ÊÑèÔºöOpenAIÂèØËÉΩ‰ºöÊõ¥ÊîπËøô‰∫õÁ´ØÁÇπ\n",
    "        subscription_url = \"https://api.openai.com/v1/dashboard/billing/subscription\"\n",
    "        response = requests.get(subscription_url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            print(\"=== ËÆ¢ÈòÖ‰ø°ÊÅØ ===\")\n",
    "            print(f\"ËÆ°ÂàíÁ±ªÂûã: {data.get('plan', {}).get('title', 'Unknown')}\")\n",
    "            print(f\"Á°¨ÈôêÂà∂: ${data.get('hard_limit_usd', 0):.2f}\")\n",
    "            print(f\"ËΩØÈôêÂà∂: ${data.get('soft_limit_usd', 0):.2f}\")\n",
    "            print(f\"Á≥ªÁªüÁ°¨ÈôêÂà∂: ${data.get('system_hard_limit_usd', 0):.2f}\")\n",
    "        else:\n",
    "            print(f\"Êó†Ê≥ïËé∑ÂèñËÆ¢ÈòÖ‰ø°ÊÅØ: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ëé∑ÂèñËÆ¢ÈòÖ‰ø°ÊÅØÊó∂Âá∫Èîô: {e}\")\n",
    "    \n",
    "    # Ê£ÄÊü•‰ΩøÁî®ÊÉÖÂÜµ\n",
    "    try:\n",
    "        usage_url = f\"https://api.openai.com/v1/dashboard/billing/usage?start_date={start_date}&end_date={end_date}\"\n",
    "        response = requests.get(usage_url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            total_usage = data.get('total_usage', 0) / 100  # ËΩ¨Êç¢‰∏∫ÁæéÂÖÉ\n",
    "            \n",
    "            print(f\"\\n=== ÊúÄËøë30Â§©‰ΩøÁî®ÊÉÖÂÜµ ===\")\n",
    "            print(f\"ÊÄª‰ΩøÁî®ÈáëÈ¢ù: ${total_usage:.2f}\")\n",
    "            \n",
    "            # ÊòæÁ§∫ÊØèÊó•‰ΩøÁî®ÊÉÖÂÜµÔºàÊúÄËøë7Â§©Ôºâ\n",
    "            daily_costs = data.get('daily_costs', [])\n",
    "            if daily_costs:\n",
    "                print(\"\\nÊúÄËøë7Â§©ËØ¶ÁªÜ‰ΩøÁî®:\")\n",
    "                for day in daily_costs[-7:]:\n",
    "                    date = day.get('timestamp')\n",
    "                    cost = day.get('line_items', [{}])[0].get('cost', 0) / 100\n",
    "                    print(f\"  {date}: ${cost:.2f}\")\n",
    "        else:\n",
    "            print(f\"Êó†Ê≥ïËé∑Âèñ‰ΩøÁî®ÊÉÖÂÜµ: {response.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Ëé∑Âèñ‰ΩøÁî®ÊÉÖÂÜµÊó∂Âá∫Èîô: {e}\")\n",
    "\n",
    "def test_api_with_simple_request():\n",
    "    \"\"\"Áî®‰∏Ä‰∏™ÁÆÄÂçïÁöÑËØ∑Ê±ÇÊµãËØïAPIÊòØÂê¶Ê≠£Â∏∏Â∑•‰Ωú\"\"\"\n",
    "    print(\"\\n=== ÊµãËØïAPIËøûÊé• ===\")\n",
    "    \n",
    "    try:\n",
    "        client = OpenAI()\n",
    "        \n",
    "        # ÂèëÈÄÅ‰∏Ä‰∏™ÈùûÂ∏∏Â∞èÁöÑËØ∑Ê±Ç\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",  # ‰ΩøÁî®‰æøÂÆúÁöÑÊ®°Âûã\n",
    "            messages=[{\"role\": \"user\", \"content\": \"Hi\"}],\n",
    "            max_tokens=1,\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        print(\"‚úÖ APIËøûÊé•Ê≠£Â∏∏ÔºÅ\")\n",
    "        print(f\"ÂìçÂ∫î: {response.choices[0].message.content}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå APIÊµãËØïÂ§±Ë¥•: {type(e).__name__}\")\n",
    "        print(f\"ÈîôËØØ‰ø°ÊÅØ: {str(e)}\")\n",
    "        \n",
    "        # Âà§Êñ≠ÈîôËØØÁ±ªÂûã\n",
    "        error_str = str(e).lower()\n",
    "        if \"insufficient_quota\" in error_str or \"exceeded your current quota\" in error_str:\n",
    "            print(\"\\n‚ö†Ô∏è ËØäÊñ≠: ÊÇ®ÁöÑAPIÈ¢ùÂ∫¶Â∑≤Áî®ÂÆåÔºåÈúÄË¶ÅÂÖÖÂÄºÔºÅ\")\n",
    "        elif \"invalid api key\" in error_str or \"incorrect api key\" in error_str:\n",
    "            print(\"\\n‚ö†Ô∏è ËØäÊñ≠: APIÂØÜÈí•Êó†ÊïàÔºÅ\")\n",
    "        elif \"502\" in error_str or \"bad gateway\" in error_str:\n",
    "            print(\"\\n‚ö†Ô∏è ËØäÊñ≠: OpenAIÊúçÂä°Âô®‰∏¥Êó∂ÊïÖÈöúÔºå‰∏é‰ΩôÈ¢ùÊó†ÂÖ≥\")\n",
    "        else:\n",
    "            print(\"\\n‚ö†Ô∏è ËØäÊñ≠: ÂÖ∂‰ªñÈîôËØØÔºåËØ∑Ê£ÄÊü•ÁΩëÁªúËøûÊé•ÂíåAPIËÆæÁΩÆ\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"‰∏ªÂáΩÊï∞\"\"\"\n",
    "    print(\"OpenAI API Áä∂ÊÄÅÊ£ÄÊü•Â∑•ÂÖ∑\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # 1. Ê£ÄÊü•ÁéØÂ¢ÉÂèòÈáè\n",
    "    api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    if not api_key:\n",
    "        print(\"‚ùå ÈîôËØØÔºöÊú™ËÆæÁΩÆOPENAI_API_KEYÁéØÂ¢ÉÂèòÈáè\")\n",
    "        print(\"\\nËØ∑ËøêË°åÔºöexport OPENAI_API_KEY='your-api-key'\")\n",
    "        return\n",
    "    \n",
    "    print(f\"‚úÖ ÊâæÂà∞APIÂØÜÈí• (Ââç8‰Ωç: {api_key[:8]}...)\")\n",
    "    \n",
    "    # 2. Ê£ÄÊü•‰ΩøÁî®ÊÉÖÂÜµ\n",
    "    check_openai_usage()\n",
    "    \n",
    "    # 3. ÊµãËØïAPI\n",
    "    test_api_with_simple_request()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"Ê£ÄÊü•ÂÆåÊàêÔºÅ\")\n",
    "    \n",
    "    # ÁªôÂá∫Âª∫ËÆÆ\n",
    "    print(\"\\nüí° Âª∫ËÆÆÔºö\")\n",
    "    print(\"1. Â¶ÇÊûúÊòØ‰ΩôÈ¢ù‰∏çË∂≥ÔºåËØ∑ÂâçÂæÄ https://platform.openai.com/account/billing ÂÖÖÂÄº\")\n",
    "    print(\"2. Â¶ÇÊûúÊòØ502ÈîôËØØÔºåËØ∑Á≠âÂæÖÂá†ÂàÜÈíüÂêéÈáçËØï\")\n",
    "    print(\"3. Êü•ÁúãOpenAIÊúçÂä°Áä∂ÊÄÅ: https://status.openai.com/\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c17416d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Âπ∂Ë°åÂà†Èô§ÊâÄÊúâ‰ªªÂä°Â∫ì‰∏≠ÁöÑ original_description Â≠óÊÆµ\n",
    "‰ΩøÁî® ThreadPoolExecutor È´òÊïàÂ§ÑÁêÜÂ§ö‰∏™Êñá‰ª∂\n",
    "\n",
    "‰ΩøÁî®ÊñπÊ≥ï:\n",
    "    # Â§ÑÁêÜÈªòËÆ§ÁõÆÂΩï (./mcp_generated_library/difficulty_versions)\n",
    "    python remove_original_descriptions.py\n",
    "    \n",
    "    # Â§ÑÁêÜÊåáÂÆöÁõÆÂΩï\n",
    "    python remove_original_descriptions.py -d ./path/to/tasks\n",
    "    \n",
    "    # ÂêåÊó∂Â§ÑÁêÜÁà∂ÁõÆÂΩïÁöÑ‰ªªÂä°Êñá‰ª∂\n",
    "    python remove_original_descriptions.py -p\n",
    "    \n",
    "    # Ëá™ÂÆö‰πâÂ∑•‰ΩúÁ∫øÁ®ãÊï∞\n",
    "    python remove_original_descriptions.py -w 20\n",
    "\"\"\"\n",
    "\n",
    "import json\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "from datetime import datetime\n",
    "\n",
    "# ËÆæÁΩÆÊó•Âøó\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def process_single_file(file_path: Path) -> Tuple[str, int, int]:\n",
    "    \"\"\"\n",
    "    Â§ÑÁêÜÂçï‰∏™‰ªªÂä°Êñá‰ª∂ÔºåÂà†Èô§ÊâÄÊúâ‰ªªÂä°ÁöÑ original_description Â≠óÊÆµ\n",
    "    \n",
    "    Args:\n",
    "        file_path: ‰ªªÂä°Êñá‰ª∂Ë∑ØÂæÑ\n",
    "        \n",
    "    Returns:\n",
    "        (Êñá‰ª∂Âêç, ÂéüÂßã‰ªªÂä°Êï∞, ‰øÆÊîπÁöÑ‰ªªÂä°Êï∞)\n",
    "    \"\"\"\n",
    "    logger.info(f\"Processing file: {file_path}\")\n",
    "    \n",
    "    # ËØªÂèñÊñá‰ª∂\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # ÁªüËÆ°‰ø°ÊÅØ\n",
    "    total_tasks = 0\n",
    "    modified_tasks = 0\n",
    "    \n",
    "    # Â§ÑÁêÜ‰∏çÂêåÁöÑÊï∞ÊçÆÊ†ºÂºè\n",
    "    if isinstance(data, dict):\n",
    "        if 'tasks' in data:\n",
    "            # Ê†ºÂºè: {\"tasks\": [...]}\n",
    "            tasks = data['tasks']\n",
    "            total_tasks = len(tasks)\n",
    "            \n",
    "            for task in tasks:\n",
    "                if 'original_description' in task:\n",
    "                    del task['original_description']\n",
    "                    modified_tasks += 1\n",
    "                    \n",
    "        else:\n",
    "            # ÂèØËÉΩÊòØÂÖ∂‰ªñÊ†ºÂºèÁöÑÂ≠óÂÖ∏\n",
    "            logger.warning(f\"Unexpected dict format in {file_path}\")\n",
    "            \n",
    "    elif isinstance(data, list):\n",
    "        # Ê†ºÂºè: [task1, task2, ...]\n",
    "        tasks = data\n",
    "        total_tasks = len(tasks)\n",
    "        \n",
    "        for task in tasks:\n",
    "            if isinstance(task, dict) and 'original_description' in task:\n",
    "                del task['original_description']\n",
    "                modified_tasks += 1\n",
    "    \n",
    "    # ‰øùÂ≠ò‰øÆÊîπÂêéÁöÑÊñá‰ª∂\n",
    "    if modified_tasks > 0:\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(data, f, indent=2, ensure_ascii=False)\n",
    "        logger.info(f\"‚úÖ {file_path.name}: Modified {modified_tasks}/{total_tasks} tasks\")\n",
    "    else:\n",
    "        logger.info(f\"‚ÑπÔ∏è {file_path.name}: No original_description fields found\")\n",
    "    \n",
    "    return (file_path.name, total_tasks, modified_tasks)\n",
    "\n",
    "\n",
    "def find_all_task_files(directory: Path) -> List[Path]:\n",
    "    \"\"\"\n",
    "    Êü•ÊâæÁõÆÂΩï‰∏ãÊâÄÊúâÁöÑ‰ªªÂä°Êñá‰ª∂\n",
    "    \n",
    "    Args:\n",
    "        directory: Ë¶ÅÊêúÁ¥¢ÁöÑÁõÆÂΩï\n",
    "        \n",
    "    Returns:\n",
    "        ‰ªªÂä°Êñá‰ª∂Ë∑ØÂæÑÂàóË°®\n",
    "    \"\"\"\n",
    "    task_files = []\n",
    "    \n",
    "    # Êü•ÊâæÊâÄÊúâ .json Êñá‰ª∂\n",
    "    for file_path in directory.glob('*.json'):\n",
    "        # Ë∑≥Ëøá‰∏Ä‰∫õÊòéÊòæ‰∏çÊòØ‰ªªÂä°Êñá‰ª∂ÁöÑ\n",
    "        if any(skip in file_path.name.lower() for skip in ['config', 'registry', 'settings']):\n",
    "            continue\n",
    "        task_files.append(file_path)\n",
    "    \n",
    "    return task_files\n",
    "\n",
    "\n",
    "def parallel_remove_original_descriptions(\n",
    "    directory_path: str = \"./mcp_generated_library/difficulty_versions\",\n",
    "    max_workers: int = 10\n",
    ") -> Dict[str, any]:\n",
    "    \"\"\"\n",
    "    Âπ∂Ë°åÂ§ÑÁêÜÁõÆÂΩï‰∏ãÊâÄÊúâ‰ªªÂä°Êñá‰ª∂ÔºåÂà†Èô§ original_description Â≠óÊÆµ\n",
    "    \n",
    "    Args:\n",
    "        directory_path: ‰ªªÂä°Êñá‰ª∂ÁõÆÂΩïË∑ØÂæÑ\n",
    "        max_workers: ÊúÄÂ§ßÂπ∂Ë°åÂ∑•‰ΩúÁ∫øÁ®ãÊï∞\n",
    "        \n",
    "    Returns:\n",
    "        Â§ÑÁêÜÁªìÊûúÁªüËÆ°\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    directory = Path(directory_path)\n",
    "    \n",
    "    # Á°Æ‰øùÁõÆÂΩïÂ≠òÂú®\n",
    "    if not directory.exists():\n",
    "        print(f\"‚ùå Directory not found: {directory}\")\n",
    "        raise FileNotFoundError(f\"Directory not found: {directory}\")\n",
    "    \n",
    "    # Êü•ÊâæÊâÄÊúâ‰ªªÂä°Êñá‰ª∂\n",
    "    task_files = find_all_task_files(directory)\n",
    "    \n",
    "    if not task_files:\n",
    "        print(f\"‚ö†Ô∏è No task files found in {directory}\")\n",
    "        return {\n",
    "            'total_files': 0,\n",
    "            'total_tasks': 0,\n",
    "            'modified_tasks': 0,\n",
    "            'execution_time': 0\n",
    "        }\n",
    "    \n",
    "    print(f\"\\nüîç Found {len(task_files)} task files to process\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ÁªìÊûúÁªüËÆ°\n",
    "    results = {\n",
    "        'total_files': len(task_files),\n",
    "        'total_tasks': 0,\n",
    "        'modified_tasks': 0,\n",
    "        'file_results': []\n",
    "    }\n",
    "    \n",
    "    # ‰ΩøÁî®Á∫øÁ®ãÊ±†Âπ∂Ë°åÂ§ÑÁêÜ\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        # Êèê‰∫§ÊâÄÊúâ‰ªªÂä°\n",
    "        future_to_file = {\n",
    "            executor.submit(process_single_file, file_path): file_path \n",
    "            for file_path in task_files\n",
    "        }\n",
    "        \n",
    "        # Â§ÑÁêÜÂÆåÊàêÁöÑ‰ªªÂä°\n",
    "        completed = 0\n",
    "        for future in as_completed(future_to_file):\n",
    "            file_path = future_to_file[future]\n",
    "            completed += 1\n",
    "            \n",
    "            # Ëé∑ÂèñÁªìÊûú\n",
    "            file_name, total_tasks, modified_tasks = future.result()\n",
    "            \n",
    "            # Êõ¥Êñ∞ÁªüËÆ°\n",
    "            results['total_tasks'] += total_tasks\n",
    "            results['modified_tasks'] += modified_tasks\n",
    "            results['file_results'].append({\n",
    "                'file': file_name,\n",
    "                'total_tasks': total_tasks,\n",
    "                'modified_tasks': modified_tasks\n",
    "            })\n",
    "            \n",
    "            # ÊòæÁ§∫ËøõÂ∫¶\n",
    "            progress = (completed / len(task_files)) * 100\n",
    "            print(f\"Progress: {completed}/{len(task_files)} files ({progress:.1f}%)\")\n",
    "    \n",
    "    # ËÆ°ÁÆóÊâßË°åÊó∂Èó¥\n",
    "    execution_time = time.time() - start_time\n",
    "    results['execution_time'] = execution_time\n",
    "    \n",
    "    # ÊòæÁ§∫ÊúÄÁªàÁªìÊûú\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"‚ú® Processing Complete!\")\n",
    "    print(f\"üìä Total files processed: {results['total_files']}\")\n",
    "    print(f\"üìã Total tasks processed: {results['total_tasks']}\")\n",
    "    print(f\"‚úèÔ∏è  Tasks modified: {results['modified_tasks']}\")\n",
    "    print(f\"‚è±Ô∏è  Execution time: {execution_time:.2f} seconds\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ‰øùÂ≠òÂ§ÑÁêÜÊä•Âëä\n",
    "    report_path = directory / f\"original_description_removal_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    print(f\"\\nüìÑ Report saved to: {report_path}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    \n",
    "    # ÂëΩ‰ª§Ë°åÂèÇÊï∞Ëß£Êûê\n",
    "    parser = argparse.ArgumentParser(description='Âπ∂Ë°åÂà†Èô§‰ªªÂä°Êñá‰ª∂‰∏≠ÁöÑ original_description Â≠óÊÆµ')\n",
    "    parser.add_argument(\n",
    "        '--directory', '-d',\n",
    "        default='./mcp_generated_library/difficulty_versions',\n",
    "        help='‰ªªÂä°Êñá‰ª∂ÁõÆÂΩïË∑ØÂæÑ (ÈªòËÆ§: ./mcp_generated_library/difficulty_versions)'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--workers', '-w',\n",
    "        type=int,\n",
    "        default=10,\n",
    "        help='ÊúÄÂ§ßÂπ∂Ë°åÂ∑•‰ΩúÁ∫øÁ®ãÊï∞ (ÈªòËÆ§: 10)'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        '--include-parent', '-p',\n",
    "        action='store_true',\n",
    "        help='ÂêåÊó∂Â§ÑÁêÜ‰∏äÁ∫ßÁõÆÂΩïÁöÑ‰ªªÂä°Êñá‰ª∂'\n",
    "    )\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # ÊâßË°å‰∏ªÂáΩÊï∞\n",
    "    try:\n",
    "        # Â§ÑÁêÜÊåáÂÆöÁõÆÂΩï\n",
    "        print(f\"üöÄ Processing directory: {args.directory}\")\n",
    "        results = parallel_remove_original_descriptions(\n",
    "            directory_path=args.directory,\n",
    "            max_workers=args.workers\n",
    "        )\n",
    "        \n",
    "        # Â¶ÇÊûúÈúÄË¶ÅÔºå‰πüÂ§ÑÁêÜ‰∏äÁ∫ßÁõÆÂΩï\n",
    "        if args.include_parent:\n",
    "            parent_dir = Path(args.directory).parent\n",
    "            print(f\"\\nüöÄ Processing parent directory: {parent_dir}\")\n",
    "            parent_results = parallel_remove_original_descriptions(\n",
    "                directory_path=str(parent_dir),\n",
    "                max_workers=args.workers\n",
    "            )\n",
    "            \n",
    "            # ÂêàÂπ∂ÁªìÊûú\n",
    "            results['total_files'] += parent_results['total_files']\n",
    "            results['total_tasks'] += parent_results['total_tasks']\n",
    "            results['modified_tasks'] += parent_results['modified_tasks']\n",
    "            results['file_results'].extend(parent_results['file_results'])\n",
    "        \n",
    "        # ÊòæÁ§∫ÊØè‰∏™Êñá‰ª∂ÁöÑËØ¶ÁªÜÁªìÊûú\n",
    "        print(\"\\nüìã Detailed Results:\")\n",
    "        print(\"-\" * 60)\n",
    "        for file_result in results['file_results']:\n",
    "            if file_result['modified_tasks'] > 0:\n",
    "                print(f\"‚úÖ {file_result['file']}: {file_result['modified_tasks']}/{file_result['total_tasks']} tasks modified\")\n",
    "            else:\n",
    "                print(f\"‚ÑπÔ∏è  {file_result['file']}: No modifications needed\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        logger.error(f\"‚ùå Error occurred: {str(e)}\")\n",
    "        raise  # Áõ¥Êé•ÊäõÂá∫ÂºÇÂ∏∏ÔºåÊñπ‰æøË∞ÉËØï"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4648af3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ËÆæÁΩÆÁéØÂ¢ÉÂèòÈáèÔºàÂú®ËøêË°å‰ª£Á†ÅÂâçÔºâ\n",
    "import os\n",
    "\n",
    "# ÁÑ∂ÂêéËøêË°åÊÇ®ÁöÑ‰ª£Á†Å\n",
    "from tool_and_task_generator import parallel_generate_tasks_from_existing_tools\n",
    "\n",
    "results = parallel_generate_tasks_from_existing_tools(\n",
    "    tool_registry_path=\"mcp_generated_library/tool_registry_consolidated.json\",\n",
    "    num_tasks=1000,\n",
    "    task_distribution={\n",
    "        'basic_task': 0.20,\n",
    "        'simple_task': 0.20,\n",
    "        'data_pipeline': 0.2,\n",
    "        'api_integration': 0.2,\n",
    "        'multi_stage_pipeline': 0.20\n",
    "    },\n",
    "    use_llm=True,  # Á°Æ‰øùÂêØÁî®LLM\n",
    "    max_workers=1000,\n",
    "    show_progress=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "852bffeb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Êâ´ÊèèÁõÆÂΩï: mcp_generated_library/difficulty_versions\n",
      "üìÅ ÊâæÂà∞ 8 ‰∏™ JSON Êñá‰ª∂\n",
      "==================================================\n",
      "\n",
      "üìÑ Â§ÑÁêÜÊñá‰ª∂: task_library_enhanced_v3_easy.json\n",
      "  ‚úì ÊâæÂà∞ 630 ‰∏™‰ªªÂä°\n",
      "\n",
      "üìÑ Â§ÑÁêÜÊñá‰ª∂: task_library_enhanced_v3_easy_biased.json\n",
      "  ‚úì ÊâæÂà∞ 630 ‰∏™‰ªªÂä°\n",
      "\n",
      "üìÑ Â§ÑÁêÜÊñá‰ª∂: task_library_enhanced_v3_hard.json\n",
      "  ‚úì ÊâæÂà∞ 630 ‰∏™‰ªªÂä°\n",
      "\n",
      "üìÑ Â§ÑÁêÜÊñá‰ª∂: task_library_enhanced_v3_hard_biased.json\n",
      "  ‚úì ÊâæÂà∞ 630 ‰∏™‰ªªÂä°\n",
      "\n",
      "üìÑ Â§ÑÁêÜÊñá‰ª∂: task_library_enhanced_v3_medium.json\n",
      "  ‚úì ÊâæÂà∞ 630 ‰∏™‰ªªÂä°\n",
      "\n",
      "üìÑ Â§ÑÁêÜÊñá‰ª∂: task_library_enhanced_v3_medium_biased.json\n",
      "  ‚úì ÊâæÂà∞ 630 ‰∏™‰ªªÂä°\n",
      "\n",
      "üìÑ Â§ÑÁêÜÊñá‰ª∂: task_library_enhanced_v3_very_easy.json\n",
      "  ‚úì ÊâæÂà∞ 630 ‰∏™‰ªªÂä°\n",
      "\n",
      "üìÑ Â§ÑÁêÜÊñá‰ª∂: task_library_enhanced_v3_very_hard.json\n",
      "  ‚úì ÊâæÂà∞ 630 ‰∏™‰ªªÂä°\n",
      "\n",
      "==================================================\n",
      "üìä ÂêàÂπ∂ÁªüËÆ°:\n",
      "  ÊÄªÊñá‰ª∂Êï∞: 8\n",
      "  ÊÄª‰ªªÂä°Êï∞: 5040\n",
      "  ÂéªÈáçÂêé‰ªªÂä°Êï∞: 5040\n",
      "  ÈáçÂ§ç‰ªªÂä°Êï∞: 0\n",
      "\n",
      "‚úÖ ÂêàÂπ∂ÂÆåÊàêÔºÅ\n",
      "üìÅ ËæìÂá∫Êñá‰ª∂: mcp_generated_library/task_library_all_difficulties.json\n",
      "\n",
      "üìä ËØ¶ÁªÜÁªüËÆ°:\n",
      "\n",
      "ÊåâÊñá‰ª∂ÁªüËÆ°:\n",
      "  task_library_enhanced_v3_easy.json: 630 ‰∏™‰ªªÂä°\n",
      "  task_library_enhanced_v3_easy_biased.json: 630 ‰∏™‰ªªÂä°\n",
      "  task_library_enhanced_v3_hard.json: 630 ‰∏™‰ªªÂä°\n",
      "  task_library_enhanced_v3_hard_biased.json: 630 ‰∏™‰ªªÂä°\n",
      "  task_library_enhanced_v3_medium.json: 630 ‰∏™‰ªªÂä°\n",
      "  task_library_enhanced_v3_medium_biased.json: 630 ‰∏™‰ªªÂä°\n",
      "  task_library_enhanced_v3_very_easy.json: 630 ‰∏™‰ªªÂä°\n",
      "  task_library_enhanced_v3_very_hard.json: 630 ‰∏™‰ªªÂä°\n",
      "\n",
      "Êåâ‰ªªÂä°Á±ªÂûãÁªüËÆ°:\n",
      "  api_integration: 1360 ‰∏™‰ªªÂä°\n",
      "  basic_task: 1200 ‰∏™‰ªªÂä°\n",
      "  data_pipeline: 1520 ‰∏™‰ªªÂä°\n",
      "  multi_stage_pipeline: 640 ‰∏™‰ªªÂä°\n",
      "  simple_task: 320 ‰∏™‰ªªÂä°\n",
      "\n",
      "ÊåâÂ§çÊùÇÂ∫¶ÁªüËÆ°:\n",
      "  easy: 1520 ‰∏™‰ªªÂä°\n",
      "  hard: 640 ‰∏™‰ªªÂä°\n",
      "  medium: 2880 ‰∏™‰ªªÂä°\n",
      "\n",
      "ÊåâÈöæÂ∫¶ÁâàÊú¨ÁªüËÆ°:\n",
      "  easy: 630 ‰∏™‰ªªÂä°\n",
      "  easy_biased: 630 ‰∏™‰ªªÂä°\n",
      "  hard: 1260 ‰∏™‰ªªÂä°\n",
      "  hard_biased: 630 ‰∏™‰ªªÂä°\n",
      "  medium: 630 ‰∏™‰ªªÂä°\n",
      "  medium_biased: 630 ‰∏™‰ªªÂä°\n",
      "  very_easy: 630 ‰∏™‰ªªÂä°\n",
      "\n",
      "üìã Á§∫‰æã‰ªªÂä°ÔºàÂâç5‰∏™Ôºâ:\n",
      "\n",
      "‰ªªÂä° 1:\n",
      "  ID: N/A\n",
      "  Á±ªÂûã: basic_task\n",
      "  Â§çÊùÇÂ∫¶: easy\n",
      "  ÈöæÂ∫¶ÁâàÊú¨: easy\n",
      "  Êù•Ê∫êÊñá‰ª∂: task_library_enhanced_v3_easy.json\n",
      "\n",
      "‰ªªÂä° 2:\n",
      "  ID: N/A\n",
      "  Á±ªÂûã: basic_task\n",
      "  Â§çÊùÇÂ∫¶: easy\n",
      "  ÈöæÂ∫¶ÁâàÊú¨: easy\n",
      "  Êù•Ê∫êÊñá‰ª∂: task_library_enhanced_v3_easy.json\n",
      "\n",
      "‰ªªÂä° 3:\n",
      "  ID: N/A\n",
      "  Á±ªÂûã: basic_task\n",
      "  Â§çÊùÇÂ∫¶: easy\n",
      "  ÈöæÂ∫¶ÁâàÊú¨: easy\n",
      "  Êù•Ê∫êÊñá‰ª∂: task_library_enhanced_v3_easy.json\n",
      "\n",
      "‰ªªÂä° 4:\n",
      "  ID: N/A\n",
      "  Á±ªÂûã: basic_task\n",
      "  Â§çÊùÇÂ∫¶: easy\n",
      "  ÈöæÂ∫¶ÁâàÊú¨: easy\n",
      "  Êù•Ê∫êÊñá‰ª∂: task_library_enhanced_v3_easy.json\n",
      "\n",
      "‰ªªÂä° 5:\n",
      "  ID: N/A\n",
      "  Á±ªÂûã: basic_task\n",
      "  Â§çÊùÇÂ∫¶: easy\n",
      "  ÈöæÂ∫¶ÁâàÊú¨: easy\n",
      "  Êù•Ê∫êÊñá‰ª∂: task_library_enhanced_v3_easy.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import time\n",
    "\n",
    "def merge_task_libraries(directory_path=\"mcp_generated_library/difficulty_versions\", \n",
    "                        output_file=\"mcp_generated_library/task_library_all_difficulties.json\"):\n",
    "    \"\"\"\n",
    "    ÂêàÂπ∂ difficulty_versions ÁõÆÂΩï‰∏ãÁöÑÊâÄÊúâ JSON Êñá‰ª∂Âà∞‰∏Ä‰∏™Êñá‰ª∂\n",
    "    \n",
    "    Args:\n",
    "        directory_path: ÂåÖÂê´ JSON Êñá‰ª∂ÁöÑÁõÆÂΩïË∑ØÂæÑ\n",
    "        output_file: ËæìÂá∫ÂêàÂπ∂ÂêéÁöÑ JSON Êñá‰ª∂Ë∑ØÂæÑ\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"üîç Êâ´ÊèèÁõÆÂΩï: {directory_path}\")\n",
    "    \n",
    "    # Â≠òÂÇ®ÊâÄÊúâ‰ªªÂä°\n",
    "    all_tasks = []\n",
    "    \n",
    "    # ÁªüËÆ°‰ø°ÊÅØ\n",
    "    stats = defaultdict(int)\n",
    "    file_task_counts = {}\n",
    "    \n",
    "    # Ëé∑ÂèñÁõÆÂΩï‰∏ãÊâÄÊúâ .json Êñá‰ª∂\n",
    "    json_files = list(Path(directory_path).glob(\"*.json\"))\n",
    "    \n",
    "    if not json_files:\n",
    "        print(f\"‚ùå Âú® {directory_path} ‰∏≠Ê≤°ÊúâÊâæÂà∞ JSON Êñá‰ª∂\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üìÅ ÊâæÂà∞ {len(json_files)} ‰∏™ JSON Êñá‰ª∂\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Â§ÑÁêÜÊØè‰∏™ JSON Êñá‰ª∂\n",
    "    for json_file in sorted(json_files):\n",
    "        file_name = json_file.name\n",
    "        print(f\"\\nüìÑ Â§ÑÁêÜÊñá‰ª∂: {file_name}\")\n",
    "        \n",
    "        # ËØªÂèñÊñá‰ª∂\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        \n",
    "        # ÊèêÂèñ‰ªªÂä°\n",
    "        tasks = []\n",
    "        if isinstance(data, dict):\n",
    "            if 'tasks' in data:\n",
    "                tasks = data['tasks']\n",
    "            else:\n",
    "                # Â¶ÇÊûúÊòØÂÖ∂‰ªñÊ†ºÂºèÁöÑÂ≠óÂÖ∏ÔºåÂ∞ùËØïÊèêÂèñÂÄº\n",
    "                for value in data.values():\n",
    "                    if isinstance(value, list):\n",
    "                        tasks.extend(value)\n",
    "        elif isinstance(data, list):\n",
    "            tasks = data\n",
    "        \n",
    "        # ‰∏∫ÊØè‰∏™‰ªªÂä°Ê∑ªÂä†Êù•Ê∫êÊñá‰ª∂‰ø°ÊÅØ\n",
    "        for task in tasks:\n",
    "            # Ê∑ªÂä†Ê∫êÊñá‰ª∂Ê†áËÆ∞\n",
    "            task['source_file'] = file_name\n",
    "            \n",
    "            # ‰ªéÊñá‰ª∂ÂêçÊé®Êñ≠ÈöæÂ∫¶ÁâàÊú¨\n",
    "            if 'very_easy' in file_name:\n",
    "                task['difficulty_version'] = 'very_easy'\n",
    "            elif 'easy_biased' in file_name:\n",
    "                task['difficulty_version'] = 'easy_biased'\n",
    "            elif 'easy' in file_name:\n",
    "                task['difficulty_version'] = 'easy'\n",
    "            elif 'medium_biased' in file_name:\n",
    "                task['difficulty_version'] = 'medium_biased'\n",
    "            elif 'medium' in file_name:\n",
    "                task['difficulty_version'] = 'medium'\n",
    "            elif 'hard_biased' in file_name:\n",
    "                task['difficulty_version'] = 'hard_biased'\n",
    "            elif 'hard' in file_name:\n",
    "                task['difficulty_version'] = 'hard'\n",
    "            elif 'very_hard' in file_name:\n",
    "                task['difficulty_version'] = 'very_hard'\n",
    "            else:\n",
    "                task['difficulty_version'] = 'unknown'\n",
    "            \n",
    "            # ÁªüËÆ°‰ªªÂä°Á±ªÂûã\n",
    "            task_type = task.get('task_type', 'unknown')\n",
    "            stats[f'task_type_{task_type}'] += 1\n",
    "            \n",
    "            # ÁªüËÆ°Â§çÊùÇÂ∫¶\n",
    "            complexity = task.get('complexity', 'unknown')\n",
    "            stats[f'complexity_{complexity}'] += 1\n",
    "            \n",
    "            # ÁªüËÆ°ÈöæÂ∫¶ÁâàÊú¨\n",
    "            stats[f'difficulty_{task[\"difficulty_version\"]}'] += 1\n",
    "        \n",
    "        # Ê∑ªÂä†Âà∞ÊÄª‰ªªÂä°ÂàóË°®\n",
    "        all_tasks.extend(tasks)\n",
    "        file_task_counts[file_name] = len(tasks)\n",
    "        \n",
    "        print(f\"  ‚úì ÊâæÂà∞ {len(tasks)} ‰∏™‰ªªÂä°\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"üìä ÂêàÂπ∂ÁªüËÆ°:\")\n",
    "    print(f\"  ÊÄªÊñá‰ª∂Êï∞: {len(json_files)}\")\n",
    "    print(f\"  ÊÄª‰ªªÂä°Êï∞: {len(all_tasks)}\")\n",
    "    \n",
    "    # ÂéªÈáçÔºàÂü∫‰∫é‰ªªÂä°IDÔºâ\n",
    "    unique_tasks = {}\n",
    "    duplicate_count = 0\n",
    "    \n",
    "    for task in all_tasks:\n",
    "        task_id = task.get('id', None)\n",
    "        if task_id:\n",
    "            if task_id not in unique_tasks:\n",
    "                unique_tasks[task_id] = task\n",
    "            else:\n",
    "                duplicate_count += 1\n",
    "                # Â¶ÇÊûúÊòØÈáçÂ§ç‰ªªÂä°ÔºåËÆ∞ÂΩïÊâÄÊúâÂá∫Áé∞ÁöÑÊñá‰ª∂\n",
    "                if 'all_source_files' not in unique_tasks[task_id]:\n",
    "                    unique_tasks[task_id]['all_source_files'] = [unique_tasks[task_id].get('source_file')]\n",
    "                unique_tasks[task_id]['all_source_files'].append(task.get('source_file'))\n",
    "        else:\n",
    "            # Ê≤°ÊúâIDÁöÑ‰ªªÂä°Ôºå‰ΩøÁî®ÂÖ∂‰ªñÂ≠óÊÆµÁîüÊàêÂîØ‰∏ÄÊ†áËØÜ\n",
    "            unique_key = f\"{task.get('task_type', 'unknown')}_{task.get('complexity', 'unknown')}_{len(unique_tasks)}\"\n",
    "            unique_tasks[unique_key] = task\n",
    "    \n",
    "    print(f\"  ÂéªÈáçÂêé‰ªªÂä°Êï∞: {len(unique_tasks)}\")\n",
    "    print(f\"  ÈáçÂ§ç‰ªªÂä°Êï∞: {duplicate_count}\")\n",
    "    \n",
    "    # ÂàõÂª∫ÊúÄÁªàÁöÑÊï∞ÊçÆÁªìÊûÑ\n",
    "    merged_data = {\n",
    "        \"metadata\": {\n",
    "            \"generated_at\": time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            \"source_directory\": directory_path,\n",
    "            \"total_files\": len(json_files),\n",
    "            \"total_tasks\": len(all_tasks),\n",
    "            \"unique_tasks\": len(unique_tasks),\n",
    "            \"duplicates_removed\": duplicate_count,\n",
    "            \"files_processed\": list(file_task_counts.keys())\n",
    "        },\n",
    "        \"statistics\": {\n",
    "            \"by_file\": file_task_counts,\n",
    "            \"by_task_type\": {k.replace('task_type_', ''): v for k, v in stats.items() if k.startswith('task_type_')},\n",
    "            \"by_complexity\": {k.replace('complexity_', ''): v for k, v in stats.items() if k.startswith('complexity_')},\n",
    "            \"by_difficulty_version\": {k.replace('difficulty_', ''): v for k, v in stats.items() if k.startswith('difficulty_')}\n",
    "        },\n",
    "        \"tasks\": list(unique_tasks.values())\n",
    "    }\n",
    "    \n",
    "    # ‰øùÂ≠òÂêàÂπ∂ÂêéÁöÑÊñá‰ª∂\n",
    "    output_path = Path(output_file)\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(merged_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\n‚úÖ ÂêàÂπ∂ÂÆåÊàêÔºÅ\")\n",
    "    print(f\"üìÅ ËæìÂá∫Êñá‰ª∂: {output_file}\")\n",
    "    \n",
    "    # ÊòæÁ§∫ËØ¶ÁªÜÁªüËÆ°\n",
    "    print(\"\\nüìä ËØ¶ÁªÜÁªüËÆ°:\")\n",
    "    print(\"\\nÊåâÊñá‰ª∂ÁªüËÆ°:\")\n",
    "    for file_name, count in sorted(file_task_counts.items()):\n",
    "        print(f\"  {file_name}: {count} ‰∏™‰ªªÂä°\")\n",
    "    \n",
    "    print(\"\\nÊåâ‰ªªÂä°Á±ªÂûãÁªüËÆ°:\")\n",
    "    for task_type, count in sorted(merged_data['statistics']['by_task_type'].items()):\n",
    "        print(f\"  {task_type}: {count} ‰∏™‰ªªÂä°\")\n",
    "    \n",
    "    print(\"\\nÊåâÂ§çÊùÇÂ∫¶ÁªüËÆ°:\")\n",
    "    for complexity, count in sorted(merged_data['statistics']['by_complexity'].items()):\n",
    "        print(f\"  {complexity}: {count} ‰∏™‰ªªÂä°\")\n",
    "    \n",
    "    print(\"\\nÊåâÈöæÂ∫¶ÁâàÊú¨ÁªüËÆ°:\")\n",
    "    for difficulty, count in sorted(merged_data['statistics']['by_difficulty_version'].items()):\n",
    "        print(f\"  {difficulty}: {count} ‰∏™‰ªªÂä°\")\n",
    "    \n",
    "    return merged_data\n",
    "\n",
    "# ÊâßË°åÂêàÂπ∂\n",
    "if __name__ == \"__main__\":\n",
    "    # ‰ΩøÁî®ÈªòËÆ§ÂèÇÊï∞ËøêË°å\n",
    "    result = merge_task_libraries()\n",
    "    \n",
    "    # ÂèØÈÄâÔºöÊòæÁ§∫Ââç5‰∏™‰ªªÂä°‰Ωú‰∏∫Á§∫‰æã\n",
    "    if result and 'tasks' in result:\n",
    "        print(\"\\nüìã Á§∫‰æã‰ªªÂä°ÔºàÂâç5‰∏™Ôºâ:\")\n",
    "        for i, task in enumerate(result['tasks'][:5]):\n",
    "            print(f\"\\n‰ªªÂä° {i+1}:\")\n",
    "            print(f\"  ID: {task.get('id', 'N/A')}\")\n",
    "            print(f\"  Á±ªÂûã: {task.get('task_type', 'N/A')}\")\n",
    "            print(f\"  Â§çÊùÇÂ∫¶: {task.get('complexity', 'N/A')}\")\n",
    "            print(f\"  ÈöæÂ∫¶ÁâàÊú¨: {task.get('difficulty_version', 'N/A')}\")\n",
    "            print(f\"  Êù•Ê∫êÊñá‰ª∂: {task.get('source_file', 'N/A')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "822b9bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ÈÖçÁΩÆÊàêÂäü! API Key: sk-proj-...\n"
     ]
    }
   ],
   "source": [
    "# ‰∏ÄË°åÂëΩ‰ª§ÁîüÊàêÈÖçÁΩÆÊñá‰ª∂\n",
    "import os, json\n",
    "from pathlib import Path\n",
    "\n",
    "# Á°Æ‰øùËÆæÁΩÆ‰∫ÜÁéØÂ¢ÉÂèòÈáè\n",
    "# os.environ['OPENAI_API_KEY'] = 'sk-...'  # ÂèñÊ∂àÊ≥®ÈáäÂπ∂Â°´ÂÖ•ÊÇ®ÁöÑ key\n",
    "\n",
    "# ÂàõÂª∫ÈÖçÁΩÆ\n",
    "Path(\"./config\").mkdir(exist_ok=True)\n",
    "config = {\n",
    "    \"api_key\": os.getenv('OPENAI_API_KEY', ''),\n",
    "    \"openai_api_key\": os.getenv('OPENAI_API_KEY', ''),\n",
    "    \"model\": \"gpt-4o-mini\"\n",
    "}\n",
    "\n",
    "# ‰øùÂ≠òÈÖçÁΩÆ\n",
    "with open(\"./config/config.json\", 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "# È™åËØÅ\n",
    "if config['api_key']:\n",
    "    print(f\"‚úÖ ÈÖçÁΩÆÊàêÂäü! API Key: {config['api_key'][:8]}...\")\n",
    "else:\n",
    "    print(\"‚ùå ËØ∑ÂÖàËÆæÁΩÆ OPENAI_API_KEY ÁéØÂ¢ÉÂèòÈáè\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
