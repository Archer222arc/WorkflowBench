#!/usr/bin/env python3
"""
Phase 2 Improvement: Simplified and Stable Scoring Mechanism
============================================================
å°†å››ä¸ªè¯„åˆ†ç»´åº¦ç®€åŒ–ä¸ºä¸¤ä¸ªæ­£äº¤ç»´åº¦ï¼šä»»åŠ¡å®Œæˆåº¦å’Œæ‰§è¡Œè´¨é‡
"""

from typing import Dict, List, Set, Tuple, Optional
from dataclasses import dataclass, field
import numpy as np
import logging

logger = logging.getLogger(__name__)


@dataclass
class SimplifiedScoringConfig:
    """ç®€åŒ–çš„è¯„åˆ†é…ç½® - åªæœ‰ä¸¤ä¸ªæ­£äº¤ç»´åº¦"""
    # ä¸»è¦ç»´åº¦æƒé‡
    task_achievement_weight: float = 0.7  # ä»»åŠ¡å®Œæˆåº¦ï¼ˆç»“æœå¯¼å‘ï¼‰
    execution_quality_weight: float = 0.3  # æ‰§è¡Œè´¨é‡ï¼ˆè¿‡ç¨‹å¯¼å‘ï¼‰
    
    # ä»»åŠ¡å®Œæˆåº¦å­å› ç´ 
    required_tools_weight: float = 0.6   # å¿…éœ€å·¥å…·ä½¿ç”¨ç‡
    output_generated_weight: float = 0.4  # è¾“å‡ºç”Ÿæˆæƒ…å†µ
    
    # æ‰§è¡Œè´¨é‡å­å› ç´ 
    workflow_adherence_weight: float = 0.5  # å·¥ä½œæµéµå¾ªåº¦
    efficiency_weight: float = 0.3          # æ‰§è¡Œæ•ˆç‡
    error_handling_weight: float = 0.2      # é”™è¯¯å¤„ç†èƒ½åŠ›


class StableScorer:
    """ç¨³å®šçš„è¯„åˆ†å™¨ - å®ç°Phase 2æ”¹è¿›"""
    
    def __init__(self, config: SimplifiedScoringConfig = None):
        self.config = config or SimplifiedScoringConfig()
        
    def calculate_stable_score(self, 
                             execution_result: Dict,
                             evaluation_context: Dict) -> Tuple[float, Dict]:
        """
        è®¡ç®—ç¨³å®šçš„ç»¼åˆè¯„åˆ†
        
        Args:
            execution_result: æ‰§è¡Œç»“æœï¼ŒåŒ…å«tool_callsç­‰ä¿¡æ¯
            evaluation_context: è¯„ä¼°ä¸Šä¸‹æ–‡ï¼ŒåŒ…å«required_toolsã€workflowç­‰
            
        Returns:
            (final_score, score_breakdown) - æœ€ç»ˆåˆ†æ•°å’Œè¯¦ç»†åˆ†è§£
        """
        # 1. è®¡ç®—ä»»åŠ¡å®Œæˆåº¦ (0-1)
        task_achievement, ta_details = self._calculate_task_achievement(
            execution_result, evaluation_context
        )
        
        # 2. è®¡ç®—æ‰§è¡Œè´¨é‡ (0-1)
        execution_quality, eq_details = self._calculate_execution_quality(
            execution_result, evaluation_context
        )
        
        # 3. ä½¿ç”¨å‡ ä½•å¹³å‡æ•°ç»„åˆï¼Œé¿å…æç«¯å€¼ï¼ˆæ·»åŠ å®‰å…¨æ£€æŸ¥ï¼‰
        # è¿™æ¯”ç®—æœ¯å¹³å‡æ›´ç¨³å®šï¼Œå› ä¸ºå®ƒæƒ©ç½šä»»ä½•ä¸€ä¸ªç»´åº¦çš„ä½åˆ†
        try:
            # ç¡®ä¿è¾“å…¥å€¼åœ¨æœ‰æ•ˆèŒƒå›´å†…
            task_achievement = max(0.0, min(1.0, task_achievement or 0.0))
            execution_quality = max(0.0, min(1.0, execution_quality or 0.0))
            
            # é˜²æ­¢æƒé‡å’Œä¸ºé›¶
            weight_sum = self.config.task_achievement_weight + self.config.execution_quality_weight
            if weight_sum <= 0:
                weight_sum = 1.0
                
            final_score = np.power(
                np.power(task_achievement, self.config.task_achievement_weight) * 
                np.power(execution_quality, self.config.execution_quality_weight),
                1.0 / weight_sum
            )
            
            # æ£€æŸ¥ç»“æœæ˜¯å¦æœ‰æ•ˆ
            if not np.isfinite(final_score):
                logger.warning(f"æ— æ•ˆçš„final_score: {final_score}, ä½¿ç”¨é»˜è®¤å€¼0.0")
                final_score = 0.0
            else:
                final_score = float(final_score)  # ç¡®ä¿è¿”å›Python floatè€Œä¸æ˜¯numpyç±»å‹
                
        except Exception as e:
            logger.error(f"è®¡ç®—final_scoreæ—¶å‡ºé”™: {e}, ä½¿ç”¨é»˜è®¤å€¼0.0")
            final_score = 0.0
        
        # 4. æ„å»ºè¯¦ç»†çš„åˆ†æ•°åˆ†è§£
        score_breakdown = {
            'final_score': final_score,
            'task_achievement': task_achievement,
            'execution_quality': execution_quality,
            'task_achievement_details': ta_details,
            'execution_quality_details': eq_details,
            'scoring_method': 'geometric_mean',
            'config': {
                'task_achievement_weight': self.config.task_achievement_weight,
                'execution_quality_weight': self.config.execution_quality_weight
            }
        }
        
        return final_score, score_breakdown
    
    def _calculate_task_achievement(self, 
                                  execution_result: Dict,
                                  evaluation_context: Dict) -> Tuple[float, Dict]:
        """è®¡ç®—ä»»åŠ¡å®Œæˆåº¦åˆ†æ•°"""
        tool_calls = execution_result.get('tool_calls', [])
        required_tools = set(evaluation_context.get('required_tools', []))
        used_tools = set(tool_calls)
        
        # 1. å¿…éœ€å·¥å…·ä½¿ç”¨ç‡ï¼ˆä½¿ç”¨Jaccardç›¸ä¼¼åº¦ï¼‰
        if required_tools:
            # Jaccardç›¸ä¼¼åº¦ï¼šäº¤é›†/å¹¶é›†ï¼Œæ›´ç¨³å®š
            tool_score = len(required_tools & used_tools) / len(required_tools | used_tools)
        else:
            # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„å¿…éœ€å·¥å…·ï¼Œæ£€æŸ¥æ˜¯å¦è‡³å°‘ä½¿ç”¨äº†ä¸€äº›å·¥å…·
            tool_score = min(1.0, len(used_tools) / 3)  # å‡è®¾å¹³å‡éœ€è¦3ä¸ªå·¥å…·
        
        # 2. è¾“å‡ºç”Ÿæˆæ£€æŸ¥
        output_indicators = ['output', 'export', 'write', 'save', 'create', 'generate', 'post']
        has_output = any(
            any(indicator in tool.lower() for indicator in output_indicators)
            for tool in tool_calls
        )
        output_score = 1.0 if has_output else 0.0
        
        # 3. è®¡ç®—ç»¼åˆä»»åŠ¡å®Œæˆåº¦
        task_achievement = (
            self.config.required_tools_weight * tool_score +
            self.config.output_generated_weight * output_score
        )
        
        details = {
            'tool_score': tool_score,
            'output_score': output_score,
            'required_tools': list(required_tools),
            'used_tools': list(used_tools),
            'matched_tools': list(required_tools & used_tools),
            'has_output': has_output
        }
        
        return task_achievement, details
    
    def _calculate_execution_quality(self,
                                   execution_result: Dict,
                                   evaluation_context: Dict) -> Tuple[float, Dict]:
        """è®¡ç®—æ‰§è¡Œè´¨é‡åˆ†æ•°"""
        tool_calls = execution_result.get('tool_calls', [])
        workflow = evaluation_context.get('workflow', {})
        
        # 1. å·¥ä½œæµéµå¾ªåº¦
        if workflow and 'adherence_scores' in execution_result:
            # å¦‚æœæœ‰adherence scoresï¼Œç›´æ¥ä½¿ç”¨
            adherence_score = execution_result['adherence_scores'].get('overall_adherence', 0.5)
        elif workflow and 'optimal_sequence' in workflow:
            # å¦åˆ™è®¡ç®—ç®€å•çš„åºåˆ—åŒ¹é…åº¦
            optimal_seq = workflow['optimal_sequence']
            adherence_score = self._calculate_sequence_similarity(tool_calls, optimal_seq)
        else:
            # æ²¡æœ‰å·¥ä½œæµä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤å€¼
            adherence_score = 0.5
        
        # 2. æ‰§è¡Œæ•ˆç‡
        if tool_calls:
            unique_tools = len(set(tool_calls))
            total_calls = len(tool_calls)
            # æ•ˆç‡ = unique/totalï¼Œè¶Šæ¥è¿‘1è¶Šå¥½ï¼ˆé¿å…é‡å¤ï¼‰
            efficiency_score = unique_tools / total_calls
            
            # åŒæ—¶è€ƒè™‘æ­¥æ•°æ•ˆç‡ï¼ˆå¤ªå¤šæˆ–å¤ªå°‘éƒ½ä¸å¥½ï¼‰
            optimal_steps = evaluation_context.get('expected_tool_count', 4)
            step_efficiency = 1.0 - min(1.0, abs(total_calls - optimal_steps) / optimal_steps * 0.5)
            efficiency_score = (efficiency_score + step_efficiency) / 2
        else:
            efficiency_score = 0.0
        
        # 3. é”™è¯¯å¤„ç†èƒ½åŠ›ï¼ˆåŸºäºæ‰§è¡Œç»“æœä¸­çš„é”™è¯¯ä¿¡æ¯ï¼‰
        error_score = 1.0  # é»˜è®¤æ²¡æœ‰é”™è¯¯
        if 'error_message' in execution_result and execution_result['error_message']:
            error_score = 0.3  # æœ‰é”™è¯¯ä½†ä»å®Œæˆäº†éƒ¨åˆ†ä»»åŠ¡
        if execution_result.get('success', False):
            error_score = min(1.0, error_score + 0.5)  # æˆåŠŸå®ŒæˆåŠ åˆ†
        
        # 4. è®¡ç®—ç»¼åˆæ‰§è¡Œè´¨é‡
        execution_quality = (
            self.config.workflow_adherence_weight * adherence_score +
            self.config.efficiency_weight * efficiency_score +
            self.config.error_handling_weight * error_score
        )
        
        details = {
            'adherence_score': adherence_score,
            'efficiency_score': efficiency_score,
            'error_score': error_score,
            'unique_tools': len(set(tool_calls)) if tool_calls else 0,
            'total_calls': len(tool_calls),
            'workflow_provided': bool(workflow)
        }
        
        return execution_quality, details
    
    def _calculate_sequence_similarity(self, actual_seq: List[str], expected_seq: List[str]) -> float:
        """è®¡ç®—åºåˆ—ç›¸ä¼¼åº¦ï¼ˆä½¿ç”¨ç¼–è¾‘è·ç¦»ï¼‰"""
        if not expected_seq:
            return 0.5
        if not actual_seq:
            return 0.0
            
        # ä½¿ç”¨Levenshteinè·ç¦»çš„ç®€åŒ–ç‰ˆæœ¬
        # è¿™é‡Œä½¿ç”¨æœ€é•¿å…¬å…±å­åºåˆ—ï¼ˆLCSï¼‰æ¥è®¡ç®—ç›¸ä¼¼åº¦
        m, n = len(actual_seq), len(expected_seq)
        
        # åŠ¨æ€è§„åˆ’è®¡ç®—LCS
        dp = [[0] * (n + 1) for _ in range(m + 1)]
        for i in range(1, m + 1):
            for j in range(1, n + 1):
                if actual_seq[i-1] == expected_seq[j-1]:
                    dp[i][j] = dp[i-1][j-1] + 1
                else:
                    dp[i][j] = max(dp[i-1][j], dp[i][j-1])
        
        lcs_length = dp[m][n]
        
        # ç›¸ä¼¼åº¦ = 2 * LCS / (len(actual) + len(expected))
        similarity = 2.0 * lcs_length / (m + n)
        
        return similarity


class EnhancedMetricsCalculator:
    """å¢å¼ºçš„æŒ‡æ ‡è®¡ç®—å™¨ - é›†æˆæ–°çš„è¯„åˆ†æœºåˆ¶"""
    
    def __init__(self):
        self.scorer = StableScorer()
        
    def calculate_execution_metrics(self, 
                                  tool_calls: List[str],
                                  task_type: str,
                                  workflow: Optional[Dict] = None,
                                  adherence_scores: Optional[Dict] = None) -> Dict[str, float]:
        """
        è®¡ç®—æ‰§è¡Œè´¨é‡æŒ‡æ ‡ - ä½¿ç”¨æ–°çš„ä¸¤ç»´åº¦è¯„åˆ†ç³»ç»Ÿ
        """
        # æ„å»ºæ‰§è¡Œç»“æœ
        execution_result = {
            'tool_calls': tool_calls,
            'success': len(tool_calls) > 0,
            'adherence_scores': adherence_scores or {}
        }
        
        # æ„å»ºè¯„ä¼°ä¸Šä¸‹æ–‡
        expected_tools = {
            'simple_task': ['reader', 'processor'],
            'basic_task': ['reader', 'processor'],
            'data_pipeline': ['reader', 'validator', 'transformer', 'writer'],
            'api_integration': ['fetcher', 'parser', 'poster'],
            'multi_stage_pipeline': ['scanner', 'validator', 'transformer', 'aggregator', 'exporter']
        }
        
        evaluation_context = {
            'task_type': task_type,
            'required_tools': expected_tools.get(task_type, []),
            'expected_tool_count': len(expected_tools.get(task_type, [])),
            'workflow': workflow
        }
        
        # ä½¿ç”¨æ–°çš„è¯„åˆ†å™¨è®¡ç®—åˆ†æ•°
        final_score, breakdown = self.scorer.calculate_stable_score(
            execution_result, evaluation_context
        )
        
        # è¿”å›å…¼å®¹çš„æŒ‡æ ‡æ ¼å¼
        return {
            # æ–°çš„æ ¸å¿ƒæŒ‡æ ‡
            'final_score': final_score,
            'task_achievement': breakdown['task_achievement'],
            'execution_quality': breakdown['execution_quality'],
            
            # è¯¦ç»†åˆ†è§£ï¼ˆç”¨äºåˆ†æï¼‰
            'tool_score': breakdown['task_achievement_details']['tool_score'],
            'output_score': breakdown['task_achievement_details']['output_score'],
            'adherence_score': breakdown['execution_quality_details']['adherence_score'],
            'efficiency_score': breakdown['execution_quality_details']['efficiency_score'],
            'error_score': breakdown['execution_quality_details']['error_score'],
            
            # ä¿æŒå‘åå…¼å®¹
            'task_completion': breakdown['task_achievement'],  # æ˜ å°„åˆ°æ–°æŒ‡æ ‡
            'tool_diversity': breakdown['execution_quality_details']['efficiency_score'],
            'operation_coverage': breakdown['task_achievement_details']['output_score']
        }


# æµ‹è¯•å’Œæ¼”ç¤ºå‡½æ•°
def test_new_scoring_system():
    """æµ‹è¯•æ–°çš„è¯„åˆ†ç³»ç»Ÿ"""
    print("ğŸ§ª Testing New Simplified Scoring System")
    print("=" * 60)
    
    calculator = EnhancedMetricsCalculator()
    
    # æµ‹è¯•åœºæ™¯
    test_cases = [
        {
            'name': 'Perfect Execution',
            'tool_calls': ['data_reader', 'data_validator', 'data_transformer', 'data_writer'],
            'task_type': 'data_pipeline',
            'expected_score': 0.9  # åº”è¯¥å¾—é«˜åˆ†
        },
        {
            'name': 'Missing Output',
            'tool_calls': ['data_reader', 'data_validator', 'data_transformer'],
            'task_type': 'data_pipeline',
            'expected_score': 0.6  # ç¼ºå°‘è¾“å‡ºï¼Œåˆ†æ•°åº”è¯¥é™ä½
        },
        {
            'name': 'Inefficient Execution',
            'tool_calls': ['data_reader', 'data_reader', 'data_validator', 'data_validator', 'data_writer'],
            'task_type': 'data_pipeline',
            'expected_score': 0.7  # æœ‰é‡å¤ï¼Œæ•ˆç‡é™ä½
        },
        {
            'name': 'Wrong Tools',
            'tool_calls': ['api_fetcher', 'json_parser', 'api_poster'],
            'task_type': 'data_pipeline',
            'expected_score': 0.4  # ä½¿ç”¨é”™è¯¯å·¥å…·ï¼Œåˆ†æ•°åº”è¯¥å¾ˆä½
        }
    ]
    
    for test in test_cases:
        metrics = calculator.calculate_execution_metrics(
            test['tool_calls'],
            test['task_type']
        )
        
        print(f"\nğŸ“‹ {test['name']}:")
        print(f"   Tool Calls: {test['tool_calls']}")
        print(f"   Final Score: {metrics['final_score']:.2f}")
        print(f"   - Task Achievement: {metrics['task_achievement']:.2f}")
        print(f"   - Execution Quality: {metrics['execution_quality']:.2f}")
        print(f"   Expected Range: ~{test['expected_score']:.1f}")
        
        # éªŒè¯åˆ†æ•°çš„åˆç†æ€§
        if abs(metrics['final_score'] - test['expected_score']) < 0.2:
            print("   âœ… Score is reasonable")
        else:
            print("   âš ï¸ Score may need adjustment")
    
    print("\nâœ¨ Scoring system test complete!")


# é›†æˆåˆ°ç°æœ‰ç³»ç»Ÿçš„ç¤ºä¾‹
def integrate_with_workflow_tester():
    """å±•ç¤ºå¦‚ä½•é›†æˆåˆ°ç°æœ‰çš„WorkflowQualityTester"""
    
    # è¿™ä¸ªå‡½æ•°å±•ç¤ºäº†å¦‚ä½•åœ¨ç°æœ‰ä»£ç ä¸­ä½¿ç”¨æ–°çš„è¯„åˆ†ç³»ç»Ÿ
    example_code = '''
    # åœ¨WorkflowQualityTester._execute_single_testä¸­ï¼š
    
    # ä½¿ç”¨æ–°çš„è¯„åˆ†ç³»ç»Ÿ
    metrics_calculator = EnhancedMetricsCalculator()
    metrics = metrics_calculator.calculate_execution_metrics(
        tool_calls=tool_calls,
        task_type=task_type,
        workflow=workflow,
        adherence_scores=adherence_scores
    )
    
    # åˆ¤æ–­æˆåŠŸçš„æ–°æ ‡å‡†ï¼ˆæ›´ç¨³å®šï¼‰
    success = metrics['final_score'] > 0.6  # ç»Ÿä¸€çš„æˆåŠŸé˜ˆå€¼
    '''
    
    print("\nğŸ“ Integration Example:")
    print(example_code)


if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    test_new_scoring_system()
    print("\n" + "="*60)
    integrate_with_workflow_tester()
