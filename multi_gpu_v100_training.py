# æ–‡ä»¶ï¼šmulti_gpu_v100_training.py
# ä½ç½®ï¼šç¬¬1-900è¡Œ
# æ³¨æ„ï¼šå®Œæ•´çš„ä¿®å¤åä»£ç ï¼Œæ”¯æŒFSDP

#!/usr/bin/env python3
"""
Multi-GPU V100 è®­ç»ƒè„šæœ¬ - æ”¯æŒ4å¡å¹¶è¡Œè®­ç»ƒå’Œæ–­ç‚¹ç»­è®­
é’ˆå¯¹4å—V100 GPUä¼˜åŒ–ï¼Œæ”¯æŒFSDPåˆ†å¸ƒå¼è®­ç»ƒå’ŒresumeåŠŸèƒ½
"""

import os
import sys
import json
import time
import torch
import torch.nn as nn
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.distributed.fsdp.fully_sharded_data_parallel import (
    CPUOffload,
    BackwardPrefetch,
    MixedPrecision,
)
from torch.distributed.fsdp.wrap import (
    size_based_auto_wrap_policy,
    enable_wrap,
    wrap,
)
from torch.distributed.fsdp.sharded_grad_scaler import ShardedGradScaler
from functools import partial
from torch.utils.data.distributed import DistributedSampler
import numpy as np
import psutil
import GPUtil
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List, Tuple
import logging
from collections import defaultdict
import socket
import multiprocessing as native_mp

# è®¾ç½®ç¯å¢ƒå˜é‡ä¼˜åŒ–NCCLé€šä¿¡
os.environ['NCCL_DEBUG'] = 'WARN'
os.environ['NCCL_TREE_THRESHOLD'] = '0'  # å§‹ç»ˆä½¿ç”¨treeç®—æ³•
os.environ['CUDA_LAUNCH_BLOCKING'] = '0'  # å¼‚æ­¥æ‰§è¡Œ
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

class V100MultiGPUConfig:
    """å¤šGPU V100ä¼˜åŒ–é…ç½® - ä¼˜åŒ–æ€§èƒ½ï¼Œä½¿ç”¨DDPä»£æ›¿FSDP"""
    
    def __init__(self, num_gpus: int = 4):
        # V100è§„æ ¼ï¼š16GB/32GB VRAM per GPU
        self.num_gpus = num_gpus
        self.gpu_memory_gb = 16  # å‡è®¾16GBç‰ˆæœ¬
        self.total_memory_gb = self.gpu_memory_gb * num_gpus
        
        # ä¼˜åŒ–çš„åŸºç¡€é…ç½® - å‡å°æ‰¹é‡å¤§å°ä»¥åŠ å¿«è®­ç»ƒ
        base_batch_size = 64  # ä»256å‡å°‘åˆ°64
        base_lr = 0.0002  # ç•¥å¾®æé«˜å­¦ä¹ ç‡
        
        # æ€§èƒ½ä¼˜åŒ–çš„ç¼©æ”¾ç­–ç•¥
        if num_gpus <= 4:
            # 4GPUä¼˜åŒ–ç­–ç•¥
            batch_size_scale = num_gpus  # çº¿æ€§æ‰©å±•
            lr_scale = np.sqrt(num_gpus) * 0.8  # ä¿å®ˆçš„å­¦ä¹ ç‡æ‰©å±•
            n_steps_scale = 1  # å‡å°‘n_stepsä»¥åŠ å¿«è®­ç»ƒå¾ªç¯
        else:
            # 8GPUä¼˜åŒ–ç­–ç•¥
            batch_size_scale = num_gpus * 0.75  # è€ƒè™‘é€šä¿¡å¼€é”€
            lr_scale = np.sqrt(num_gpus) * 0.7
            n_steps_scale = 1.5  # ç•¥å¾®å¢åŠ ä½†ä¸è¦å¤ªå¤š
        
        # ä¼˜åŒ–åçš„è®­ç»ƒé…ç½®
        self.config = {
            'num_gpus': num_gpus,
            'batch_size': int(base_batch_size * batch_size_scale),
            'per_gpu_batch_size': base_batch_size,  # æ¯ä¸ªGPUçš„æ‰¹é‡å¤§å°
            'learning_rate': base_lr * lr_scale,
            'mini_batch_size': 32,  # PPO mini-batch
            'gradient_accumulation_steps': 1,  # ä¸ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
            'device': 'cuda',
            
            # ç®—æ³•é…ç½®
            'algorithm': 'ppo',  # æˆ– 'ppo'
            'hidden_dim': 1024,
            'num_layers': 6,
            'num_heads': 16,
            'dropout': 0.1,
            
            # DQN specific
            'memory_size': 50000 * num_gpus,  # é€‚åº¦çš„replay buffer
            'epsilon_start': 1.0,
            'epsilon_min': 0.05,
            'epsilon_decay': 0.9995,
            'gamma': 0.99,
            'target_update_freq': 50,  # æ›´é¢‘ç¹çš„targetæ›´æ–°
            
            # PPO specific - å…³é”®ä¼˜åŒ–
            'n_steps': 512 * n_steps_scale,  # å¤§å¹…å‡å°‘ä»8192åˆ°512-768
            'n_epochs': 4,  # å‡å°‘ä»10åˆ°4
            'clip_range': 0.2,
            'ent_coef': 0.01,
            'vf_coef': 0.5,
            'gae_lambda': 0.95,

            # Teacher guidance - å¯ç”¨å¹¶ä¼˜åŒ–å‚æ•°
            'use_teacher_guidance': True,  # å¯ç”¨teacher guidance
            'teacher_guidance_start_prob': 0.95,  # æé«˜åˆå§‹æ¦‚ç‡ä»¥åŠ é€Ÿå­¦ä¹ 
            'teacher_guidance_decay': 0.99,  # è¾ƒæ…¢çš„è¡°å‡é€Ÿåº¦
            'teacher_guidance_min_prob': 0.001,  # ä¿æŒæœ€ä½5%çš„guidance
            'episode_guidance_mode': True,  # ä½¿ç”¨episodeçº§åˆ«çš„guidance
            
            # æ–°å¢ï¼šè‡ªé€‚åº”guidanceå‚æ•°
            'adaptive_guidance': True,  # æ ¹æ®ä»»åŠ¡éš¾åº¦è‡ªé€‚åº”è°ƒæ•´
            'guidance_temperature': 0.5,  # soft guidanceçš„æ¸©åº¦å‚æ•°
            'guidance_blend_factor': 0.7,  # æ··åˆteacherå’Œmodelé¢„æµ‹çš„æƒé‡
            
            # ä¼˜åŒ–å™¨é…ç½®
            'weight_decay': 0.0001,
            'adam_eps': 1e-5,
            'max_grad_norm': 1.0,
            
            # å¹¶è¡Œä¼˜åŒ– - ä½¿ç”¨DDPè€ŒéFSDP
            'use_mixed_precision': True,  # V100æ”¯æŒæ··åˆç²¾åº¦
            'use_fsdp': False,  # å…³é—­FSDPï¼Œä½¿ç”¨æ ‡å‡†DDP
            'ddp_find_unused_parameters': False,
            'ddp_bucket_cap_mb': 25,
            'gradient_compression': False,
            
            
            # ç›‘æ§å’Œæ—¥å¿— - ä¼˜åŒ–checkpointé¢‘ç‡
            'log_interval': 10,
            'checkpoint_frequency': 500,  # ä»100å¢åŠ åˆ°500ï¼Œå‡å°‘IOå¼€é”€
            'checkpoint_keep_recent': 3,  # å‡å°‘ä¿å­˜çš„checkpointæ•°é‡
            'sync_frequency': 2000,  # ä»500å¢åŠ åˆ°2000ï¼Œå‡å°‘åŒæ­¥å¼€é”€
            
            # Task-awareé…ç½®
            'use_task_aware_state': True,
            'use_task_aware_buffer': True,
            'buffer_capacity_per_task': 10000,  # å‡å°‘å†…å­˜ä½¿ç”¨
            'min_episodes_per_task': 10,
            'task_mix_ratio': 0.7,
            
            # è®­ç»ƒå‚æ•°
            'max_episode_length': 30,
            'use_action_masking': True,
            'use_curriculum': True,  # å…³é”®ä¿®å¤ï¼šç¡®ä¿curriculumåŠŸèƒ½å¯ç”¨
            'evaluation_frequency': 100,  # å‡å°‘è¯„ä¼°é¢‘ç‡
            'evaluation_episodes': 5,  # å‡å°‘è¯„ä¼°episodes
            
            # æ€§èƒ½ç›‘æ§
            'profile_training': True,  # å¯ç”¨æ€§èƒ½åˆ†æ
            'log_episode_time': True,  # è®°å½•æ¯ä¸ªepisodeçš„æ—¶é—´
            'target_episode_time': 1.0  # ç›®æ ‡ï¼šæ¯ä¸ªepisode 1ç§’å†…å®Œæˆ
        }

        if 'per_gpu_batch_size' not in self.config:
            self.config['per_gpu_batch_size'] = self.config['batch_size'] // self.num_gpus

        # å¦‚æœæŸäº›FSDPé…ç½®ç¼ºå¤±ï¼Œæ·»åŠ é»˜è®¤å€¼
        fsdp_defaults = {
            'fsdp_min_num_params': 1e6,
            'fsdp_cpu_offload': False,
            'fsdp_backward_prefetch': True,
            'fsdp_forward_prefetch': True,
            'fsdp_use_orig_params': True,
        }
        
        for key, default_value in fsdp_defaults.items():
            if key not in self.config:
                self.config[key] = default_value
        
        # æ·»åŠ è°ƒè¯•æ—¥å¿—ï¼Œç¡®è®¤curriculumå·²å¯ç”¨
        print(f"[V100MultiGPUConfig] Curriculum learning enabled: {self.config['use_curriculum']}")
    def _validate_batch_sizes(self):
        """éªŒè¯å¹¶è°ƒæ•´æ‰¹é‡å¤§å°ä»¥ç¡®ä¿å…¼å®¹æ€§"""
        # ç¡®ä¿æ€»æ‰¹é‡å¤§å°æ˜¯GPUæ•°é‡çš„å€æ•°
        if self.config["batch_size"] % self.num_gpus != 0:
            self.config["batch_size"] = (self.config["batch_size"] // self.num_gpus) * self.num_gpus
            print(f"Adjusted batch_size to {self.config['batch_size']} for {self.num_gpus} GPUs")
        
        # ç¡®ä¿æ˜¯mini_batch_sizeçš„å€æ•°
        if self.config["batch_size"] % self.config["mini_batch_size"] != 0:
            self.config["batch_size"] = (self.config["batch_size"] // self.config["mini_batch_size"]) * self.config["mini_batch_size"]
            print(f"Adjusted batch_size to {self.config['batch_size']} to be divisible by mini_batch_size")
        
        # è®¡ç®—æ¯ä¸ªGPUçš„æ‰¹é‡å¤§å°
        self.config["per_gpu_batch_size"] = self.config["batch_size"] // self.num_gpus
        
        # ç¡®ä¿n_stepsæ˜¯batch_sizeçš„å€æ•°ï¼ˆPPOè¦æ±‚ï¼‰
        if self.config["n_steps"] % self.config["batch_size"] != 0:
            self.config["n_steps"] = (self.config["n_steps"] // self.config["batch_size"]) * self.config["batch_size"]
            print(f"Adjusted n_steps to {self.config['n_steps']} to be divisible by batch_size")
        
        # æ‰“å°éªŒè¯ä¿¡æ¯
        print(f"\nğŸ“Š Batch Size Configuration:")
        print(f"  â€¢ Total batch size: {self.config['batch_size']}")
        print(f"  â€¢ Per-GPU batch size: {self.config['per_gpu_batch_size']}")
        print(f"  â€¢ Mini-batch size: {self.config['mini_batch_size']}")
        print(f"  â€¢ N-steps (rollout): {self.config['n_steps']}")
        print(f"  â€¢ Steps per update: {self.config['n_steps'] // self.config['batch_size']}")

def find_free_port():
    """åŠ¨æ€æŸ¥æ‰¾ä¸€ä¸ªå¯ç”¨çš„ç«¯å£"""
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        # ç»‘å®šåˆ°ç«¯å£0è®©ç³»ç»Ÿè‡ªåŠ¨åˆ†é…ä¸€ä¸ªå¯ç”¨ç«¯å£
        s.bind(('', 0))
        s.listen(1)
        port = s.getsockname()[1]
    return port


def setup_distributed(rank: int, world_size: int):
    """ä¼˜åŒ–çš„åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒåˆå§‹åŒ– - å¢å¼ºç‰ˆæœ¬"""
    
    print(f"[Rank {rank}] Starting enhanced distributed setup for {world_size} GPUs...")
    
    # ä»ç¯å¢ƒå˜é‡è·å–master port
    master_port = os.environ['MASTER_PORT']
    print(f"[Rank {rank}] Using master port: {master_port}")
    
    # è®¾ç½®å½“å‰GPUè®¾å¤‡
    torch.cuda.set_device(rank)
    print(f"[Rank {rank}] Set CUDA device to GPU {rank}")
    
    # ç›´æ¥åˆå§‹åŒ–è¿›ç¨‹ç»„ - å¢åŠ è¶…æ—¶æ—¶é—´
    print(f"[Rank {rank}] Initializing process group...")
    
    dist.init_process_group(
        backend="nccl",
        rank=rank,
        world_size=world_size,
        timeout=timedelta(minutes=30)  # å¢åŠ åˆ°30åˆ†é’Ÿï¼Œé€‚åº”å¤§è§„æ¨¡è®­ç»ƒ
    )
    
    print(f"[Rank {rank}] Successfully initialized process group")
    
    # éªŒè¯åˆå§‹åŒ–æˆåŠŸ
    if dist.is_initialized():
        # è®¾ç½®CUDAæµä¼˜å…ˆçº§ï¼ˆ8GPUæ—¶ç‰¹åˆ«é‡è¦ï¼‰
        if world_size > 4:
            torch.cuda.set_stream(torch.cuda.Stream(priority=-1))
        
        # ä½¿ç”¨å¼‚æ­¥barrieré¿å…æ­»é”
        print(f"[Rank {rank}] Performing initial barrier sync...")
        barrier_handle = dist.barrier(async_op=True)
        
        # ç­‰å¾…barrierå®Œæˆï¼Œä½†æœ‰è¶…æ—¶ä¿æŠ¤
        if not barrier_handle.wait(timeout=timedelta(seconds=30)):
            print(f"[Rank {rank}] WARNING: Initial barrier timed out")
        
        if rank == 0:
            print(f"âœ… Distributed training initialized with {world_size} GPUs")
            print(f"   Backend: {dist.get_backend()}")
            print(f"   World size: {dist.get_world_size()}")
            print(f"   Master port: {master_port}")
            print(f"   NCCL timeout: {os.environ.get('NCCL_TIMEOUT', 'default')} seconds")


def cleanup_distributed():
    """æ¸…ç†åˆ†å¸ƒå¼è®­ç»ƒç¯å¢ƒ"""
    if dist.is_initialized():
        dist.destroy_process_group()


def create_fsdp_model(model: nn.Module, rank: int, config: Dict[str, Any]) -> FSDP:
    """åˆ›å»ºFSDPåŒ…è£…çš„æ¨¡å‹ - ä¿®å¤ç‰ˆæœ¬ï¼Œé¿å…åµŒå¥—FSDPé—®é¢˜"""
    
    print(f"[Rank {rank}] Creating FSDP model with improved wrapping strategy")
    
    # æ··åˆç²¾åº¦é…ç½®
    mixed_precision_policy = None
    if config.get('use_mixed_precision', True):
        from torch.distributed.fsdp import MixedPrecision
        mixed_precision_policy = MixedPrecision(
            param_dtype=torch.float32,
            reduce_dtype=torch.float32,
            buffer_dtype=torch.float32,
        )
    
    # CPU offloadé…ç½®ï¼ˆV100é€šå¸¸ä¸éœ€è¦ï¼‰
    cpu_offload = None
    if config.get('fsdp_cpu_offload', False):
        cpu_offload = CPUOffload(offload_params=True)
    
    # ä¿®å¤ï¼šæ›´ä¿å®ˆçš„åŒ…è£…ç­–ç•¥ï¼Œé¿å…è¿‡åº¦åŒ…è£…
    # 1. å¢åŠ æœ€å°å‚æ•°é˜ˆå€¼ï¼Œå‡å°‘è‡ªåŠ¨åŒ…è£…çš„æ¨¡å—æ•°é‡
    # 2. åªåŒ…è£…çœŸæ­£éœ€è¦çš„å¤§å‹æ¨¡å—
    min_params = config.get('fsdp_min_num_params', 1e6)
    
    # å¯¹äºActorCriticNetworkï¼Œæˆ‘ä»¬éœ€è¦æ›´ç²¾ç»†çš„æ§åˆ¶
    # å¢åŠ é˜ˆå€¼ä»¥é¿å…è¿‡åº¦åŒ…è£…å°æ¨¡å—
    adjusted_min_params = max(min_params, 5e6)  # è‡³å°‘5Må‚æ•°æ‰åŒ…è£…
    
    print(f"[Rank {rank}] Using adjusted min_params threshold: {adjusted_min_params}")
    
    # è‡ªå®šä¹‰åŒ…è£…ç­–ç•¥ - æ›´ç²¾ç»†çš„æ§åˆ¶
    def custom_auto_wrap_policy(module, recurse, nonwrapped_numel):
        """è‡ªå®šä¹‰åŒ…è£…ç­–ç•¥ï¼Œé¿å…åµŒå¥—FSDPé—®é¢˜"""
        # è®¡ç®—æ¨¡å—å‚æ•°æ•°é‡
        num_params = sum(p.numel() for p in module.parameters(recurse=False))
        
        # æ£€æŸ¥æ˜¯å¦åº”è¯¥åŒ…è£…
        should_wrap = num_params >= adjusted_min_params
        
        # ç‰¹æ®Šå¤„ç†ï¼šé¿å…åŒ…è£…å·²ç»æ˜¯FSDPçš„æ¨¡å—
        if hasattr(module, '_is_root'):
            print(f"[Rank {rank}] Module {module.__class__.__name__} already has _is_root, skipping wrap")
            should_wrap = False
        
        # é¿å…åŒ…è£…æŸäº›ç‰¹å®šç±»å‹çš„æ¨¡å—
        excluded_modules = (nn.LayerNorm, nn.Dropout, nn.Embedding)
        if isinstance(module, excluded_modules):
            should_wrap = False
        
        # åªåŒ…è£…å¤§å‹çš„Transformerå±‚æˆ–æ•´ä¸ªå­ç½‘ç»œ
        if should_wrap:
            print(f"[Rank {rank}] Wrapping {module.__class__.__name__} with {num_params} params")
        
        return should_wrap
    
    # ä½¿ç”¨è‡ªå®šä¹‰åŒ…è£…ç­–ç•¥
    auto_wrap_policy = custom_auto_wrap_policy
    
    # æ£€æŸ¥PyTorchç‰ˆæœ¬ï¼Œè°ƒæ•´å‚æ•°
    torch_version = tuple(map(int, torch.__version__.split('.')[:2]))
    
    # åˆ›å»ºFSDPæ¨¡å‹æ—¶çš„å‚æ•°
    fsdp_kwargs = {
        'module': model,
        'auto_wrap_policy': auto_wrap_policy,
        'mixed_precision': mixed_precision_policy,
        'cpu_offload': cpu_offload,
        'device_id': rank,
    }
    
    # ç‰ˆæœ¬å…¼å®¹æ€§å¤„ç†
    if torch_version >= (2, 0):
        # PyTorch 2.0+ æ”¯æŒçš„å‚æ•°
        fsdp_kwargs.update({
            'use_orig_params': config.get('fsdp_use_orig_params', False),  # ä¿®æ”¹ä¸ºFalseä»¥é¿å…å†²çª
            'sync_module_states': True,
        })
        if config.get('fsdp_backward_prefetch', True):
            fsdp_kwargs['backward_prefetch'] = BackwardPrefetch.BACKWARD_PRE
    else:
        # æ—§ç‰ˆæœ¬PyTorch
        print(f"[Rank {rank}] Using legacy FSDP parameters for PyTorch {torch.__version__}")
    
    # åˆ›å»ºFSDPæ¨¡å‹
    print(f"[Rank {rank}] Creating FSDP wrapper with kwargs: {list(fsdp_kwargs.keys())}")
    fsdp_model = FSDP(**fsdp_kwargs)
    
    print(f"[Rank {rank}] FSDP model created successfully")
    return fsdp_model

def save_checkpoint_fsdp(
    rank: int,
    model: FSDP,
    trainer,
    manager,
    episode: int,
    checkpoint_dir: Path,
    is_best: bool = False,
    config: Dict[str, Any] = None
):
    """åœ¨FSDPç¯å¢ƒä¸­ä¿å­˜checkpoint - å…¼å®¹ä¸åŒPyTorchç‰ˆæœ¬"""
    
    try:
        # å°è¯•å¯¼å…¥FSDPç›¸å…³ç±»
        from torch.distributed.fsdp import (
            FullStateDictConfig,
            StateDictType,
        )
        
        # æ£€æŸ¥StateDictTypeæ˜¯å¦æœ‰æ­£ç¡®çš„å±æ€§
        if hasattr(StateDictType, 'FULL_STATE_DICT') and StateDictType.FULL_STATE_DICT is not None:
            # ä½¿ç”¨æ–°ç‰ˆAPI
            save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)
            
            with FSDP.state_dict_type(
                model, StateDictType.FULL_STATE_DICT, save_policy
            ):
                # åªåœ¨rank 0ä¿å­˜
                if rank == 0:
                    _save_checkpoint_content(model, trainer, manager, episode, checkpoint_dir, is_best, config)
        else:
            # åå¤‡æ–¹æ¡ˆï¼šä½¿ç”¨æ—§ç‰ˆAPIæˆ–ç›´æ¥ä¿å­˜
            print(f"[Rank {rank}] Warning: StateDictType.FULL_STATE_DICT not available, using fallback")
            if rank == 0:
                # å°è¯•ä½¿ç”¨æ—§ç‰ˆAPI
                with FSDP.summon_full_params(model, writeback=False, rank0_only=True):
                    _save_checkpoint_content(model, trainer, manager, episode, checkpoint_dir, is_best, config)
                    
    except ImportError as e:
        # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œä½¿ç”¨æœ€åŸºç¡€çš„æ–¹æ³•
        print(f"[Rank {rank}] Warning: FSDP state dict API not available: {e}")
        if rank == 0:
            # ç›´æ¥ä¿å­˜æ¨¡å‹çŠ¶æ€
            _save_checkpoint_content(model, trainer, manager, episode, checkpoint_dir, is_best, config)
    
    except Exception as e:
        # æ•è·å…¶ä»–é”™è¯¯
        print(f"[Rank {rank}] Error in save_checkpoint_fsdp: {e}")
        if rank == 0:
            # å°è¯•åŸºç¡€ä¿å­˜
            try:
                _save_checkpoint_content(model, trainer, manager, episode, checkpoint_dir, is_best, config)
            except:
                print(f"[Rank {rank}] Failed to save checkpoint")
                raise
    
    # æ‰€æœ‰rankåŒæ­¥
    dist.barrier()

def _save_checkpoint_content(model, trainer, manager, episode: int, checkpoint_dir: Path, 
                            is_best: bool, config: Dict[str, Any]):
    """å®é™…ä¿å­˜checkpointå†…å®¹çš„è¾…åŠ©å‡½æ•°"""
    # å‡†å¤‡checkpointæ•°æ®
    checkpoint = {
        'episode': episode,
        'algorithm': config.get('algorithm', 'ppo'),
        'config': config,
        'timestamp': datetime.now().isoformat(),
        'num_gpus_trained': config.get('num_gpus', 4),
        'training_complete': False,
        'fsdp_trained': True  # æ ‡è®°ä½¿ç”¨FSDPè®­ç»ƒ
    }
    
    # ä¿å­˜æ¨¡å‹çŠ¶æ€
    try:
        checkpoint['network_state_dict'] = model.state_dict()
    except:
        # å¦‚æœstate_dictå¤±è´¥ï¼Œå°è¯•module
        if hasattr(model, 'module'):
            checkpoint['network_state_dict'] = model.module.state_dict()
        else:
            print(f"Warning: Could not save model state dict")
            checkpoint['network_state_dict'] = {}
    
    checkpoint['model_state_dict'] = checkpoint['network_state_dict']  # å…¼å®¹æ€§
    
    # å¯¹äºDQNï¼Œä¿å­˜target network
    if hasattr(trainer, 'target_network'):
        try:
            # Target networkå¯èƒ½ä¹Ÿè¢«FSDPåŒ…è£…
            if isinstance(trainer.target_network, FSDP):
                # å°è¯•ä½¿ç”¨FSDP API
                try:
                    from torch.distributed.fsdp import StateDictType, FullStateDictConfig
                    if hasattr(StateDictType, 'FULL_STATE_DICT') and StateDictType.FULL_STATE_DICT is not None:
                        save_policy = FullStateDictConfig(offload_to_cpu=True, rank0_only=True)
                        with FSDP.state_dict_type(
                            trainer.target_network, StateDictType.FULL_STATE_DICT, save_policy
                        ):
                            checkpoint['target_network_state_dict'] = trainer.target_network.state_dict()
                    else:
                        # ä½¿ç”¨æ—§ç‰ˆAPI
                        with FSDP.summon_full_params(trainer.target_network, writeback=False, rank0_only=True):
                            checkpoint['target_network_state_dict'] = trainer.target_network.state_dict()
                except:
                    # ç›´æ¥å°è¯•è·å–state_dict
                    checkpoint['target_network_state_dict'] = trainer.target_network.state_dict()
            else:
                checkpoint['target_network_state_dict'] = trainer.target_network.state_dict()
        except Exception as e:
            print(f"Warning: Could not save target network state: {e}")
    
    # ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€
    if hasattr(trainer, 'optimizer'):
        checkpoint['optimizer_state_dict'] = trainer.optimizer.state_dict()
    
    # ä¿å­˜å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€
    if hasattr(trainer, 'lr_scheduler'):
        checkpoint['lr_scheduler_state_dict'] = trainer.lr_scheduler.state_dict()
    
    # ä¿å­˜è®­ç»ƒå™¨çŠ¶æ€
    if hasattr(trainer, 'training_steps'):
        checkpoint['training_steps'] = trainer.training_steps
    if hasattr(trainer, 'total_timesteps'):
        checkpoint['total_timesteps'] = trainer.total_timesteps
    if hasattr(trainer, 'epsilon'):
        checkpoint['epsilon'] = trainer.epsilon
    
    # ä¿å­˜managerçŠ¶æ€
    if manager:
        if hasattr(manager, 'training_history'):
            checkpoint['training_history'] = dict(manager.training_history)
        if hasattr(manager, 'best_success_rate'):
            checkpoint['best_success_rate'] = manager.best_success_rate
        if hasattr(manager, 'best_model_path'):
            checkpoint['best_model_path'] = str(manager.best_model_path) if manager.best_model_path else None
    
    # ç¡®å®šä¿å­˜è·¯å¾„
    if is_best:
        checkpoint_path = checkpoint_dir / "best_model.pt"
    else:
        checkpoint_path = checkpoint_dir / f"multi_gpu_checkpoint_{episode}.pt"
    
    # ä¿å­˜checkpoint
    torch.save(checkpoint, checkpoint_path)
    logger.info(f"Saved FSDP checkpoint to {checkpoint_path}")
    
    # æ¸…ç†æ—§çš„checkpointsï¼ˆä¿ç•™æœ€è¿‘çš„å‡ ä¸ªï¼‰
    if not is_best:
        keep_recent = config.get('checkpoint_keep_recent', 5)
        all_checkpoints = sorted(checkpoint_dir.glob("multi_gpu_checkpoint_*.pt"), key=lambda p: p.stat().st_mtime)
        
        if len(all_checkpoints) > keep_recent:
            for old_checkpoint in all_checkpoints[:-keep_recent]:
                old_checkpoint.unlink()
                logger.info(f"Removed old checkpoint: {old_checkpoint}")


def load_checkpoint_fsdp(
    rank: int, 
    checkpoint_path: Path, 
    model: FSDP,
    trainer, 
    manager, 
    config: Dict[str, Any]
) -> int:
    """åœ¨FSDPç¯å¢ƒä¸­åŠ è½½checkpoint - å…¼å®¹ä¸åŒPyTorchç‰ˆæœ¬"""
    
    if rank == 0:
        logger.info(f"Loading checkpoint from {checkpoint_path}")
    
    # åŠ è½½checkpoint
    checkpoint = torch.load(checkpoint_path, map_location=f'cuda:{rank}', weights_only=False)
    
    try:
        # å°è¯•ä½¿ç”¨æ–°ç‰ˆFSDP API
        from torch.distributed.fsdp import (
            FullStateDictConfig,
            StateDictType,
        )
        
        # æ£€æŸ¥StateDictTypeæ˜¯å¦æœ‰æ­£ç¡®çš„å±æ€§
        if hasattr(StateDictType, 'FULL_STATE_DICT') and StateDictType.FULL_STATE_DICT is not None:
            # æ¢å¤æ¨¡å‹çŠ¶æ€
            with FSDP.state_dict_type(
                model, StateDictType.FULL_STATE_DICT
            ):
                model.load_state_dict(checkpoint.get('network_state_dict', checkpoint.get('model_state_dict', {})))
        else:
            # ä½¿ç”¨æ—§ç‰ˆAPI
            print(f"[Rank {rank}] Warning: StateDictType.FULL_STATE_DICT not available, using fallback")
            with FSDP.summon_full_params(model, writeback=True):
                model.load_state_dict(checkpoint.get('network_state_dict', checkpoint.get('model_state_dict', {})))
                
    except Exception as e:
        # åå¤‡æ–¹æ¡ˆï¼šç›´æ¥åŠ è½½
        print(f"[Rank {rank}] Warning: FSDP state dict API failed: {e}, trying direct load")
        try:
            model.load_state_dict(checkpoint.get('network_state_dict', checkpoint.get('model_state_dict', {})))
        except:
            if hasattr(model, 'module'):
                model.module.load_state_dict(checkpoint.get('network_state_dict', checkpoint.get('model_state_dict', {})))
    
    # å¯¹äºDQNï¼Œè¿˜éœ€è¦æ¢å¤target network
    if hasattr(trainer, 'target_network') and 'target_network_state_dict' in checkpoint:
        try:
            if isinstance(trainer.target_network, FSDP):
                # å°è¯•ä½¿ç”¨FSDP API
                try:
                    from torch.distributed.fsdp import StateDictType
                    if hasattr(StateDictType, 'FULL_STATE_DICT') and StateDictType.FULL_STATE_DICT is not None:
                        with FSDP.state_dict_type(
                            trainer.target_network, StateDictType.FULL_STATE_DICT
                        ):
                            trainer.target_network.load_state_dict(checkpoint['target_network_state_dict'])
                    else:
                        with FSDP.summon_full_params(trainer.target_network, writeback=True):
                            trainer.target_network.load_state_dict(checkpoint['target_network_state_dict'])
                except:
                    trainer.target_network.load_state_dict(checkpoint['target_network_state_dict'])
            else:
                trainer.target_network.load_state_dict(checkpoint['target_network_state_dict'])
        except Exception as e:
            print(f"[Rank {rank}] Warning: Could not restore target network: {e}")
    
    # æ¢å¤ä¼˜åŒ–å™¨çŠ¶æ€
    if 'optimizer_state_dict' in checkpoint and hasattr(trainer, 'optimizer'):
        trainer.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        if rank == 0:
            logger.info("Restored optimizer state")
    
    # æ¢å¤å­¦ä¹ ç‡è°ƒåº¦å™¨
    if hasattr(trainer, 'lr_scheduler') and 'lr_scheduler_state_dict' in checkpoint:
        trainer.lr_scheduler.load_state_dict(checkpoint['lr_scheduler_state_dict'])
        if rank == 0:
            logger.info("Restored learning rate scheduler")
    
    # æ¢å¤è®­ç»ƒå™¨ç‰¹å®šçŠ¶æ€
    if hasattr(trainer, 'training_steps'):
        trainer.training_steps = checkpoint.get('training_steps', 0)
    if hasattr(trainer, 'total_timesteps'):
        trainer.total_timesteps = checkpoint.get('total_timesteps', 0)
    
    # å¯¹äºDQNï¼Œæ¢å¤epsilon
    if hasattr(trainer, 'epsilon'):
        trainer.epsilon = checkpoint.get('epsilon', config.get('epsilon_start', 1.0))
    
    # æ¢å¤managerçŠ¶æ€ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
    if rank == 0 and manager:
        if 'training_history' in checkpoint:
            manager.training_history = defaultdict(list, checkpoint['training_history'])
        if 'best_success_rate' in checkpoint:
            manager.best_success_rate = checkpoint['best_success_rate']
        if 'best_model_path' in checkpoint:
            manager.best_model_path = Path(checkpoint['best_model_path']) if checkpoint['best_model_path'] else None
    
    # è¿”å›ä¸‹ä¸€ä¸ªè¦å¼€å§‹çš„episode
    start_episode = checkpoint.get('episode', 0) + 1
    
    if rank == 0:
        logger.info(f"Checkpoint loaded successfully, resuming from episode {start_episode}")
        
        # æ‰“å°æ¢å¤çš„çŠ¶æ€ä¿¡æ¯
        if hasattr(trainer, 'training_steps'):
            logger.info(f"Training steps: {trainer.training_steps}")
        if hasattr(trainer, 'epsilon'):
            logger.info(f"Epsilon: {trainer.epsilon:.4f}")
    
    return start_episode

def find_latest_checkpoint(checkpoint_dir: Path) -> Optional[Tuple[Path, int]]:
    """æŸ¥æ‰¾æœ€æ–°çš„checkpointæ–‡ä»¶"""
    # é¦–å…ˆå°è¯•æ‰¾best_model.pt
    best_model_path = checkpoint_dir / "best_model.pt"
    if best_model_path.exists():
        checkpoint = torch.load(best_model_path, map_location='cpu', weights_only=False)
        episode = checkpoint.get('episode', 0)
        return best_model_path, episode
    
    # æŸ¥æ‰¾checkpoint_*.ptæ–‡ä»¶
    checkpoints = list(checkpoint_dir.glob("checkpoint_*.pt"))
    if not checkpoints:
        # æŸ¥æ‰¾multi_gpuç‰¹å®šçš„checkpoint
        checkpoints = list(checkpoint_dir.glob("multi_gpu_checkpoint_*.pt"))
    
    if not checkpoints:
        return None
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°çš„
    checkpoints.sort(key=lambda p: p.stat().st_mtime)
    latest_checkpoint = checkpoints[-1]
    
    checkpoint = torch.load(latest_checkpoint, map_location='cpu', weights_only=False)
    episode = checkpoint.get('episode', 0)
    return latest_checkpoint, episode



def train_worker(rank: int, world_size: int, config: Dict[str, Any], episodes: int, resume: bool = False):
    """æ¯ä¸ªGPUçš„è®­ç»ƒè¿›ç¨‹ - æ”¯æŒFSDPå’ŒCurriculum"""
    
    # åˆå§‹åŒ–å˜é‡ï¼Œé˜²æ­¢åœ¨finallyå—ä¸­å¼•ç”¨æœªå®šä¹‰å˜é‡
    trainer = None
    manager = None
    last_sync_time = time.time()
    sync_failures = 0
    curriculum = None  # æ·»åŠ curriculumå˜é‡
    
    # è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒ
    setup_distributed(rank, world_size)
    
    # è®¾ç½®æ—¥å¿—ï¼ˆåªåœ¨ä¸»è¿›ç¨‹æ‰“å°ï¼‰
    if rank == 0:
        logger.setLevel(logging.INFO)
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
    else:
        logger.setLevel(logging.WARNING)
    
    # å¯¼å…¥å¿…è¦çš„æ¨¡å—
    sys.path.append(str(Path(__file__).parent))
    from unified_training_manager import UnifiedTrainingManager, CurriculumScheduler  # å¯¼å…¥CurriculumScheduler
    
    # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆåªåœ¨ä¸»è¿›ç¨‹ï¼‰
    output_dir = Path(".")
    checkpoint_dir = output_dir / "checkpoints"
    if rank == 0:
        checkpoint_dir.mkdir(exist_ok=True)
    
    # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å‡†å¤‡å°±ç»ª
    dist.barrier()
    
    try:
        # åˆ›å»ºè®­ç»ƒç®¡ç†å™¨
        config_path = output_dir / "training_config.json" 
        if rank == 0:
            with open(config_path, 'w') as f:
                json.dump(config, f, indent=2)
        
        # ç¡®ä¿é…ç½®æ–‡ä»¶å†™å…¥å®Œæˆ
        dist.barrier()
        
        # æ‰€æœ‰è¿›ç¨‹åˆ›å»ºmanager
        manager = UnifiedTrainingManager(
            config_path=str(config_path),
            use_task_aware_state=True,
            algorithm=config['algorithm']
        )
        
        # è®¾ç½®ç¯å¢ƒ
        if not manager.setup_environment():
            print(f"[Rank {rank}] Failed to setup environment!")
            return
        
        # è·å–trainer
        trainer = manager.trainer
        if not trainer:
            print(f"[Rank {rank}] Trainer not initialized!")
            return
        
        # å°†æ¨¡å‹è½¬æ¢ä¸ºFSDPï¼ˆä»…é€‚ç”¨äºPPOï¼‰
        if config['algorithm'] == 'ppo' and hasattr(trainer, 'network'):
            trainer.network = create_fsdp_model(trainer.network, rank, config)
            print(f"[Rank {rank}] Model converted to FSDP")
        
        # æ¢å¤checkpointï¼ˆå¦‚æœéœ€è¦ï¼‰
        start_episode = 0
        if resume and rank == 0:
            checkpoint_info = find_latest_checkpoint(checkpoint_dir)
            if checkpoint_info:
                checkpoint_path, _ = checkpoint_info
                model_to_load = trainer.network if hasattr(trainer, 'network') else trainer.q_network
                start_episode = load_checkpoint_fsdp(
                    rank, checkpoint_path, model_to_load, trainer, manager, config
                )

        # å¹¿æ’­å¼€å§‹episodeåˆ°æ‰€æœ‰è¿›ç¨‹
        start_episode_tensor = torch.tensor(start_episode, dtype=torch.long).cuda(rank)
        dist.broadcast(start_episode_tensor, src=0)
        start_episode = start_episode_tensor.item()

        print(f"[Rank {rank}] Starting from episode {start_episode}")
        
        # åˆå§‹åŒ–curriculumï¼ˆæ–°å¢ï¼‰
        if config.get('use_curriculum', True):
            curriculum = CurriculumScheduler(episodes)
            if resume and start_episode > 0:
                curriculum.current_episode = start_episode
            if rank == 0:
                print(f"[CURRICULUM] Initialized with {episodes} total episodes")
                print(f"[CURRICULUM] Starting from episode {start_episode}")
        
        # è®­ç»ƒå¾ªç¯
        episode_rewards = []
        episode_success = []
        episode_lengths = []
        start_time = time.time()
        
        # åˆå§‹åŒ–managerçš„training_historyï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if not hasattr(manager, 'training_history'):
            manager.training_history = {
                'rewards': [],
                'success': [],
                'lengths': []
            }
        
        for episode in range(start_episode, episodes):
            # è·å–curriculumé˜¶æ®µï¼ˆæ–°å¢ï¼‰
            curriculum_stage = None
            if curriculum:
                curriculum_stage = curriculum.get_stage()
                # åªåœ¨rank 0ä¸Šæ‰“å°curriculumä¿¡æ¯
                if rank == 0 and episode % 100 == 0:
                    print(f"[CURRICULUM] Episode {episode}, Stage: {curriculum_stage}, "
                          f"Progress: {curriculum.current_episode/curriculum.total_episodes:.1%}")
            
            # é‡ç½®ç¯å¢ƒï¼Œä¼ é€’curriculum_stageï¼ˆä¿®å¤çš„å…³é”®è¡Œï¼‰
            state = manager.env.reset(curriculum_stage=curriculum_stage)
            done = False
            episode_reward = 0
            episode_trajectory = []
            
            # Episodeå¾ªç¯
            while not done and manager.env.episode_steps < config['max_episode_length']:
                # è·å–æœ‰æ•ˆåŠ¨ä½œ
                valid_actions = manager.env.get_valid_actions() if config.get('use_action_masking', True) else None
                
                # é€‰æ‹©åŠ¨ä½œ
                action = trainer.select_action(state, valid_actions)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_state, reward, done, info = manager.env.step(action)
                
                # è®°å½•è½¨è¿¹
                episode_trajectory.append({
                    'state': state.copy() if isinstance(state, np.ndarray) else state,
                    'action': action,
                    'reward': reward,
                    'next_state': next_state.copy() if isinstance(next_state, np.ndarray) else next_state,
                    'done': done,
                    'info': info
                })
                
                # å­˜å‚¨ç»éªŒ
                task_type = None
                if hasattr(manager.env, 'current_task') and hasattr(manager.env.current_task, 'task_type'):
                    task_type = manager.env.current_task.task_type
                
                trainer.store_experience(state, action, reward, next_state, done, task_type=task_type)
                trainer.step_completed()
                
                # æ›´æ–°çŠ¶æ€
                state = next_state
                episode_reward += reward
                
                # è®­ç»ƒï¼ˆå¦‚æœéœ€è¦ï¼‰
                if trainer.should_train():
                    loss = trainer.train_step()
                    if rank == 0 and episode % 10 == 0:
                        logger.debug(f"Episode {episode}, Loss: {loss:.4f}")
            
            # Episodeç»“æŸå¤„ç†
            trainer.on_episode_end()
            
            # è®°å½•episodeç»“æœ
            success = info.get('success', False)
            episode_rewards.append(episode_reward)
            episode_success.append(float(success))
            episode_lengths.append(manager.env.episode_steps)
            
            # æ›´æ–°exploration
            trainer.update_exploration()
            
            # æ›´æ–°curriculumï¼ˆæ–°å¢ï¼‰
            if curriculum:
                curriculum.update()
                # æ£€æŸ¥stageå˜åŒ–
                new_stage = curriculum.get_stage()
                if rank == 0 and episode > 0 and new_stage != curriculum_stage:
                    print(f"[CURRICULUM] Advanced to Stage {new_stage} at episode {episode}")
            
            # å®šæœŸåŒæ­¥å’Œä¿å­˜ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
            if episode % config.get('sync_frequency', 500) == 0 and episode > 0:
                if rank == 0:
                    logger.info(f"Episode {episode}/{episodes} - "
                               f"Reward: {np.mean(episode_rewards[-100:]):.2f}, "
                               f"Success: {np.mean(episode_success[-100:]):.2%}, "
                               f"Stage: {curriculum_stage if curriculum else 'N/A'}")
                
                # åŒæ­¥è®­ç»ƒçŠ¶æ€
                try:
                    trainer.sync_parameters()
                    last_sync_time = time.time()
                    sync_failures = 0
                except Exception as e:
                    sync_failures += 1
                    if rank == 0:
                        logger.warning(f"Sync failed: {e} (failure #{sync_failures})")
                    
                    if sync_failures >= 3:
                        logger.error(f"[Rank {rank}] Too many sync failures, stopping training")
                        break
            
            # ä¿å­˜checkpointï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
            if rank == 0 and episode % config.get('checkpoint_frequency', 500) == 0 and episode > 0:
                model_to_save = trainer.network if hasattr(trainer, 'network') else trainer.q_network
                save_checkpoint_fsdp(
                    rank, model_to_save, trainer, manager, episode, checkpoint_dir,
                    is_best=False, config=config
                )

            hours_limit = config.get('hours_limit', None)
            # æ£€æŸ¥æ—¶é—´é™åˆ¶ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
            if hours_limit and (time.time() - start_time) / 3600 > hours_limit:
                if rank == 0:
                    logger.info(f"Time limit reached ({hours_limit} hours)")
                break
        
        # è®­ç»ƒå®Œæˆï¼Œä¿å­˜æœ€ç»ˆæ¨¡å‹ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        if rank == 0:
            elapsed_time = (time.time() - start_time) / 3600
            logger.info(f"Training completed in {elapsed_time:.2f} hours")
            
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            model_to_save = trainer.network if hasattr(trainer, 'network') else trainer.q_network
            save_checkpoint_fsdp(
                rank, model_to_save, trainer, manager, episodes, checkpoint_dir,
                is_best=False, config=config
            )
            
            # æ‰“å°curriculumæœ€ç»ˆçŠ¶æ€ï¼ˆæ–°å¢ï¼‰
            if curriculum:
                print(f"\n[CURRICULUM FINAL] Training completed!")
                print(f"[CURRICULUM FINAL] Total episodes run: {curriculum.current_episode}")
                print(f"[CURRICULUM FINAL] Final stage: {curriculum.get_stage()}")
                print(f"[CURRICULUM FINAL] Final progress: {curriculum.current_episode/curriculum.total_episodes:.1%}")
            
            # ä¿å­˜è®­ç»ƒç»Ÿè®¡
            training_stats = {
                'total_episodes': episodes,
                'start_episode': start_episode,
                'training_time_hours': elapsed_time,
                'final_success_rate': np.mean(episode_success[-100:]) if episode_success else 0,
                'best_success_rate': manager.best_success_rate if hasattr(manager, 'best_success_rate') else 0,
                'num_gpus': world_size,
                'resumed': resume and start_episode > 0,
                'fsdp_enabled': True,
                'curriculum_enabled': curriculum is not None,
                'final_curriculum_stage': curriculum.get_stage() if curriculum else None
            }
            
            with open(output_dir / "training_stats.json", 'w') as f:
                json.dump(training_stats, f, indent=2)
    
    finally:
        # æ¸…ç†åˆ†å¸ƒå¼ç¯å¢ƒ
        print(f"[Rank {rank}] Cleaning up distributed environment...")
        cleanup_distributed()


def train_multi_gpu_v100(
    episodes: int = 10000,
    num_gpus: int = 4,
    hours_limit: Optional[float] = None,
    resume: bool = False
):
    """ä½¿ç”¨4å—V100è¿›è¡ŒFSDPåˆ†å¸ƒå¼è®­ç»ƒçš„ä¸»å‡½æ•°
    
    Args:
        episodes: è®­ç»ƒçš„æ€»episodeæ•°
        num_gpus: ä½¿ç”¨çš„GPUæ•°é‡
        hours_limit: æ—¶é—´é™åˆ¶ï¼ˆå°æ—¶ï¼‰
        resume: æ˜¯å¦ä»checkpointæ¢å¤è®­ç»ƒ
    """
    
    print("=" * 60)
    print("ğŸš€ Multi-GPU V100 Training Script with FSDP")
    print("=" * 60)
    
    # æ£€æŸ¥GPUå¯ç”¨æ€§
    if not torch.cuda.is_available():
        print("âŒ CUDA is not available!")
        return
    
    available_gpus = torch.cuda.device_count()
    if available_gpus < num_gpus:
        print(f"âŒ Requested {num_gpus} GPUs but only {available_gpus} available!")
        return
    
    # æ‰“å°GPUä¿¡æ¯
    print(f"\nğŸ“Š System Information:")
    print(f"  â€¢ PyTorch version: {torch.__version__}")
    print(f"  â€¢ CUDA version: {torch.version.cuda}")
    print(f"  â€¢ Available GPUs: {available_gpus}")
    
    for i in range(num_gpus):
        gpu_name = torch.cuda.get_device_name(i)
        gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1e9
        print(f"  â€¢ GPU {i}: {gpu_name} ({gpu_memory:.1f} GB)")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰å¯æ¢å¤çš„checkpoint
    checkpoint_dir = Path(".") / "checkpoints"
    if resume and checkpoint_dir.exists():
        checkpoint_info = find_latest_checkpoint(checkpoint_dir)
        if checkpoint_info:
            checkpoint_path, last_episode = checkpoint_info
            print(f"\nğŸ“ Found checkpoint: {checkpoint_path}")
            print(f"  â€¢ Last episode: {last_episode}")
            print(f"  â€¢ Will resume from episode: {last_episode + 1}")
            
            # è°ƒæ•´episodesæ•°
            if last_episode >= episodes:
                print(f"âš ï¸  Training already completed {last_episode} episodes, increasing target to {last_episode + 1000}")
                episodes = last_episode + 1000
        else:
            print("\nâš ï¸  No checkpoint found, will start from scratch")
            resume = False
    
    # åˆ›å»ºé…ç½®
    gpu_config = V100MultiGPUConfig(num_gpus=num_gpus)
    config = gpu_config.config
    config['hours_limit'] = hours_limit

    print(f"\nâš™ï¸  Training Configuration:")
    print(f"  â€¢ Algorithm: {config['algorithm'].upper()}")
    print(f"  â€¢ Network: {config['hidden_dim']}Ã—{config['num_layers']} with {config['num_heads']} heads")
    print(f"  â€¢ Total batch size: {config['batch_size']}")
    print(f"  â€¢ Per-GPU batch size: {config['per_gpu_batch_size']}")
    print(f"  â€¢ Learning rate: {config['learning_rate']:.6f}")
    print(f"  â€¢ Mixed precision: {config['use_mixed_precision']}")
    print(f"  â€¢ FSDP enabled: YES")
    print(f"  â€¢ FSDP min params: {config['fsdp_min_num_params']:.0e}")
    print(f"  â€¢ Resume training: {resume}")
    
    # é¢„ä¼°è®­ç»ƒæ—¶é—´
    estimated_time = episodes * 0.002  # ç²—ç•¥ä¼°è®¡ï¼ŒFSDPå¯èƒ½ä¼šç•¥æ…¢
    print(f"\nâ±ï¸  Estimated training time: {estimated_time:.1f} hours")
    
    if hours_limit:
        print(f"  â€¢ Time limit set: {hours_limit} hours")
        if estimated_time > hours_limit:
            episodes = int(hours_limit / 0.002)
            print(f"  â€¢ Adjusted episodes to {episodes} to fit time limit")
    
    # ç¡®è®¤å¼€å§‹è®­ç»ƒ
    confirm = input("\nStart training with FSDP? (y/n): ").strip().lower()
    if confirm != 'y':
        print("Training cancelled.")
        return
    
    # è®¾ç½®åˆ†å¸ƒå¼è®­ç»ƒå¿…éœ€çš„ç¯å¢ƒå˜é‡
    os.environ['MASTER_ADDR'] = 'localhost'  # å•æœºå¤šGPUä½¿ç”¨localhost
    
    # é¢„å…ˆæ‰¾åˆ°ä¸€ä¸ªå¯ç”¨ç«¯å£ï¼ˆé¿å…è¿›ç¨‹é—´ç«äº‰ï¼‰
    master_port = find_free_port()
    print(f"\nğŸš€ Using master address: localhost")
    print(f"ğŸš€ Using master port: {master_port}")
    os.environ['MASTER_PORT'] = str(master_port)
    
    # ä½¿ç”¨æ ‡å‡†çš„multiprocessingå¯åŠ¨
    print("\nğŸš€ Launching FSDP distributed training...")
    mp.spawn(
        train_worker,
        args=(num_gpus, config, episodes, resume),
        nprocs=num_gpus,
        join=True
    )
    
    print("\nâœ… All training processes completed!")
    
    # æ‰“å°æœ€ç»ˆç»Ÿè®¡ï¼ˆå¦‚æœè®­ç»ƒå®Œæˆï¼‰
    stats_path = Path(".") / "training_stats.json"
    if stats_path.exists():
        with open(stats_path, 'r') as f:
            stats = json.load(f)
        
        print("\nğŸ“Š Training Statistics:")
        print(f"  â€¢ Total episodes: {stats['total_episodes']}")
        print(f"  â€¢ Training time: {stats['training_time_hours']:.2f} hours")
        print(f"  â€¢ Final success rate: {stats['final_success_rate']:.2%}")
        print(f"  â€¢ Best success rate: {stats['best_success_rate']:.2%}")
        print(f"  â€¢ FSDP enabled: {stats.get('fsdp_enabled', False)}")
        if stats.get('resumed', False):
            print(f"  â€¢ Resumed from episode: {stats['start_episode']}")


def main():
    """ä¸»å…¥å£"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Multi-GPU V100 Training with FSDP Support')
    parser.add_argument('--episodes', type=int, default=10000, help='Number of episodes')
    parser.add_argument('--num-gpus', type=int, default=4, help='Number of GPUs to use')
    parser.add_argument('--hours-limit', type=float, help='Time limit in hours')
    parser.add_argument('--resume', action='store_true', help='Resume from latest checkpoint')
    
    args = parser.parse_args()
    
    train_multi_gpu_v100(
        episodes=args.episodes,
        num_gpus=args.num_gpus,
        hours_limit=args.hours_limit,
        resume=args.resume
    )


if __name__ == "__main__":
    main()