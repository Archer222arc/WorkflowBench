#!/usr/bin/env python3
"""
Ultra Parallel Runner - æœ€å¤§åŒ–å¹¶è¡Œåº¦çš„æµ‹è¯•æ‰§è¡Œå™¨
===============================================

æ ¸å¿ƒè®¾è®¡ï¼š
1. æ™ºèƒ½å®ä¾‹æ± ç®¡ç†ï¼šç»Ÿä¸€è°ƒåº¦9ä¸ªAzureå®ä¾‹
2. ä»»åŠ¡æ™ºèƒ½åˆ†ç‰‡ï¼šå°†å¤§ä»»åŠ¡æ‹†åˆ†åˆ°å¤šå®ä¾‹
3. åŠ¨æ€è´Ÿè½½å‡è¡¡ï¼šæ ¹æ®å®ä¾‹æ€§èƒ½è‡ªé€‚åº”åˆ†é…
4. èšåˆç»“æœç®¡ç†ï¼šç»Ÿä¸€æ”¶é›†å’Œå­˜å‚¨ç»“æœ

ç›®æ ‡ï¼šå°†èµ„æºåˆ©ç”¨ç‡ä»11%æå‡åˆ°90%+
"""

import asyncio
import concurrent.futures
import json
import os
import time
import logging
from dataclasses import dataclass
from typing import List, Dict, Set, Optional, Tuple
from pathlib import Path
import subprocess
import threading
from queue import Queue, Empty

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å¯é€‰å¯¼å…¥æ–°çš„ResultCollector
try:
    from result_collector import ResultCollector, ResultAggregator
    RESULT_COLLECTOR_AVAILABLE = True
except ImportError:
    RESULT_COLLECTOR_AVAILABLE = False
    logger.info("ResultCollectorä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼")

@dataclass
class InstanceConfig:
    """Azureå®ä¾‹é…ç½®"""
    name: str
    model_family: str  # "deepseek-v3", "deepseek-r1", "llama-3.3"
    max_workers: int = 100
    max_qps: float = 200.0
    is_busy: bool = False
    current_load: int = 0
    performance_score: float = 1.0  # æ€§èƒ½è¯„åˆ†ï¼ŒåŠ¨æ€è°ƒæ•´

@dataclass
class TaskShard:
    """ä»»åŠ¡åˆ†ç‰‡"""
    shard_id: str
    model: str
    prompt_types: str
    difficulty: str
    task_types: str
    num_instances: int
    instance_name: str
    tool_success_rate: float = 0.8

class UltraParallelRunner:
    """è¶…é«˜å¹¶è¡Œåº¦æµ‹è¯•æ‰§è¡Œå™¨"""
    
    def __init__(self, use_result_collector: bool = None):
        """
        åˆå§‹åŒ–UltraParallelRunner
        
        Args:
            use_result_collector: æ˜¯å¦ä½¿ç”¨æ–°çš„ResultCollectoræ¨¡å¼
                                 None=è‡ªåŠ¨æ£€æµ‹, True=å¼ºåˆ¶å¯ç”¨, False=ç¦ç”¨
        """
        self.instance_pool = self._initialize_instance_pool()
        self.active_tasks: Set[str] = set()
        self.task_queue = Queue()
        self.results_lock = threading.Lock()
        self.performance_stats = {}
        
        # ç»“æœæ”¶é›†æ¨¡å¼é…ç½®
        if use_result_collector is None:
            # è‡ªåŠ¨æ£€æµ‹ï¼šä»ç¯å¢ƒå˜é‡æˆ–é…ç½®å†³å®š
            use_collector = os.environ.get('USE_RESULT_COLLECTOR', 'false').lower() == 'true'
        else:
            use_collector = use_result_collector
            
        if use_collector and RESULT_COLLECTOR_AVAILABLE:
            self.result_collector = ResultCollector()
            self.result_aggregator = ResultAggregator()
            self.use_collector_mode = True
            logger.info("ğŸ†• å¯ç”¨ResultCollectoræ¨¡å¼ï¼Œæ”¯æŒé›¶å†²çªå¹¶å‘")
        else:
            self.result_collector = None
            self.result_aggregator = None
            self.use_collector_mode = False
            if use_collector and not RESULT_COLLECTOR_AVAILABLE:
                logger.warning("âš ï¸ ResultCollectorä¸å¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼")
            else:
                logger.info("ğŸ“œ ä½¿ç”¨ä¼ ç»Ÿæ•°æ®åº“å†™å…¥æ¨¡å¼")
        
    def _initialize_instance_pool(self) -> Dict[str, InstanceConfig]:
        """åˆå§‹åŒ–Azureå®ä¾‹æ± """
        instances = {}
        
        # 6ä¸ªDeepSeekå®ä¾‹ï¼ˆä¿æŒAzure APIè¦æ±‚çš„å¤§å°å†™ï¼‰
        deepseek_v3_instances = [
            "DeepSeek-V3-0324",
            "DeepSeek-V3-0324-2",  # ç»Ÿä¸€å¤§å°å†™
            "DeepSeek-V3-0324-3"   # ç»Ÿä¸€å¤§å°å†™
        ]
        
        deepseek_r1_instances = [
            "DeepSeek-R1-0528",
            "DeepSeek-R1-0528-2",  # ç»Ÿä¸€å¤§å°å†™
            "DeepSeek-R1-0528-3"   # ç»Ÿä¸€å¤§å°å†™
        ]
        
        # 3ä¸ªLlamaå®ä¾‹ï¼ˆä¿æŒAzure APIè¦æ±‚çš„å¤§å°å†™ï¼‰
        llama_instances = [
            "Llama-3.3-70B-Instruct",
            "Llama-3.3-70B-Instruct-2",  # ç»Ÿä¸€å¤§å°å†™
            "Llama-3.3-70B-Instruct-3"   # ç»Ÿä¸€å¤§å°å†™
        ]
        
        # 2ä¸ªIdealLab API Keyå¯¹åº”çš„è™šæ‹Ÿå®ä¾‹
        # ä¸ºå¼€æºæ¨¡å‹qwenåˆ›å»º2ä¸ªè™šæ‹Ÿå®ä¾‹ï¼Œå¯¹åº”2ä¸ªå¯ç”¨çš„API keys
        # è¿™æ ·å¯ä»¥å®ç°çœŸæ­£çš„å¹¶å‘ï¼ˆç¬¬3ä¸ªkeyæš‚æ—¶ä¸å¯ç”¨ï¼‰
        ideallab_qwen_instances = [
            "qwen-key0",      # å¯¹åº”API key 0 (baselineåå¥½)
            "qwen-key1"       # å¯¹åº”API key 1 (cot+optimalåå¥½)
        ]
        
        # æ³¨å†Œæ‰€æœ‰å®ä¾‹
        for name in deepseek_v3_instances:
            instances[name] = InstanceConfig(
                name=name,
                model_family="deepseek-v3",
                max_workers=100,
                max_qps=200.0
            )
            
        for name in deepseek_r1_instances:
            instances[name] = InstanceConfig(
                name=name,
                model_family="deepseek-r1", 
                max_workers=100,
                max_qps=200.0
            )
            
        for name in llama_instances:
            instances[name] = InstanceConfig(
                name=name,
                model_family="llama-3.3",
                max_workers=100,
                max_qps=200.0
            )
            
        # æ³¨å†ŒIdealLabå®ä¾‹ (API Keyçº§åˆ«å¹¶è¡Œ)
        for name in ideallab_qwen_instances:
            instances[name] = InstanceConfig(
                name=name,
                model_family="qwen",
                max_workers=1,   # é™åˆ¶ä¸º1é¿å…é™æµé—®é¢˜
                max_qps=5.0      # æ›´ä¿å®ˆçš„QPSé™åˆ¶
            )
        
        # æ·»åŠ é—­æºAzureæ¨¡å‹å®ä¾‹ (åªæœ‰ä¸€ä¸ªdeploymentï¼Œä½†å¯ä»¥model levelå¹¶å‘)
        azure_closed_models = [
            "gpt-4o-mini",
            "gpt-5-mini"
        ]
        
        for model in azure_closed_models:
            # é—­æºæ¨¡å‹åªæœ‰å•ä¸ªdeploymentï¼Œä½†å¯ä»¥ç”¨æ›´é«˜å¹¶å‘
            instances[model] = InstanceConfig(
                name=model,
                model_family=f"azure-{model}",
                max_workers=200,  # å•å®ä¾‹æ›´é«˜å¹¶å‘
                max_qps=400.0
            )
        
        # æ·»åŠ IdealLabé—­æºæ¨¡å‹å®ä¾‹ (åªæœ‰ä¸€ä¸ªAPI Keyå¯ç”¨ï¼Œä½†å¯ä»¥model levelå¹¶å‘)
        ideallab_closed_models = [
            "o3-0416-global",
            "gemini-2.5-flash-06-17", 
            "kimi-k2",
            "claude_sonnet4"
        ]
        
        for model in ideallab_closed_models:
            # é—­æºæ¨¡å‹åªèƒ½ç”¨ä¸€ä¸ªAPI Keyï¼Œä¸”æœ‰ä¸¥æ ¼é€Ÿç‡é™åˆ¶
            instances[model] = InstanceConfig(
                name=model,
                model_family=f"ideallab-{model}",
                max_workers=1,   # é™åˆ¶ä¸º1é¿å…é™æµé—®é¢˜
                max_qps=5.0      # æ›´ä¿å®ˆçš„QPSé™åˆ¶
            )
            
        logger.info(f"åˆå§‹åŒ–å®ä¾‹æ± : {len(instances)}ä¸ªå®ä¾‹ ({len([i for i in instances.values() if 'azure' in i.model_family])}ä¸ªAzure + {len([i for i in instances.values() if 'ideallab' in i.model_family or i.model_family == 'qwen'])}ä¸ªIdealLab)")
        return instances
        
    def get_available_instances(self, model_family: str) -> List[InstanceConfig]:
        """è·å–æŒ‡å®šæ¨¡å‹æ—çš„å¯ç”¨å®ä¾‹"""
        available = []
        for instance in self.instance_pool.values():
            if (instance.model_family == model_family and 
                not instance.is_busy and 
                instance.current_load < instance.max_workers * 0.8):
                available.append(instance)
        
        # æŒ‰æ€§èƒ½è¯„åˆ†æ’åºï¼Œä¼˜å…ˆä½¿ç”¨é«˜æ€§èƒ½å®ä¾‹
        available.sort(key=lambda x: x.performance_score, reverse=True)
        return available
        
    def _create_qwen_smart_shards(self, model: str, prompt_types: str, difficulty: str,
                                  task_types: str, num_instances: int, tool_success_rate: float) -> List[TaskShard]:
        """ä¸ºqwenæ¨¡å‹åˆ›å»ºæ™ºèƒ½åˆ†ç‰‡ï¼Œé¿å…åŒä¸€æ¨¡å‹åŒä¸€keyå¹¶å‘
        
        é‡è¦ï¼š5.3æµ‹è¯•ä¼šä¼ å…¥å¤šä¸ªprompt_typesï¼ˆé€—å·åˆ†éš”ï¼‰ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
        """
        shards = []
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯5.3çš„å¤šprompt_typesæƒ…å†µï¼ˆä»…å¤„ç†flawedç±»å‹ï¼‰
        if "," in prompt_types and "flawed" in prompt_types:
            # 5.3åœºæ™¯ï¼šå¤šä¸ªflawedç±»å‹ä¸€èµ·æµ‹è¯•
            # ç®€å•ç­–ç•¥ï¼š3ä¸ªç»„å›ºå®šä½¿ç”¨3ä¸ªä¸åŒçš„keyï¼Œå¯ä»¥å®‰å…¨å¹¶è¡Œ
            
            # åˆ¤æ–­æ˜¯å“ªä¸€ç»„ç¼ºé™·ï¼Œåˆ†é…å›ºå®šçš„key
            if "sequence_disorder" in prompt_types:
                # ç»„1ï¼šç»“æ„ç¼ºé™·ç»„ï¼Œå›ºå®šä½¿ç”¨key0
                key_idx = 0
                group_name = "struct_defects"
            elif "missing_step" in prompt_types:
                # ç»„2ï¼šæ“ä½œç¼ºé™·ç»„ï¼Œå›ºå®šä½¿ç”¨key1
                key_idx = 1
                group_name = "operation_defects"
            elif "logical_inconsistency" in prompt_types:
                # ç»„3ï¼šé€»è¾‘ç¼ºé™·ç»„ï¼Œå›ºå®šä½¿ç”¨key2
                key_idx = 2
                group_name = "logic_defects"
            else:
                # é»˜è®¤ä½¿ç”¨key0
                key_idx = 0
                group_name = "unknown_defects"
            
            shard = TaskShard(
                shard_id=f"{model}_{difficulty}_{group_name}_key{key_idx}",
                model=model,
                prompt_types=prompt_types,  # ä¿æŒåŸå§‹çš„å¤šä¸ªprompt_types
                difficulty=difficulty,
                task_types=task_types,
                num_instances=num_instances,
                instance_name=f"qwen-key{key_idx}",  # æŒ‡å®šä½¿ç”¨çš„key
                tool_success_rate=tool_success_rate
            )
            shards.append(shard)
            logger.info(f"Qwen 5.3æ¨¡å¼: {model}çš„{group_name}ä½¿ç”¨key{key_idx}")
            return shards
        
        # 5.1/5.2/5.4/5.5åœºæ™¯ï¼šå•ä¸ªprompt_typeï¼Œå¯ä»¥åˆ†ç‰‡
        # å‡åŒ€åˆ†é…åˆ°3ä¸ªkeys
        instances_per_key = max(1, num_instances // 3)
        remainder = num_instances % 3
        
        for key_idx in range(3):
            shard_instances = instances_per_key + (1 if key_idx < remainder else 0)
            
            if shard_instances > 0:
                shard = TaskShard(
                    shard_id=f"{model}_{difficulty}_key{key_idx}",
                    model=model,  # ä¿æŒåŸå§‹æ¨¡å‹åï¼Œä¸è½¬æ¢ä¸ºå°å†™
                    prompt_types=prompt_types,  # ä¿æŒåŸå§‹prompt_types
                    difficulty=difficulty,
                    task_types=task_types,
                    num_instances=shard_instances,
                    instance_name=f"qwen-key{key_idx}",  # ä½¿ç”¨è™šæ‹Ÿå®ä¾‹å
                    tool_success_rate=tool_success_rate
                )
                shards.append(shard)
        
        logger.info(f"Qwenæ ‡å‡†æ¨¡å¼: ä»»åŠ¡å‡åŒ€åˆ†é…åˆ°3ä¸ªkeysï¼Œæ¯keyçº¦{instances_per_key}ä¸ªå®ä¾‹")
        
        return shards
    
    def create_task_shards(self, model: str, prompt_types: str, difficulty: str, 
                          task_types: str, num_instances: int, tool_success_rate: float = 0.8) -> List[TaskShard]:
        """æ™ºèƒ½åˆ›å»ºä»»åŠ¡åˆ†ç‰‡"""
        
        # ç‰¹æ®Šå¤„ç†ï¼šqwenæ¨¡å‹ä½¿ç”¨æ™ºèƒ½åˆ†ç‰‡ç­–ç•¥
        if "qwen" in model.lower():
            logger.info(f"ğŸ¯ ä½¿ç”¨qwenæ™ºèƒ½åˆ†ç‰‡ç­–ç•¥: {model}")
            return self._create_qwen_smart_shards(model, prompt_types, difficulty,
                                                 task_types, num_instances, tool_success_rate)
        
        # ç¡®å®šæ¨¡å‹æ—
        if "deepseek-v3" in model.lower():
            model_family = "deepseek-v3"
            base_model = "DeepSeek-V3-0324"
        elif "deepseek-r1" in model.lower():
            model_family = "deepseek-r1"
            base_model = "DeepSeek-R1-0528"
        elif "llama" in model.lower():
            model_family = "llama-3.3"
            base_model = "Llama-3.3-70B-Instruct"
        # Azureé—­æºæ¨¡å‹
        elif model == "gpt-4o-mini":
            model_family = "azure-gpt-4o-mini"
            base_model = "gpt-4o-mini"
        elif model == "gpt-5-mini":
            model_family = "azure-gpt-5-mini"
            base_model = "gpt-5-mini"
        # IdealLabé—­æºæ¨¡å‹
        elif model == "o3-0416-global":
            model_family = "ideallab-o3-0416-global"
            base_model = "o3-0416-global"
        elif model == "gemini-2.5-flash-06-17":
            model_family = "ideallab-gemini-2.5-flash-06-17"
            base_model = "gemini-2.5-flash-06-17"
        elif model == "kimi-k2":
            model_family = "ideallab-kimi-k2"
            base_model = "kimi-k2"
        elif model == "claude_sonnet4":
            model_family = "ideallab-claude_sonnet4"
            base_model = "claude_sonnet4"
        else:
            logger.warning(f"æœªçŸ¥æ¨¡å‹æ—: {model}")
            return []
            
        # è·å–å¯ç”¨å®ä¾‹
        available_instances = self.get_available_instances(model_family)
        
        if not available_instances:
            logger.warning(f"æ²¡æœ‰å¯ç”¨çš„{model_family}å®ä¾‹")
            return []
            
        # è®¡ç®—åˆ†ç‰‡ç­–ç•¥
        shards = []
        
        # å¯¹äºé—­æºæ¨¡å‹ï¼Œä½¿ç”¨ç‰¹æ®Šçš„åˆ†ç‰‡ç­–ç•¥
        if model_family.startswith("ideallab-"):
            instances_to_use = 1  # IdealLabé—­æºæ¨¡å‹åªèƒ½ä½¿ç”¨å•åˆ†ç‰‡é¿å…API Keyå†²çª
            logger.info(f"IdealLabé—­æºæ¨¡å‹ {model} ä½¿ç”¨å•åˆ†ç‰‡ç­–ç•¥ï¼ˆé¿å…API Keyå†²çªï¼‰")
        elif model_family.startswith("azure-"):
            instances_to_use = 1  # Azureé—­æºæ¨¡å‹ä½¿ç”¨å•åˆ†ç‰‡é«˜å¹¶å‘ç­–ç•¥
            logger.info(f"Azureé—­æºæ¨¡å‹ {model} ä½¿ç”¨å•åˆ†ç‰‡é«˜å¹¶å‘ç­–ç•¥ï¼ˆå•deploymentä¼˜åŒ–ï¼‰")
        elif model_family.startswith("deepseek"):
            # DeepSeekæ¨¡å‹æš‚æ—¶åªä½¿ç”¨ç¬¬ä¸€ä¸ªéƒ¨ç½²å®ä¾‹ï¼Œé¿å…å¤šéƒ¨ç½²å¯èƒ½çš„å¹¶å‘é—®é¢˜
            instances_to_use = 1
            logger.info(f"DeepSeekæ¨¡å‹ {model} æš‚æ—¶ä½¿ç”¨å•éƒ¨ç½²ç­–ç•¥ï¼ˆé¿å…å¤šéƒ¨ç½²å¹¶å‘é—®é¢˜ï¼‰")
        elif model_family.startswith("llama"):
            # Llamaæ¨¡å‹ä¹Ÿæš‚æ—¶åªä½¿ç”¨ç¬¬ä¸€ä¸ªéƒ¨ç½²å®ä¾‹ï¼Œé¿å…å¤šéƒ¨ç½²å¯èƒ½çš„å¹¶å‘é—®é¢˜
            instances_to_use = 1
            logger.info(f"Llamaæ¨¡å‹ {model} æš‚æ—¶ä½¿ç”¨å•éƒ¨ç½²ç­–ç•¥ï¼ˆé¿å…å¤šéƒ¨ç½²å¹¶å‘é—®é¢˜ï¼‰")
        else:
            instances_to_use = min(len(available_instances), 3)  # å…¶ä»–å¼€æºæ¨¡å‹æœ€å¤šç”¨3ä¸ªå®ä¾‹
            
        instances_per_shard = max(1, num_instances // instances_to_use)
        
        logger.info(f"åˆ›å»ºä»»åŠ¡åˆ†ç‰‡: {instances_to_use}ä¸ªå®ä¾‹å¹¶è¡Œ")
        
        for i in range(instances_to_use):
            instance = available_instances[i]
            shard_instances = instances_per_shard
            
            # æœ€åä¸€ä¸ªåˆ†ç‰‡å¤„ç†ä½™æ•°
            if i == instances_to_use - 1:
                shard_instances += num_instances % instances_to_use
                
            shard = TaskShard(
                shard_id=f"{model}_{difficulty}_{i}",
                model=base_model,  # ä¿æŒåŸå§‹å¤§å°å†™ï¼Œnormalizeä¼šåœ¨å­˜å‚¨æ—¶å¤„ç†
                prompt_types=prompt_types,
                difficulty=difficulty,
                task_types=task_types,
                num_instances=shard_instances,
                instance_name=instance.name,  # ä¿ç•™åŸå§‹å¤§å°å†™ç”¨äºAPIè°ƒç”¨
                tool_success_rate=tool_success_rate
            )
            shards.append(shard)
            
        return shards
        
    def execute_shard_async(self, shard: TaskShard, rate_mode: str = "adaptive", result_suffix: str = "", silent: bool = False, max_workers: int = None, shard_index: int = 0) -> subprocess.Popen:
        """å¼‚æ­¥æ‰§è¡Œä»»åŠ¡åˆ†ç‰‡
        
        Args:
            shard: ä»»åŠ¡åˆ†ç‰‡
            rate_mode: é€Ÿç‡æ¨¡å¼ - "adaptive" æˆ– "fixed"
        """
        
        # æ ‡è®°å®ä¾‹ä¸ºå¿™ç¢Œ
        if shard.instance_name in self.instance_pool:
            self.instance_pool[shard.instance_name].is_busy = True
            self.instance_pool[shard.instance_name].current_load += shard.num_instances
            
        # è®¡ç®—promptæ•°é‡ï¼ˆç”¨äºåŠ¨æ€è°ƒæ•´workersï¼‰
        prompt_count = len(shard.prompt_types.split(",")) if "," in shard.prompt_types else 1
        use_prompt_parallel = "--prompt-parallel" if prompt_count > 1 else ""
        
        # ä¿æŒåŸå§‹çš„instance_nameä½œä¸ºdeployment
        deployment_name = shard.instance_name
        # æ³¨æ„ï¼šä¸è¦åœ¨è¿™é‡Œä¿®æ”¹deployment_nameï¼
        # qwen-key0/1 è™šæ‹Ÿå®ä¾‹åä¼šåœ¨ batch_test_runner.py ä¸­æ­£ç¡®å¤„ç†
        
        # æ ¹æ®æ¨¡å‹ç±»å‹å’Œrate_modeè°ƒæ•´å‚æ•°
        if shard.instance_name in self.instance_pool:
            instance = self.instance_pool[shard.instance_name]
            
            # IdealLabå¼€æºæ¨¡å‹ï¼ˆqwenç³»åˆ—ï¼‰
            if instance.model_family == "qwen" or shard.instance_name.startswith("qwen-key"):
                # IdealLab APIä¸¥æ ¼é™åˆ¶ï¼Œå¿…é¡»ä½¿ç”¨ä½å¹¶å‘
                # æ— è®ºç”¨æˆ·è®¾ç½®ä»€ä¹ˆï¼ˆåŒ…æ‹¬--max-workersï¼‰ï¼Œéƒ½å¼ºåˆ¶é™åˆ¶ä¸º1ä¸ªworker
                max_workers = 1  # å¼ºåˆ¶é™åˆ¶ï¼šæ¯ä¸ªkeyåªèƒ½1ä¸ªworkerï¼ˆä¸¥æ ¼é™æµï¼‰
                qps = 10  # QPSé™åˆ¶ä¸º10
                logger.info(f"  IdealLab qwenæ¨¡å‹é™åˆ¶: {shard.instance_name} å¼ºåˆ¶ä½¿ç”¨ max_workers={max_workers}, qps={qps}")
                logger.info(f"    æ³¨æ„: IdealLab APIå¹¶å‘é™åˆ¶ä¸¥æ ¼ï¼Œå¿½ç•¥--max-workersè®¾ç½®")
            # Azureå¼€æºæ¨¡å‹ï¼ˆDeepSeek, Llamaç­‰ï¼‰
            elif instance.model_family in ["deepseek-v3", "deepseek-r1", "llama-3.3"]:
                # å¦‚æœç”¨æˆ·æŒ‡å®šäº†max_workersï¼Œä¼˜å…ˆä½¿ç”¨
                if max_workers is not None:
                    base_workers = max_workers
                    max_workers = base_workers * prompt_count if prompt_count > 1 else base_workers
                    qps = None  # ç§»é™¤QPSé™åˆ¶
                    logger.info(f"  Azureå¼€æºæ¨¡å‹è‡ªå®šä¹‰: {prompt_count}ä¸ªprompt Ã— {base_workers} = {max_workers} workers")
                elif rate_mode == "fixed":
                    # å›ºå®šæ¨¡å¼ï¼šæ¯ä¸ªprompt 50 workers
                    base_workers = 50
                    max_workers = base_workers * prompt_count if prompt_count > 1 else base_workers
                    qps = None  # ç§»é™¤QPSé™åˆ¶
                    logger.info(f"  Azureå¼€æºæ¨¡å‹å›ºå®šæ¨¡å¼: {prompt_count}ä¸ªprompt Ã— {base_workers} = {max_workers} workers")
                else:
                    # è‡ªé€‚åº”æ¨¡å¼ï¼šæ¯ä¸ªprompt 100 workers
                    base_workers = 100
                    max_workers = base_workers * prompt_count if prompt_count > 1 else base_workers
                    qps = None  # adaptiveä¸éœ€è¦QPS
                    logger.info(f"  Azureå¼€æºæ¨¡å‹è‡ªé€‚åº”æ¨¡å¼: {prompt_count}ä¸ªprompt Ã— {base_workers} = {max_workers} workers")
            # IdealLabé—­æºæ¨¡å‹ï¼ˆå•API Keyé™åˆ¶ï¼‰
            elif instance.model_family.startswith("ideallab-"):
                # IdealLabé—­æºæ¨¡å‹ä¸¥æ ¼é™åˆ¶ä¸º1ä¸ªworkerï¼Œä¸ç®¡æœ‰å¤šå°‘prompts
                max_workers = 1  # å¼ºåˆ¶1ä¸ªworkerï¼Œå¿½ç•¥ç”¨æˆ·è®¾ç½®å’Œpromptæ•°é‡
                qps = 10  # QPSé™åˆ¶ä¸º10
                logger.info(f"  IdealLabé—­æºæ¨¡å‹: max_workers={max_workers}, qps={qps} (ä¸¥æ ¼é™æµï¼Œå¿½ç•¥promptæ•°é‡)")
            # Azureé—­æºæ¨¡å‹ï¼ˆå•deploymentä½†æ”¯æŒé«˜å¹¶å‘ï¼‰
            elif instance.model_family.startswith("azure-"):
                # å¦‚æœç”¨æˆ·æŒ‡å®šäº†max_workersï¼Œä¼˜å…ˆä½¿ç”¨
                if max_workers is not None:
                    base_workers = max_workers
                    max_workers = base_workers * prompt_count if prompt_count > 1 else base_workers
                    qps = None  # ç§»é™¤QPSé™åˆ¶
                    logger.info(f"  Azureé—­æºæ¨¡å‹è‡ªå®šä¹‰: {prompt_count}ä¸ªprompt Ã— {base_workers} = {max_workers} workers")
                elif rate_mode == "fixed":
                    # é—­æºæ¨¡å‹å›ºå®šæ¨¡å¼ï¼šå•deploymenté«˜å¹¶å‘
                    base_workers = 100  # æ›´é«˜çš„åŸºç¡€å¹¶å‘
                    max_workers = base_workers * prompt_count if prompt_count > 1 else base_workers
                    qps = None  # ç§»é™¤QPSé™åˆ¶
                    logger.info(f"  Azureé—­æºæ¨¡å‹å›ºå®šæ¨¡å¼: {prompt_count}ä¸ªprompt Ã— {base_workers} = {max_workers} workers")
                else:
                    # é—­æºæ¨¡å‹è‡ªé€‚åº”æ¨¡å¼ï¼šå•deploymentè¶…é«˜å¹¶å‘
                    base_workers = 200  # è¶…é«˜åŸºç¡€å¹¶å‘
                    max_workers = base_workers * prompt_count if prompt_count > 1 else base_workers
                    qps = None
                    logger.info(f"  Azureé—­æºæ¨¡å‹è‡ªé€‚åº”æ¨¡å¼: {prompt_count}ä¸ªprompt Ã— {base_workers} = {max_workers} workers")
            else:
                # å…¶ä»–æœªåˆ†ç±»æ¨¡å‹ - ä½¿ç”¨ä¿å®ˆé…ç½®
                logger.warning(f"  æœªè¯†åˆ«çš„æ¨¡å‹æ— {instance.model_family}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                if max_workers is not None:
                    base_workers = max_workers
                    max_workers = base_workers * prompt_count if prompt_count > 1 else base_workers
                    qps = None  # ç§»é™¤QPSé™åˆ¶
                elif rate_mode == "fixed":
                    base_workers = 30  # ä¿å®ˆçš„å›ºå®šæ¨¡å¼é…ç½®
                    max_workers = base_workers * prompt_count if prompt_count > 1 else base_workers
                    qps = None  # ç§»é™¤QPSé™åˆ¶
                else:
                    base_workers = 50  # ä¿å®ˆçš„è‡ªé€‚åº”æ¨¡å¼é…ç½®
                    max_workers = base_workers * prompt_count if prompt_count > 1 else base_workers
                    qps = None
        else:
            # é»˜è®¤é…ç½®
            # å¦‚æœç”¨æˆ·æŒ‡å®šäº†max_workersï¼Œä¼˜å…ˆä½¿ç”¨
            if max_workers is not None:
                base_workers = max_workers
                max_workers = base_workers * prompt_count if prompt_count > 1 else base_workers
                qps = None  # ç§»é™¤QPSé™åˆ¶
            elif rate_mode == "fixed":
                base_workers = 30
                max_workers = base_workers * prompt_count if prompt_count > 1 else base_workers
                qps = None  # ç§»é™¤QPSé™åˆ¶
            else:
                base_workers = 50
                max_workers = base_workers * prompt_count if prompt_count > 1 else base_workers
                qps = None
        
        # æ„å»ºå‘½ä»¤
        cmd = [
            "python", "smart_batch_runner.py",
            "--model", shard.model,  # å°å†™çš„åŸºç¡€æ¨¡å‹åï¼ˆç”¨äºç»Ÿè®¡ï¼‰
            "--deployment", deployment_name,  # ä½¿ç”¨æ˜ å°„åçš„éƒ¨ç½²åï¼ˆå¯¹qwenæ˜¯æ¨¡å‹åï¼Œå¯¹Azureæ˜¯å®ä¾‹åï¼‰
            "--prompt-types", shard.prompt_types,
            "--difficulty", shard.difficulty,
            "--task-types", shard.task_types,
            "--num-instances", str(shard.num_instances),
            "--max-workers", str(max_workers),
            "--tool-success-rate", str(shard.tool_success_rate),
            "--batch-commit",
            "--checkpoint-interval", "20",
            "--ai-classification",
            "--no-save-logs"  # é¿å…æ—¥å¿—å†²çª
        ]
        
        # å¯¹äºqwenè™šæ‹Ÿå®ä¾‹ï¼Œæ·»åŠ API keyç´¢å¼•å‚æ•°
        if shard.instance_name.startswith("qwen-key"):
            key_index = int(shard.instance_name[-1])  # ä» "qwen-key0" æå– 0
            cmd.extend(["--idealab-key-index", str(key_index)])
            logger.info(f"  ä½¿ç”¨IdealLab API Key {key_index}")
        elif shard.instance_name == "qwen-serial":
            # ä¸²è¡Œæ¨¡å¼ï¼šä¸æŒ‡å®škeyï¼Œè®©smart_batch_runnerè‡ªå·±è½®è¯¢ä½¿ç”¨
            logger.info(f"  ä¸²è¡Œæ¨¡å¼: å°†åœ¨å†…éƒ¨è½®è¯¢ä½¿ç”¨å¤šä¸ªAPI keys")
        
        # æ·»åŠ é™é»˜æ¨¡å¼å‚æ•°ï¼ˆå¦‚æœæ˜¯è°ƒè¯•è¿›ç¨‹åˆ™ä¸é™é»˜ï¼‰
        debug_process_num = int(os.environ.get('DEBUG_PROCESS_NUM', '1'))
        if os.environ.get('DEBUG_LOG', 'false').lower() == 'true' and shard_index == debug_process_num:
            # è¿™æ˜¯è¦è°ƒè¯•çš„è¿›ç¨‹ï¼Œä¸ä½¿ç”¨é™é»˜æ¨¡å¼
            logger.info(f"ğŸ” è¿›ç¨‹ {shard_index} å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼ˆè¯¦ç»†æ—¥å¿—ï¼‰")
        elif silent:
            cmd.append("--silent")
        
        # æ ¹æ®rate_modeæ·»åŠ å‚æ•°
        if rate_mode == "fixed":
            if qps is not None:
                cmd.extend(["--no-adaptive", "--qps", str(qps)])
            else:
                cmd.append("--no-adaptive")  # å›ºå®šæ¨¡å¼ä½†æ— QPSé™åˆ¶
        else:
            cmd.append("--adaptive")
        
        # æœ‰å¤šä¸ªpromptæ—¶æ‰æ·»åŠ prompt-parallel
        if use_prompt_parallel:
            cmd.append(use_prompt_parallel)
        
        # æ·»åŠ ç»“æœæ–‡ä»¶åç¼€
        if result_suffix:
            cmd.extend(["--result-suffix", result_suffix])
        
        logger.info(f"ğŸš€ å¯åŠ¨åˆ†ç‰‡ {shard.shard_id}: {shard.instance_name}")
        logger.info(f"   å®ä¾‹æ•°: {shard.num_instances}, æ¨¡å‹: {shard.model}")
        
        # å¯åŠ¨è¿›ç¨‹ï¼ˆä¿ç•™é”™è¯¯è¾“å‡ºä»¥ä¾¿è°ƒè¯•ï¼‰
        # ç¡®ä¿ä¼ é€’ç¯å¢ƒå˜é‡
        env = os.environ.copy()
        
        # ç¡®ä¿æ‰€æœ‰å…³é”®å†…å­˜ä¼˜åŒ–ç¯å¢ƒå˜é‡è¢«ä¼ é€’
        critical_env_vars = {
            'STORAGE_FORMAT': os.environ.get('STORAGE_FORMAT', 'json'),
            'USE_PARTIAL_LOADING': os.environ.get('USE_PARTIAL_LOADING', 'true'),
            'TASK_LOAD_COUNT': os.environ.get('TASK_LOAD_COUNT', '20'),
            'SKIP_MODEL_LOADING': os.environ.get('SKIP_MODEL_LOADING', 'true'),
            'USE_RESULT_COLLECTOR': os.environ.get('USE_RESULT_COLLECTOR', 'true'),
            'KMP_DUPLICATE_LIB_OK': 'TRUE',
            'PYTHONMALLOC': 'malloc'
        }
        
        # æ›´æ–°ç¯å¢ƒå˜é‡
        for key, value in critical_env_vars.items():
            if key not in env or not env.get(key):
                env[key] = value
                logger.info(f"   è®¾ç½®{key}={value}ç»™å­è¿›ç¨‹")
            else:
                logger.info(f"   ä¼ é€’{key}={env[key]}ç»™å­è¿›ç¨‹")
        
        # æ„å»ºç¯å¢ƒå˜é‡å‰ç¼€å‘½ä»¤ï¼ˆç¡®ä¿æ‰€æœ‰å…³é”®å˜é‡éƒ½ä¼ é€’ï¼‰
        env_prefix = []
        for key, value in critical_env_vars.items():
            env_prefix.append(f'{key}={env[key]}')
        
        cmd_with_env = ['env'] + env_prefix + cmd
        
        process = subprocess.Popen(
            cmd_with_env,
            stdout=subprocess.DEVNULL,  # ä¸æ•è·æ ‡å‡†è¾“å‡º
            stderr=subprocess.PIPE,      # æ•è·é”™è¯¯ä»¥ä¾¿è°ƒè¯•
            text=True,
            env=env  # æ˜¾å¼ä¼ é€’ç¯å¢ƒå˜é‡
        )
        
        self.active_tasks.add(shard.shard_id)
        return process
        
    def run_ultra_parallel_test(self, model: str, prompt_types: str, difficulty: str,
                               task_types: str = "all", num_instances: int = 20,
                               rate_mode: str = "adaptive", result_suffix: str = "",
                               silent: bool = False, tool_success_rate: float = 0.8,
                               max_workers: int = None) -> bool:
        """è¿è¡Œè¶…é«˜å¹¶è¡Œåº¦æµ‹è¯•
        
        Args:
            model: æ¨¡å‹åç§°
            prompt_types: æç¤ºç±»å‹
            difficulty: éš¾åº¦
            task_types: ä»»åŠ¡ç±»å‹
            num_instances: å®ä¾‹æ•°
            rate_mode: é€Ÿç‡æ¨¡å¼ - "adaptive" æˆ– "fixed"
        """
        
        logger.info(f"\nğŸ”¥ å¯åŠ¨è¶…é«˜å¹¶è¡Œæµ‹è¯•")
        logger.info(f"   æ¨¡å‹: {model}")
        logger.info(f"   Promptç±»å‹: {prompt_types}") 
        logger.info(f"   éš¾åº¦: {difficulty}")
        logger.info(f"   å®ä¾‹æ•°: {num_instances}")
        logger.info(f"   é€Ÿç‡æ¨¡å¼: {rate_mode}")
        
        # åˆ›å»ºä»»åŠ¡åˆ†ç‰‡
        shards = self.create_task_shards(model, prompt_types, difficulty, 
                                        task_types, num_instances, tool_success_rate)
        
        if not shards:
            logger.error("æ— æ³•åˆ›å»ºä»»åŠ¡åˆ†ç‰‡")
            return False
            
        logger.info(f"ğŸ“Š åˆ›å»ºäº† {len(shards)} ä¸ªå¹¶è¡Œåˆ†ç‰‡")
        
        # å¹¶è¡Œå¯åŠ¨æ‰€æœ‰åˆ†ç‰‡ï¼ˆé”™å¼€å¯åŠ¨é¿å…workflowç”Ÿæˆå†²çªï¼‰
        processes = []
        start_time = time.time()
        
        # æ™ºèƒ½åˆ†ç»„å¯åŠ¨ï¼Œå¹³è¡¡å¹¶å‘æ€§å’Œç¨³å®šæ€§
        # ç­–ç•¥ï¼šç¬¬ä¸€ä¸ªåˆ†ç‰‡ç«‹å³å¯åŠ¨ï¼Œåç»­åˆ†ç‰‡å»¶è¿Ÿå¯åŠ¨
        # è¿™æ ·ç¬¬ä¸€ä¸ªåˆ†ç‰‡çš„workflowç”Ÿæˆå¯ä»¥ä¸åç»­åˆ†ç‰‡çš„å¯åŠ¨å¹¶è¡Œ
        for i, shard in enumerate(shards):
            if i == 0:
                # ç¬¬ä¸€ä¸ªåˆ†ç‰‡ç«‹å³å¯åŠ¨
                process = self.execute_shard_async(shard, rate_mode=rate_mode, result_suffix=result_suffix, silent=silent, max_workers=max_workers, shard_index=i+1)
                processes.append((shard, process))
                logger.info(f"ğŸš€ ç¬¬ä¸€ä¸ªåˆ†ç‰‡ {shard.shard_id} ç«‹å³å¯åŠ¨")
            elif i == 1:
                # ç¬¬äºŒä¸ªåˆ†ç‰‡å»¶è¿Ÿ5ç§’ï¼ˆç°åœ¨ä½¿ç”¨é¢„åŠ è½½workflowï¼Œæ— éœ€é•¿å»¶è¿Ÿï¼‰
                logger.info(f"â±ï¸  å»¶è¿Ÿ5ç§’åå¯åŠ¨ç¬¬äºŒä¸ªåˆ†ç‰‡...")
                time.sleep(5)
                process = self.execute_shard_async(shard, rate_mode=rate_mode, result_suffix=result_suffix, silent=silent, max_workers=max_workers, shard_index=i+1)
                processes.append((shard, process))
            else:
                # ç¬¬ä¸‰ä¸ªåŠåç»­åˆ†ç‰‡å»¶è¿Ÿ5ç§’ï¼ˆé¢„åŠ è½½workflowï¼Œå¿«é€Ÿå¯åŠ¨ï¼‰
                logger.info(f"â±ï¸  å»¶è¿Ÿ5ç§’åå¯åŠ¨åˆ†ç‰‡ {i+1}...")
                time.sleep(5)
                process = self.execute_shard_async(shard, rate_mode=rate_mode, result_suffix=result_suffix, silent=silent, max_workers=max_workers, shard_index=i+1)
                processes.append((shard, process))
            
        # å¹¶å‘ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆï¼ˆçœŸæ­£çš„å¹¶å‘ï¼ï¼‰
        logger.info(f"â³ å¹¶å‘ç­‰å¾… {len(processes)} ä¸ªåˆ†ç‰‡å®Œæˆ...")
        
        success_count = 0
        failed_shards = []
        completed_shards = set()
        
        # ä½¿ç”¨è½®è¯¢æ–¹å¼æ£€æŸ¥æ‰€æœ‰è¿›ç¨‹çŠ¶æ€ï¼ˆéé˜»å¡ï¼‰
        while len(completed_shards) < len(processes):
            for shard, process in processes:
                if shard.shard_id in completed_shards:
                    continue
                    
                # éé˜»å¡æ£€æŸ¥è¿›ç¨‹çŠ¶æ€
                poll_result = process.poll()
                if poll_result is not None:  # è¿›ç¨‹å·²ç»“æŸ
                    completed_shards.add(shard.shard_id)
                    
                    if poll_result == 0:
                        logger.info(f"âœ… åˆ†ç‰‡ {shard.shard_id} å®Œæˆ")
                        success_count += 1
                        self._update_performance_score(shard.instance_name, True)
                    else:
                        # è¯»å–é”™è¯¯è¾“å‡º
                        stderr_output = process.stderr.read() if process.stderr else ""
                        logger.error(f"âŒ åˆ†ç‰‡ {shard.shard_id} å¤±è´¥ (é€€å‡ºç : {poll_result})")
                        if stderr_output:
                            # æ˜¾ç¤ºæ›´å¤šé”™è¯¯ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯æœ€åçš„éƒ¨åˆ†
                            lines = stderr_output.strip().split('\n')
                            if len(lines) > 10:
                                logger.error(f"   é”™è¯¯ä¿¡æ¯ï¼ˆæœ€å10è¡Œï¼‰:")
                                for line in lines[-10:]:
                                    logger.error(f"     {line}")
                            else:
                                logger.error(f"   é”™è¯¯ä¿¡æ¯: {stderr_output}")
                        failed_shards.append(shard.shard_id)
                        self._update_performance_score(shard.instance_name, False)
                    
                    # é‡Šæ”¾å®ä¾‹
                    if shard.instance_name in self.instance_pool:
                        instance = self.instance_pool[shard.instance_name]
                        instance.is_busy = False
                        instance.current_load = max(0, instance.current_load - shard.num_instances)
                    
                    self.active_tasks.discard(shard.shard_id)
            
            # çŸ­æš‚ä¼‘çœ é¿å…CPUå ç”¨è¿‡é«˜
            if len(completed_shards) < len(processes):
                time.sleep(1)
                
        end_time = time.time()
        duration = end_time - start_time
        
        # æŠ¥å‘Šç»“æœ
        logger.info(f"\nğŸ“Š å¹¶è¡Œæµ‹è¯•å®Œæˆ")
        logger.info(f"   æˆåŠŸ: {success_count}/{len(shards)} ä¸ªåˆ†ç‰‡")
        logger.info(f"   æ€»è€—æ—¶: {duration:.1f}ç§’")
        logger.info(f"   ç†è®ºåŠ é€Ÿæ¯”: {len(shards)}x")
        
        if failed_shards:
            logger.warning(f"   å¤±è´¥åˆ†ç‰‡: {failed_shards}")
        
        # æ–°åŠŸèƒ½ï¼šæ”¶é›†å’Œèšåˆæ‰€æœ‰ç»“æœï¼ˆå¦‚æœå¯ç”¨äº†collectoræ¨¡å¼ï¼‰
        if self.use_collector_mode and success_count > 0:
            self._collect_and_aggregate_results(model)
            
        return len(failed_shards) == 0
    
    def _collect_and_aggregate_results(self, model: str):
        """æ”¶é›†å¹¶èšåˆæ‰€æœ‰åˆ†ç‰‡çš„ç»“æœï¼ˆæ–°åŠŸèƒ½ï¼‰"""
        logger.info("ğŸ”„ å¼€å§‹æ”¶é›†æ‰€æœ‰åˆ†ç‰‡çš„æµ‹è¯•ç»“æœ...")
        
        try:
            # æ”¶é›†æ‰€æœ‰å¾…å¤„ç†çš„ç»“æœ
            all_results = self.result_collector.collect_all_results(cleanup=True)
            
            if not all_results:
                logger.warning("âš ï¸ æœªå‘ç°ä»»ä½•å¾…å¤„ç†çš„ç»“æœ")
                return
            
            # èšåˆç»“æœ
            logger.info("ğŸ“Š å¼€å§‹èšåˆç»“æœ...")
            aggregated_db = self.result_aggregator.aggregate_results(all_results)
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            self._save_aggregated_results(aggregated_db)
            
            logger.info("âœ… ç»“æœæ”¶é›†å’Œèšåˆå®Œæˆï¼Œæ•°æ®å·²å®‰å…¨ä¿å­˜")
            
        except Exception as e:
            logger.error(f"âŒ ç»“æœæ”¶é›†å¤±è´¥: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè®©æµ‹è¯•ç»§ç»­å®Œæˆ
            
    def _save_aggregated_results(self, aggregated_db: Dict):
        """ä¿å­˜èšåˆåçš„ç»“æœåˆ°æ•°æ®åº“"""
        # ä½¿ç”¨ä¼ ç»Ÿçš„æ•°æ®åº“ä¿å­˜æœºåˆ¶ï¼Œä½†ç°åœ¨æ˜¯å•çº¿ç¨‹å®‰å…¨çš„
        from pathlib import Path
        import json
        
        db_file = Path("pilot_bench_cumulative_results/master_database.json")
        db_file.parent.mkdir(exist_ok=True)
        
        # å¦‚æœå·²æœ‰æ•°æ®åº“ï¼Œéœ€è¦åˆå¹¶
        if db_file.exists():
            try:
                with open(db_file, 'r', encoding='utf-8') as f:
                    existing_db = json.load(f)
                    
                # åˆå¹¶æ•°æ®åº“ï¼ˆè¿™é‡Œç°åœ¨æ˜¯å®‰å…¨çš„ï¼Œå› ä¸ºåªæœ‰ä¸€ä¸ªå†™å…¥è€…ï¼‰
                merged_db = self._merge_databases(existing_db, aggregated_db)
            except Exception as e:
                logger.warning(f"è¯»å–ç°æœ‰æ•°æ®åº“å¤±è´¥ï¼Œå°†åˆ›å»ºæ–°æ•°æ®åº“: {e}")
                merged_db = aggregated_db
        else:
            merged_db = aggregated_db
        
        # åŸå­å†™å…¥
        temp_file = db_file.with_suffix('.tmp')
        with open(temp_file, 'w', encoding='utf-8') as f:
            json.dump(merged_db, f, indent=2, ensure_ascii=False)
        
        temp_file.replace(db_file)
        logger.info(f"ğŸ’¾ æ•°æ®åº“å·²ä¿å­˜åˆ°: {db_file}")
        
    def _merge_databases(self, existing: Dict, new: Dict) -> Dict:
        """åˆå¹¶ä¸¤ä¸ªæ•°æ®åº“ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„åˆå¹¶é€»è¾‘
        # ç°åœ¨å…ˆåšç®€å•çš„æ¨¡å‹çº§åˆ«åˆå¹¶
        merged = existing.copy()
        
        if 'models' in new:
            if 'models' not in merged:
                merged['models'] = {}
            merged['models'].update(new['models'])
        
        merged['last_updated'] = new.get('last_updated', existing.get('last_updated'))
        return merged
        
    def _update_performance_score(self, instance_name: str, success: bool):
        """æ›´æ–°å®ä¾‹æ€§èƒ½è¯„åˆ†"""
        if instance_name not in self.performance_stats:
            self.performance_stats[instance_name] = {'success': 0, 'total': 0}
            
        stats = self.performance_stats[instance_name]
        stats['total'] += 1
        if success:
            stats['success'] += 1
            
        # è®¡ç®—æˆåŠŸç‡ä½œä¸ºæ€§èƒ½è¯„åˆ†
        success_rate = stats['success'] / stats['total']
        self.instance_pool[instance_name].performance_score = success_rate
        
    def get_resource_utilization(self) -> Dict:
        """è·å–èµ„æºåˆ©ç”¨ç‡ç»Ÿè®¡"""
        total_capacity = sum(inst.max_workers for inst in self.instance_pool.values())
        current_load = sum(inst.current_load for inst in self.instance_pool.values())
        busy_instances = sum(1 for inst in self.instance_pool.values() if inst.is_busy)
        
        return {
            "total_instances": len(self.instance_pool),
            "busy_instances": busy_instances,
            "total_capacity": total_capacity,
            "current_load": current_load,
            "utilization_rate": current_load / total_capacity if total_capacity > 0 else 0,
            "active_tasks": len(self.active_tasks)
        }

def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    import argparse
    import os
    
    parser = argparse.ArgumentParser(description="Ultra Parallel Test Runner")
    parser.add_argument("--model", required=True, help="æ¨¡å‹åç§°")
    parser.add_argument("--prompt-types", required=True, help="Promptç±»å‹")
    parser.add_argument("--difficulty", default="easy", help="éš¾åº¦")
    parser.add_argument("--task-types", default="all", help="ä»»åŠ¡ç±»å‹")
    parser.add_argument("--num-instances", type=int, default=20, help="å®ä¾‹æ•°é‡")
    parser.add_argument("--rate-mode", default="adaptive", 
                       choices=["adaptive", "fixed"],
                       help="é€Ÿç‡æ¨¡å¼: adaptive(åŠ¨æ€è°ƒæ•´) æˆ– fixed(å›ºå®šé€Ÿç‡)")
    parser.add_argument("--result-suffix", default="", 
                       help="ç»“æœæ–‡ä»¶åç¼€(ç”¨äºåŒºåˆ†é—­æº/å¼€æºæ¨¡å‹)")
    parser.add_argument("--silent", action="store_true",
                       help="é™é»˜æ¨¡å¼ï¼Œå‡å°‘è¾“å‡º")
    parser.add_argument("--tool-success-rate", type=float, default=0.8,
                       help="å·¥å…·æˆåŠŸç‡(0.0-1.0)")
    parser.add_argument("--max-workers", type=int, default=None,
                       help="æ¯ä¸ªåˆ†ç‰‡çš„æœ€å¤§å¹¶å‘workersæ•°")
    
    args = parser.parse_args()
    
    # ä¹Ÿå¯ä»¥ä»ç¯å¢ƒå˜é‡è¯»å–rate_mode
    rate_mode = args.rate_mode
    if os.environ.get("RATE_MODE"):
        rate_mode = os.environ.get("RATE_MODE")
        logger.info(f"ä½¿ç”¨ç¯å¢ƒå˜é‡ RATE_MODE: {rate_mode}")
    
    # æ³¨æ„ï¼šä¿ç•™loggerçš„INFOçº§åˆ«ä»¥æ˜¾ç¤ºé«˜å±‚çº§è¿›åº¦ä¿¡æ¯
    # silentå‚æ•°åªæ§åˆ¶è¯¦ç»†çš„æ‰§è¡Œè¾“å‡ºï¼Œä¸å½±å“ä¸»è¦è¿›åº¦æ˜¾ç¤º
    
    runner = UltraParallelRunner()
    
    # æ˜¾ç¤ºèµ„æºçŠ¶æ€
    util = runner.get_resource_utilization()
    logger.info(f"èµ„æºæ± çŠ¶æ€: {util['total_instances']}ä¸ªå®ä¾‹, å®¹é‡{util['total_capacity']}")
    
    # æ‰§è¡Œæµ‹è¯•
    success = runner.run_ultra_parallel_test(
        model=args.model,
        prompt_types=args.prompt_types,
        difficulty=args.difficulty,
        task_types=args.task_types,
        num_instances=args.num_instances,
        rate_mode=rate_mode,
        result_suffix=args.result_suffix,
        silent=args.silent,
        tool_success_rate=args.tool_success_rate,
        max_workers=args.max_workers
    )
    
    # æœ€ç»ˆç»Ÿè®¡
    final_util = runner.get_resource_utilization()
    logger.info(f"æœ€ç»ˆåˆ©ç”¨ç‡: {final_util['utilization_rate']:.1%}")
    
    return 0 if success else 1

if __name__ == "__main__":
    exit(main())