#!/usr/bin/env python3
"""
ç»Ÿä¸€çš„Flawed Workflowæµ‹è¯•ç»“æœå¯è§†åŒ–ç³»ç»Ÿ
=========================================
é‡æ–°è®¾è®¡ï¼Œç¡®ä¿æ•°æ®å±•ç¤ºçš„ä¸€è‡´æ€§å’Œå¯ç†è§£æ€§
"""

import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path
from collections import defaultdict
from datetime import datetime
import logging
from typing import Dict, List, Any, Tuple

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class UnifiedFlawedVisualizer:
    """ç»Ÿä¸€çš„å¯è§†åŒ–å™¨ - ç¡®ä¿æ‰€æœ‰å›¾è¡¨ä½¿ç”¨ç›¸åŒçš„æ•°æ®å¤„ç†é€»è¾‘"""
    
    def __init__(self, data_path: str = None):
        self.output_dir = Path("output")
        self.output_dir.mkdir(exist_ok=True)
        
        # å…¨å±€é…ç½®
        self.SCORE_WEIGHTS = {'success': 0.7, 'accuracy': 0.3}
        self.COLORS = {
            'baseline': '#FF6B6B',
            'optimal': '#4ECDC4', 
            'flawed': '#45B7D1',
            'light': '#FFE66D',
            'medium': '#FFA500',
            'severe': '#FF4444'
        }
        
        # åŠ è½½æ•°æ®
        self.raw_data = self._load_all_data(data_path)
        self.processed_data = self._process_data()
        
    def _load_all_data(self, data_path: str = None) -> Dict:
        """åŠ è½½æ‰€æœ‰æµ‹è¯•æ•°æ®"""
        if data_path and Path(data_path).exists():
            with open(data_path, 'r') as f:
                return json.load(f)
        
        # å¦åˆ™å°è¯•ä»å¤šä¸ªæ¥æºåŠ è½½
        all_data = {}
        patterns = [
            "output/flawed_test_*_*_*.json",
            "output/flawed_test_combined_results.json"
        ]
        
        for pattern in patterns:
            for file in Path(".").glob(pattern):
                try:
                    with open(file, 'r') as f:
                        data = json.load(f)
                        # æ ¹æ®æ–‡ä»¶ç±»å‹å¤„ç†æ•°æ®
                        if 'combined' in str(file):
                            all_data.update(self._parse_combined_data(data))
                        else:
                            all_data.update(self._parse_single_file(data))
                except Exception as e:
                    logger.warning(f"Failed to load {file}: {e}")
        
        return all_data
    
    def _parse_combined_data(self, data: Dict) -> Dict:
        """è§£æcombinedæ ¼å¼çš„æ•°æ®"""
        result = {}
        for task_type, severity_data in data.items():
            for severity, test_data in severity_data.items():
                if 'flaw_test_results' in test_data:
                    for flaw_name, flaw_data in test_data['flaw_test_results'].items():
                        key = f"{task_type}_{severity}_{flaw_name}"
                        result[key] = flaw_data
        return result
    
    def _parse_single_file(self, data: Dict) -> Dict:
        """è§£æå•ä¸ªæ–‡ä»¶çš„æ•°æ®"""
        result = {}
        task_type = data.get('task_type', 'unknown')
        severity = data.get('severity', 'unknown')
        
        if 'flaw_test_results' in data:
            for flaw_name, flaw_data in data['flaw_test_results'].items():
                key = f"{task_type}_{severity}_{flaw_name}"
                result[key] = flaw_data
        
        return result
    
    # æ–‡ä»¶ï¼šunified_visualization_system.py
    # ä½ç½®ï¼šç¬¬90-150è¡Œ
    # å‡½æ•°ï¼š_process_data

    def _process_data(self) -> Dict:
        """å¤„ç†åŸå§‹æ•°æ® - ç›´æ¥ä½¿ç”¨æµ‹è¯•ä»£ç çš„è¾“å‡ºï¼Œä¸é‡æ–°è®¡ç®—"""
        processed = {
            'by_prompt': defaultdict(list),  # æŒ‰promptç±»å‹åˆ†ç»„
            'by_severity': defaultdict(lambda: defaultdict(list)),  # æŒ‰severityå’Œpromptåˆ†ç»„
            'by_flaw_type': defaultdict(lambda: defaultdict(list)),  # æŒ‰flawç±»å‹å’Œpromptåˆ†ç»„
            'summary': {
                'total_tests': 0,
                'severities': set(),
                'flaw_types': set(),
                'task_types': set()
            }
        }
        
        for key, flaw_data in self.raw_data.items():
            # æå–å…ƒä¿¡æ¯
            parts = key.split('_')
            task_type = parts[0] if len(parts) > 0 else 'unknown'
            severity = parts[1] if len(parts) > 1 else 'unknown'
            flaw_name = '_'.join(parts[2:]) if len(parts) > 2 else 'unknown'
            
            # ä»flaw_infoè·å–æ›´å‡†ç¡®çš„ä¿¡æ¯
            flaw_info = flaw_data.get('flaw_info', {})
            severity = flaw_info.get('severity', severity)
            flaw_type = flaw_info.get('type', flaw_name.split('_')[0] if flaw_name != 'unknown' else 'unknown')
            
            # æ›´æ–°summary
            processed['summary']['severities'].add(severity)
            processed['summary']['flaw_types'].add(flaw_type)
            processed['summary']['task_types'].add(task_type)
            
            # === ä¿®æ”¹å¼€å§‹ï¼šç›´æ¥ä½¿ç”¨æµ‹è¯•ç»“æœï¼Œä¸é‡æ–°è®¡ç®— ===
            perf = flaw_data.get('performance_comparison', {})
            
            for prompt_type in ['baseline_prompt', 'optimal_prompt', 'flawed_optimal_prompt']:
                if prompt_type in perf:
                    # ç›´æ¥ä½¿ç”¨æµ‹è¯•ä»£ç è®¡ç®—çš„æ‰€æœ‰æŒ‡æ ‡
                    metrics = perf[prompt_type]
                    
                    data_point = {
                        # ä½¿ç”¨åŸå§‹æ•°æ®ï¼Œä¸é‡æ–°è®¡ç®—
                        'success_rate': metrics.get('success_rate', 0),
                        'tool_accuracy': metrics.get('tool_accuracy', 0),
                        'avg_execution_time': metrics.get('avg_execution_time', 0),
                        'error_rate': metrics.get('error_rate', 0),
                        'total_tests': metrics.get('total_tests', 0),
                        # å…ƒæ•°æ®
                        'severity': severity,
                        'flaw_type': flaw_type,
                        'task_type': task_type,
                        'flaw_name': flaw_name,
                        'key': key,
                        # å¦‚æœæµ‹è¯•ä»£ç æä¾›äº†composite_scoreï¼Œä½¿ç”¨å®ƒï¼›å¦åˆ™ä¸è®¡ç®—
                        'composite_score': metrics.get('composite_score', metrics.get('success_rate', 0))
                    }
                    
                    # å­˜å‚¨åˆ°ä¸åŒçš„åˆ†ç»„
                    processed['by_prompt'][prompt_type].append(data_point)
                    processed['by_severity'][severity][prompt_type].append(data_point)
                    processed['by_flaw_type'][flaw_type][prompt_type].append(data_point)
                    
                    processed['summary']['total_tests'] += data_point['total_tests']
            # === ä¿®æ”¹ç»“æŸ ===
        
        return processed
    
    # æ–‡ä»¶ï¼šunified_visualization_system.py
    # ä½ç½®ï¼šåœ¨ç±»ä¸­æ·»åŠ æ–°æ–¹æ³•ï¼Œçº¦ç¬¬180è¡Œå

    def validate_data_consistency(self):
        """éªŒè¯æ•°æ®ä¸€è‡´æ€§ï¼Œç¡®ä¿ä¸æµ‹è¯•ä»£ç è¾“å‡ºåŒ¹é…"""
        logger.info("Validating data consistency...")
        
        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ•°æ®ç‚¹éƒ½æœ‰å¿…è¦çš„å­—æ®µ
        required_fields = ['success_rate', 'total_tests']
        missing_data = []
        
        for prompt_type, data_points in self.processed_data['by_prompt'].items():
            for dp in data_points:
                for field in required_fields:
                    if field not in dp or dp[field] is None:
                        missing_data.append({
                            'prompt_type': prompt_type,
                            'key': dp.get('key', 'unknown'),
                            'missing_field': field
                        })
        
        if missing_data:
            logger.warning(f"Found {len(missing_data)} data points with missing fields")
            for item in missing_data[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                logger.warning(f"  {item}")
        
        # æ£€æŸ¥æ•°ValueèŒƒå›´
        for prompt_type, data_points in self.processed_data['by_prompt'].items():
            for dp in data_points:
                sr = dp.get('success_rate', 0)
                if not 0 <= sr <= 1:
                    logger.warning(f"Invalid success_rate {sr} for {dp.get('key')}")
        
        return len(missing_data) == 0
    # æ–‡ä»¶ï¼šunified_visualization_system.py
    # ä½ç½®ï¼šç¬¬160-180è¡Œ
    # å‡½æ•°ï¼š_calculate_stats

    def _calculate_stats(self, data_points: List[Dict], metric: str = 'success_rate') -> Dict:
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯ - åŸºäºæµ‹è¯•ä»£ç çš„åŸå§‹æ•°æ®"""
        if not data_points:
            return {
                'mean': 0, 
                'std': 0, 
                'count': 0, 
                'total_tests': 0,
                'raw_values': []  # ä¿å­˜åŸå§‹Valueä¾›è°ƒè¯•
            }
        
        # === ä¿®æ”¹å¼€å§‹ï¼šä¿ç•™åŸå§‹æ•°æ®çš„ç²¾ç¡®æ€§ ===
        values = [d[metric] for d in data_points]
        total_tests = sum(d.get('total_tests', 0) for d in data_points)
        
        return {
            'mean': np.mean(values) if values else 0,
            'std': np.std(values) if values else 0,
            'count': len(values),
            'total_tests': total_tests,
            'values': values,
            'raw_values': values,  # ä¿å­˜åŸå§‹Value
            'min': min(values) if values else 0,
            'max': max(values) if values else 0,
            'median': np.median(values) if values else 0
        }
        # === ä¿®æ”¹ç»“æŸ ===
    
    def visualize_unified_comparison(self):
        """åˆ›å»ºç»Ÿä¸€çš„å¯¹æ¯”å›¾ - å±•ç¤ºçœŸå®çš„æ•°æ®åˆ†å¸ƒ"""
        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        
        # 1. æ•´ä½“æ€§èƒ½å¯¹æ¯”ï¼ˆå·¦ä¸Šï¼‰
        ax1 = axes[0, 0]
        self._plot_overall_performance(ax1)
        
        # 2. æŒ‰Severityåˆ†ç»„å¯¹æ¯”ï¼ˆå³ä¸Šï¼‰
        ax2 = axes[0, 1]
        self._plot_by_severity(ax2)
        
        # 3. æ•°æ®åˆ†å¸ƒç®±çº¿å›¾ï¼ˆå·¦ä¸‹ï¼‰
        ax3 = axes[1, 0]
        self._plot_distribution_boxplot(ax3)
        
        # 4. æ ·æœ¬æ•°é‡ç»Ÿè®¡ï¼ˆå³ä¸‹ï¼‰
        ax4 = axes[1, 1]
        self._plot_sample_counts(ax4)
        
        plt.suptitle('Unified Flawed Workflow Test Analysis', fontsize=16, y=0.98)
        plt.tight_layout()
        plt.savefig(self.output_dir / 'unified_analysis.png', dpi=300, bbox_inches='tight')
        plt.close()
        logger.info("Generated: unified_analysis.png")
    
    def _plot_overall_performance(self, ax):
        """ç»˜åˆ¶æ•´ä½“æ€§èƒ½å¯¹æ¯”"""
        # è®¡ç®—æ¯ä¸ªprompt typeçš„æ•´ä½“ç»Ÿè®¡
        prompt_types = ['baseline_prompt', 'optimal_prompt', 'flawed_optimal_prompt']
        labels = ['Baseline\n(No MDP)', 'Optimal\n(With MDP)', 'Flawed\n(All Severities)']
        
        means = []
        stds = []
        counts = []
        
        for prompt_type in prompt_types:
            stats = self._calculate_stats(self.processed_data['by_prompt'][prompt_type])
            means.append(stats['mean'])
            stds.append(stats['std'])
            counts.append(stats['total_tests'])
        
        x = np.arange(len(labels))
        colors = [self.COLORS['baseline'], self.COLORS['optimal'], self.COLORS['flawed']]
        
        bars = ax.bar(x, means, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)
        ax.errorbar(x, means, yerr=stds, fmt='none', color='black', capsize=5, linewidth=2)
        
        # æ·»åŠ æ•°Valueæ ‡ç­¾
        for i, (bar, mean, count) in enumerate(zip(bars, means, counts)):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                   f'{mean:.1%}\n(n={count})', ha='center', va='bottom')
        
        ax.set_ylabel('Success Rate', fontsize=12)
        ax.set_title('Overall Performance Comparison\n(All Data Combined)', fontsize=14)
        ax.set_xticks(x)
        ax.set_xticklabels(labels)
        ax.set_ylim(0, max(means) * 1.3 if means else 1.0)
        ax.grid(axis='y', alpha=0.3)
    
    # ç›¸åŒä½ç½®çš„ä¿®å¤ä»£ç 
    def _plot_by_severity(self, ax):
        """æ˜¾ç¤º5ä¸ªç‹¬ç«‹çš„ç­–ç•¥ï¼šbaseline, optimal, flawed(light/medium/severe)"""
        # å‡†å¤‡5ä¸ªç‹¬ç«‹çš„æ¡å½¢æ•°æ®
        strategies = []
        means = []
        stds = []
        counts = []
        colors = []
        
        # 1. Baseline (æ‰€æœ‰severityåˆå¹¶)
        baseline_stats = self._calculate_stats(self.processed_data['by_prompt']['baseline_prompt'])
        strategies.append('Baseline\n(No MDP)')
        means.append(baseline_stats['mean'])
        stds.append(baseline_stats['std'])
        counts.append(baseline_stats['total_tests'])
        colors.append(self.COLORS['baseline'])
        
        # 2. Optimal (æ‰€æœ‰severityåˆå¹¶)
        optimal_stats = self._calculate_stats(self.processed_data['by_prompt']['optimal_prompt'])
        strategies.append('Optimal\n(With MDP)')
        means.append(optimal_stats['mean'])
        stds.append(optimal_stats['std'])
        counts.append(optimal_stats['total_tests'])
        colors.append(self.COLORS['optimal'])
        
        # 3-5. FlawedæŒ‰severityåˆ†å¼€
        for severity in ['light', 'medium', 'severe']:
            flawed_stats = self._calculate_stats(
                self.processed_data['by_severity'][severity]['flawed_optimal_prompt'])
            strategies.append(f'Flawed\n{severity.capitalize()}')
            means.append(flawed_stats['mean'])
            stds.append(flawed_stats['std'])
            counts.append(flawed_stats['total_tests'])
            colors.append(self.COLORS[severity])
        
        # ç»˜åˆ¶æ¡å½¢å›¾
        x = np.arange(len(strategies))
        bars = ax.bar(x, means, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)
        ax.errorbar(x, means, yerr=stds, fmt='none', color='black', capsize=5, linewidth=2)
        
        # æ·»åŠ æ•°Valueæ ‡ç­¾
        for i, (bar, mean, count) in enumerate(zip(bars, means, counts)):
            height = bar.get_height()
            ax.text(bar.get_x() + bar.get_width()/2., height + 0.02,
                f'{mean:.1%}\n(n={count})', ha='center', va='bottom', fontsize=9)
        
        ax.set_xlabel('Strategy', fontsize=12)
        ax.set_ylabel('Success Rate', fontsize=12)
        ax.set_title('Performance by Strategy\n(Unified View)', fontsize=14)
        ax.set_xticks(x)
        ax.set_xticklabels(strategies)
        ax.set_ylim(0, max(means) * 1.3 if means else 1.0)
        ax.grid(axis='y', alpha=0.3)
    
    def _plot_distribution_boxplot(self, ax):
        """ç»˜åˆ¶æ•°æ®åˆ†å¸ƒç®±çº¿å›¾"""
        # å‡†å¤‡æ•°æ®
        data_for_plot = []
        labels = []
        colors = []
        
        # Baselineå’ŒOptimalï¼ˆæ‰€æœ‰severityåˆå¹¶ï¼‰
        for prompt_type, label, color in [
            ('baseline_prompt', 'Baseline', self.COLORS['baseline']),
            ('optimal_prompt', 'Optimal', self.COLORS['optimal'])
        ]:
            values = [d['success_rate'] for d in self.processed_data['by_prompt'][prompt_type]]
            if values:
                data_for_plot.append(values)
                labels.append(label)
                colors.append(color)
        
        # FlawedæŒ‰severityåˆ†ç»„
        for severity in ['light', 'medium', 'severe']:
            values = [d['success_rate'] for d in 
                     self.processed_data['by_severity'][severity]['flawed_optimal_prompt']]
            if values:
                data_for_plot.append(values)
                labels.append(f'Flawed\n{severity.capitalize()}')
                colors.append(self.COLORS[severity])
        
        # ç»˜åˆ¶ç®±çº¿å›¾
        bp = ax.boxplot(data_for_plot, labels=labels, patch_artist=True, notch=True)
        
        # è®¾ç½®é¢œè‰²
        for patch, color in zip(bp['boxes'], colors):
            patch.set_facecolor(color)
            patch.set_alpha(0.7)
        
        # æ·»åŠ æ•°æ®ç‚¹æ•°é‡
        for i, data in enumerate(data_for_plot):
            ax.text(i + 1, -0.08, f'n={len(data)}', ha='center', 
                   transform=ax.get_xaxis_transform(), fontsize=9)
        
        ax.set_ylabel('Success Rate Distribution', fontsize=12)
        ax.set_title('Success Rate Distribution\n(Showing Data Spread)', fontsize=14)
        ax.set_ylim(-0.1, 1.1)
        ax.grid(axis='y', alpha=0.3)
    
    def _plot_sample_counts(self, ax):
        """ç»˜åˆ¶æ ·æœ¬æ•°é‡ç»Ÿè®¡"""
        # åˆ›å»ºè¡¨æ ¼æ•°æ®
        severities = ['light', 'medium', 'severe']
        prompt_types = ['baseline_prompt', 'optimal_prompt', 'flawed_optimal_prompt']
        
        # æ”¶é›†æ•°æ®
        table_data = []
        for severity in severities:
            row = [severity.capitalize()]
            for prompt_type in prompt_types:
                stats = self._calculate_stats(
                    self.processed_data['by_severity'][severity][prompt_type])
                row.append(f"{stats['count']} flaws\n{stats['total_tests']} tests")
            table_data.append(row)
        
        # æ·»åŠ æ€»è®¡è¡Œ
        total_row = ['Total']
        for prompt_type in prompt_types:
            stats = self._calculate_stats(self.processed_data['by_prompt'][prompt_type])
            total_row.append(f"{stats['count']} flaws\n{stats['total_tests']} tests")
        table_data.append(total_row)
        
        # åˆ›å»ºè¡¨æ ¼
        col_labels = ['Severity', 'Baseline', 'Optimal', 'Flawed']
        
        # éšè—åæ ‡è½´
        ax.axis('tight')
        ax.axis('off')
        
        # åˆ›å»ºè¡¨æ ¼
        table = ax.table(cellText=table_data, colLabels=col_labels,
                        cellLoc='center', loc='center',
                        colWidths=[0.2, 0.25, 0.25, 0.25])
        
        # è®¾ç½®æ ·å¼
        table.auto_set_font_size(False)
        table.set_fontsize(10)
        table.scale(1, 2)
        
        # è®¾ç½®æ ‡é¢˜
        ax.set_title('Sample Size Summary\n(Flaw Types and Test Counts)', 
                    fontsize=14, pad=20)
    
    def visualize_detailed_severity_analysis(self):
        """åˆ›å»ºè¯¦ç»†çš„severityåˆ†æå›¾"""
        fig, axes = plt.subplots(1, 3, figsize=(18, 6))
        
        severities = ['light', 'medium', 'severe']
        
        for i, severity in enumerate(severities):
            ax = axes[i]
            self._plot_severity_detail(ax, severity)
        
        plt.suptitle('Detailed Analysis by Severity Level', fontsize=16)
        plt.tight_layout()
        plt.savefig(self.output_dir / 'severity_detailed_analysis.png', 
                   dpi=300, bbox_inches='tight')
        plt.close()
        logger.info("Generated: severity_detailed_analysis.png")
    
    def _plot_severity_detail(self, ax, severity):
        """ç»˜åˆ¶å•ä¸ªseverityçš„è¯¦ç»†åˆ†æ"""
        prompt_types = ['baseline_prompt', 'optimal_prompt', 'flawed_optimal_prompt']
        labels = ['Baseline', 'Optimal', 'Flawed']
        colors = [self.COLORS['baseline'], self.COLORS['optimal'], self.COLORS['flawed']]
        
        # æ”¶é›†è¯¥severityä¸‹çš„æ‰€æœ‰æ•°æ®
        data_by_prompt = []
        for prompt_type in prompt_types:
            values = [d['success_rate'] for d in 
                     self.processed_data['by_severity'][severity][prompt_type]]
            data_by_prompt.append(values)
        
        # åˆ›å»ºå°æç´å›¾
        positions = np.arange(len(prompt_types))
        
        # ç»˜åˆ¶å°æç´å›¾
        parts = ax.violinplot(data_by_prompt, positions=positions, 
                             showmeans=True, showmedians=True)
        
        # è®¾ç½®é¢œè‰²
        for pc, color in zip(parts['bodies'], colors):
            pc.set_facecolor(color)
            pc.set_alpha(0.7)
        
        # æ·»åŠ æ•£ç‚¹å›¾æ˜¾ç¤ºå®é™…æ•°æ®ç‚¹
        for i, (data, color) in enumerate(zip(data_by_prompt, colors)):
            if data:
                y = data
                x = np.random.normal(i, 0.04, size=len(y))
                ax.scatter(x, y, alpha=0.4, s=30, color=color)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        stats_text = []
        for i, (prompt_type, data) in enumerate(zip(prompt_types, data_by_prompt)):
            if data:
                mean = np.mean(data)
                std = np.std(data)
                stats_text.append(f"{labels[i]}: {mean:.1%}Â±{std:.1%} (n={len(data)})")
        
        ax.text(0.02, 0.98, '\n'.join(stats_text), 
               transform=ax.transAxes, va='top', fontsize=10,
               bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))
        
        ax.set_xticks(positions)
        ax.set_xticklabels(labels)
        ax.set_ylabel('Success Rate')
        ax.set_title(f'{severity.capitalize()} Severity', fontsize=14)
        ax.set_ylim(-0.05, 1.05)
        ax.grid(axis='y', alpha=0.3)
    
    # æ–‡ä»¶ï¼šunified_visualization_system.py  
    # ä½ç½®ï¼šä¿®æ”¹generate_comprehensive_reportæ–¹æ³•ï¼Œçº¦ç¬¬400è¡Œ

    def generate_comprehensive_report(self):
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š - åŒ…å«æ•°æ®æ¥æºä¿¡æ¯"""
        report_path = self.output_dir / 'comprehensive_flawed_analysis_report.md'
        
        with open(report_path, 'w') as f:
            f.write("# Comprehensive Flawed Workflow Analysis Report\n\n")
            f.write(f"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # === æ–°å¢ï¼šæ•°æ®æ¥æºè¯´æ˜ ===
            f.write("## Data Source\n\n")
            f.write("This report is based on the raw output from workflow_quality_test.py\n")
            f.write("All success rates and metrics are directly from the test execution results.\n\n")
            
            # æ•°æ®éªŒè¯Status
            f.write("### Data Validation\n\n")
            if self.validate_data_consistency():
                f.write("âœ… All data points have required fields\n\n")
            else:
                f.write("âš ï¸ Some data points have missing fields\n\n")
            
            # æ•°æ®æ¦‚è§ˆ
            f.write("## Dataset Overview\n\n")
            summary = self.processed_data['summary']
            f.write(f"- Total tests conducted: {summary['total_tests']}\n")
            f.write(f"- Severities tested: {', '.join(sorted(summary['severities']))}\n")
            f.write(f"- Flaw types: {len(summary['flaw_types'])}\n")
            f.write(f"- Task types: {', '.join(sorted(summary['task_types']))}\n\n")
            
            # === æ–°å¢ï¼šæ˜¾ç¤ºè¯„åˆ†æ ‡å‡†æ¥æº ===
            f.write("## Scoring Standards\n\n")
            f.write("**Note**: All scores shown are from the test execution, not recalculated.\n")
            f.write("The test code uses the following evaluation:\n")
            f.write("- Task requirements from generated task instances\n")
            f.write("- Tool dependencies from tool_task_generator\n")
            f.write("- Objective scoring based on tool coverage and dependency satisfaction\n\n")
            
            # Key Findingï¼ˆç»§ç»­ä½¿ç”¨åŸæœ‰é€»è¾‘ï¼Œä½†æ˜ç¡®è¯´æ˜æ•°æ®æ¥æºï¼‰
            # ... å…¶ä½™ä»£ç ä¿æŒä¸å˜
            
            # è¯„åˆ†æ ‡å‡†
            f.write("## Scoring Standards\n\n")
            f.write(f"**Composite Score** = {self.SCORE_WEIGHTS['success']}Ã—Success Rate + ")
            f.write(f"{self.SCORE_WEIGHTS['accuracy']}Ã—Tool Accuracy\n\n")
            
            # Key Finding
            f.write("## Key Findings\n\n")
            
            # æ•´ä½“æ€§èƒ½
            f.write("### Overall Performance\n\n")
            f.write("| Prompt Type | Mean Success Rate | Std Dev | Total Tests |\n")
            f.write("|-------------|-------------------|---------|-------------|\n")
            
            for prompt_type, label in [
                ('baseline_prompt', 'Baseline (No MDP)'),
                ('optimal_prompt', 'Optimal (With MDP)'),
                ('flawed_optimal_prompt', 'Flawed (All)')
            ]:
                stats = self._calculate_stats(self.processed_data['by_prompt'][prompt_type])
                f.write(f"| {label} | {stats['mean']:.1%} | {stats['std']:.1%} | ")
                f.write(f"{stats['total_tests']} |\n")
            
            # æŒ‰Severityåˆ†æ
            f.write("\n### Performance by Severity\n\n")
            
            for severity in ['light', 'medium', 'severe']:
                f.write(f"\n**{severity.capitalize()} Severity:**\n\n")
                f.write("| Prompt Type | Mean Success Rate | Sample Size |\n")
                f.write("|-------------|-------------------|-------------|\n")
                
                for prompt_type, label in [
                    ('baseline_prompt', 'Baseline'),
                    ('optimal_prompt', 'Optimal'),
                    ('flawed_optimal_prompt', 'Flawed')
                ]:
                    stats = self._calculate_stats(
                        self.processed_data['by_severity'][severity][prompt_type])
                    f.write(f"| {label} | {stats['mean']:.1%} | {stats['count']} |\n")
            
            # æ•°æ®è´¨é‡æ£€æŸ¥
            f.write("\n## Data Quality Check\n\n")
            
            # æ£€æŸ¥æ•°æ®å¹³è¡¡æ€§
            f.write("### Data Balance\n\n")
            severity_counts = defaultdict(int)
            for severity in ['light', 'medium', 'severe']:
                for prompt_type in ['baseline_prompt', 'optimal_prompt', 'flawed_optimal_prompt']:
                    count = len(self.processed_data['by_severity'][severity][prompt_type])
                    severity_counts[severity] += count
            
            imbalance_ratio = max(severity_counts.values()) / min(severity_counts.values()) \
                             if min(severity_counts.values()) > 0 else float('inf')
            
            if imbalance_ratio > 2:
                f.write("âš ï¸ **Warning**: Significant data imbalance detected between severities.\n")
                f.write(f"Imbalance ratio: {imbalance_ratio:.1f}\n\n")
            else:
                f.write("âœ… Data is reasonably balanced across severities.\n\n")
            
            # å»ºè®®
            f.write("## Recommendations\n\n")
            
            # åŸºäºæ•°æ®çš„å»ºè®®
            baseline_stats = self._calculate_stats(self.processed_data['by_prompt']['baseline_prompt'])
            optimal_stats = self._calculate_stats(self.processed_data['by_prompt']['optimal_prompt'])
            
            improvement = (optimal_stats['mean'] - baseline_stats['mean']) / baseline_stats['mean'] \
                         if baseline_stats['mean'] > 0 else float('inf')
            
            if improvement > 0:
                f.write(f"1. MDP guidance shows {improvement:.0%} improvement over baseline.\n")
            
            # æ£€æŸ¥flawedæ€§èƒ½
            for severity in ['light', 'medium', 'severe']:
                flawed_stats = self._calculate_stats(
                    self.processed_data['by_severity'][severity]['flawed_optimal_prompt'])
                if flawed_stats['mean'] > optimal_stats['mean']:
                    f.write(f"2. Flawed workflows with {severity} severity ")
                    f.write(f"outperform optimal ({flawed_stats['mean']:.1%} vs {optimal_stats['mean']:.1%}), ")
                    f.write("suggesting the model may be overfitting to specific patterns.\n")
                    break
        
        logger.info("Generated: comprehensive_flawed_analysis_report.md")
    # æ–‡ä»¶ï¼šunified_visualization_system.py
    # ä½ç½®ï¼šæ·»åŠ æ–°æ–¹æ³•ï¼Œç”¨äºè°ƒè¯•æ•°æ®æµ

    def trace_data_point(self, key: str):
        """è·Ÿè¸ªç‰¹å®šæ•°æ®ç‚¹ï¼Œç”¨äºè°ƒè¯•"""
        logger.info(f"Tracing data point: {key}")
        
        # åœ¨åŸå§‹æ•°æ®ä¸­æŸ¥æ‰¾
        if key in self.raw_data:
            raw = self.raw_data[key]
            logger.info("Raw data found:")
            logger.info(f"  Flaw info: {raw.get('flaw_info')}")
            perf = raw.get('performance_comparison', {})
            for prompt_type, metrics in perf.items():
                logger.info(f"  {prompt_type}:")
                logger.info(f"    - success_rate: {metrics.get('success_rate')}")
                logger.info(f"    - total_tests: {metrics.get('total_tests')}")
        
        # åœ¨å¤„ç†åçš„æ•°æ®ä¸­æŸ¥æ‰¾
        for prompt_type, data_points in self.processed_data['by_prompt'].items():
            for dp in data_points:
                if dp.get('key') == key:
                    logger.info(f"Processed data in {prompt_type}:")
                    logger.info(f"  - success_rate: {dp.get('success_rate')}")
                    logger.info(f"  - Used in visualization: Yes")
                    break

    # æ–‡ä»¶ï¼šworkflow_quality_test_flawed.py
    # ä½ç½®ï¼š_calculate_flaw_performanceæ–¹æ³•ï¼Œçº¦ç¬¬1400è¡Œ

    def _calculate_flaw_performance(self, test_results: Dict[str, List]) -> Dict:
        """è®¡ç®—ä¸åŒpromptåœ¨flawed workflowä¸Šçš„æ€§èƒ½"""
        performance = {}
        
        for prompt_type, results in test_results.items():
            if results:
                success_count = sum(1 for r in results if r.success)
                total_count = len(results)
                avg_time = sum(r.execution_time for r in results) / total_count
                
                # è®¡ç®—å·¥å…·è°ƒç”¨å‡†ç¡®ç‡
                correct_calls = 0
                total_calls = 0
                for r in results:
                    total_calls += len(r.tool_calls)
                    # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„æ­£ç¡®æ€§åˆ¤æ–­é€»è¾‘
                    if r.success and r.tool_calls:
                        correct_calls += len(r.tool_calls)
                
                performance[prompt_type] = {
                    'success_rate': success_count / total_count if total_count > 0 else 0,
                    'avg_execution_time': avg_time,
                    'tool_accuracy': correct_calls / total_calls if total_calls > 0 else 0,
                    'error_rate': 1 - (success_count / total_count) if total_count > 0 else 1,
                    'total_tests': total_count,
                    # === æ–°å¢ï¼šä¿å­˜æ›´å¤šç»†èŠ‚ä¾›å¯è§†åŒ–ä½¿ç”¨ ===
                    'success_count': success_count,
                    'total_calls': total_calls,
                    'correct_calls': correct_calls,
                    'test_ids': [r.test_id for r in results],  # ç”¨äºè¿½è¸ª
                    'individual_scores': [getattr(r, 'task_score', 0.0) for r in results]  # å¦‚æœæœ‰çš„è¯
                }
            else:
                performance[prompt_type] = {
                    'success_rate': 0,
                    'avg_execution_time': 0,
                    'tool_accuracy': 0,
                    'error_rate': 1,
                    'total_tests': 0,
                    'success_count': 0,
                    'total_calls': 0,
                    'correct_calls': 0,
                    'test_ids': [],
                    'individual_scores': []
                }
        
        return performance

    def run_all_visualizations(self):
        """è¿è¡Œæ‰€æœ‰å¯è§†åŒ–"""
        logger.info("Starting unified visualization generation...")
        
        # è®¾ç½®ç»Ÿä¸€æ ·å¼
        plt.style.use('seaborn-v0_8-whitegrid')
        plt.rcParams['figure.facecolor'] = 'white'
        plt.rcParams['axes.facecolor'] = 'white'
        
        # ç”Ÿæˆå¯è§†åŒ–
        self.visualize_unified_comparison()
        self.visualize_detailed_severity_analysis()
        self.generate_comprehensive_report()
        
        logger.info("All visualizations completed!")
        
        # æ‰“å°æ•°æ®æ‘˜è¦
        print("\nğŸ“Š Data Summary:")
        print(f"Total test entries: {len(self.raw_data)}")
        print(f"Total tests conducted: {self.processed_data['summary']['total_tests']}")
        print(f"Severities: {', '.join(sorted(self.processed_data['summary']['severities']))}")
        print(f"Unique flaw types: {len(self.processed_data['summary']['flaw_types'])}")


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description='Generate unified flawed workflow visualizations')
    parser.add_argument('--data', type=str, help='Path to data file')
    parser.add_argument('--output-dir', type=str, default='output', help='Output directory')
    
    args = parser.parse_args()
    
    try:
        visualizer = UnifiedFlawedVisualizer(data_path=args.data)
        visualizer.run_all_visualizations()
        print("\nâœ… All visualizations generated successfully!")
        print(f"ğŸ“ Check the '{visualizer.output_dir}' directory for results.")
        
    except Exception as e:
        print(f"\nâŒ Error: {e}")
        import traceback
        traceback.print_exc()