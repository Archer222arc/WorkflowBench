#!/usr/bin/env python3
"""
æ™ºèƒ½æ‰¹æµ‹è¯•è¿è¡Œå™¨ - å¢å¼ºç‰ˆ
ä¿æŒåŸæœ‰æ¥å£ï¼Œä½†å†…éƒ¨ä½¿ç”¨BatchTestRunnerä»¥è·å¾—å®Œæ•´åŠŸèƒ½
æ”¯æŒç¼“å­˜æ¨¡å¼é¿å…å¹¶å‘ç«äº‰æ¡ä»¶
æ–°å¢ï¼šè‡ªåŠ¨å¤±è´¥ç»´æŠ¤å’ŒåŸºäºè¿›åº¦çš„å¢é‡é‡æµ‹
"""
import json
import os
import sys
import subprocess
import argparse
import tempfile
import shutil
from typing import List, Dict, Optional, Any
from pathlib import Path
from datetime import datetime
import time

sys.path.insert(0, str(Path(__file__).parent))
from batch_test_runner import BatchTestRunner, TestTask
# æ”¯æŒå­˜å‚¨æ ¼å¼é€‰æ‹©
import os
storage_format = os.environ.get('STORAGE_FORMAT', 'json').lower()

# å¯é€‰æ”¯æŒResultCollector
try:
    from result_collector import ResultCollector
    from result_collector_adapter import AdaptiveResultCollector
    RESULT_COLLECTOR_AVAILABLE = True
except ImportError:
    RESULT_COLLECTOR_AVAILABLE = False
if storage_format == 'parquet':
    try:
        from parquet_cumulative_manager import ParquetCumulativeManager as EnhancedCumulativeManager
        print(f"[INFO] ä½¿ç”¨Parquetå­˜å‚¨æ ¼å¼")
    except ImportError as e:
        print(f"[WARNING] Parquetå¯¼å…¥å¤±è´¥: {e}")
        from enhanced_cumulative_manager import EnhancedCumulativeManager
        print(f"[INFO] å›é€€åˆ°JSONå­˜å‚¨æ ¼å¼")
else:
    from enhanced_cumulative_manager import EnhancedCumulativeManager
    print(f"[INFO] ä½¿ç”¨JSONå­˜å‚¨æ ¼å¼")
# TestRecordæ€»æ˜¯ä»cumulative_test_managerå¯¼å…¥
from cumulative_test_manager import TestRecord
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor, as_completed
from collections import defaultdict
import threading
from api_client_manager import MODEL_PROVIDER_MAP
from database_utils import load_database, get_completed_count

# å…¨å±€managerå®ä¾‹ç¼“å­˜ï¼Œé¿å…é‡å¤åˆ›å»ºå¯¼è‡´ç¼“å­˜ä¸¢å¤±
# è¿™æ˜¯å…³é”®ä¿®å¤ï¼šParquetæ ¼å¼éœ€è¦ç´¯ç§¯ç¼“å­˜æ‰èƒ½æ­£ç¡®ä¿å­˜æ±‡æ€»æ•°æ®
_global_manager_cache = {}
_manager_lock = threading.Lock()

def _get_or_create_manager(use_ai_classification=False, result_suffix=''):
    """è·å–æˆ–åˆ›å»ºmanagerå®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    cache_key = f"{use_ai_classification}_{result_suffix}"
    
    with _manager_lock:
        if cache_key not in _global_manager_cache:
            # åˆ›å»ºæ–°çš„managerå®ä¾‹
            _global_manager_cache[cache_key] = EnhancedCumulativeManager(
                use_ai_classification=use_ai_classification, 
                db_suffix=result_suffix
            )
            print(f"[DEBUG] åˆ›å»ºæ–°çš„managerå®ä¾‹: key={cache_key}")
        
        return _global_manager_cache[cache_key]

def _flush_all_managers():
    """åˆ·æ–°æ‰€æœ‰managerçš„ç¼“å­˜åˆ°ç£ç›˜"""
    with _manager_lock:
        for cache_key, manager in _global_manager_cache.items():
            if hasattr(manager, '_flush_buffer'):
                manager._flush_buffer()
                print(f"[INFO] åˆ·æ–°managerç¼“å­˜: key={cache_key}")

# æ³¨å†Œé€€å‡ºæ—¶çš„æ¸…ç†å‡½æ•°
import atexit
atexit.register(_flush_all_managers)

def _save_results_to_database(results, model, difficulty, use_ai_classification=False, result_suffix=''):
    """ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ•°æ®åº“"""
    if not results:
        return 0
    
    # ä½¿ç”¨å•ä¾‹managerï¼Œé¿å…é‡å¤åˆ›å»ºå¯¼è‡´ç¼“å­˜ä¸¢å¤±
    # è¿™æ˜¯å…³é”®ä¿®å¤ï¼šç¡®ä¿Parquetæ ¼å¼èƒ½ç´¯ç§¯ç¼“å­˜
    manager = _get_or_create_manager(use_ai_classification=use_ai_classification, result_suffix=result_suffix)
    success_commit = 0
    
    for result in results:
        if result and not result.get('_saved', False):
            record = TestRecord(
                model=model,
                task_type=result.get("task_type", "unknown"),
                prompt_type=result.get("prompt_type", "baseline"),
                difficulty=result.get("difficulty", difficulty)
            )
            # è®¾ç½®å…¶ä»–å­—æ®µ
            for field in ['timestamp', 'success', 'success_level', 'execution_time', 'turns', 
                        'tool_calls', 'workflow_score', 'phase2_score', 'quality_score', 
                        'final_score', 'error_type', 'tool_success_rate', 'is_flawed', 
                        'flaw_type', 'format_error_count', 'api_issues', 'executed_tools', 
                        'required_tools', 'tool_coverage_rate', 'task_instance', 'execution_history']:
                if field in result:
                    setattr(record, field, result[field])
            
            try:
                manager.add_test_result_with_classification(record)
                result['_saved'] = True  # æ ‡è®°å·²ä¿å­˜
                success_commit += 1
            except Exception as e:
                print(f"âš ï¸  æäº¤å¤±è´¥: {e}")
    
    # å®šæœŸåˆ·æ–°ç¼“å†²åŒºï¼ˆæ¯10ä¸ªç»“æœåˆ·æ–°ä¸€æ¬¡ï¼Œé¿å…é¢‘ç¹IOï¼‰
    # å¯¹äºParquetæ ¼å¼è¿™å¾ˆé‡è¦ï¼Œå› ä¸ºéœ€è¦ç´¯ç§¯ä¸€å®šæ•°é‡æ‰å€¼å¾—å†™å…¥
    if success_commit > 0 and success_commit % 10 == 0:
        if hasattr(manager, '_flush_buffer'):
            manager._flush_buffer()
            print(f"[INFO] åˆ·æ–°ç¼“å­˜åˆ°ç£ç›˜ (å·²ç´¯ç§¯{success_commit}ä¸ªç»“æœ)")
    
    if success_commit > 0:
        print(f"âœ… å·²ä¿å­˜ {success_commit} ä¸ªæµ‹è¯•ç»“æœåˆ°æ•°æ®åº“")
        # æœ€åå†åˆ·æ–°ä¸€æ¬¡ï¼Œç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½å†™å…¥
        if hasattr(manager, '_flush_buffer'):
            manager._flush_buffer()
    
    return success_commit

# load_databaseå‡½æ•°å·²ç§»åŠ¨åˆ°database_utils.py

def get_incomplete_tests_for_model(model: str) -> Dict[str, List[Dict]]:
    """è·å–æŒ‡å®šæ¨¡å‹æœªå®Œæˆçš„æµ‹è¯•é…ç½®"""
    # é¿å…å¾ªç¯å¯¼å…¥ï¼Œåœ¨å‡½æ•°å†…éƒ¨å¯¼å…¥
    from auto_failure_maintenance_system import AutoFailureMaintenanceSystem
    maintenance_system = AutoFailureMaintenanceSystem(enable_auto_retry=False)
    incomplete_tests = maintenance_system.get_incomplete_tests([model])
    return incomplete_tests.get(model, [])

# get_completed_countå‡½æ•°å·²ç§»åŠ¨åˆ°database_utils.py

def run_batch_test_smart(model: str, prompt_types: str, difficulty: str, 
                         task_types: str, num_instances: int, deployment: str = None,
                         adaptive: bool = True, tool_success_rate: float = 0.8, 
                         batch_commit: bool = True, checkpoint_interval: int = 10, 
                         use_provider_parallel: bool = False, 
                         use_result_collector: bool = None, **kwargs):
    """
    æ™ºèƒ½è¿è¡Œæ‰¹æµ‹è¯•ï¼Œè‡ªåŠ¨è®¡ç®—éœ€è¦è¡¥å……çš„æµ‹è¯•æ•°é‡
    ä¿æŒåŸæœ‰æ¥å£ï¼Œä½†ä½¿ç”¨BatchTestRunneræ‰§è¡Œ
    æ”¯æŒbatch_commitæ‰¹é‡æäº¤é¿å…å¹¶å‘ç«äº‰
    æ”¯æŒcheckpoint_intervalä¸­é—´ä¿å­˜ï¼ˆæ¯Nä¸ªæµ‹è¯•ä¿å­˜ä¸€æ¬¡ï¼‰
    æ”¯æŒå¤šprompt typeså¹¶è¡Œæ‰§è¡Œ
    """
    # è§£æprompt types - æ”¯æŒå¤šä¸ªprompt types
    if prompt_types == "all":
        prompt_list = ["baseline", "cot", "optimal"]
    elif prompt_types == "all_with_flawed":
        prompt_list = ["baseline", "cot", "optimal"] + [
            f"flawed_{ft}" for ft in [
                "sequence_disorder", "tool_misuse", "parameter_error",
                "missing_step", "redundant_operations", 
                "logical_inconsistency", "semantic_drift"
            ]
        ]
    elif "," in prompt_types:
        # æ”¯æŒé€—å·åˆ†éš”çš„å¤šä¸ªprompt types
        prompt_list = [p.strip() for p in prompt_types.split(",")]
    else:
        # å•ä¸ªprompt type
        prompt_list = [prompt_types]
    
    # è§£æä»»åŠ¡ç±»å‹
    if task_types == "all":
        task_list = ["simple_task", "basic_task", "data_pipeline", 
                    "api_integration", "multi_stage_pipeline"]
    else:
        task_list = task_types.split(",")
    
    # åˆ¤æ–­æ˜¯å¦åº”è¯¥ä½¿ç”¨å¤špromptå¹¶è¡Œ
    use_prompt_parallel = len(prompt_list) > 1 and kwargs.get('prompt_parallel', True)
    provider = MODEL_PROVIDER_MAP.get(model, 'idealab')
    silent = kwargs.get('silent', False)
    
    if not silent:
        print(f"\n{'='*60}")
        print(f"æ™ºèƒ½æ‰¹æµ‹è¯•: {model} ({provider})")
        print(f"Prompt types: {prompt_list}")
        print(f"éš¾åº¦: {difficulty}")
        print(f"ç›®æ ‡: æ¯ç§é…ç½® {num_instances} ä¸ªå®ä¾‹")
        if use_prompt_parallel:
            if provider in ['azure', 'user_azure']:
                print(f"ç­–ç•¥: æ‰€æœ‰{len(prompt_list)}ä¸ªprompt typesåŒæ—¶å¹¶è¡Œï¼ˆé«˜å¹¶å‘ï¼‰")
            else:
                print(f"ç­–ç•¥: {len(prompt_list)}ä¸ªprompt typesä½¿ç”¨ä¸åŒAPI keyså¹¶è¡Œ")
        print(f"{'='*60}")
    
    # å¦‚æœä½¿ç”¨å¤špromptå¹¶è¡Œï¼Œç›´æ¥æ„å»ºæ‰€æœ‰ä»»åŠ¡
    if use_prompt_parallel:
        return _run_multi_prompt_parallel(
            model=model,
            prompt_list=prompt_list,
            task_list=task_list,
            difficulty=difficulty,
            num_instances=num_instances,
            tool_success_rate=tool_success_rate,
            provider=provider,
            adaptive=adaptive,
            batch_commit=batch_commit,
            checkpoint_interval=checkpoint_interval,
            deployment=deployment,
            **kwargs
        )
    
    # å•prompt typeçš„åŸæœ‰é€»è¾‘
    prompt_type = prompt_list[0]
    
    # æ£€æŸ¥æ¯ç§ä»»åŠ¡ç±»å‹çš„å®Œæˆæƒ…å†µ
    total_needed = 0
    tasks_to_run = []
    
    # åˆ¤æ–­æ˜¯å¦æ˜¯ç¼ºé™·æµ‹è¯•
    is_flawed = prompt_type.startswith("flawed_")
    flaw_type = prompt_type.replace("flawed_", "") if is_flawed else None
    
    for task_type in task_list:
        completed = get_completed_count(model, prompt_type, difficulty, task_type, tool_success_rate)
        needed = max(0, num_instances - completed)
        total_needed += needed
        
        status_symbol = "âœ“" if needed == 0 else "â—‹"
        if not silent:
            print(f"{status_symbol} {task_type:20s}: {completed:3d}/{num_instances:3d} å·²å®Œæˆ", end="")
            if needed > 0:
                print(f" (éœ€è¦è¡¥å…… {needed} ä¸ª)")
            else:
                print(" [è·³è¿‡]")
        
        if needed > 0:
            # è®°å½•éœ€è¦è¿è¡Œçš„ä»»åŠ¡
            tasks_to_run.append({
                'task_type': task_type,
                'count': needed
            })
    
    if total_needed == 0:
        if not silent:
            print(f"\nâœ… æ‰€æœ‰æµ‹è¯•å·²å®Œæˆï¼Œè·³è¿‡æ­¤é…ç½®")
        return True
    
    if not silent:
        print(f"\nâ³ éœ€è¦è¿è¡Œ {total_needed} ä¸ªæ–°æµ‹è¯•")
    
    # æ„å»ºæ‰€æœ‰æµ‹è¯•ä»»åŠ¡
    all_tasks = []
    for task_info in tasks_to_run:
        task_type = task_info['task_type']
        count = task_info['count']
        
        if not silent:
            print(f"\nâ–¶ å‡†å¤‡ {task_type} ({count} ä¸ªå®ä¾‹)...")
        
        for _ in range(count):
            task = TestTask(
                model=model,  # ä¿æŒåŸå§‹å¤§å°å†™ï¼Œnormalizeä¼šåœ¨å­˜å‚¨æ—¶å¤„ç†
                deployment=deployment,  # APIè°ƒç”¨ç”¨çš„éƒ¨ç½²å
                task_type=task_type,
                prompt_type=prompt_type,  # ä¿æŒåŸå§‹çš„prompt_typeï¼Œä¸è¦ç®€åŒ–
                difficulty=difficulty,
                is_flawed=is_flawed,
                flaw_type=flaw_type,
                tool_success_rate=tool_success_rate
            )
            all_tasks.append(task)
    
    # è¿è¡Œæ‰¹é‡æµ‹è¯•
    if not silent:
        print(f"\nâ–¶ å¼€å§‹æ‰§è¡Œ {len(all_tasks)} ä¸ªæµ‹è¯•...")
    
    # æ™ºèƒ½checkpoint_intervalå¤„ç†
    if batch_commit:
        if checkpoint_interval is None:
            # æ ¹æ®æµ‹è¯•è§„æ¨¡è‡ªåŠ¨è°ƒæ•´
            if num_instances <= 5:
                checkpoint_interval = max(1, num_instances)  # å°è§„æ¨¡æµ‹è¯•ä½¿ç”¨å°é˜ˆå€¼
            else:
                checkpoint_interval = min(10, num_instances // 2)  # å¤§è§„æ¨¡æµ‹è¯•ä½¿ç”¨é€‚ä¸­é˜ˆå€¼
        
        if not silent:
            print(f"ğŸ“Š è‡ªé€‚åº”checkpoint_interval: {checkpoint_interval}")
            if checkpoint_interval > 0:
                print(f"ğŸ“¦ æ‰¹é‡æäº¤æ¨¡å¼ï¼šæ¯{checkpoint_interval}ä¸ªæµ‹è¯•ä¿å­˜ä¸€æ¬¡")
            else:
                print("ğŸ“¦ æ‰¹é‡æäº¤æ¨¡å¼ï¼šæµ‹è¯•å®Œæˆåç»Ÿä¸€å†™å…¥æ•°æ®åº“")
    
    save_logs = kwargs.get('save_logs', True)
    workers = kwargs.get('max_workers', 10)
    qps = kwargs.get('qps', 20.0)
    use_ai_classification = kwargs.get('ai_classification', True)  # è·å–AIåˆ†ç±»å‚æ•°
    
    # å¦‚æœä½¿ç”¨åŸºäºæä¾›å•†çš„å¹¶è¡Œç­–ç•¥
    if use_provider_parallel:
        if not silent:
            print("\nğŸš€ ä½¿ç”¨åŸºäºAPIæä¾›å•†çš„å¹¶è¡Œç­–ç•¥ï¼ˆé›†æˆç‰ˆï¼‰")
        
        # æŒ‰æä¾›å•†åˆ†ç»„ä»»åŠ¡
        provider_tasks = defaultdict(list)
        for task in all_tasks:
            provider = MODEL_PROVIDER_MAP.get(task.model, 'idealab')
            provider_tasks[provider].append(task)
        
        if not silent:
            print(f"\nğŸ“¦ ä»»åŠ¡åˆ†å¸ƒï¼š")
            for provider, tasks in provider_tasks.items():
                print(f"  {provider}: {len(tasks)} ä¸ªä»»åŠ¡")
        
        # è®¾ç½®æä¾›å•†çº§åˆ«çš„å¹¶å‘å‚æ•°
        provider_configs = {
            'azure': {'max_workers': 100, 'initial_qps': 200.0},  # æé«˜åˆ°100 workersï¼Œæ”¯æŒ20*5ä¸€æ¬¡æ€§å®Œæˆ
            'user_azure': {'max_workers': 50, 'initial_qps': 100.0},  # ä¹Ÿç›¸åº”æé«˜
            'idealab': {'max_workers': 24, 'initial_qps': 30.0}  # 3 keys Ã— 8 å¹¶å‘
        }
        
        all_results = []
        
        # å¹¶è¡Œè¿è¡Œä¸åŒæä¾›å•†çš„ä»»åŠ¡
        with ProcessPoolExecutor(max_workers=len(provider_tasks)) as executor:
            futures = []
            
            for provider, tasks in provider_tasks.items():
                config = provider_configs.get(provider, {'max_workers': 10, 'initial_qps': 20.0})
                
                # åˆ›å»ºä¸“é—¨çš„runner
                future = executor.submit(
                    _run_provider_tasks,
                    tasks,
                    config['max_workers'],
                    config['initial_qps'],
                    adaptive,
                    save_logs,
                    silent,
                    not batch_commit,
                    kwargs.get('ai_classification', True),
                    checkpoint_interval if batch_commit else 0
                )
                futures.append((provider, future))
            
            # æ”¶é›†ç»“æœ
            for provider, future in futures:
                try:
                    provider_results = future.result()
                    all_results.extend(provider_results)
                    if not silent:
                        print(f"\nâœ… {provider} æä¾›å•†ä»»åŠ¡å®Œæˆï¼š{len(provider_results)} ä¸ªç»“æœ")
                except Exception as e:
                    if not silent:
                        print(f"\nâŒ {provider} æä¾›å•†ä»»åŠ¡å¤±è´¥ï¼š{e}")
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in all_results if r and r.get('success', False))
        if not silent:
            print(f"\nâœ… æ‰€æœ‰ä»»åŠ¡å®Œæˆ")
            print(f"   æˆåŠŸ: {success_count}/{len(all_results)}")
            print(f"   å¤±è´¥: {len(all_results) - success_count}/{len(all_results)}")
        
        # ä¿å­˜ç»“æœ
        if batch_commit and all_results:
            unsaved_results = [r for r in all_results if r and not r.get('_saved', False)]
            if unsaved_results:
                if not silent:
                    print(f"\nğŸ“¤ ä¿å­˜{len(unsaved_results)}ä¸ªæµ‹è¯•ç»“æœ...")
                _save_results_to_database(unsaved_results, model, difficulty, kwargs.get('ai_classification', True), kwargs.get('result_suffix', ''))
        
        return True
    
    # æ ¹æ®æ¨¡å‹å’Œæ¨¡å¼è°ƒæ•´å‚æ•°
    if any(x in model.lower() for x in ['qwen', 'llama-4-scout', 'o1']):
        # IdealLab API: é€‚åº¦ä¿å®ˆä½†ä¸è¦å¤ªä¿å®ˆ
        if adaptive:
            workers = min(workers, 5)  # æé«˜åˆ°5
            qps = None  # ç§»é™¤QPSé™åˆ¶
        else:
            # éadaptiveæ¨¡å¼ä¹Ÿå¯ä»¥ç¨å¾®æ¿€è¿›ä¸€ç‚¹
            workers = 2  # ä¼˜åŒ–ï¼šé™ä¸º2ï¼Œé¿å…è¿‡è½½idealab API
            qps = None  # ç§»é™¤QPSé™åˆ¶
        if not silent:
            print(f"âš ï¸  æ£€æµ‹åˆ°idealab APIï¼Œè°ƒæ•´å¹¶å‘: workers={workers}, qps={qps}")
    elif any(x in model.lower() for x in ['deepseek', 'llama-3.3', 'gpt-4o-mini', 'gpt-5']):
        # Azure API: éå¸¸æ¿€è¿›ï¼Œæ”¯æŒ100å¹¶å‘ä¸€æ¬¡æ€§å®Œæˆ20*5æµ‹è¯•
        if adaptive:
            workers = max(workers, 100)  # ç›´æ¥ä»100å¼€å§‹ï¼Œæ”¯æŒ20*5ä¸€æ¬¡æ€§å¹¶è¡Œ
            qps = None  # ç§»é™¤QPSé™åˆ¶
        else:
            # éadaptiveæ¨¡å¼ä½¿ç”¨å›ºå®šå€¼ï¼Œä¸ultra_parallel_runner.pyä¿æŒä¸€è‡´
            workers = 50   # ä¿®æ­£ï¼šAzureå›ºå®šæ¨¡å¼50 workers
            qps = None     # ç§»é™¤QPSé™åˆ¶
        if not silent:
            print(f"ğŸš€ æ£€æµ‹åˆ°Azure APIï¼Œä½¿ç”¨è¶…é«˜å¹¶å‘: workers={workers}, qps={qps}")
    else:
        # é»˜è®¤æ¨¡å‹ï¼šä¹Ÿæ›´æ¿€è¿›ä¸€äº›
        if not adaptive:
            workers = min(workers, 20)  # æé«˜åˆ°20
            qps = min(qps, 30.0)  # æé«˜åˆ°30
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ResultCollectoræ¨¡å¼
    if use_result_collector is None:
        # ä»ç¯å¢ƒå˜é‡å†³å®š
        use_collector = os.environ.get('USE_RESULT_COLLECTOR', 'false').lower() == 'true'
    else:
        use_collector = use_result_collector
    
        # åˆ›å»ºæ™ºèƒ½ç»“æœæ”¶é›†å™¨
    result_collector = None
    if use_collector:
        try:
            # å°è¯•ä½¿ç”¨è‡ªé€‚åº”æ”¶é›†å™¨
            from result_collector_adapter import create_adaptive_collector
            
            # æ™ºèƒ½é…ç½®ï¼ˆæš‚æ—¶ä¸ä¼ å…¥database_managerï¼Œç¨ååœ¨runneråˆ›å»ºåè®¾ç½®ï¼‰
            collector_config = {
                'temp_dir': 'temp_results',
                'max_memory_results': max(5, checkpoint_interval or 5),  # è‡ªé€‚åº”é˜ˆå€¼
                'max_time_seconds': 300,  # 5åˆ†é’Ÿè¶…æ—¶
                'auto_save_interval': 60,  # 1åˆ†é’Ÿè‡ªåŠ¨ä¿å­˜
                'adaptive_threshold': True
            }
            
            result_collector = create_adaptive_collector(**collector_config)
            
            if not silent:
                print("ğŸ§  å¯ç”¨SmartResultCollectoræ¨¡å¼ï¼Œæ™ºèƒ½æ•°æ®ç®¡ç†")
        except ImportError:
            # å›é€€åˆ°åŸå§‹ResultCollector
            try:
                result_collector = ResultCollector()
                if not silent:
                    print("ğŸ†• å¯ç”¨ResultCollectoræ¨¡å¼ï¼Œæµ‹è¯•å®Œæˆåç»Ÿä¸€å†™å…¥")
            except:
                result_collector = None
                if not silent:
                    print("âš ï¸ ResultCollectorä¸å¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼")
        except Exception as e:
            print(f"âš ï¸ æ™ºèƒ½æ”¶é›†å™¨åˆ›å»ºå¤±è´¥ï¼Œä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼: {e}")
            result_collector = None
    
    # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
    runner = BatchTestRunner(
        debug=False,
        silent=silent,
        use_adaptive=adaptive,
        save_logs=save_logs,
        enable_database_updates=not use_collector,  # collectoræ¨¡å¼ç¦ç”¨å®æ—¶å†™å…¥
        is_subprocess=True,  # æ ‡è®°ä¸ºå­è¿›ç¨‹
        use_ai_classification=use_ai_classification,  # ä¼ é€’AIåˆ†ç±»å‚æ•°
        idealab_key_index=kwargs.get('idealab_key_index'),  # ä¼ é€’API keyç´¢å¼•
        checkpoint_interval=checkpoint_interval if batch_commit else 0  # ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„é—´éš”  # ä¸­é—´ä¿å­˜é—´éš”
    )
    
    # å¦‚æœä½¿ç”¨æ™ºèƒ½æ”¶é›†å™¨ï¼Œè®¾ç½®database_manager
    if result_collector and hasattr(result_collector, 'set_database_manager'):
        database_manager = getattr(runner, 'manager', None)
        if database_manager:
            result_collector.set_database_manager(database_manager)
            if not silent:
                print("ğŸ“Š å·²å…³è”æ•°æ®åº“ç®¡ç†å™¨åˆ°æ™ºèƒ½æ”¶é›†å™¨")
    
    try:
        if adaptive:
            results = runner.run_adaptive_concurrent_batch(
                all_tasks, 
                initial_workers=workers, 
                initial_qps=qps
            )
        else:
            results = runner.run_concurrent_batch(
                all_tasks, 
                workers=workers, 
                qps=qps
            )
        
        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for r in results if r and r.get('success', False))
        if not silent:
            print(f"\nâœ… æ‰¹æµ‹è¯•å®Œæˆ")
            print(f"   æˆåŠŸ: {success_count}/{len(results)}")
            print(f"   å¤±è´¥: {len(results) - success_count}/{len(results)}")
        
        # ResultCollectoræ¨¡å¼ï¼šæäº¤ç»“æœåˆ°æ”¶é›†å™¨
        if result_collector:
            # ä½¿ç”¨å®é™…çš„resultsï¼Œè€Œä¸æ˜¯runner.pending_results
            # å› ä¸ºåœ¨collectoræ¨¡å¼ä¸‹ï¼Œrunnerä¸ä¼šç´¯ç§¯pending_results
            pending_results = results if results else getattr(runner, 'pending_results', [])
            if pending_results:
                if not silent:
                    print(f"ğŸ“¤ æäº¤ {len(pending_results)} ä¸ªç»“æœåˆ°ResultCollector...")
                
                # æ·»åŠ è¿›ç¨‹ä¿¡æ¯
                process_info = {
                    'pid': os.getpid(),
                    'model': model,
                    'prompt_types': prompt_types,
                    'difficulty': difficulty,
                    'task_types': task_types,
                    'workers': workers,
                    'deployment': deployment
                }
                
                result_collector.add_batch_result(model, pending_results, process_info)
                # åªæœ‰å½“æˆ‘ä»¬ä½¿ç”¨äº†runner.pending_resultsæ—¶æ‰æ¸…ç©º
                if hasattr(runner, 'pending_results'):
                    runner.pending_results = []  # æ¸…ç©º
            else:
                if not silent:
                    print("âš ï¸ æ²¡æœ‰å¾…æäº¤çš„ç»“æœ")
        
        # ä¼ ç»Ÿæ¨¡å¼ï¼šå¦‚æœæ˜¯æ‰¹é‡æäº¤æ¨¡å¼ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æœªä¿å­˜çš„ç»“æœ
        elif batch_commit:
            # è·å–runnerä¸­çš„pending_resultsï¼ˆå¦‚æœæœ‰ï¼‰
            pending_results = getattr(runner, 'pending_results', [])
            if pending_results:
                if not silent:
                    print(f"\nğŸ“¤ ä¿å­˜{len(pending_results)}ä¸ªæœªæäº¤çš„æµ‹è¯•ç»“æœ...")
                _save_results_to_database(pending_results, model, difficulty, kwargs.get('ai_classification', True), kwargs.get('result_suffix', ''))
                runner.pending_results = []  # æ¸…ç©º
            
            # å¦‚æœè¿˜æœ‰å…¶ä»–æœªå¤„ç†çš„results
            if results:
                unsaved_results = [r for r in results if r and not r.get('_saved', False)]
                if unsaved_results:
                    if not silent:
                        print(f"\nğŸ“¤ æœ€ç»ˆä¿å­˜{len(unsaved_results)}ä¸ªæµ‹è¯•ç»“æœ...")
                    _save_results_to_database(unsaved_results, model, difficulty, kwargs.get('ai_classification', True), kwargs.get('result_suffix', ''))
        
        # å¼ºåˆ¶åˆ·æ–°æ‰€æœ‰ç¼“å†²åŒºï¼Œç¡®ä¿æ•°æ®å†™å…¥ç£ç›˜
        print("\nğŸ’¾ æœ€ç»ˆåˆ·æ–°æ‰€æœ‰ç¼“å†²åŒº...")
        _flush_all_managers()
        print("âœ… æ‰€æœ‰æ•°æ®å·²å®‰å…¨å†™å…¥ç£ç›˜")
        
        return True
        
    except Exception as e:
        if not silent:
            print(f"\nâœ— æ‰¹æµ‹è¯•å¤±è´¥: {e}")
            # æ˜¾ç¤ºå®Œæ•´tracebackä»¥ä¾¿è°ƒè¯•
            import traceback
            print("å®Œæ•´é”™è¯¯ä¿¡æ¯:")
            traceback.print_exc()
        return False

def run_auto_maintenance_mode(models: Optional[List[str]] = None) -> bool:
    """
    è‡ªåŠ¨ç»´æŠ¤æ¨¡å¼ï¼šåˆ†æå¤±è´¥ï¼Œç”Ÿæˆé‡æµ‹è®¡åˆ’å¹¶æ‰§è¡Œ
    """
    print("ğŸ”§ è¿›å…¥è‡ªåŠ¨ç»´æŠ¤æ¨¡å¼...")
    
    # é¿å…å¾ªç¯å¯¼å…¥
    from auto_failure_maintenance_system import AutoFailureMaintenanceSystem
    # åˆå§‹åŒ–ç»´æŠ¤ç³»ç»Ÿ
    maintenance_system = AutoFailureMaintenanceSystem(enable_auto_retry=True)
    
    # æ‰§è¡Œè‡ªåŠ¨ç»´æŠ¤
    maintenance_result = maintenance_system.auto_maintain_failures()
    
    # è·å–æœªå®Œæˆçš„æµ‹è¯•
    incomplete_tests = maintenance_system.get_incomplete_tests(models)
    
    if not incomplete_tests:
        print("âœ… æ‰€æœ‰æ¨¡å‹æµ‹è¯•å®Œæ•´ï¼Œæ— éœ€è¡¥å……")
        return True
    
    print(f"\nğŸ“‹ å‘ç° {len(incomplete_tests)} ä¸ªæ¨¡å‹æœ‰æœªå®Œæˆæµ‹è¯•")
    
    # ç”Ÿæˆå¹¶æ‰§è¡Œé‡æµ‹è„šæœ¬
    total_executed = 0
    for model, configs in incomplete_tests.items():
        print(f"\nâ–¶ å¤„ç†æ¨¡å‹: {model}")
        
        for config in configs:
            if config["missing_count"] <= 0:
                continue
                
            print(f"  è¡¥å…… {config['prompt_type']} / {config['task_type']}: {config['missing_count']} ä¸ªæµ‹è¯•")
            
            # æ‰§è¡Œå•ä¸ªé…ç½®çš„è¡¥å……æµ‹è¯•
            success = run_batch_test_smart(
                model=model,
                prompt_types=config["prompt_type"],
                difficulty=config["difficulty"],
                task_types=config["task_type"],
                num_instances=config["missing_count"],
                adaptive=True,
                tool_success_rate=0.8,
                batch_commit=True,
                checkpoint_interval=10,
                max_workers=5,
                save_logs=False,
                silent=True
            )
            
            if success:
                total_executed += config["missing_count"]
                print(f"    âœ… å®Œæˆ {config['missing_count']} ä¸ªæµ‹è¯•")
            else:
                print(f"    âŒ æµ‹è¯•å¤±è´¥")
                # è®°å½•å¤±è´¥
                maintenance_system.failed_manager.record_test_failure(
                    model=model,
                    group_name=f"{config['prompt_type']}_{config['task_type']}",
                    prompt_types=config["prompt_type"],
                    test_type="auto_retest",
                    failure_reason="Auto retest failed"
                )
    
    print(f"\nğŸ¯ è‡ªåŠ¨ç»´æŠ¤å®Œæˆï¼Œæ‰§è¡Œäº† {total_executed} ä¸ªæµ‹è¯•")
    return True

def run_incremental_retest_mode(models: Optional[List[str]] = None, 
                               completion_threshold: float = 0.8) -> bool:
    """
    å¢é‡é‡æµ‹æ¨¡å¼ï¼šåŸºäºç°æœ‰è¿›åº¦ï¼Œåªæµ‹è¯•ç¼ºå¤±çš„éƒ¨åˆ†
    """
    print(f"ğŸ”„ è¿›å…¥å¢é‡é‡æµ‹æ¨¡å¼ï¼ˆå®Œæˆç‡é˜ˆå€¼: {completion_threshold:.1%}ï¼‰...")
    
    # é¿å…å¾ªç¯å¯¼å…¥
    from auto_failure_maintenance_system import AutoFailureMaintenanceSystem
    maintenance_system = AutoFailureMaintenanceSystem(enable_auto_retry=False)
    
    # åˆ†æå®Œæˆæƒ…å†µ
    analysis = maintenance_system.analyze_test_completion(models)
    
    models_to_retest = []
    for model in analysis["models_analyzed"]:
        model_analysis = analysis["completion_summary"][model]
        if (model_analysis.get("status") == "analyzed" and 
            model_analysis.get("completion_rate", 0) < completion_threshold):
            models_to_retest.append(model)
    
    if not models_to_retest:
        print(f"âœ… æ‰€æœ‰æ¨¡å‹å®Œæˆç‡éƒ½è¶…è¿‡ {completion_threshold:.1%}ï¼Œæ— éœ€é‡æµ‹")
        return True
    
    print(f"\nğŸ“‹ éœ€è¦é‡æµ‹çš„æ¨¡å‹: {models_to_retest}")
    
    # å¯¹æ¯ä¸ªéœ€è¦é‡æµ‹çš„æ¨¡å‹æ‰§è¡Œå¢é‡æµ‹è¯•
    for model in models_to_retest:
        model_analysis = analysis["completion_summary"][model]
        print(f"\nâ–¶ é‡æµ‹æ¨¡å‹: {model}")
        print(f"   å½“å‰å®Œæˆç‡: {model_analysis.get('completion_rate', 0):.1%}")
        print(f"   å¤±è´¥ç‡: {model_analysis.get('failure_rate', 0):.1%}")
        
        # è·å–è¯¥æ¨¡å‹çš„æœªå®Œæˆæµ‹è¯•
        incomplete_configs = get_incomplete_tests_for_model(model)
        
        for config in incomplete_configs:
            if config["missing_count"] <= 0:
                continue
                
            print(f"    è¡¥å……: {config['prompt_type']} / {config['task_type']} ({config['missing_count']} ä¸ª)")
            
            success = run_batch_test_smart(
                model=model,
                prompt_types=config["prompt_type"],
                difficulty=config["difficulty"],
                task_types=config["task_type"],
                num_instances=config["missing_count"],
                adaptive=True,
                tool_success_rate=0.8,
                batch_commit=True,
                checkpoint_interval=5,
                max_workers=3,
                save_logs=False,
                silent=True
            )
            
            if success:
                print(f"      âœ… å®Œæˆ")
            else:
                print(f"      âŒ å¤±è´¥")
    
    print("\nğŸ¯ å¢é‡é‡æµ‹å®Œæˆ")
    return True

def main():
    
    # æ˜¾ç¤ºå½“å‰å­˜å‚¨æ ¼å¼
    
    storage_format = os.environ.get('STORAGE_FORMAT', 'json').upper()
    
    print(f"[INFO] ä½¿ç”¨{storage_format}å­˜å‚¨æ ¼å¼")
    
    
    parser = argparse.ArgumentParser(description='æ™ºèƒ½æ‰¹æµ‹è¯•è¿è¡Œå™¨ - å¢å¼ºç‰ˆ')
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--model', help='æ¨¡å‹åç§°ï¼ˆç”¨äºç»Ÿè®¡ï¼‰')
    parser.add_argument('--deployment', help='éƒ¨ç½²å®ä¾‹åï¼ˆç”¨äºAPIè°ƒç”¨ï¼Œå¦‚æœªæŒ‡å®šåˆ™ä½¿ç”¨modelï¼‰')
    parser.add_argument('--prompt-types', help='Promptç±»å‹(é€—å·åˆ†éš”æˆ–all)')
    parser.add_argument('--difficulty', default='easy', help='éš¾åº¦')
    parser.add_argument('--task-types', default='all', help='ä»»åŠ¡ç±»å‹')
    parser.add_argument('--num-instances', type=int, default=20, help='æ¯ç§ä»»åŠ¡çš„å®ä¾‹æ•°')
    parser.add_argument('--tool-success-rate', type=float, help='å·¥å…·æˆåŠŸç‡')
    
    # è¿è¡Œå‚æ•°
    parser.add_argument('--max-workers', type=int, default=20, help='æœ€å¤§å¹¶å‘æ•°ï¼ˆAzureæ¨¡å‹è‡ªåŠ¨ä½¿ç”¨50+ï¼‰')
    parser.add_argument('--adaptive', action='store_true', default=True, help='ä½¿ç”¨adaptiveæ¨¡å¼ï¼ˆé»˜è®¤å¼€å¯ï¼‰')
    parser.add_argument('--no-adaptive', dest='adaptive', action='store_false', help='ç¦ç”¨adaptiveæ¨¡å¼')
    parser.add_argument('--save-logs', action='store_true', default=True, help='ä¿å­˜è¯¦ç»†æ—¥å¿—ï¼ˆé»˜è®¤å¼€å¯ï¼‰')
    parser.add_argument('--no-save-logs', dest='save_logs', action='store_false', help='ç¦ç”¨æ—¥å¿—ä¿å­˜')
    parser.add_argument('--silent', action='store_true', help='é™é»˜æ¨¡å¼')
    parser.add_argument('--qps', type=float, default=20.0, help='QPSé™åˆ¶ï¼ˆéadaptiveæ¨¡å¼ä¸‹ä½¿ç”¨ï¼‰')
    parser.add_argument('--batch-commit', action='store_true', help='æ‰¹é‡æäº¤æ¨¡å¼é¿å…å¹¶å‘ç«äº‰')
    parser.add_argument('--checkpoint-interval', type=int, default=20, help='ä¸­é—´ä¿å­˜é—´éš”(æ¯Nä¸ªæµ‹è¯•ä¿å­˜ä¸€æ¬¡ï¼Œé»˜è®¤20)')
    parser.add_argument('--ai-classification', dest='ai_classification', action='store_true', default=True, help='å¯ç”¨AIé”™è¯¯åˆ†ç±»(ä½¿ç”¨gpt-5-nanoï¼Œé»˜è®¤å¯ç”¨)')
    parser.add_argument('--no-ai-classification', dest='ai_classification', action='store_false', help='ç¦ç”¨AIé”™è¯¯åˆ†ç±»')
    parser.add_argument('--provider-parallel', action='store_true', help='ä½¿ç”¨åŸºäºAPIæä¾›å•†çš„å¹¶è¡Œç­–ç•¥(è·¨æä¾›å•†å¹¶è¡Œ)')
    parser.add_argument('--prompt-parallel', action='store_true', help='å¹¶è¡Œè¿è¡Œå¤šä¸ªprompt types(Azureç›´æ¥å¹¶è¡Œ,IdealLabä½¿ç”¨ä¸åŒkeys)')
    parser.add_argument('--result-suffix', default='', help='ç»“æœæ–‡ä»¶åç¼€(ç”¨äºåŒºåˆ†é—­æº/å¼€æºæ¨¡å‹)')
    
    # æ–°å¢ï¼šè‡ªåŠ¨ç»´æŠ¤æ¨¡å¼
    parser.add_argument('--auto-maintain', action='store_true', help='è‡ªåŠ¨ç»´æŠ¤æ¨¡å¼ï¼šè‡ªåŠ¨æ£€æµ‹å¤±è´¥å¹¶é‡æµ‹')
    parser.add_argument('--incremental-retest', action='store_true', help='å¢é‡é‡æµ‹æ¨¡å¼ï¼šåŸºäºç°æœ‰è¿›åº¦è¡¥å……ç¼ºå¤±æµ‹è¯•')
    parser.add_argument('--completion-threshold', type=float, default=0.8, help='å¢é‡é‡æµ‹çš„å®Œæˆç‡é˜ˆå€¼(é»˜è®¤80%%)')
    
    # æ–°å¢ï¼šIdealLab API keyç´¢å¼•
    parser.add_argument('--idealab-key-index', dest='idealab_key_index', type=int, default=None, help='æŒ‡å®šä½¿ç”¨çš„IdealLab API keyç´¢å¼•(0-1)')
    parser.add_argument('--models', nargs='*', help='æŒ‡å®šè¦å¤„ç†çš„æ¨¡å‹åˆ—è¡¨(ç”¨äºè‡ªåŠ¨ç»´æŠ¤æ¨¡å¼)')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥æ˜¯å¦è¿›å…¥ç‰¹æ®Šæ¨¡å¼
    if args.auto_maintain:
        print("ğŸ”§ å¯åŠ¨è‡ªåŠ¨ç»´æŠ¤æ¨¡å¼")
        success = run_auto_maintenance_mode(args.models)
        sys.exit(0 if success else 1)
    
    if args.incremental_retest:
        print("ğŸ”„ å¯åŠ¨å¢é‡é‡æµ‹æ¨¡å¼")
        success = run_incremental_retest_mode(args.models, args.completion_threshold)
        sys.exit(0 if success else 1)
    
    # éªŒè¯å¿…éœ€å‚æ•°
    if not args.model or not args.prompt_types:
        print("âŒ å¸¸è§„æ¨¡å¼éœ€è¦æŒ‡å®š --model å’Œ --prompt-types å‚æ•°")
        print("ğŸ’¡ æˆ–ä½¿ç”¨ä»¥ä¸‹ç‰¹æ®Šæ¨¡å¼ï¼š")
        print("   --auto-maintain          è‡ªåŠ¨ç»´æŠ¤æ¨¡å¼")
        print("   --incremental-retest     å¢é‡é‡æµ‹æ¨¡å¼")
        print("")
        print("ğŸ“– ç¤ºä¾‹ç”¨æ³•ï¼š")
        print("   # å¸¸è§„æµ‹è¯•")
        print("   python smart_batch_runner.py --model gpt-4o-mini --prompt-types baseline")
        print("   # è‡ªåŠ¨ç»´æŠ¤æ‰€æœ‰æ¨¡å‹")
        print("   python smart_batch_runner.py --auto-maintain")
        print("   # å¢é‡é‡æµ‹ç‰¹å®šæ¨¡å‹")
        print("   python smart_batch_runner.py --incremental-retest --models gpt-4o-mini claude-3-sonnet")
        sys.exit(1)
    
    # å¸¸è§„æµ‹è¯•æ¨¡å¼
    success = run_batch_test_smart(
        model=args.model,
        deployment=args.deployment,  # æ–°å¢ï¼šä¼ é€’éƒ¨ç½²å®ä¾‹å
        prompt_types=args.prompt_types,
        difficulty=args.difficulty,
        task_types=args.task_types,
        num_instances=args.num_instances,
        adaptive=args.adaptive,
        tool_success_rate=args.tool_success_rate if args.tool_success_rate else 0.8,
        batch_commit=args.batch_commit,
        checkpoint_interval=args.checkpoint_interval,
        use_provider_parallel=args.provider_parallel,
        max_workers=args.max_workers,
        save_logs=args.save_logs,
        silent=args.silent,
        qps=args.qps,
        ai_classification=args.ai_classification,
        prompt_parallel=args.prompt_parallel,  # ä¼ é€’prompt_parallelå‚æ•°
        result_suffix=args.result_suffix,  # ä¼ é€’ç»“æœæ–‡ä»¶åç¼€
        idealab_key_index=args.idealab_key_index  # ä¼ é€’API keyç´¢å¼•
    )
    
    sys.exit(0 if success else 1)

def _run_multi_prompt_parallel(model: str, prompt_list: List[str], task_list: List[str],
                               difficulty: str, num_instances: int, tool_success_rate: float,
                               provider: str, adaptive: bool, batch_commit: bool,
                               checkpoint_interval: int, deployment: str = None, **kwargs):
    """
    å¹¶è¡Œè¿è¡Œå¤šä¸ªprompt types
    Azureæ¨¡å‹ï¼šæ‰€æœ‰prompt typesåŒæ—¶å¹¶è¡Œ
    IdealLabæ¨¡å‹ï¼šæ¯ä¸ªprompt typeä½¿ç”¨ä¸åŒçš„API keyå¹¶è¡Œ
    """
    # æ„å»ºæ‰€æœ‰æµ‹è¯•ä»»åŠ¡
    all_tasks = []
    task_groups = {}  # æŒ‰prompt_typeåˆ†ç»„
    
    for prompt_type in prompt_list:
        task_groups[prompt_type] = []
        is_flawed = prompt_type.startswith("flawed_")
        flaw_type = prompt_type.replace("flawed_", "") if is_flawed else None
        
        for task_type in task_list:
            # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆ
            completed = get_completed_count(model, prompt_type, difficulty, task_type, tool_success_rate)
            needed = max(0, num_instances - completed)
            
            if needed > 0:
                for _ in range(needed):
                    task = TestTask(
                        model=model,  # ä¿æŒåŸå§‹å¤§å°å†™ï¼Œnormalizeä¼šåœ¨å­˜å‚¨æ—¶å¤„ç†
                        deployment=deployment,  # APIè°ƒç”¨ç”¨çš„éƒ¨ç½²å
                        task_type=task_type,
                        prompt_type=prompt_type,  # ä¿æŒåŸå§‹çš„prompt_typeï¼Œä¸è¦ç®€åŒ–
                        difficulty=difficulty,
                        is_flawed=is_flawed,
                        flaw_type=flaw_type,
                        tool_success_rate=tool_success_rate
                    )
                    all_tasks.append(task)
                    task_groups[prompt_type].append(task)
    
    if not all_tasks:
        print(f"\nâœ… æ‰€æœ‰æµ‹è¯•å·²å®Œæˆï¼Œè·³è¿‡æ­¤é…ç½®")
        return True
    
    print(f"\næ€»è®¡: {len(all_tasks)} ä¸ªæµ‹è¯•ä»»åŠ¡")
    for pt, tasks in task_groups.items():
        if tasks:
            print(f"  {pt}: {len(tasks)} ä¸ªä»»åŠ¡")
    
    # æ ¹æ®providerå†³å®šå¹¶è¡Œç­–ç•¥
    if provider in ['azure', 'user_azure']:
        # Azureï¼šç›´æ¥å¹¶è¡Œæ‰€æœ‰ä»»åŠ¡
        return _run_azure_parallel_tasks(all_tasks, model, difficulty, provider, 
                                        adaptive, batch_commit, checkpoint_interval, **kwargs)
    else:
        # IdealLabï¼šæŒ‰prompt typeåˆ†ç»„å¹¶è¡Œ
        return _run_idealab_parallel_tasks(task_groups, model, difficulty,
                                          adaptive, batch_commit, checkpoint_interval, **kwargs)

def _run_azure_parallel_tasks(all_tasks: List, model: str, difficulty: str, provider: str,
                              adaptive: bool, batch_commit: bool, checkpoint_interval: int, **kwargs):
    """Azureæ¨¡å‹çš„å¹¶è¡Œç­–ç•¥ï¼šæ‰€æœ‰prompt typesåŒæ—¶è¿è¡Œ"""
    # è®¾ç½®Azureçš„æ¿€è¿›å¹¶å‘å‚æ•° - æ”¯æŒ100å¹¶å‘ä¸€æ¬¡æ€§å®Œæˆ20*5æµ‹è¯•
    if provider == 'azure':
        max_workers = max(kwargs.get('max_workers', 100), 100)  # æé«˜åˆ°100
        initial_qps = max(kwargs.get('qps', 200.0), 200.0)  # æé«˜åˆ°200
    else:  # user_azure
        max_workers = max(kwargs.get('max_workers', 50), 50)  # ä¹Ÿç›¸åº”æé«˜
        initial_qps = max(kwargs.get('qps', 100.0), 100.0)
    
    print(f"\nâš¡ ä½¿ç”¨é«˜å¹¶å‘å‚æ•°: workers={max_workers}, QPS={initial_qps}")
    
    # è¿è¡Œæ‰¹é‡æµ‹è¯•
    runner = BatchTestRunner(
        debug=False,
        silent=kwargs.get('silent', False),
        use_adaptive=adaptive,
        save_logs=kwargs.get('save_logs', True),
        enable_database_updates=True,  # æ€»æ˜¯å¯ç”¨æ•°æ®åº“æ›´æ–°ä»¥é˜²æ•°æ®ä¸¢å¤±
        is_subprocess=True,  # æ ‡è®°ä¸ºå­è¿›ç¨‹
        use_ai_classification=kwargs.get('ai_classification', True),  # ä¼ é€’AIåˆ†ç±»å‚æ•°
        checkpoint_interval=checkpoint_interval if batch_commit else 0  # ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„é—´éš”
    )
    
    if adaptive:
        results = runner.run_adaptive_concurrent_batch(
            all_tasks,
            initial_workers=max_workers,
            initial_qps=initial_qps
        )
    else:
        results = runner.run_concurrent_batch(
            all_tasks,
            workers=max_workers,
            qps=initial_qps
        )
    
    # ç»Ÿè®¡ç»“æœ
    success_count = sum(1 for r in results if r and r.get('success', False))
    print(f"\nâœ… æ‰¹æµ‹è¯•å®Œæˆ")
    print(f"   æˆåŠŸ: {success_count}/{len(results)}")
    print(f"   å¤±è´¥: {len(results) - success_count}/{len(results)}")
    
    # ä¿å­˜ç»“æœï¼ˆæ— è®ºæ˜¯å¦batch_commitï¼‰
    unsaved_results = [r for r in results if r and not r.get('_saved', False)]
    if unsaved_results:
        if not batch_commit:
            print(f"\nğŸ“¤ è‡ªåŠ¨ä¿å­˜{len(unsaved_results)}ä¸ªæµ‹è¯•ç»“æœ...")
        else:
            print(f"\nğŸ“¤ æ‰¹é‡ä¿å­˜{len(unsaved_results)}ä¸ªæµ‹è¯•ç»“æœ...")
            _save_results_to_database(unsaved_results, model, difficulty, 
                                    kwargs.get('ai_classification', True),
                                    kwargs.get('result_suffix', ''))
    
    return True

def _run_idealab_parallel_tasks(task_groups: Dict, model: str, difficulty: str,
                                adaptive: bool, batch_commit: bool, checkpoint_interval: int, **kwargs):
    """IdealLabæ¨¡å‹çš„å¹¶è¡Œç­–ç•¥ï¼šæ¯ä¸ªprompt typeä½¿ç”¨ä¸åŒçš„API keyå¹¶è¡Œ"""
    from concurrent.futures import ThreadPoolExecutor, as_completed
    import time
    
    # API keyåˆ†é…è¯´æ˜
    print(f"\nğŸ“¦ IdealLabå¹¶è¡Œç­–ç•¥ï¼š")
    for i, pt in enumerate(task_groups.keys()):
        if pt in ['baseline', 'cot', 'optimal']:
            print(f"  {pt} â†’ API Key {i+1}")
        else:
            print(f"  {pt} â†’ è½®è¯¢ä½¿ç”¨3ä¸ªkeys")
    
    print(f"\nâš¡ å¯åŠ¨{len(task_groups)}ä¸ªå¹¶è¡Œä»»åŠ¡...")
    start_time = time.time()
    
    with ThreadPoolExecutor(max_workers=len(task_groups)) as executor:
        futures = []
        
        for prompt_type, tasks in task_groups.items():
            if not tasks:
                continue
                
            # ä¸ºæ¯ä¸ªprompt typeæäº¤ä¸€ä¸ªä»»åŠ¡
            future = executor.submit(
                _run_single_prompt_tasks,
                tasks, model, prompt_type, difficulty,
                adaptive, batch_commit, **kwargs
            )
            futures.append((prompt_type, future))
        
        # æ”¶é›†ç»“æœ
        all_results = []
        prompt_results = {}
        
        for prompt_type, future in futures:
            try:
                results = future.result(timeout=600)  # 10åˆ†é’Ÿè¶…æ—¶
                all_results.extend(results)
                success_count = sum(1 for r in results if r and r.get('success', False))
                prompt_results[prompt_type] = {
                    'success': True,
                    'count': len(results),
                    'success_count': success_count
                }
                print(f"âœ… {prompt_type} å®Œæˆ: {success_count}/{len(results)} æˆåŠŸ")
            except Exception as e:
                print(f"âŒ {prompt_type} å¤±è´¥: {e}")
                prompt_results[prompt_type] = {
                    'success': False,
                    'error': str(e)
                }
    
    elapsed = time.time() - start_time
    
    # ç»Ÿè®¡æ€»ç»“æœ
    total_success = sum(1 for r in all_results if r and r.get('success', False))
    print(f"\nâœ… æ‰€æœ‰prompt typesæµ‹è¯•å®Œæˆ")
    print(f"   æ€»æ—¶é—´: {elapsed:.2f}ç§’")
    print(f"   æ€»æµ‹è¯•: {len(all_results)}")
    print(f"   æˆåŠŸ: {total_success}")
    print(f"   å¤±è´¥: {len(all_results) - total_success}")
    
    # ä¿å­˜ç»“æœï¼ˆæ— è®ºæ˜¯å¦batch_commitï¼‰
    if all_results:
        unsaved_results = [r for r in all_results if r and not r.get('_saved', False)]
        if unsaved_results:
            if not batch_commit:
                print(f"\nğŸ“¤ è‡ªåŠ¨ä¿å­˜{len(unsaved_results)}ä¸ªæµ‹è¯•ç»“æœ...")
            else:
                print(f"\nğŸ“¤ æ‰¹é‡ä¿å­˜{len(unsaved_results)}ä¸ªæµ‹è¯•ç»“æœ...")
            _save_results_to_database(unsaved_results, model, difficulty,
                                    kwargs.get('ai_classification', True),
                                    kwargs.get('result_suffix', ''))
    
    return True

def _run_single_prompt_tasks(tasks: List, model: str, prompt_type: str, difficulty: str,
                            adaptive: bool, batch_commit: bool, **kwargs):
    """è¿è¡Œå•ä¸ªprompt typeçš„æ‰€æœ‰ä»»åŠ¡"""
    # IdealLabä½¿ç”¨ä¿å®ˆå‚æ•°
    max_workers = min(kwargs.get('max_workers', 5), 5)
    initial_qps = min(kwargs.get('qps', 10.0), 10.0)
    
    # åˆ›å»ºrunnerå¹¶è¿è¡Œ
    runner = BatchTestRunner(
        debug=False,
        silent=True,  # å‡å°‘è¾“å‡ºé¿å…æ··ä¹±
        use_adaptive=adaptive,
        save_logs=kwargs.get('save_logs', True),
        enable_database_updates=True,  # æ€»æ˜¯å¯ç”¨æ•°æ®åº“æ›´æ–°ä»¥é˜²æ•°æ®ä¸¢å¤±
        is_subprocess=True,  # æ ‡è®°ä¸ºå­è¿›ç¨‹
        use_ai_classification=kwargs.get('ai_classification', True),  # ä¼ é€’AIåˆ†ç±»å‚æ•°
        checkpoint_interval=0  # å•ä¸ªprompt typeä¸éœ€è¦checkpoint
    )
    
    if adaptive:
        results = runner.run_adaptive_concurrent_batch(
            tasks,
            initial_workers=max_workers,
            initial_qps=initial_qps
        )
    else:
        results = runner.run_concurrent_batch(
            tasks,
            workers=max_workers,
            qps=initial_qps
        )
    
    return results

def _run_provider_tasks(tasks, max_workers, initial_qps, adaptive, save_logs, silent, 
                        enable_database_updates, use_ai_classification, checkpoint_interval):
    """è¿è¡Œå•ä¸ªæä¾›å•†çš„ä»»åŠ¡ï¼ˆç”¨äºè¿›ç¨‹æ± ï¼‰"""
    runner = BatchTestRunner(
        debug=False,
        silent=silent,
        use_adaptive=adaptive,
        save_logs=save_logs,
        enable_database_updates=enable_database_updates,
        is_subprocess=True,  # æ ‡è®°ä¸ºå­è¿›ç¨‹
        use_ai_classification=use_ai_classification,  # ä¼ é€’AIåˆ†ç±»å‚æ•°
        checkpoint_interval=checkpoint_interval
    )
    
    if adaptive:
        return runner.run_adaptive_concurrent_batch(
            tasks,
            initial_workers=max_workers,
            initial_qps=initial_qps
        )
    else:
        return runner.run_concurrent_batch(
            tasks,
            workers=max_workers,
            qps=initial_qps
        )

if __name__ == "__main__":
    main()