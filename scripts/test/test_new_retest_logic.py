#!/usr/bin/env python3
"""
æµ‹è¯•æ–°çš„é‡æµ‹é€»è¾‘ï¼šåŸºäºå®Œå…¨å¤±è´¥+å¹³å‡æ‰§è¡Œæ—¶é—´è¾¾åˆ°ä¸Šé™
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent))

from auto_failure_maintenance_system import AutoFailureMaintenanceSystem

def test_new_retest_logic():
    """æµ‹è¯•æ–°çš„é‡æµ‹é€»è¾‘"""
    print("ğŸ§ª æµ‹è¯•æ–°çš„é‡æµ‹é€»è¾‘ï¼šåŸºäºå®Œå…¨å¤±è´¥+å¹³å‡æ‰§è¡Œæ—¶é—´è¾¾åˆ°ä¸Šé™")
    print("=" * 60)
    
    # åˆ›å»ºç»´æŠ¤ç³»ç»Ÿ
    system = AutoFailureMaintenanceSystem(enable_auto_retry=False)
    
    # æ˜¾ç¤ºé…ç½®
    print(f"ğŸ“‹ å½“å‰é…ç½®:")
    print(f"  - å®Œå…¨å¤±è´¥é˜ˆå€¼: {system.auto_retest_config.complete_failure_threshold}")
    print(f"  - æœ€å¤§æ‰§è¡Œæ—¶é—´: {system.auto_retest_config.max_execution_time}ç§’")
    print()
    
    # åˆ†ææµ‹è¯•å®Œæˆæƒ…å†µ
    print("ğŸ” åˆ†ææµ‹è¯•å®Œæˆæƒ…å†µ...")
    analysis = system.analyze_test_completion()
    
    print(f"åˆ†æäº† {len(analysis['models_analyzed'])} ä¸ªæ¨¡å‹")
    print(f"å‘ç°å¤±è´¥æ¨¡å¼: {len(analysis['failure_patterns'])} ä¸ª")
    print()
    
    # æ˜¾ç¤ºè¯¦ç»†åˆ†æç»“æœ
    retry_models = []
    for model in analysis['models_analyzed']:
        summary = analysis['completion_summary'][model]
        if summary['status'] == 'analyzed':
            completion_rate = summary['completion_rate']
            avg_exec_time = summary.get('avg_execution_time', 0)
            complete_failure = summary.get('needs_retry_complete_failure', False)
            high_exec_time = summary.get('needs_retry_high_execution_time', False)
            
            print(f"ğŸ“Š æ¨¡å‹: {model}")
            print(f"   - å®Œæˆç‡: {completion_rate:.1%}")
            print(f"   - å¹³å‡æ‰§è¡Œæ—¶é—´: {avg_exec_time:.1f}ç§’")
            
            reasons = []
            if complete_failure:
                failure_configs = summary.get('complete_failure_configs', [])
                reasons.append(f"å®Œå…¨å¤±è´¥é…ç½®: {len(failure_configs)}ä¸ª")
                print(f"   - âš ï¸  å®Œå…¨å¤±è´¥é…ç½®: {len(failure_configs)}ä¸ª")
                for config in failure_configs[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
                    print(f"     â€¢ {config['config']} ({config['total_tests']}ä¸ªæµ‹è¯•)")
            
            if high_exec_time:
                reasons.append(f"æ‰§è¡Œæ—¶é—´è¿‡é•¿: {avg_exec_time:.1f}ç§’")
                print(f"   - âš ï¸  æ‰§è¡Œæ—¶é—´è¿‡é•¿: {avg_exec_time:.1f}ç§’ (é˜ˆå€¼: {system.auto_retest_config.max_execution_time}ç§’)")
            
            needs_retry = summary.get('needs_retry', False)
            if needs_retry:
                retry_models.append(model)
                print(f"   - ğŸ”„ éœ€è¦é‡æµ‹: æ˜¯ ({', '.join(reasons)})")
            else:
                print(f"   - âœ… çŠ¶æ€: æ­£å¸¸")
            
            print()
    
    # æ€»ç»“
    print("=" * 60)
    print(f"ğŸ“ˆ é‡æµ‹å»ºè®®æ€»ç»“:")
    if retry_models:
        print(f"ğŸ”„ éœ€è¦é‡æµ‹çš„æ¨¡å‹ ({len(retry_models)}ä¸ª):")
        for model in retry_models:
            print(f"   - {model}")
    else:
        print("âœ… æ‰€æœ‰æ¨¡å‹éƒ½åœ¨æ­£å¸¸èŒƒå›´å†…ï¼Œæ— éœ€é‡æµ‹")
    
    print()
    print("ğŸ’¡ é‡æµ‹é€»è¾‘è¯´æ˜:")
    print("  1. å®Œå…¨å¤±è´¥: æŸé…ç½®çš„æˆåŠŸç‡ä¸º0%")
    print("  2. æ‰§è¡Œæ—¶é—´è¿‡é•¿: å¹³å‡æ‰§è¡Œæ—¶é—´è¶…è¿‡è®¾å®šé˜ˆå€¼")
    print("  3. æ»¡è¶³ä»»ä¸€æ¡ä»¶å³å»ºè®®é‡æµ‹")

if __name__ == "__main__":
    test_new_retest_logic()