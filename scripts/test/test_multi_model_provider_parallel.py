#!/usr/bin/env python3
"""
Multi-Model Provider-Parallel Test
===================================
æµ‹è¯•å¤šæ¨¡å‹å¹¶è¡Œæ‰§è¡Œï¼Œåˆ©ç”¨æä¾›å•†çº§åˆ«çš„é€Ÿç‡é™åˆ¶
"""

import sys
import time
import argparse
from pathlib import Path
from typing import List, Dict
from concurrent.futures import ThreadPoolExecutor, as_completed

sys.path.insert(0, str(Path(__file__).parent))

from batch_test_runner import TestTask
from provider_parallel_runner import ProviderParallelRunner
from api_client_manager import MODEL_PROVIDER_MAP


def run_multi_model_test(models: List[str], num_tests_per_model: int = 5,
                         task_types: List[str] = None, prompt_type: str = 'baseline',
                         difficulty: str = 'easy', tool_success_rate: float = 0.8):
    """
    è¿è¡Œå¤šæ¨¡å‹å¹¶è¡Œæµ‹è¯•
    
    Args:
        models: æ¨¡å‹åˆ—è¡¨
        num_tests_per_model: æ¯ä¸ªæ¨¡å‹çš„æµ‹è¯•æ•°
        task_types: ä»»åŠ¡ç±»å‹åˆ—è¡¨
        prompt_type: æç¤ºç±»å‹
        difficulty: éš¾åº¦
        tool_success_rate: å·¥å…·æˆåŠŸç‡
    """
    if task_types is None:
        task_types = ['simple_task', 'basic_task']
    
    print("="*70)
    print("ğŸš€ å¤šæ¨¡å‹æä¾›å•†å¹¶è¡Œæµ‹è¯•")
    print("="*70)
    
    # æŒ‰æä¾›å•†åˆ†ç»„æ¨¡å‹
    provider_models = {}
    for model in models:
        provider = MODEL_PROVIDER_MAP.get(model, 'idealab')
        if provider not in provider_models:
            provider_models[provider] = []
        provider_models[provider].append(model)
    
    print("\nğŸ“Š æ¨¡å‹åˆ†å¸ƒ:")
    for provider, pmodels in provider_models.items():
        print(f"  {provider}: {', '.join(pmodels)}")
    
    # åˆ›å»ºæ‰€æœ‰æµ‹è¯•ä»»åŠ¡
    all_tasks = []
    for model in models:
        for task_type in task_types:
            for i in range(num_tests_per_model):
                task = TestTask(
                    model=model,
                    task_type=task_type,
                    prompt_type=prompt_type,
                    difficulty=difficulty,
                    tool_success_rate=tool_success_rate
                )
                all_tasks.append(task)
    
    total_tasks = len(all_tasks)
    print(f"\nğŸ“ æ€»ä»»åŠ¡æ•°: {total_tasks}")
    print(f"   = {len(models)} æ¨¡å‹ Ã— {len(task_types)} ä»»åŠ¡ç±»å‹ Ã— {num_tests_per_model} å®ä¾‹")
    
    # ä¼°ç®—æ—¶é—´
    print("\nâ±ï¸ æ—¶é—´ä¼°ç®—:")
    # ä¸²è¡Œæ—¶é—´ï¼ˆå‡è®¾æ¯ä¸ªæµ‹è¯•10ç§’ï¼‰
    serial_time = total_tasks * 10
    print(f"  ä¸²è¡Œæ‰§è¡Œ: ~{serial_time}ç§’ ({serial_time/60:.1f}åˆ†é’Ÿ)")
    
    # å¹¶è¡Œæ—¶é—´ä¼°ç®—ï¼ˆåŸºäºæä¾›å•†ï¼‰
    if len(provider_models) > 1:
        # è·¨æä¾›å•†å¯ä»¥å®Œå…¨å¹¶è¡Œ
        max_provider_tasks = max(
            len([t for t in all_tasks if MODEL_PROVIDER_MAP.get(t.model, 'idealab') == p])
            for p in provider_models.keys()
        )
        if 'idealab' in provider_models and len(provider_models['idealab']) > 1:
            # IdealLabå†…éƒ¨éœ€è¦é™åˆ¶å¹¶å‘
            idealab_tasks = len([t for t in all_tasks if MODEL_PROVIDER_MAP.get(t.model, 'idealab') == 'idealab'])
            idealab_time = idealab_tasks * 10 / 2  # å‡è®¾2ä¸ªå¹¶å‘
            parallel_time = max(idealab_time, max_provider_tasks * 10 / 5)  # å…¶ä»–æä¾›å•†5ä¸ªå¹¶å‘
        else:
            parallel_time = max_provider_tasks * 10 / 5  # 5ä¸ªå¹¶å‘
    else:
        # å•æä¾›å•†
        if 'idealab' in provider_models:
            parallel_time = total_tasks * 10 / 2  # IdealLabé™åˆ¶ä¸º2ä¸ªå¹¶å‘
        else:
            parallel_time = total_tasks * 10 / 5  # å…¶ä»–æä¾›å•†5ä¸ªå¹¶å‘
    
    print(f"  å¹¶è¡Œæ‰§è¡Œ: ~{parallel_time}ç§’ ({parallel_time/60:.1f}åˆ†é’Ÿ)")
    print(f"  é¢„æœŸåŠ é€Ÿ: {serial_time/parallel_time:.1f}x")
    
    # è¿è¡Œæµ‹è¯•
    print("\n" + "="*70)
    print("ğŸƒ å¼€å§‹æ‰§è¡Œ...")
    print("="*70)
    
    start_time = time.time()
    
    # ä½¿ç”¨æä¾›å•†å¹¶è¡Œè¿è¡Œå™¨
    runner = ProviderParallelRunner(
        debug=False,
        silent=False,
        save_logs=True,
        use_ai_classification=False
    )
    
    results, stats = runner.run_parallel_by_provider(all_tasks)
    
    actual_time = time.time() - start_time
    
    # åˆ†æç»“æœ
    print("\n" + "="*70)
    print("ğŸ“Š æµ‹è¯•ç»“æœåˆ†æ")
    print("="*70)
    
    print(f"\nâ±ï¸ å®é™…æ‰§è¡Œæ—¶é—´: {actual_time:.1f}ç§’ ({actual_time/60:.1f}åˆ†é’Ÿ)")
    print(f"ğŸ“ˆ å®é™…åŠ é€Ÿæ¯”: {serial_time/actual_time:.2f}x")
    
    # æŒ‰æ¨¡å‹ç»Ÿè®¡
    print("\nğŸ“Š æŒ‰æ¨¡å‹ç»Ÿè®¡:")
    for model in models:
        model_results = [r for r in results if r and r.get('model') == model]
        success = sum(1 for r in model_results if r.get('success', False))
        total = len(model_results)
        if total > 0:
            print(f"  {model}: {success}/{total} æˆåŠŸ ({success/total*100:.1f}%)")
    
    # æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡
    print("\nğŸ“Š æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡:")
    for task_type in task_types:
        task_results = [r for r in results if r and r.get('task_type') == task_type]
        success = sum(1 for r in task_results if r.get('success', False))
        total = len(task_results)
        if total > 0:
            print(f"  {task_type}: {success}/{total} æˆåŠŸ ({success/total*100:.1f}%)")
    
    return results, stats


def main():
    parser = argparse.ArgumentParser(description='å¤šæ¨¡å‹æä¾›å•†å¹¶è¡Œæµ‹è¯•')
    parser.add_argument('--models', type=str, default='gpt-4o-mini,qwen2.5-3b-instruct,DeepSeek-V3-671B',
                       help='è¦æµ‹è¯•çš„æ¨¡å‹åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰')
    parser.add_argument('--num-tests', type=int, default=5,
                       help='æ¯ä¸ªæ¨¡å‹çš„æµ‹è¯•æ•°é‡')
    parser.add_argument('--task-types', type=str, default='simple_task,basic_task',
                       help='ä»»åŠ¡ç±»å‹åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰')
    parser.add_argument('--prompt-type', type=str, default='baseline',
                       help='æç¤ºç±»å‹')
    parser.add_argument('--difficulty', type=str, default='easy',
                       help='éš¾åº¦çº§åˆ«')
    parser.add_argument('--tool-success-rate', type=float, default=0.8,
                       help='å·¥å…·æˆåŠŸç‡')
    
    args = parser.parse_args()
    
    # è§£ææ¨¡å‹åˆ—è¡¨
    models = [m.strip() for m in args.models.split(',') if m.strip()]
    task_types = [t.strip() for t in args.task_types.split(',') if t.strip()]
    
    if not models:
        print("âŒ è¯·æŒ‡å®šè‡³å°‘ä¸€ä¸ªæ¨¡å‹")
        sys.exit(1)
    
    # è¿è¡Œæµ‹è¯•
    run_multi_model_test(
        models=models,
        num_tests_per_model=args.num_tests,
        task_types=task_types,
        prompt_type=args.prompt_type,
        difficulty=args.difficulty,
        tool_success_rate=args.tool_success_rate
    )


if __name__ == "__main__":
    main()