#!/usr/bin/env python3
"""åˆ†æè½®æ•°ä½¿ç”¨æƒ…å†µå’Œmax_turnsé”™è¯¯"""

import json
from pathlib import Path

def analyze_turns_usage():
    """åˆ†ææ¨¡å‹çš„è½®æ•°ä½¿ç”¨æƒ…å†µå’Œmax_turnsé”™è¯¯"""
    db_path = Path('pilot_bench_cumulative_results/master_database.json')
    with open(db_path, 'r') as f:
        db = json.load(f)

    models_to_analyze = ['DeepSeek-V3-0324', 'DeepSeek-R1-0528']
    
    print('=' * 70)
    print('ğŸ” DeepSeek-R1 vs V3: è½®æ•°ä½¿ç”¨å’Œmax_turnsé”™è¯¯åˆ†æ')
    print('=' * 70)
    print()

    for model_name in models_to_analyze:
        if model_name not in db['models']:
            continue
            
        model_data = db['models'][model_name]
        print(f'ğŸ“Š {model_name}:')
        print('-' * 60)
        
        for prompt_type, prompt_data in model_data.get('by_prompt_type', {}).items():
            for tool_rate, rate_data in prompt_data.get('by_tool_success_rate', {}).items():
                for diff, diff_data in rate_data.get('by_difficulty', {}).items():
                    print(f'  ğŸ“ˆ {prompt_type} - {tool_rate} - {diff}:')
                    
                    # æ±‡æ€»ç»Ÿè®¡
                    total_tests = 0
                    total_success = 0
                    total_max_turns_errors = 0
                    total_format_errors = 0
                    weighted_avg_turns = 0
                    weighted_execution_time = 0
                    weighted_tool_calls = 0
                    
                    for task, task_data in diff_data.get('by_task_type', {}).items():
                        total = task_data.get('total', 0)
                        success = task_data.get('success', 0)
                        avg_turns = task_data.get('avg_turns', 0)
                        avg_time = task_data.get('avg_execution_time', 0)
                        avg_tool_calls = task_data.get('avg_tool_calls', 0)
                        max_turns_errors = task_data.get('max_turns_errors', 0)
                        format_errors = task_data.get('tool_call_format_errors', 0)
                        
                        total_tests += total
                        total_success += success
                        total_max_turns_errors += max_turns_errors
                        total_format_errors += format_errors
                        
                        # åŠ æƒå¹³å‡
                        if total > 0:
                            weighted_avg_turns += avg_turns * total
                            weighted_execution_time += avg_time * total
                            weighted_tool_calls += avg_tool_calls * total
                        
                        success_rate = success / total if total > 0 else 0
                        print(f'    {task:<20}: {success:>2}/{total:<2} ({success_rate:.1%}) | '
                              f'è½®æ•°:{avg_turns:>4.1f} | æ—¶é—´:{avg_time:>5.1f}s | '
                              f'å·¥å…·è°ƒç”¨:{avg_tool_calls:>4.1f} | '
                              f'max_turnsé”™è¯¯:{max_turns_errors} | æ ¼å¼é”™è¯¯:{format_errors}')
                    
                    # è®¡ç®—æ±‡æ€»çš„åŠ æƒå¹³å‡
                    if total_tests > 0:
                        weighted_avg_turns /= total_tests
                        weighted_execution_time /= total_tests
                        weighted_tool_calls /= total_tests
                    
                    overall_success_rate = total_success / total_tests if total_tests > 0 else 0
                    
                    print(f'    {"="*20}')
                    print(f'    {"æ±‡æ€»":<20}: {total_success:>2}/{total_tests:<2} ({overall_success_rate:.1%}) | '
                          f'è½®æ•°:{weighted_avg_turns:>4.1f} | æ—¶é—´:{weighted_execution_time:>5.1f}s | '
                          f'å·¥å…·è°ƒç”¨:{weighted_tool_calls:>4.1f} | '
                          f'max_turnsé”™è¯¯:{total_max_turns_errors} | æ ¼å¼é”™è¯¯:{total_format_errors}')
                    
                    print()
        print()

    print('=' * 70)
    print('ğŸ¯ å…³é”®æ´å¯Ÿåˆ†æ')
    print('=' * 70)
    
    # æå–å…³é”®æ•°æ®è¿›è¡Œå¯¹æ¯”
    v3_data = db['models']['DeepSeek-V3-0324']['by_prompt_type']['optimal']['by_tool_success_rate']['0.8']['by_difficulty']['easy']['by_task_type']
    r1_data = db['models']['DeepSeek-R1-0528']['by_prompt_type']['optimal']['by_tool_success_rate']['0.8']['by_difficulty']['easy']['by_task_type']
    
    print('1. ğŸ“Š è½®æ•°ä½¿ç”¨å¯¹æ¯”:')
    print(f'{"ä»»åŠ¡ç±»å‹":<20} {"V3è½®æ•°":<8} {"R1è½®æ•°":<8} {"å·®å¼‚":<8} {"åˆ†æ":<30}')
    print('-' * 75)
    
    for task in v3_data.keys():
        if task in r1_data:
            v3_turns = v3_data[task].get('avg_turns', 0)
            r1_turns = r1_data[task].get('avg_turns', 0)
            diff = r1_turns - v3_turns
            
            v3_success = v3_data[task].get('success', 0) / v3_data[task].get('total', 1)
            r1_success = r1_data[task].get('success', 0) / r1_data[task].get('total', 1)
            
            analysis = ""
            if abs(diff) < 1:
                analysis = "è½®æ•°ç›¸è¿‘"
            elif diff > 0:
                analysis = f"R1å¤šç”¨{diff:.1f}è½®"
            else:
                analysis = f"R1å°‘ç”¨{abs(diff):.1f}è½®"
            
            # æ£€æŸ¥æ˜¯å¦å› ä¸ºè½®æ•°ä¸å¤Ÿ
            if r1_turns < v3_turns and r1_success < v3_success:
                analysis += " (å¯èƒ½è½®æ•°ä¸å¤Ÿ)"
            elif r1_turns > v3_turns and r1_success < v3_success:
                analysis += " (è½®æ•°å¤šä½†æ•ˆæœå·®)"
                
            print(f'{task:<20} {v3_turns:<8.1f} {r1_turns:<8.1f} {diff:<8.1f} {analysis:<30}')

    print()
    print('2. ğŸš¨ Max_turnsé”™è¯¯åˆ†æ:')
    
    # æ£€æŸ¥max_turnsé”™è¯¯
    v3_max_turns_total = sum(task_data.get('max_turns_errors', 0) for task_data in v3_data.values())
    r1_max_turns_total = sum(task_data.get('max_turns_errors', 0) for task_data in r1_data.values())
    
    print(f'   DeepSeek-V3-0324: {v3_max_turns_total} ä¸ªmax_turnsé”™è¯¯')
    print(f'   DeepSeek-R1-0528: {r1_max_turns_total} ä¸ªmax_turnsé”™è¯¯')
    
    if r1_max_turns_total > v3_max_turns_total:
        print(f'   âš ï¸  R1ç¡®å®æœ‰æ›´å¤šmax_turnsé”™è¯¯ (+{r1_max_turns_total - v3_max_turns_total}ä¸ª)')
        print('   è¿™æ”¯æŒäº†"R1é—®å¾—å¤ªä»”ç»†å¯¼è‡´è½®æ•°ä¸å¤Ÿ"çš„å‡è®¾')
    else:
        print('   ğŸ“Š ä¸¤ä¸ªæ¨¡å‹çš„max_turnsé”™è¯¯æ•°é‡ç›¸è¿‘')
    
    print()
    print('3. ğŸ’¡ ç»“è®º:')
    
    # è®¡ç®—æ•´ä½“æ¨¡å¼
    v3_avg_turns = sum(task_data.get('avg_turns', 0) * task_data.get('total', 0) for task_data in v3_data.values()) / sum(task_data.get('total', 0) for task_data in v3_data.values())
    r1_avg_turns = sum(task_data.get('avg_turns', 0) * task_data.get('total', 0) for task_data in r1_data.values()) / sum(task_data.get('total', 0) for task_data in r1_data.values())
    
    v3_success_rate = sum(task_data.get('success', 0) for task_data in v3_data.values()) / sum(task_data.get('total', 0) for task_data in v3_data.values())
    r1_success_rate = sum(task_data.get('success', 0) for task_data in r1_data.values()) / sum(task_data.get('total', 0) for task_data in r1_data.values())
    
    if r1_max_turns_total > 0:
        print(f'   âœ… "é—®å¾—å¤ªä»”ç»†å¯¼è‡´è½®æ•°ä¸å¤Ÿ"çš„å‡è®¾ **æœ‰ä¸€å®šé“ç†**')
        print(f'      - R1å¹³å‡ä½¿ç”¨ {r1_avg_turns:.1f} è½® vs V3çš„ {v3_avg_turns:.1f} è½®')
        print(f'      - R1æœ‰ {r1_max_turns_total} ä¸ªmax_turnsé”™è¯¯ vs V3çš„ {v3_max_turns_total} ä¸ª')
        print(f'      - ä½†è¿™åªæ˜¯éƒ¨åˆ†åŸå› ï¼Œæ ¼å¼é”™è¯¯ä»ç„¶æ˜¯ä¸»è¦é—®é¢˜')
    else:
        print(f'   ğŸ“Š è½®æ•°ä½¿ç”¨æ¨¡å¼åˆ†æ:')
        print(f'      - R1å¹³å‡ä½¿ç”¨ {r1_avg_turns:.1f} è½® vs V3çš„ {v3_avg_turns:.1f} è½®')
        print(f'      - R1æˆåŠŸç‡ {r1_success_rate:.1%} vs V3çš„ {v3_success_rate:.1%}')
        if r1_avg_turns < v3_avg_turns:
            print(f'      - R1å®é™…ç”¨è½®æ•°æ›´å°‘ï¼Œä¸»è¦é—®é¢˜å¯èƒ½æ˜¯æ ¼å¼é”™è¯¯å¯¼è‡´æ—©æœŸå¤±è´¥')
        else:
            print(f'      - R1ç”¨è½®æ•°ç›¸è¿‘æˆ–æ›´å¤šï¼Œé—®é¢˜ä¸åœ¨è½®æ•°ä¸å¤Ÿ')

if __name__ == "__main__":
    analyze_turns_usage()