#!/usr/bin/env python3
"""
æœ€å°åŒ–çš„JSONåˆ°ParquetåŒæ­¥è„šæœ¬
åªåŒæ­¥JSONä¸­å®é™…å­˜åœ¨çš„å­—æ®µï¼Œä¸æ·»åŠ è™šå‡çš„é›¶å€¼
"""

import json
import pandas as pd
from pathlib import Path
from datetime import datetime
import shutil

def sync_json_to_parquet_minimal():
    """ä»JSONåŒæ­¥åˆ°Parquetï¼Œåªä¿ç•™å®é™…å­˜åœ¨çš„å­—æ®µ"""
    
    # æ–‡ä»¶è·¯å¾„
    json_file = Path('pilot_bench_cumulative_results/master_database.json')
    parquet_file = Path('pilot_bench_parquet_data/test_results.parquet')
    backup_dir = Path('pilot_bench_parquet_data/backups')
    backup_dir.mkdir(exist_ok=True)
    
    if not json_file.exists():
        print("âŒ JSONæ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    # å¤‡ä»½ç°æœ‰Parquetæ–‡ä»¶
    if parquet_file.exists():
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_file = backup_dir / f'test_results_{timestamp}.parquet'
        shutil.copy2(parquet_file, backup_file)
        print(f"âœ… å·²å¤‡ä»½Parquet: {backup_file.name}")
    
    print("ğŸ“– è¯»å–JSONæ•°æ®...")
    with open(json_file, 'r', encoding='utf-8') as f:
        json_data = json.load(f)
    
    # æå–æ‰€æœ‰è®°å½•ï¼Œåªä¿ç•™JSONä¸­å®é™…å­˜åœ¨çš„å­—æ®µ
    records = []
    
    for model_name, model_data in json_data.get('models', {}).items():
        # è·³è¿‡llama-4-scout-17b
        if 'llama-4' in model_name.lower():
            continue
            
        for prompt_type, prompt_data in model_data.get('by_prompt_type', {}).items():
            # è·³è¿‡æ— æ•ˆçš„å•ç‹¬flawedè®°å½•
            if prompt_type == 'flawed':
                continue
                
            for tool_rate, rate_data in prompt_data.get('by_tool_success_rate', {}).items():
                for difficulty, diff_data in rate_data.get('by_difficulty', {}).items():
                    for task_type, task_data in diff_data.get('by_task_type', {}).items():
                        # è·³è¿‡unknown task_type
                        if task_type == 'unknown':
                            continue
                            
                        if not isinstance(task_data, dict):
                            continue
                        
                        # åˆ›å»ºè®°å½•ï¼ŒåªåŒ…å«JSONä¸­å®é™…å­˜åœ¨çš„å­—æ®µ
                        record = {
                            # æ ‡è¯†å­—æ®µ
                            'model': model_name,
                            'prompt_type': prompt_type,
                            'tool_success_rate': float(tool_rate),
                            'difficulty': difficulty,
                            'task_type': task_type,
                            
                            # JSONä¸­å®é™…å­˜åœ¨çš„ç»Ÿè®¡å­—æ®µ
                            'total': task_data.get('total', 0),
                            'successful': task_data.get('successful', 0),
                            'partial': task_data.get('partial', 0),
                            'failed': task_data.get('failed', 0),
                            'success_rate': task_data.get('success_rate', 0.0),
                            'partial_rate': task_data.get('partial_rate', 0.0),
                            'failure_rate': task_data.get('failure_rate', 0.0),
                            'weighted_success_score': task_data.get('weighted_success_score', 0.0),
                            'avg_execution_time': task_data.get('avg_execution_time', 0.0),
                            'avg_turns': task_data.get('avg_turns', 0.0),
                            'tool_coverage_rate': task_data.get('tool_coverage_rate', 0.0),
                            'avg_tool_calls': task_data.get('avg_tool_calls', 0.0),
                            
                            # æ—¶é—´æˆ³
                            'last_updated': datetime.now().isoformat()
                        }
                        
                        records.append(record)
    
    print(f"âœ… æå–äº† {len(records)} æ¡è®°å½•")
    
    if records:
        # åˆ›å»ºDataFrame
        df = pd.DataFrame(records)
        
        # æ˜¾ç¤ºç»Ÿè®¡
        print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"  è®°å½•æ•°: {len(df)}")
        print(f"  å­—æ®µæ•°: {len(df.columns)}")
        print(f"  æ¨¡å‹æ•°: {len(df['model'].unique())}")
        
        # æ˜¾ç¤ºæ¨¡å‹åˆ—è¡¨
        print(f"\næ¨¡å‹åˆ—è¡¨:")
        for model in sorted(df['model'].unique()):
            count = len(df[df['model'] == model])
            print(f"  - {model}: {count} æ¡è®°å½•")
        
        # æ£€æŸ¥æ•°æ®è´¨é‡
        print(f"\næ•°æ®è´¨é‡æ£€æŸ¥:")
        if 'llama-4-scout-17b' in df['model'].values:
            print("  âŒ åŒ…å«llama-4-scout-17b")
        else:
            print("  âœ… ä¸åŒ…å«llama-4-scout-17b")
            
        if 'unknown' in df['task_type'].values:
            print("  âŒ åŒ…å«unknown task_type")
        else:
            print("  âœ… ä¸åŒ…å«unknown task_type")
            
        if 'flawed' in df['prompt_type'].values:
            print("  âŒ åŒ…å«å•ç‹¬çš„flawed prompt_type")
        else:
            print("  âœ… ä¸åŒ…å«å•ç‹¬çš„flawed prompt_type")
        
        # ä¿å­˜åˆ°Parquet
        df.to_parquet(parquet_file, index=False)
        print(f"\nâœ… å·²ä¿å­˜åˆ°Parquet: {parquet_file}")
        print(f"  æ–‡ä»¶å¤§å°: {parquet_file.stat().st_size / 1024:.1f} KB")
        
        # éªŒè¯
        print(f"\nğŸ“Š éªŒè¯ç»“æœ:")
        df_verify = pd.read_parquet(parquet_file)
        print(f"  è®°å½•æ•°: {len(df_verify)}")
        print(f"  å­—æ®µæ•°: {len(df_verify.columns)}")
        
        # æ˜¾ç¤ºå­—æ®µåˆ—è¡¨
        print(f"\nå®é™…ä¿å­˜çš„å­—æ®µ:")
        for field in df_verify.columns:
            non_zero = len(df_verify[df_verify[field] != 0]) if field not in ['model', 'prompt_type', 'difficulty', 'task_type', 'last_updated'] else '---'
            if non_zero != '---' and non_zero > 0:
                print(f"  âœ… {field}: {non_zero} ä¸ªéé›¶å€¼")
            elif non_zero == 0:
                print(f"  âš ï¸ {field}: å…¨éƒ¨ä¸º0")
            else:
                print(f"  ğŸ“ {field}: æ–‡æœ¬å­—æ®µ")
    else:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆè®°å½•")

if __name__ == "__main__":
    sync_json_to_parquet_minimal()