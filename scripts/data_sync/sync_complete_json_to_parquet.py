#!/usr/bin/env python3
"""
å®Œæ•´çš„JSONåˆ°ParquetåŒæ­¥è„šæœ¬
ä¿ç•™æ‰€æœ‰å­—æ®µï¼Œç¡®ä¿æ•°æ®å®Œæ•´æ€§
"""

import json
import pandas as pd
from pathlib import Path
from datetime import datetime
import shutil

def sync_json_to_parquet_complete():
    """ä»JSONå®Œæ•´åŒæ­¥åˆ°Parquetï¼Œä¿ç•™æ‰€æœ‰å­—æ®µ"""
    
    # æ–‡ä»¶è·¯å¾„
    json_file = Path('pilot_bench_cumulative_results/master_database.json')
    parquet_file = Path('pilot_bench_parquet_data/test_results.parquet')
    backup_dir = Path('pilot_bench_parquet_data/backups')
    backup_dir.mkdir(exist_ok=True)
    
    if not json_file.exists():
        print("âŒ JSONæ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    # å¤‡ä»½ç°æœ‰Parquetæ–‡ä»¶
    if parquet_file.exists():
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        backup_file = backup_dir / f'test_results_{timestamp}.parquet'
        shutil.copy2(parquet_file, backup_file)
        print(f"âœ… å·²å¤‡ä»½Parquet: {backup_file.name}")
    
    print("ğŸ“– è¯»å–JSONæ•°æ®...")
    with open(json_file, 'r', encoding='utf-8') as f:
        json_data = json.load(f)
    
    # æå–æ‰€æœ‰è®°å½•ï¼Œä¿ç•™å®Œæ•´å­—æ®µ
    records = []
    
    for model_name, model_data in json_data.get('models', {}).items():
        # è·³è¿‡llama-4-scout-17b
        if 'llama-4' in model_name.lower():
            print(f"  âš ï¸ è·³è¿‡æ¨¡å‹: {model_name}")
            continue
            
        for prompt_type, prompt_data in model_data.get('by_prompt_type', {}).items():
            # è·³è¿‡æ— æ•ˆçš„å•ç‹¬flawedè®°å½•ï¼ˆæ²¡æœ‰å…·ä½“ç±»å‹çš„ï¼‰
            if prompt_type == 'flawed':
                continue
                
            for tool_rate, rate_data in prompt_data.get('by_tool_success_rate', {}).items():
                for difficulty, diff_data in rate_data.get('by_difficulty', {}).items():
                    for task_type, task_data in diff_data.get('by_task_type', {}).items():
                        # è·³è¿‡unknown task_type
                        if task_type == 'unknown':
                            continue
                            
                        if not isinstance(task_data, dict):
                            continue
                        
                        # åˆ›å»ºå®Œæ•´è®°å½•ï¼ŒåŒ…å«æ‰€æœ‰å­—æ®µ
                        record = {
                            # æ ‡è¯†å­—æ®µ
                            'model': model_name,
                            'prompt_type': prompt_type,
                            'tool_success_rate': float(tool_rate),
                            'difficulty': difficulty,
                            'task_type': task_type,
                            
                            # åŸºæœ¬ç»Ÿè®¡å­—æ®µï¼ˆä»JSONä¸­çš„successful/partial/failedæ˜ å°„ï¼‰
                            'total': task_data.get('total', 0),
                            'success': task_data.get('success', 0),
                            'full_success': task_data.get('full_success', 0),
                            'partial_success': task_data.get('partial_success', 0),
                            
                            # å¦‚æœJSONä¸­ä½¿ç”¨äº†ä¸åŒçš„å­—æ®µåï¼Œè¿›è¡Œæ˜ å°„
                            'successful': task_data.get('successful', task_data.get('success', 0)),
                            'partial': task_data.get('partial', task_data.get('partial_success', 0)),
                            'failed': task_data.get('failed', 0),
                            
                            # æˆåŠŸç‡å­—æ®µ
                            'success_rate': task_data.get('success_rate', 0.0),
                            'full_success_rate': task_data.get('full_success_rate', 0.0),
                            'partial_success_rate': task_data.get('partial_success_rate', 0.0),
                            'partial_rate': task_data.get('partial_rate', task_data.get('partial_success_rate', 0.0)),
                            'failure_rate': task_data.get('failure_rate', 0.0),
                            'weighted_success_score': task_data.get('weighted_success_score', 0.0),
                            
                            # æ‰§è¡ŒæŒ‡æ ‡
                            'avg_execution_time': task_data.get('avg_execution_time', 0.0),
                            'avg_turns': task_data.get('avg_turns', 0.0),
                            'avg_tool_calls': task_data.get('avg_tool_calls', 0.0),
                            'tool_coverage_rate': task_data.get('tool_coverage_rate', 0.0),
                            
                            # è´¨é‡åˆ†æ•°
                            'avg_workflow_score': task_data.get('avg_workflow_score', 0.0),
                            'avg_phase2_score': task_data.get('avg_phase2_score', 0.0),
                            'avg_quality_score': task_data.get('avg_quality_score', 0.0),
                            'avg_final_score': task_data.get('avg_final_score', 0.0),
                            
                            # é”™è¯¯ç»Ÿè®¡
                            'total_errors': task_data.get('total_errors', 0),
                            'tool_call_format_errors': task_data.get('tool_call_format_errors', 0),
                            'timeout_errors': task_data.get('timeout_errors', 0),
                            'dependency_errors': task_data.get('dependency_errors', 0),
                            'parameter_config_errors': task_data.get('parameter_config_errors', 0),
                            'tool_selection_errors': task_data.get('tool_selection_errors', 0),
                            'sequence_order_errors': task_data.get('sequence_order_errors', 0),
                            'max_turns_errors': task_data.get('max_turns_errors', 0),
                            'other_errors': task_data.get('other_errors', 0),
                            
                            # é”™è¯¯ç‡
                            'tool_selection_error_rate': task_data.get('tool_selection_error_rate', 0.0),
                            'parameter_error_rate': task_data.get('parameter_error_rate', 0.0),
                            'sequence_error_rate': task_data.get('sequence_error_rate', 0.0),
                            'dependency_error_rate': task_data.get('dependency_error_rate', 0.0),
                            'timeout_error_rate': task_data.get('timeout_error_rate', 0.0),
                            'format_error_rate': task_data.get('format_error_rate', 0.0),
                            'max_turns_error_rate': task_data.get('max_turns_error_rate', 0.0),
                            'other_error_rate': task_data.get('other_error_rate', 0.0),
                            
                            # è¾…åŠ©ç»Ÿè®¡
                            'assisted_failure': task_data.get('assisted_failure', 0),
                            'assisted_success': task_data.get('assisted_success', 0),
                            'total_assisted_turns': task_data.get('total_assisted_turns', 0),
                            'tests_with_assistance': task_data.get('tests_with_assistance', 0),
                            'avg_assisted_turns': task_data.get('avg_assisted_turns', 0.0),
                            'assisted_success_rate': task_data.get('assisted_success_rate', 0.0),
                            'assistance_rate': task_data.get('assistance_rate', 0.0),
                            
                            # æ—¶é—´æˆ³
                            'last_updated': task_data.get('last_updated', datetime.now().isoformat())
                        }
                        
                        records.append(record)
    
    print(f"âœ… æå–äº† {len(records)} æ¡è®°å½•")
    
    if records:
        # åˆ›å»ºDataFrame
        df = pd.DataFrame(records)
        
        # æ˜¾ç¤ºå­—æ®µç»Ÿè®¡
        print(f"\nğŸ“Š å­—æ®µç»Ÿè®¡:")
        print(f"  æ€»å­—æ®µæ•°: {len(df.columns)}")
        
        # æŒ‰ç±»åˆ«æ˜¾ç¤ºå­—æ®µ
        categories = {
            'æ ‡è¯†å­—æ®µ': ['model', 'prompt_type', 'tool_success_rate', 'difficulty', 'task_type'],
            'åŸºæœ¬ç»Ÿè®¡': ['total', 'success', 'full_success', 'partial_success', 'successful', 'partial', 'failed'],
            'æˆåŠŸç‡': ['success_rate', 'full_success_rate', 'partial_success_rate', 'failure_rate', 'weighted_success_score'],
            'æ‰§è¡ŒæŒ‡æ ‡': ['avg_execution_time', 'avg_turns', 'avg_tool_calls', 'tool_coverage_rate'],
            'è´¨é‡åˆ†æ•°': ['avg_workflow_score', 'avg_phase2_score', 'avg_quality_score', 'avg_final_score'],
            'é”™è¯¯ç»Ÿè®¡': ['total_errors', 'tool_call_format_errors', 'timeout_errors', 'dependency_errors', 
                        'parameter_config_errors', 'tool_selection_errors', 'sequence_order_errors', 
                        'max_turns_errors', 'other_errors'],
            'é”™è¯¯ç‡': ['tool_selection_error_rate', 'parameter_error_rate', 'sequence_error_rate',
                      'dependency_error_rate', 'timeout_error_rate', 'format_error_rate', 
                      'max_turns_error_rate', 'other_error_rate'],
            'è¾…åŠ©ç»Ÿè®¡': ['assisted_failure', 'assisted_success', 'total_assisted_turns',
                        'tests_with_assistance', 'avg_assisted_turns', 'assisted_success_rate', 'assistance_rate']
        }
        
        for category, fields in categories.items():
            existing = [f for f in fields if f in df.columns]
            print(f"\n  {category}: {len(existing)}/{len(fields)} å­—æ®µ")
            if len(existing) < len(fields):
                missing = set(fields) - set(existing)
                print(f"    ç¼ºå¤±: {missing}")
        
        # ä¿å­˜åˆ°Parquet
        df.to_parquet(parquet_file, index=False)
        print(f"\nâœ… å·²ä¿å­˜åˆ°Parquet: {parquet_file}")
        print(f"  æ–‡ä»¶å¤§å°: {parquet_file.stat().st_size / 1024:.1f} KB")
        
        # éªŒè¯
        print(f"\nğŸ“Š éªŒè¯ç»“æœ:")
        df_verify = pd.read_parquet(parquet_file)
        print(f"  è®°å½•æ•°: {len(df_verify)}")
        print(f"  å­—æ®µæ•°: {len(df_verify.columns)}")
        print(f"  æ¨¡å‹æ•°: {len(df_verify['model'].unique())}")
        
        # æ•°æ®è´¨é‡æ£€æŸ¥
        print(f"\nâœ… æ•°æ®è´¨é‡æ£€æŸ¥:")
        if 'llama-4-scout-17b' in df_verify['model'].values:
            print(f"  âŒ åŒ…å«llama-4-scout-17b")
        else:
            print(f"  âœ… ä¸åŒ…å«llama-4-scout-17b")
            
        if 'unknown' in df_verify['task_type'].values:
            print(f"  âŒ åŒ…å«unknown task_type")
        else:
            print(f"  âœ… ä¸åŒ…å«unknown task_type")
            
        if 'flawed' in df_verify['prompt_type'].values:
            print(f"  âŒ åŒ…å«å•ç‹¬çš„flawed prompt_type")
        else:
            print(f"  âœ… ä¸åŒ…å«å•ç‹¬çš„flawed prompt_type")
        
        # æ£€æŸ¥å…³é”®å­—æ®µçš„éé›¶å€¼
        key_fields = ['total_errors', 'avg_workflow_score', 'assisted_success', 'tool_selection_errors']
        print(f"\n  å…³é”®å­—æ®µéé›¶è®°å½•æ•°:")
        for field in key_fields:
            if field in df_verify.columns:
                non_zero = df_verify[df_verify[field] != 0]
                percentage = (len(non_zero) / len(df_verify)) * 100
                print(f"    {field}: {len(non_zero)}/{len(df_verify)} ({percentage:.1f}%)")
    else:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆè®°å½•")

if __name__ == "__main__":
    sync_json_to_parquet_complete()