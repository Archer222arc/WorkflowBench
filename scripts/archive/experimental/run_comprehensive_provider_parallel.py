#!/usr/bin/env python3
"""
Comprehensive Provider-Parallel Batch Testing
=============================================
åˆ©ç”¨æä¾›å•†çº§åˆ«é€Ÿç‡é™åˆ¶ä¼˜åŒ–çš„ç»¼åˆæ‰¹æµ‹è¯•è„šæœ¬
"""

import sys
import json
import time
import argparse
from pathlib import Path
from typing import List, Dict, Tuple
from datetime import datetime
from collections import defaultdict

sys.path.insert(0, str(Path(__file__).parent))

from batch_test_runner import TestTask
from provider_parallel_runner import ProviderParallelRunner
from api_client_manager import MODEL_PROVIDER_MAP, SUPPORTED_MODELS


def load_database(db_path="pilot_bench_cumulative_results/master_database.json"):
    """åŠ è½½æµ‹è¯•æ•°æ®åº“"""
    if not Path(db_path).exists():
        return {}
    with open(db_path, 'r') as f:
        return json.load(f)


def analyze_missing_tests(models: List[str] = None, 
                         prompt_types: List[str] = None,
                         task_types: List[str] = None,
                         difficulties: List[str] = None,
                         target_instances: int = 20) -> Dict:
    """
    åˆ†æç¼ºå¤±çš„æµ‹è¯•
    
    Returns:
        ç¼ºå¤±æµ‹è¯•çš„è¯¦ç»†ä¿¡æ¯
    """
    if models is None:
        models = SUPPORTED_MODELS
    if prompt_types is None:
        prompt_types = ['baseline', 'cot', 'optimal', 'flawed']
    if task_types is None:
        task_types = ['simple_task', 'basic_task', 'data_pipeline', 
                     'api_integration', 'multi_stage_pipeline']
    if difficulties is None:
        difficulties = ['easy', 'medium', 'hard']
    
    db = load_database()
    missing_tests = defaultdict(lambda: defaultdict(list))
    total_missing = 0
    
    # æ£€æŸ¥æ¯ä¸ªç»„åˆ
    for model in models:
        if model not in SUPPORTED_MODELS:
            continue
            
        model_data = db.get('models', {}).get(model, {})
        
        for prompt_type in prompt_types:
            # å¤„ç†flawedç±»å‹
            if prompt_type == 'flawed':
                flaw_types = ['sequence_disorder', 'tool_misuse', 'parameter_error', 
                            'missing_step', 'redundant_operations', 
                            'logical_inconsistency', 'semantic_drift']
                for flaw_type in flaw_types:
                    full_prompt = f'flawed_{flaw_type}'
                    for difficulty in difficulties:
                        for task_type in task_types:
                            # æ£€æŸ¥å·²å®Œæˆæ•°é‡
                            completed = 0
                            if 'by_prompt_type' in model_data:
                                prompt_data = model_data.get('by_prompt_type', {}).get('flawed', {})
                                if 'by_tool_success_rate' in prompt_data:
                                    for rate_data in prompt_data['by_tool_success_rate'].values():
                                        diff_data = rate_data.get('by_difficulty', {}).get(difficulty, {})
                                        task_data = diff_data.get('by_task_type', {}).get(task_type, {})
                                        if task_data.get('flaw_type') == flaw_type:
                                            completed += task_data.get('total', 0)
                            
                            needed = max(0, target_instances - completed)
                            if needed > 0:
                                missing_tests[model][full_prompt].append({
                                    'difficulty': difficulty,
                                    'task_type': task_type,
                                    'needed': needed,
                                    'completed': completed
                                })
                                total_missing += needed
            else:
                # å¸¸è§„promptç±»å‹
                for difficulty in difficulties:
                    for task_type in task_types:
                        # æ£€æŸ¥å·²å®Œæˆæ•°é‡
                        completed = 0
                        if 'by_prompt_type' in model_data:
                            prompt_data = model_data.get('by_prompt_type', {}).get(prompt_type, {})
                            if 'by_tool_success_rate' in prompt_data:
                                for rate_data in prompt_data['by_tool_success_rate'].values():
                                    diff_data = rate_data.get('by_difficulty', {}).get(difficulty, {})
                                    task_data = diff_data.get('by_task_type', {}).get(task_type, {})
                                    completed += task_data.get('total', 0)
                        
                        needed = max(0, target_instances - completed)
                        if needed > 0:
                            missing_tests[model][prompt_type].append({
                                'difficulty': difficulty,
                                'task_type': task_type,
                                'needed': needed,
                                'completed': completed
                            })
                            total_missing += needed
    
    return {
        'missing_tests': dict(missing_tests),
        'total_missing': total_missing,
        'models_with_missing': len(missing_tests),
        'target_instances': target_instances
    }


def create_test_tasks_from_analysis(analysis: Dict, 
                                   max_tasks_per_model: int = None,
                                   priority_models: List[str] = None) -> List[TestTask]:
    """
    æ ¹æ®åˆ†æç»“æœåˆ›å»ºæµ‹è¯•ä»»åŠ¡
    
    Args:
        analysis: ç¼ºå¤±æµ‹è¯•åˆ†æç»“æœ
        max_tasks_per_model: æ¯ä¸ªæ¨¡å‹çš„æœ€å¤§ä»»åŠ¡æ•°
        priority_models: ä¼˜å…ˆæµ‹è¯•çš„æ¨¡å‹åˆ—è¡¨
        
    Returns:
        æµ‹è¯•ä»»åŠ¡åˆ—è¡¨
    """
    tasks = []
    missing_tests = analysis['missing_tests']
    
    # å¦‚æœæŒ‡å®šäº†ä¼˜å…ˆæ¨¡å‹ï¼Œå…ˆå¤„ç†è¿™äº›
    if priority_models:
        for model in priority_models:
            if model in missing_tests:
                model_tasks = _create_model_tasks(model, missing_tests[model], max_tasks_per_model)
                tasks.extend(model_tasks)
        
        # ç„¶åå¤„ç†å…¶ä»–æ¨¡å‹
        for model in missing_tests:
            if model not in priority_models:
                model_tasks = _create_model_tasks(model, missing_tests[model], max_tasks_per_model)
                tasks.extend(model_tasks)
    else:
        # æŒ‰æä¾›å•†åˆ†ç»„ï¼Œä¼˜åŒ–å¹¶è¡Œ
        provider_groups = defaultdict(list)
        for model in missing_tests:
            provider = MODEL_PROVIDER_MAP.get(model, 'idealab')
            provider_groups[provider].append(model)
        
        # äº¤æ›¿ä»ä¸åŒæä¾›å•†é€‰æ‹©æ¨¡å‹ï¼Œå®ç°è´Ÿè½½å‡è¡¡
        while any(provider_groups.values()):
            for provider in ['azure', 'user_azure', 'idealab']:
                if provider_groups[provider]:
                    model = provider_groups[provider].pop(0)
                    model_tasks = _create_model_tasks(model, missing_tests[model], max_tasks_per_model)
                    tasks.extend(model_tasks)
    
    return tasks


def _create_model_tasks(model: str, prompt_configs: Dict, max_tasks: int = None) -> List[TestTask]:
    """ä¸ºå•ä¸ªæ¨¡å‹åˆ›å»ºæµ‹è¯•ä»»åŠ¡"""
    tasks = []
    task_count = 0
    
    for prompt_type, configs in prompt_configs.items():
        is_flawed = prompt_type.startswith('flawed_')
        flaw_type = prompt_type.replace('flawed_', '') if is_flawed else None
        
        for config in configs:
            needed = config['needed']
            if max_tasks and task_count + needed > max_tasks:
                needed = max_tasks - task_count
            
            for _ in range(needed):
                task = TestTask(
                    model=model,
                    task_type=config['task_type'],
                    prompt_type='flawed' if is_flawed else prompt_type,
                    difficulty=config['difficulty'],
                    is_flawed=is_flawed,
                    flaw_type=flaw_type,
                    tool_success_rate=0.8
                )
                tasks.append(task)
                task_count += 1
                
                if max_tasks and task_count >= max_tasks:
                    return tasks
    
    return tasks


def run_comprehensive_test(models: List[str] = None,
                          prompt_types: List[str] = None,
                          difficulties: List[str] = None,
                          task_types: List[str] = None,
                          target_instances: int = 20,
                          max_tasks: int = None,
                          dry_run: bool = False):
    """
    è¿è¡Œç»¼åˆæµ‹è¯•
    
    Args:
        models: è¦æµ‹è¯•çš„æ¨¡å‹åˆ—è¡¨
        prompt_types: æç¤ºç±»å‹åˆ—è¡¨
        difficulties: éš¾åº¦åˆ—è¡¨
        task_types: ä»»åŠ¡ç±»å‹åˆ—è¡¨
        target_instances: æ¯ä¸ªç»„åˆçš„ç›®æ ‡å®ä¾‹æ•°
        max_tasks: æœ€å¤§ä»»åŠ¡æ•°é™åˆ¶
        dry_run: ä»…åˆ†æä¸æ‰§è¡Œ
    """
    print("="*70)
    print("ğŸ¯ ç»¼åˆæä¾›å•†å¹¶è¡Œæ‰¹æµ‹è¯•")
    print("="*70)
    print(f"æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # åˆ†æç¼ºå¤±æµ‹è¯•
    print("\nğŸ“Š åˆ†æç¼ºå¤±æµ‹è¯•...")
    analysis = analyze_missing_tests(
        models=models,
        prompt_types=prompt_types,
        task_types=task_types,
        difficulties=difficulties,
        target_instances=target_instances
    )
    
    print(f"\nğŸ“ˆ åˆ†æç»“æœ:")
    print(f"  éœ€è¦æµ‹è¯•çš„æ¨¡å‹æ•°: {analysis['models_with_missing']}")
    print(f"  æ€»ç¼ºå¤±æµ‹è¯•æ•°: {analysis['total_missing']}")
    print(f"  ç›®æ ‡å®ä¾‹æ•°: {analysis['target_instances']}")
    
    if analysis['total_missing'] == 0:
        print("\nâœ… æ‰€æœ‰æµ‹è¯•å·²å®Œæˆï¼")
        return
    
    # æ˜¾ç¤ºè¯¦ç»†ç¼ºå¤±ä¿¡æ¯
    print("\nğŸ“‹ ç¼ºå¤±æµ‹è¯•è¯¦æƒ…:")
    for model, prompts in analysis['missing_tests'].items():
        provider = MODEL_PROVIDER_MAP.get(model, 'idealab')
        model_total = sum(
            sum(c['needed'] for c in configs)
            for configs in prompts.values()
        )
        print(f"\n  {model} ({provider}): {model_total} ä¸ªæµ‹è¯•")
        for prompt_type, configs in prompts.items():
            prompt_total = sum(c['needed'] for c in configs)
            print(f"    {prompt_type}: {prompt_total} ä¸ª")
    
    if dry_run:
        print("\nğŸ” è¯•è¿è¡Œæ¨¡å¼ï¼Œä¸æ‰§è¡Œå®é™…æµ‹è¯•")
        return
    
    # åˆ›å»ºæµ‹è¯•ä»»åŠ¡
    print("\nğŸ“ åˆ›å»ºæµ‹è¯•ä»»åŠ¡...")
    tasks = create_test_tasks_from_analysis(analysis, max_tasks_per_model=max_tasks)
    
    if max_tasks and len(tasks) > max_tasks:
        tasks = tasks[:max_tasks]
        print(f"âš ï¸ é™åˆ¶ä»»åŠ¡æ•°åˆ° {max_tasks}")
    
    print(f"âœ… åˆ›å»ºäº† {len(tasks)} ä¸ªæµ‹è¯•ä»»åŠ¡")
    
    # æŒ‰æä¾›å•†ç»Ÿè®¡
    provider_counts = defaultdict(int)
    for task in tasks:
        provider = MODEL_PROVIDER_MAP.get(task.model, 'idealab')
        provider_counts[provider] += 1
    
    print("\nğŸ“Š ä»»åŠ¡åˆ†å¸ƒï¼ˆæŒ‰æä¾›å•†ï¼‰:")
    for provider, count in provider_counts.items():
        print(f"  {provider}: {count} ä¸ªä»»åŠ¡")
    
    # ä¼°ç®—æ—¶é—´
    print("\nâ±ï¸ æ—¶é—´ä¼°ç®—:")
    serial_time = len(tasks) * 10  # å‡è®¾æ¯ä¸ªæµ‹è¯•10ç§’
    print(f"  ä¸²è¡Œæ‰§è¡Œ: ~{serial_time}ç§’ ({serial_time/60:.1f}åˆ†é’Ÿ)")
    
    # å¹¶è¡Œæ—¶é—´ä¼°ç®—
    if len(provider_counts) > 1:
        # æœ€æ…¢çš„æä¾›å•†å†³å®šæ€»æ—¶é—´
        idealab_tasks = provider_counts.get('idealab', 0)
        azure_tasks = provider_counts.get('azure', 0)
        user_azure_tasks = provider_counts.get('user_azure', 0)
        
        # å„æä¾›å•†çš„é¢„è®¡æ—¶é—´
        idealab_time = idealab_tasks * 10 / 3  # IdealLab 3ä¸ªå¹¶å‘
        azure_time = azure_tasks * 10 / 50  # Azure 50ä¸ªå¹¶å‘
        user_azure_time = user_azure_tasks * 10 / 30  # User Azure 30ä¸ªå¹¶å‘
        
        parallel_time = max(idealab_time, azure_time, user_azure_time)
    else:
        # å•æä¾›å•†
        provider = list(provider_counts.keys())[0]
        if provider == 'idealab':
            parallel_time = len(tasks) * 10 / 3
        elif provider == 'azure':
            parallel_time = len(tasks) * 10 / 50
        else:
            parallel_time = len(tasks) * 10 / 30
    
    print(f"  å¹¶è¡Œæ‰§è¡Œ: ~{parallel_time}ç§’ ({parallel_time/60:.1f}åˆ†é’Ÿ)")
    print(f"  é¢„æœŸåŠ é€Ÿ: {serial_time/parallel_time:.1f}x")
    
    # ç¡®è®¤æ‰§è¡Œ
    print("\n" + "="*70)
    response = input("æ˜¯å¦å¼€å§‹æ‰§è¡Œï¼Ÿ(y/n): ")
    if response.lower() != 'y':
        print("å–æ¶ˆæ‰§è¡Œ")
        return
    
    # æ‰§è¡Œæµ‹è¯•
    print("\nğŸš€ å¼€å§‹æ‰§è¡Œæµ‹è¯•...")
    start_time = time.time()
    
    runner = ProviderParallelRunner(
        debug=False,
        silent=False,
        save_logs=True,
        use_ai_classification=False
    )
    
    results, stats = runner.run_parallel_by_provider(tasks)
    
    actual_time = time.time() - start_time
    
    # æ˜¾ç¤ºç»“æœ
    print("\n" + "="*70)
    print("ğŸ“Š æµ‹è¯•å®Œæˆ")
    print("="*70)
    
    print(f"\nâ±ï¸ å®é™…æ‰§è¡Œæ—¶é—´: {actual_time:.1f}ç§’ ({actual_time/60:.1f}åˆ†é’Ÿ)")
    print(f"ğŸ“ˆ å®é™…åŠ é€Ÿæ¯”: {serial_time/actual_time:.2f}x")
    
    # ä¿å­˜ç»“æœåˆ°æ•°æ®åº“
    print("\nğŸ’¾ ä¿å­˜ç»“æœåˆ°æ•°æ®åº“...")
    for model in set(t.model for t in tasks):
        model_results = [r for r in results if r and r.get('model') == model]
        if model_results:
            runner.save_results_to_database(model_results, model, 'mixed')
    
    print("\nâœ… æ‰¹æµ‹è¯•å®Œæˆï¼")


def main():
    parser = argparse.ArgumentParser(description='ç»¼åˆæä¾›å•†å¹¶è¡Œæ‰¹æµ‹è¯•')
    parser.add_argument('--models', type=str, help='æ¨¡å‹åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰ï¼Œé»˜è®¤æ‰€æœ‰æ”¯æŒçš„æ¨¡å‹')
    parser.add_argument('--prompt-types', type=str, help='æç¤ºç±»å‹ï¼ˆé€—å·åˆ†éš”ï¼‰ï¼Œé»˜è®¤æ‰€æœ‰ç±»å‹')
    parser.add_argument('--difficulties', type=str, default='easy', help='éš¾åº¦çº§åˆ«ï¼ˆé€—å·åˆ†éš”ï¼‰')
    parser.add_argument('--task-types', type=str, help='ä»»åŠ¡ç±»å‹ï¼ˆé€—å·åˆ†éš”ï¼‰ï¼Œé»˜è®¤æ‰€æœ‰ç±»å‹')
    parser.add_argument('--target-instances', type=int, default=20, help='æ¯ä¸ªç»„åˆçš„ç›®æ ‡å®ä¾‹æ•°')
    parser.add_argument('--max-tasks', type=int, help='æœ€å¤§ä»»åŠ¡æ•°é™åˆ¶')
    parser.add_argument('--dry-run', action='store_true', help='ä»…åˆ†æä¸æ‰§è¡Œ')
    
    args = parser.parse_args()
    
    # è§£æå‚æ•°
    models = [m.strip() for m in args.models.split(',')] if args.models else None
    prompt_types = [p.strip() for p in args.prompt_types.split(',')] if args.prompt_types else None
    difficulties = [d.strip() for d in args.difficulties.split(',')] if args.difficulties else ['easy']
    task_types = [t.strip() for t in args.task_types.split(',')] if args.task_types else None
    
    # è¿è¡Œæµ‹è¯•
    run_comprehensive_test(
        models=models,
        prompt_types=prompt_types,
        difficulties=difficulties,
        task_types=task_types,
        target_instances=args.target_instances,
        max_tasks=args.max_tasks,
        dry_run=args.dry_run
    )


if __name__ == "__main__":
    main()