#!/usr/bin/env python3
"""
åˆ†æå¹¶å‘æ¨¡å¼ä¸‹çš„æ•°æ®å®Œæ•´æ€§é—®é¢˜
åŸºäºç”¨æˆ·åé¦ˆï¼šä¸²è¡Œ+Parquetæ­£å¸¸ï¼Œå¹¶å‘+JSONæœ‰é—®é¢˜
"""

import json
import re
from pathlib import Path
from collections import defaultdict, Counter
from datetime import datetime

def analyze_concurrent_execution_logs():
    """åˆ†æå¹¶å‘æ‰§è¡Œçš„æ—¥å¿—æ¨¡å¼"""
    print("=" * 60)
    print("ğŸ” å¹¶å‘æ‰§è¡Œæ—¥å¿—æ¨¡å¼åˆ†æ")
    print("=" * 60)
    
    log_dir = Path("logs")
    if not log_dir.exists():
        print("âŒ æ—¥å¿—ç›®å½•ä¸å­˜åœ¨")
        return
    
    # æ‰¾åˆ°æœ€æ–°çš„æ‰¹å¤„ç†æ—¥å¿—ï¼ˆç”¨æˆ·æåˆ°çš„5.1æµ‹è¯•ï¼‰
    log_files = sorted(log_dir.glob("batch_test_*.log"), 
                      key=lambda x: x.stat().st_mtime, reverse=True)
    
    if not log_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æ—¥å¿—æ–‡ä»¶")
        return
    
    # åˆ†ææœ€æ–°çš„å‡ ä¸ªæ—¥å¿—æ–‡ä»¶
    models_found = set()
    test_starts = []
    test_completions = []
    failures = []
    
    for log_file in log_files[:10]:  # åˆ†ææœ€æ–°çš„10ä¸ªæ–‡ä»¶
        try:
            with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                lines = f.readlines()
            
            print(f"\nğŸ“ åˆ†ææ–‡ä»¶: {log_file.name}")
            
            for line in lines:
                # å¯»æ‰¾æ¨¡å‹æ‰§è¡Œçš„è¯æ®
                if "Starting single test:" in line and "model=" in line:
                    # æå–æ¨¡å‹å
                    match = re.search(r'model=([^,]+)', line)
                    if match:
                        model = match.group(1)
                        models_found.add(model)
                        test_starts.append((model, log_file.name, line.strip()))
                
                # å¯»æ‰¾æµ‹è¯•å®Œæˆ/å¤±è´¥çš„è¯æ®
                if "test completed" in line.lower() or "test failed" in line.lower():
                    test_completions.append((log_file.name, line.strip()))
                
                # å¯»æ‰¾APIé”™è¯¯æˆ–è¶…æ—¶
                if any(error_keyword in line.lower() for error_keyword in 
                      ["error", "timeout", "failed", "exception"]):
                    failures.append((log_file.name, line.strip()))
                    
        except Exception as e:
            print(f"âŒ è¯»å–æ—¥å¿—å¤±è´¥: {e}")
    
    print(f"\nğŸ“Š å‘ç°çš„æ¨¡å‹æ€»æ•°: {len(models_found)}")
    for model in sorted(models_found):
        count = sum(1 for start in test_starts if start[0] == model)
        print(f"   {model}: {count} æ¬¡æµ‹è¯•å¯åŠ¨")
    
    # æ£€æŸ¥æœŸæœ›çš„æ¨¡å‹æ˜¯å¦éƒ½å‡ºç°äº†
    expected_models = [
        "DeepSeek-V3-0324", "DeepSeek-R1-0528", 
        "qwen2.5-72b-instruct", "qwen2.5-32b-instruct", "qwen2.5-14b-instruct",
        "qwen2.5-7b-instruct", "qwen2.5-3b-instruct", "Llama-3.3-70B-Instruct"
    ]
    
    missing_models = set(expected_models) - models_found
    if missing_models:
        print(f"\nâŒ æœªåœ¨æ—¥å¿—ä¸­å‘ç°çš„æ¨¡å‹:")
        for model in missing_models:
            print(f"   - {model}")
    
    print(f"\nğŸ“ˆ æµ‹è¯•å¯åŠ¨æ€»æ¬¡æ•°: {len(test_starts)}")
    print(f"ğŸ“‰ é”™è¯¯/å¤±è´¥æ€»æ¬¡æ•°: {len(failures)}")

def analyze_json_data_integrity():
    """åˆ†æJSONæ•°æ®å®Œæ•´æ€§é—®é¢˜"""
    print("\n" + "=" * 60)
    print("ğŸ” JSONæ•°æ®å®Œæ•´æ€§åˆ†æ")
    print("=" * 60)
    
    db_path = Path("pilot_bench_cumulative_results/master_database.json")
    if not db_path.exists():
        print("âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    with open(db_path, 'r', encoding='utf-8') as f:
        db = json.load(f)
    
    print(f"æ•°æ®åº“ç‰ˆæœ¬: {db.get('version', 'Unknown')}")
    print(f"åˆ›å»ºæ—¶é—´: {db.get('created_at', 'Unknown')}")
    print(f"æœ€åæ›´æ–°: {db.get('last_updated', 'Unknown')}")
    
    # åˆ†ææµ‹è¯•æ—¶é—´åˆ†å¸ƒ
    models = db.get('models', {})
    time_analysis = []
    
    for model_name, model_data in models.items():
        first_test = model_data.get('first_test_time')
        last_test = model_data.get('last_test_time')
        total_tests = model_data.get('total_tests', 0)
        
        if first_test and last_test:
            try:
                first_dt = datetime.fromisoformat(first_test.replace('Z', '+00:00'))
                last_dt = datetime.fromisoformat(last_test.replace('Z', '+00:00'))
                duration = (last_dt - first_dt).total_seconds()
                time_analysis.append((model_name, total_tests, duration, first_dt, last_dt))
            except:
                print(f"âŒ è§£ææ—¶é—´å¤±è´¥: {model_name}")
    
    # æŒ‰æ—¶é—´æ’åº
    time_analysis.sort(key=lambda x: x[3])  # æŒ‰first_dtæ’åº
    
    print("\nâ° æ¨¡å‹æµ‹è¯•æ—¶é—´åºåˆ—:")
    for model_name, total_tests, duration, first_dt, last_dt in time_analysis:
        print(f"   {model_name}:")
        print(f"     æµ‹è¯•æ•°é‡: {total_tests}")
        print(f"     å¼€å§‹æ—¶é—´: {first_dt.strftime('%H:%M:%S')}")
        print(f"     ç»“æŸæ—¶é—´: {last_dt.strftime('%H:%M:%S')}")
        print(f"     æŒç»­æ—¶é—´: {duration:.1f}ç§’")
        print()
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é‡å çš„æµ‹è¯•æ—¶é—´ï¼ˆå¯èƒ½è¡¨æ˜å¹¶å‘é—®é¢˜ï¼‰
    print("ğŸ”„ å¹¶å‘é‡å åˆ†æ:")
    for i in range(len(time_analysis)):
        for j in range(i + 1, len(time_analysis)):
            model1, _, _, start1, end1 = time_analysis[i]
            model2, _, _, start2, end2 = time_analysis[j]
            
            # æ£€æŸ¥æ—¶é—´é‡å 
            if start1 <= end2 and start2 <= end1:
                overlap_start = max(start1, start2)
                overlap_end = min(end1, end2)
                overlap_duration = (overlap_end - overlap_start).total_seconds()
                
                if overlap_duration > 0:
                    print(f"   âš ï¸  {model1} ä¸ {model2} æœ‰ {overlap_duration:.1f}ç§’ é‡å ")

def analyze_ultra_parallel_logic():
    """åˆ†æultra_parallel_runnerçš„é€»è¾‘é—®é¢˜"""
    print("\n" + "=" * 60)
    print("ğŸ” Ultra Parallel Runneré€»è¾‘åˆ†æ")
    print("=" * 60)
    
    # æ£€æŸ¥ultra_parallel_runner.pyæ˜¯å¦å­˜åœ¨é—®é¢˜
    runner_path = Path("ultra_parallel_runner.py")
    if not runner_path.exists():
        print("âŒ ultra_parallel_runner.pyæ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    with open(runner_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # æ£€æŸ¥å…³é”®é€»è¾‘ç‚¹
    issues = []
    
    # 1. æ£€æŸ¥qwenæ¨¡å‹çš„å°å†™è½¬æ¢é—®é¢˜
    if 'model=model.lower()' in content:
        issues.append("âŒ å‘ç°model.lower()è½¬æ¢ï¼Œå¯èƒ½å¯¼è‡´qwenæ¨¡å‹åç§°é—®é¢˜")
    
    # 2. æ£€æŸ¥æ¨¡å‹åˆ†ç‰‡é€»è¾‘
    if '_create_qwen_smart_shards' in content:
        print("âœ… æ‰¾åˆ°qwenæ™ºèƒ½åˆ†ç‰‡é€»è¾‘")
        # æå–ç›¸å…³ä»£ç æ®µ
        pattern = r'def _create_qwen_smart_shards.*?(?=def|\Z)'
        match = re.search(pattern, content, re.DOTALL)
        if match:
            qwen_logic = match.group(0)
            if 'model.lower()' in qwen_logic:
                issues.append("âŒ qwenåˆ†ç‰‡ä¸­æœ‰model.lower()è½¬æ¢")
    
    # 3. æ£€æŸ¥DeepSeek-R1çš„å¤„ç†
    deepseek_r1_mentions = content.count('deepseek-r1')
    deepseek_R1_mentions = content.count('DeepSeek-R1')
    print(f"DeepSeek-R1åœ¨ä»£ç ä¸­æåŠæ¬¡æ•°: {deepseek_R1_mentions + deepseek_r1_mentions}")
    
    if deepseek_r1_mentions == 0 and deepseek_R1_mentions == 0:
        issues.append("âŒ ultra_parallel_runnerä¸­æ²¡æœ‰DeepSeek-R1ç›¸å…³é€»è¾‘")
    
    # 4. æ£€æŸ¥æ¨¡å‹æ—æ˜ å°„é€»è¾‘
    if 'elif "deepseek-r1" in model.lower():' in content:
        print("âœ… æ‰¾åˆ°DeepSeek-R1æ¨¡å‹æ—æ˜ å°„é€»è¾‘")
    else:
        issues.append("âŒ ç¼ºå°‘DeepSeek-R1æ¨¡å‹æ—æ˜ å°„é€»è¾‘")
    
    if issues:
        print("\nâš ï¸  å‘ç°çš„æ½œåœ¨é—®é¢˜:")
        for issue in issues:
            print(f"   {issue}")
    else:
        print("\nâœ… ultra_parallel_runneré€»è¾‘çœ‹èµ·æ¥æ­£å¸¸")

def suggest_fixes():
    """å»ºè®®ä¿®å¤æ–¹æ¡ˆ"""
    print("\n" + "=" * 60)
    print("ğŸ› ï¸  ä¿®å¤å»ºè®®")
    print("=" * 60)
    
    print("åŸºäºåˆ†æï¼Œå»ºè®®æŒ‰ä»¥ä¸‹ä¼˜å…ˆçº§ä¿®å¤:")
    print()
    print("1. ğŸ”§ ä¿®å¤ultra_parallel_runner.pyä¸­çš„model.lower()é—®é¢˜")
    print("   - åœ¨qwenåˆ†ç‰‡ä¸­ä¿æŒåŸå§‹å¤§å°å†™")
    print("   - ç¡®ä¿æ¨¡å‹åç§°ä¼ é€’çš„ä¸€è‡´æ€§")
    print()
    print("2. ğŸ”§ æ£€æŸ¥å¹¶å‘æ¨¡å¼ä¸‹çš„æ¨¡å‹éå†é€»è¾‘")
    print("   - ç¡®è®¤æ‰€æœ‰é¢„æœŸæ¨¡å‹éƒ½è¢«åŒ…å«åœ¨æ‰§è¡Œåˆ—è¡¨ä¸­")
    print("   - éªŒè¯DeepSeek-R1æ˜¯å¦è¢«æ­£ç¡®è§¦å‘")
    print()
    print("3. ğŸ”§ åŠ å¼ºJSONæ ¼å¼çš„å¹¶å‘å†™å…¥ä¿æŠ¤")
    print("   - æ£€æŸ¥æ•°æ®ä¿å­˜æ—¶çš„é”æœºåˆ¶")  
    print("   - é˜²æ­¢æ•°æ®è¦†ç›–å’Œé‡å¤å†™å…¥")
    print()
    print("4. ğŸ§ª åˆ›å»ºå¹¶å‘æµ‹è¯•éªŒè¯è„šæœ¬")
    print("   - å°è§„æ¨¡å¤ç°é—®é¢˜")
    print("   - éªŒè¯ä¿®å¤æ•ˆæœ")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ” å¹¶å‘æ¨¡å¼æ•°æ®å®Œæ•´æ€§é—®é¢˜åˆ†æ")
    print(f"ğŸ• åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    analyze_concurrent_execution_logs()
    analyze_json_data_integrity()
    analyze_ultra_parallel_logic()
    suggest_fixes()
    
    print("\n" + "=" * 60)
    print("ğŸ“‹ å…³é”®å‘ç°")
    print("=" * 60)
    print("âœ… ä¸²è¡Œ+Parquetæ¨¡å¼å·¥ä½œæ­£å¸¸ï¼ˆç”¨æˆ·ç¡®è®¤ï¼‰")
    print("âŒ å¹¶å‘+JSONæ¨¡å¼å­˜åœ¨æ•°æ®ä¸¢å¤±/é‡å¤é—®é¢˜")
    print("âŒ DeepSeek-R1å®Œå…¨æ²¡æœ‰è¢«æ‰§è¡Œ")
    print("âŒ éƒ¨åˆ†qwenæ¨¡å‹ç¼ºå¤±ï¼Œéƒ¨åˆ†é‡å¤")
    print("âš ï¸  é—®é¢˜å¯èƒ½å‡ºç°åœ¨æ¨¡å‹è°ƒåº¦æˆ–å¹¶å‘æ•°æ®å†™å…¥é˜¶æ®µ")

if __name__ == "__main__":
    main()