#!/usr/bin/env python3
"""
Ultraå¹¶å‘æ¨¡å¼æ•°æ®åˆå¹¶å®Œæ•´æ€§è¯Šæ–­å·¥å…·
=================================

æ£€æŸ¥ä»¥ä¸‹å¹¶å‘å±‚çº§çš„æ•°æ®åˆå¹¶é—®é¢˜ï¼š
1. æ¨¡å‹çº§å¹¶å‘ - ä¸åŒæ¨¡å‹åŒæ—¶æµ‹è¯•æ—¶çš„æ•°æ®åˆ†ç¦»
2. APIçº§å¹¶å‘ - åŒä¸€æ¨¡å‹å¤šä¸ªAPIå®ä¾‹çš„æ•°æ®åˆå¹¶
3. Promptçº§å¹¶å‘ - å¤šç§promptç±»å‹å¹¶è¡Œæ—¶çš„æ•°æ®åˆ†ç¦»
4. åˆ†ç‰‡çº§åˆå¹¶ - ultra_parallel_runneråˆ†ç‰‡ç»“æœèšåˆ
5. è·¨è¿›ç¨‹åˆå¹¶ - å¤šä¸ªsmart_batch_runnerè¿›ç¨‹çš„æ•°æ®åˆå¹¶
"""

import json
import os
import time
from pathlib import Path
from datetime import datetime
from collections import defaultdict
from typing import Dict, List, Set, Any
import subprocess

os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

class UltraConcurrentMergingDiagnostic:
    """Ultraå¹¶å‘æ¨¡å¼æ•°æ®åˆå¹¶è¯Šæ–­å™¨"""
    
    def __init__(self):
        self.db_path = Path("pilot_bench_cumulative_results/master_database.json")
        self.log_dir = Path("logs")
        self.results = {}
        
        print("ğŸ” å¯åŠ¨Ultraå¹¶å‘æ¨¡å¼æ•°æ®åˆå¹¶è¯Šæ–­")
        print("=" * 60)
    
    def load_database(self) -> Dict:
        """åŠ è½½æ•°æ®åº“"""
        if not self.db_path.exists():
            print("âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨")
            return {}
        
        with open(self.db_path, 'r') as f:
            return json.load(f)
    
    def analyze_model_level_concurrency(self, db: Dict) -> Dict:
        """åˆ†ææ¨¡å‹çº§åˆ«å¹¶å‘çš„æ•°æ®åˆå¹¶é—®é¢˜"""
        print("\nğŸ“Š 1. æ¨¡å‹çº§å¹¶å‘æ•°æ®åˆå¹¶åˆ†æ")
        print("-" * 40)
        
        models = db.get('models', {})
        analysis = {
            'total_models': len(models),
            'models_with_data': 0,
            'models_without_data': [],
            'data_mixing_issues': [],
            'expected_vs_actual': {}
        }
        
        expected_models = [
            'DeepSeek-V3-0324', 'DeepSeek-R1-0528', 'Llama-3.3-70B-Instruct',
            'qwen2.5-72b-instruct', 'qwen2.5-32b-instruct', 'qwen2.5-14b-instruct',
            'qwen2.5-7b-instruct', 'qwen2.5-3b-instruct'
        ]
        
        print(f"é¢„æœŸæ¨¡å‹æ•°é‡: {len(expected_models)}")
        print(f"å®é™…æ¨¡å‹æ•°é‡: {len(models)}")
        
        # æ£€æŸ¥æ¯ä¸ªæ¨¡å‹çš„æ•°æ®å®Œæ•´æ€§
        for model_name, model_data in models.items():
            if isinstance(model_data, dict):
                overall_stats = model_data.get('overall_stats', {})
                total_tests = overall_stats.get('total_tests', 0)
                
                if total_tests > 0:
                    analysis['models_with_data'] += 1
                    print(f"  âœ… {model_name}: {total_tests} æµ‹è¯•")
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸é«˜çš„æµ‹è¯•æ•°é‡ï¼ˆå¯èƒ½è¡¨ç¤ºæ•°æ®æ··åˆï¼‰
                    if total_tests > 50:  # æ­£å¸¸5.1æµ‹è¯•åº”è¯¥æ˜¯10ä¸ªæµ‹è¯•/æ¨¡å‹
                        analysis['data_mixing_issues'].append({
                            'model': model_name,
                            'issue': 'abnormally_high_test_count',
                            'count': total_tests,
                            'expected': 10
                        })
                        print(f"    âš ï¸  å¼‚å¸¸é«˜æµ‹è¯•æ•°é‡: {total_tests} (æœŸæœ›: 10)")
                else:
                    analysis['models_without_data'].append(model_name)
                    print(f"  âŒ {model_name}: æ— æµ‹è¯•æ•°æ®")
        
        # æ£€æŸ¥ç¼ºå¤±çš„æ¨¡å‹
        missing_models = set(expected_models) - set(models.keys())
        if missing_models:
            analysis['missing_models'] = list(missing_models)
            print(f"\nâŒ ç¼ºå¤±æ¨¡å‹: {missing_models}")
        else:
            print(f"\nâœ… æ‰€æœ‰æœŸæœ›æ¨¡å‹éƒ½å­˜åœ¨")
        
        self.results['model_level'] = analysis
        return analysis
    
    def analyze_api_level_concurrency(self, db: Dict) -> Dict:
        """åˆ†æAPIçº§åˆ«å¹¶å‘çš„æ•°æ®åˆå¹¶é—®é¢˜"""
        print("\nğŸ“Š 2. APIçº§å¹¶å‘æ•°æ®åˆå¹¶åˆ†æ")
        print("-" * 40)
        
        analysis = {
            'azure_instances_merged': {},
            'ideallab_keys_merged': {},
            'potential_duplication': []
        }
        
        models = db.get('models', {})
        
        # æ£€æŸ¥Azureæ¨¡å‹çš„å®ä¾‹åˆå¹¶ï¼ˆDeepSeek, Llamaï¼‰
        azure_models = ['DeepSeek-V3-0324', 'DeepSeek-R1-0528', 'Llama-3.3-70B-Instruct']
        
        for model in azure_models:
            if model in models:
                model_data = models[model]
                overall_stats = model_data.get('overall_stats', {})
                total_tests = overall_stats.get('total_tests', 0)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰3ä¸ªå®ä¾‹çš„æ•°æ®è¢«æ­£ç¡®åˆå¹¶
                # æ¯ä¸ªå®ä¾‹åº”è¯¥è´¡çŒ®ç›¸ç­‰æ•°é‡çš„æµ‹è¯•
                analysis['azure_instances_merged'][model] = {
                    'total_tests': total_tests,
                    'expected_per_instance': total_tests / 3 if total_tests > 0 else 0,
                    'status': 'merged' if total_tests > 0 else 'no_data'
                }
                
                print(f"  Azureæ¨¡å‹ {model}:")
                print(f"    æ€»æµ‹è¯•: {total_tests}")
                print(f"    é¢„æœŸæ¯å®ä¾‹: {total_tests / 3:.1f}")
        
        # æ£€æŸ¥IdealLab qwenæ¨¡å‹çš„API keyåˆå¹¶
        qwen_models = ['qwen2.5-72b-instruct', 'qwen2.5-32b-instruct', 'qwen2.5-14b-instruct',
                      'qwen2.5-7b-instruct', 'qwen2.5-3b-instruct']
        
        for model in qwen_models:
            if model in models:
                model_data = models[model]
                overall_stats = model_data.get('overall_stats', {})
                total_tests = overall_stats.get('total_tests', 0)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰3ä¸ªAPI keyçš„æ•°æ®è¢«æ­£ç¡®åˆå¹¶
                analysis['ideallab_keys_merged'][model] = {
                    'total_tests': total_tests,
                    'expected_per_key': total_tests / 3 if total_tests > 0 else 0,
                    'status': 'merged' if total_tests > 0 else 'no_data'
                }
                
                print(f"  IdealLabæ¨¡å‹ {model}:")
                print(f"    æ€»æµ‹è¯•: {total_tests}")
                print(f"    é¢„æœŸæ¯key: {total_tests / 3:.1f}")
        
        self.results['api_level'] = analysis
        return analysis
    
    def analyze_prompt_level_concurrency(self, db: Dict) -> Dict:
        """åˆ†æPromptçº§åˆ«å¹¶å‘çš„æ•°æ®åˆ†ç¦»"""
        print("\nğŸ“Š 3. Promptçº§å¹¶å‘æ•°æ®åˆ†ç¦»åˆ†æ")
        print("-" * 40)
        
        analysis = {
            'prompt_types_found': set(),
            'models_with_mixed_prompts': [],
            'prompt_separation_issues': []
        }
        
        models = db.get('models', {})
        
        for model_name, model_data in models.items():
            if not isinstance(model_data, dict):
                continue
                
            by_prompt = model_data.get('by_prompt_type', {})
            prompt_types = list(by_prompt.keys())
            
            analysis['prompt_types_found'].update(prompt_types)
            
            print(f"  æ¨¡å‹ {model_name}:")
            print(f"    Promptç±»å‹: {prompt_types}")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¤šç§promptç±»å‹ï¼ˆè¯´æ˜åˆ†ç¦»æ­£ç¡®ï¼‰
            if len(prompt_types) > 1:
                analysis['models_with_mixed_prompts'].append(model_name)
                print(f"    âœ… æ­£ç¡®åˆ†ç¦»äº† {len(prompt_types)} ç§promptç±»å‹")
            elif len(prompt_types) == 1:
                print(f"    âš ï¸  åªæœ‰å•ä¸€promptç±»å‹: {prompt_types[0]}")
            else:
                print(f"    âŒ æ²¡æœ‰promptæ•°æ®")
            
            # æ£€æŸ¥æ¯ç§promptç±»å‹çš„æ•°æ®å®Œæ•´æ€§
            for prompt_type, prompt_data in by_prompt.items():
                by_rate = prompt_data.get('by_tool_success_rate', {})
                rates = list(by_rate.keys())
                
                if len(rates) == 0:
                    analysis['prompt_separation_issues'].append({
                        'model': model_name,
                        'prompt_type': prompt_type,
                        'issue': 'no_rate_data'
                    })
                    print(f"      âŒ {prompt_type}: æ— tool_success_rateæ•°æ®")
                else:
                    print(f"      âœ… {prompt_type}: {len(rates)} ç§æˆåŠŸç‡")
        
        analysis['prompt_types_found'] = list(analysis['prompt_types_found'])
        print(f"\nå‘ç°çš„Promptç±»å‹: {analysis['prompt_types_found']}")
        
        self.results['prompt_level'] = analysis
        return analysis
    
    def analyze_shard_aggregation(self) -> Dict:
        """åˆ†æåˆ†ç‰‡æ•°æ®èšåˆé€»è¾‘"""
        print("\nğŸ“Š 4. åˆ†ç‰‡æ•°æ®èšåˆé€»è¾‘åˆ†æ")
        print("-" * 40)
        
        analysis = {
            'recent_shard_processes': [],
            'shard_completion_pattern': {},
            'aggregation_issues': []
        }
        
        # æ£€æŸ¥æœ€è¿‘çš„æ—¥å¿—ï¼Œå¯»æ‰¾åˆ†ç‰‡æ‰§è¡Œæ¨¡å¼
        if self.log_dir.exists():
            log_files = list(self.log_dir.glob("batch_test_*.log"))
            log_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            
            recent_logs = log_files[:5]  # æœ€è¿‘5ä¸ªæ—¥å¿—
            
            for log_file in recent_logs:
                try:
                    with open(log_file, 'r') as f:
                        content = f.read()
                    
                    # å¯»æ‰¾åˆ†ç‰‡ç›¸å…³çš„æ—¥å¿—
                    if 'shard' in content.lower() or 'åˆ†ç‰‡' in content:
                        analysis['recent_shard_processes'].append({
                            'file': str(log_file),
                            'size': len(content),
                            'has_shard_info': True
                        })
                        print(f"  ğŸ“„ {log_file.name}: åŒ…å«åˆ†ç‰‡ä¿¡æ¯")
                    else:
                        analysis['recent_shard_processes'].append({
                            'file': str(log_file),
                            'size': len(content),
                            'has_shard_info': False
                        })
                        print(f"  ğŸ“„ {log_file.name}: æ— åˆ†ç‰‡ä¿¡æ¯")
                
                except Exception as e:
                    print(f"  âŒ æ— æ³•è¯»å– {log_file.name}: {e}")
        
        self.results['shard_aggregation'] = analysis
        return analysis
    
    def analyze_cross_process_merging(self, db: Dict) -> Dict:
        """åˆ†æè·¨è¿›ç¨‹æ•°æ®åˆå¹¶"""
        print("\nğŸ“Š 5. è·¨è¿›ç¨‹æ•°æ®åˆå¹¶åˆ†æ")  
        print("-" * 40)
        
        analysis = {
            'concurrent_write_evidence': [],
            'file_lock_usage': False,
            'data_consistency_issues': [],
            'timestamp_analysis': {}
        }
        
        # æ£€æŸ¥æ•°æ®æ—¶é—´æˆ³åˆ†å¸ƒï¼Œåˆ¤æ–­æ˜¯å¦æœ‰å¹¶å‘å†™å…¥
        models = db.get('models', {})
        
        all_timestamps = []
        for model_name, model_data in models.items():
            if isinstance(model_data, dict):
                last_test = model_data.get('last_test_time', '')
                if last_test:
                    all_timestamps.append(last_test)
        
        if all_timestamps:
            all_timestamps.sort()
            analysis['timestamp_analysis'] = {
                'earliest': all_timestamps[0],
                'latest': all_timestamps[-1],
                'total_entries': len(all_timestamps)
            }
            
            print(f"  æ—¶é—´æˆ³åˆ†æ:")
            print(f"    æœ€æ—©: {all_timestamps[0]}")
            print(f"    æœ€æ™š: {all_timestamps[-1]}")
            print(f"    æ€»æ¡ç›®: {len(all_timestamps)}")
        
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨é”æ–‡ä»¶ï¼ˆè¯´æ˜ä½¿ç”¨äº†æ–‡ä»¶é”ä¿æŠ¤ï¼‰
        lock_files = list(Path(".").glob("*.lock"))
        if lock_files:
            analysis['file_lock_usage'] = True
            print(f"  âœ… å‘ç°é”æ–‡ä»¶: {[str(f) for f in lock_files]}")
        else:
            print(f"  âš ï¸  æœªå‘ç°é”æ–‡ä»¶ï¼Œå¯èƒ½å­˜åœ¨å¹¶å‘å†™å…¥é£é™©")
        
        # æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§ï¼ˆæ˜¯å¦æœ‰é‡å¤æˆ–å†²çªçš„æ¡ç›®ï¼‰
        # é€šè¿‡æ£€æŸ¥test_groupsçš„æ—¶é—´æˆ³æ¥è¯†åˆ«å¯èƒ½çš„é‡å¤å†™å…¥
        test_groups = db.get('test_groups', {})
        timestamp_groups = defaultdict(list)
        
        for group_id, group_data in test_groups.items():
            timestamp = group_data.get('timestamp', '')
            if timestamp:
                timestamp_groups[timestamp].append(group_id)
        
        # å¦‚æœåŒä¸€æ—¶é—´æˆ³æœ‰å¤šä¸ªæµ‹è¯•ç»„ï¼Œå¯èƒ½è¡¨ç¤ºå¹¶å‘å†™å…¥
        for timestamp, groups in timestamp_groups.items():
            if len(groups) > 1:
                analysis['concurrent_write_evidence'].append({
                    'timestamp': timestamp,
                    'group_count': len(groups),
                    'groups': groups[:3]  # åªæ˜¾ç¤ºå‰3ä¸ª
                })
                print(f"  âš ï¸  {timestamp}: {len(groups)} ä¸ªæµ‹è¯•ç»„ï¼ˆå¯èƒ½çš„å¹¶å‘å†™å…¥ï¼‰")
        
        self.results['cross_process'] = analysis
        return analysis
    
    def check_data_integrity(self, db: Dict) -> Dict:
        """æ£€æŸ¥æ•´ä½“æ•°æ®å®Œæ•´æ€§"""
        print("\nğŸ“Š 6. æ•°æ®å®Œæ•´æ€§æ£€æŸ¥")
        print("-" * 40)
        
        analysis = {
            'total_expected_tests': 0,
            'total_actual_tests': 0,
            'missing_test_ratio': 0.0,
            'integrity_score': 0.0,
            'critical_issues': []
        }
        
        # è®¡ç®—æœŸæœ›çš„æµ‹è¯•æ•°é‡
        # 8ä¸ªæ¨¡å‹ Ã— 5ä¸ªä»»åŠ¡ç±»å‹ Ã— 2ä¸ªå®ä¾‹ = 80ä¸ªæµ‹è¯•ï¼ˆ5.1åŸºå‡†æµ‹è¯•ï¼‰
        expected_models = 8
        expected_task_types = 5
        expected_instances = 2
        analysis['total_expected_tests'] = expected_models * expected_task_types * expected_instances
        
        # è®¡ç®—å®é™…æµ‹è¯•æ•°é‡
        models = db.get('models', {})
        actual_tests = 0
        
        for model_name, model_data in models.items():
            if isinstance(model_data, dict):
                overall_stats = model_data.get('overall_stats', {})
                total_tests = overall_stats.get('total_tests', 0)
                actual_tests += total_tests
        
        analysis['total_actual_tests'] = actual_tests
        
        if analysis['total_expected_tests'] > 0:
            analysis['missing_test_ratio'] = 1.0 - (actual_tests / analysis['total_expected_tests'])
            analysis['integrity_score'] = max(0, actual_tests / analysis['total_expected_tests'])
        
        print(f"  æœŸæœ›æµ‹è¯•æ€»æ•°: {analysis['total_expected_tests']}")
        print(f"  å®é™…æµ‹è¯•æ€»æ•°: {analysis['total_actual_tests']}")
        print(f"  æ•°æ®å®Œæ•´æ€§è¯„åˆ†: {analysis['integrity_score']:.2%}")
        
        # è¯†åˆ«å…³é”®é—®é¢˜
        if analysis['integrity_score'] < 0.5:
            analysis['critical_issues'].append("æ•°æ®ä¸¥é‡ç¼ºå¤±ï¼ˆ<50%ï¼‰")
        if analysis['missing_test_ratio'] > 0.3:
            analysis['critical_issues'].append("æµ‹è¯•ç¼ºå¤±æ¯”ä¾‹è¿‡é«˜ï¼ˆ>30%ï¼‰")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•æ¨¡å‹çš„æµ‹è¯•æ•°é‡å¼‚å¸¸
        for model_name, model_data in models.items():
            if isinstance(model_data, dict):
                overall_stats = model_data.get('overall_stats', {})
                total_tests = overall_stats.get('total_tests', 0)
                
                # å•ä¸ªæ¨¡å‹æœŸæœ›10ä¸ªæµ‹è¯•
                expected_per_model = 10
                if total_tests > expected_per_model * 2:  # è¶…è¿‡æœŸæœ›2å€
                    analysis['critical_issues'].append(f"{model_name}: æµ‹è¯•æ•°é‡å¼‚å¸¸ï¼ˆ{total_tests}ï¼ŒæœŸæœ›{expected_per_model}ï¼‰")
                elif total_tests == 0:
                    analysis['critical_issues'].append(f"{model_name}: å®Œå…¨æ— æµ‹è¯•æ•°æ®")
        
        if analysis['critical_issues']:
            print(f"  âŒ å‘ç°å…³é”®é—®é¢˜:")
            for issue in analysis['critical_issues']:
                print(f"    - {issue}")
        else:
            print(f"  âœ… æ•°æ®å®Œæ•´æ€§è‰¯å¥½")
        
        self.results['data_integrity'] = analysis
        return analysis
    
    def generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆä¿®å¤å»ºè®®"""
        recommendations = []
        
        # åŸºäºåˆ†æç»“æœç”Ÿæˆå»ºè®®
        if 'model_level' in self.results:
            model_analysis = self.results['model_level']
            if model_analysis['data_mixing_issues']:
                recommendations.append("ğŸ”§ ä¿®å¤æ¨¡å‹çº§æ•°æ®æ··åˆé—®é¢˜ - æ£€æŸ¥normalize_model_nameå‡½æ•°")
            if model_analysis['models_without_data']:
                recommendations.append("ğŸ”§ è°ƒæŸ¥æ¨¡å‹ç¼ºå¤±æ•°æ®é—®é¢˜ - æ£€æŸ¥APIé…ç½®å’Œæ‰§è¡Œæ—¥å¿—")
        
        if 'api_level' in self.results:
            api_analysis = self.results['api_level']
            no_data_models = []
            for model, info in api_analysis['azure_instances_merged'].items():
                if info['status'] == 'no_data':
                    no_data_models.append(model)
            if no_data_models:
                recommendations.append(f"ğŸ”§ ä¿®å¤Azureæ¨¡å‹æ•°æ®åˆå¹¶ - æ£€æŸ¥å®ä¾‹: {no_data_models}")
        
        if 'cross_process' in self.results:
            cross_analysis = self.results['cross_process']
            if not cross_analysis['file_lock_usage']:
                recommendations.append("ğŸ”§ å¯ç”¨æ–‡ä»¶é”ä¿æŠ¤ - é˜²æ­¢å¹¶å‘å†™å…¥å†²çª")
            if cross_analysis['concurrent_write_evidence']:
                recommendations.append("ğŸ”§ æ£€æŸ¥å¹¶å‘å†™å…¥å†²çª - å¯èƒ½å­˜åœ¨æ•°æ®ç«äº‰")
        
        if 'data_integrity' in self.results:
            integrity_analysis = self.results['data_integrity']
            if integrity_analysis['integrity_score'] < 0.8:
                recommendations.append("ğŸ”§ æé«˜æ•°æ®å®Œæ•´æ€§ - å½“å‰å®Œæ•´æ€§ä½äº80%")
        
        return recommendations
    
    def run_comprehensive_diagnosis(self) -> Dict:
        """è¿è¡Œå®Œæ•´è¯Šæ–­"""
        print(f"å¼€å§‹æ—¶é—´: {datetime.now()}")
        
        # åŠ è½½æ•°æ®åº“
        db = self.load_database()
        
        if not db:
            return {'error': 'Cannot load database'}
        
        # æ‰§è¡Œå„é¡¹åˆ†æ
        self.analyze_model_level_concurrency(db)
        self.analyze_api_level_concurrency(db)
        self.analyze_prompt_level_concurrency(db)
        self.analyze_shard_aggregation()
        self.analyze_cross_process_merging(db)
        self.check_data_integrity(db)
        
        # ç”Ÿæˆå»ºè®®
        recommendations = self.generate_recommendations()
        
        # æ±‡æ€»æŠ¥å‘Š
        print("\n" + "=" * 60)
        print("ğŸ“‹ Ultraå¹¶å‘æ•°æ®åˆå¹¶è¯Šæ–­æŠ¥å‘Š")
        print("=" * 60)
        
        # å…³é”®æŒ‡æ ‡
        if 'data_integrity' in self.results:
            integrity_score = self.results['data_integrity']['integrity_score']
            print(f"ğŸ¯ æ•°æ®å®Œæ•´æ€§è¯„åˆ†: {integrity_score:.1%}")
        
        if 'model_level' in self.results:
            models_with_data = self.results['model_level']['models_with_data']
            total_models = self.results['model_level']['total_models']
            print(f"ğŸ“Š æœ‰æ•°æ®æ¨¡å‹: {models_with_data}/{total_models}")
        
        # å»ºè®®
        if recommendations:
            print(f"\nğŸ’¡ ä¿®å¤å»ºè®®:")
            for i, rec in enumerate(recommendations, 1):
                print(f"   {i}. {rec}")
        else:
            print(f"\nâœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼Œæœªå‘ç°æ˜æ˜¾é—®é¢˜")
        
        print(f"\nå®Œæˆæ—¶é—´: {datetime.now()}")
        
        return {
            'timestamp': datetime.now().isoformat(),
            'analysis_results': self.results,
            'recommendations': recommendations
        }

def main():
    """ä¸»å‡½æ•°"""
    diagnostic = UltraConcurrentMergingDiagnostic()
    results = diagnostic.run_comprehensive_diagnosis()
    
    # ä¿å­˜è¯Šæ–­ç»“æœ
    output_file = Path(f"diagnose_ultra_concurrent_merging_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ è¯Šæ–­ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

if __name__ == "__main__":
    main()