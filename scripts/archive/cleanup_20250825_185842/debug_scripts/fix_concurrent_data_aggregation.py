#!/usr/bin/env python3
"""
ä¿®å¤Ultraå¹¶å‘æ¨¡å¼ä¸‹çš„æ•°æ®èšåˆé—®é¢˜
==============================

ä¸»è¦é—®é¢˜ï¼š
1. overall_statsæ²¡æœ‰è¢«æ­£ç¡®èšåˆ
2. å­—æ®µåä¸ä¸€è‡´ï¼ˆsuccess vs successfulï¼‰  
3. summaryç»Ÿè®¡æ²¡æœ‰æ›´æ–°
4. ç¼ºä¹ä»åº•å±‚æ•°æ®é‡æ–°èšåˆçš„æœºåˆ¶
"""

import json
import os
from pathlib import Path
from datetime import datetime
from typing import Dict, Any
from collections import defaultdict

os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'

class ConcurrentDataAggregationFixer:
    """å¹¶å‘æ•°æ®èšåˆä¿®å¤å™¨"""
    
    def __init__(self):
        self.db_path = Path("pilot_bench_cumulative_results/master_database.json")
        self.backup_path = Path(f"pilot_bench_cumulative_results/master_database_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
        
    def load_database(self) -> Dict[str, Any]:
        """åŠ è½½æ•°æ®åº“"""
        if not self.db_path.exists():
            print("âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨")
            return {}
        
        with open(self.db_path, 'r') as f:
            return json.load(f)
    
    def backup_database(self, db: Dict[str, Any]):
        """å¤‡ä»½æ•°æ®åº“"""
        with open(self.backup_path, 'w') as f:
            json.dump(db, f, indent=2, ensure_ascii=False)
        print(f"âœ… æ•°æ®åº“å·²å¤‡ä»½åˆ°: {self.backup_path}")
    
    def fix_field_naming_consistency(self, db: Dict[str, Any]) -> int:
        """ä¿®å¤å­—æ®µå‘½åä¸ä¸€è‡´é—®é¢˜"""
        print("ğŸ”§ ä¿®å¤å­—æ®µå‘½åä¸ä¸€è‡´...")
        fixed_count = 0
        
        models = db.get('models', {})
        for model_name, model_data in models.items():
            if not isinstance(model_data, dict):
                continue
                
            # éå†æ‰€æœ‰å±‚çº§çš„æ•°æ®
            by_prompt = model_data.get('by_prompt_type', {})
            for prompt_type, prompt_data in by_prompt.items():
                by_rate = prompt_data.get('by_tool_success_rate', {})
                for rate, rate_data in by_rate.items():
                    by_diff = rate_data.get('by_difficulty', {})
                    for diff, diff_data in by_diff.items():
                        by_task = diff_data.get('by_task_type', {})
                        for task_type, task_data in by_task.items():
                            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†successè€Œä¸æ˜¯successful
                            if 'success' in task_data and 'successful' not in task_data:
                                task_data['successful'] = task_data['success']
                                print(f"  ä¿®å¤ {model_name}/{prompt_type}/{rate}/{diff}/{task_type}: success -> successful")
                                fixed_count += 1
                            
                            # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº†failureè€Œä¸æ˜¯failed
                            if 'failure' in task_data and 'failed' not in task_data:
                                task_data['failed'] = task_data['failure']
                                print(f"  ä¿®å¤ {model_name}/{prompt_type}/{rate}/{diff}/{task_type}: failure -> failed")
                                fixed_count += 1
        
        print(f"âœ… ä¿®å¤äº† {fixed_count} ä¸ªå­—æ®µå‘½åé—®é¢˜")
        return fixed_count
    
    def rebuild_overall_stats(self, db: Dict[str, Any]) -> int:
        """é‡å»ºoverall_statsèšåˆç»Ÿè®¡"""
        print("ğŸ”§ é‡å»ºoverall_statsèšåˆç»Ÿè®¡...")
        rebuilt_count = 0
        
        models = db.get('models', {})
        for model_name, model_data in models.items():
            if not isinstance(model_data, dict):
                continue
            
            print(f"  é‡å»º {model_name} çš„overall_stats...")
            
            # èšåˆæ‰€æœ‰ç»Ÿè®¡
            total_tests = 0
            total_success = 0
            total_partial = 0
            total_failed = 0
            total_execution_time = 0.0
            total_turns = 0
            total_tool_calls = 0
            test_count_for_avg = 0
            
            # èšåˆæ‰€æœ‰åˆ†æ•°
            total_workflow_score = 0.0
            total_phase2_score = 0.0 
            total_quality_score = 0.0
            total_final_score = 0.0
            score_count = 0
            
            by_prompt = model_data.get('by_prompt_type', {})
            for prompt_type, prompt_data in by_prompt.items():
                by_rate = prompt_data.get('by_tool_success_rate', {})
                for rate, rate_data in by_rate.items():
                    by_diff = rate_data.get('by_difficulty', {})
                    for diff, diff_data in by_diff.items():
                        by_task = diff_data.get('by_task_type', {})
                        for task_type, task_data in by_task.items():
                            # ç´¯åŠ åŸºç¡€ç»Ÿè®¡
                            total_tests += task_data.get('total', 0)
                            total_success += task_data.get('successful', 0)
                            total_partial += task_data.get('partial', 0)
                            total_failed += task_data.get('failed', 0)
                            
                            # ç´¯åŠ æ—¶é—´å’Œè½®æ¬¡ï¼ˆå¦‚æœæœ‰ï¼‰
                            if 'avg_execution_time' in task_data and task_data['avg_execution_time'] > 0:
                                total_execution_time += task_data['avg_execution_time'] * task_data.get('total', 0)
                                test_count_for_avg += task_data.get('total', 0)
                            
                            if 'avg_turns' in task_data and task_data['avg_turns'] > 0:
                                total_turns += task_data['avg_turns'] * task_data.get('total', 0)
                            
                            # ç´¯åŠ åˆ†æ•°ï¼ˆå¦‚æœæœ‰ï¼‰
                            for score_field in ['avg_workflow_score', 'avg_phase2_score', 'avg_quality_score', 'avg_final_score']:
                                if score_field in task_data and task_data[score_field] > 0:
                                    score_value = task_data[score_field] * task_data.get('total', 0)
                                    if score_field == 'avg_workflow_score':
                                        total_workflow_score += score_value
                                    elif score_field == 'avg_phase2_score':
                                        total_phase2_score += score_value
                                    elif score_field == 'avg_quality_score':
                                        total_quality_score += score_value
                                    elif score_field == 'avg_final_score':
                                        total_final_score += score_value
                                    score_count = max(score_count, task_data.get('total', 0))
            
            # è®¡ç®—èšåˆæŒ‡æ ‡
            success_rate = total_success / total_tests if total_tests > 0 else 0.0
            partial_rate = total_partial / total_tests if total_tests > 0 else 0.0
            failure_rate = total_failed / total_tests if total_tests > 0 else 0.0
            
            avg_execution_time = total_execution_time / test_count_for_avg if test_count_for_avg > 0 else 0.0
            avg_turns = total_turns / test_count_for_avg if test_count_for_avg > 0 else 0.0
            
            # è®¡ç®—å¹³å‡åˆ†æ•°
            avg_workflow_score = total_workflow_score / score_count if score_count > 0 else 0.0
            avg_phase2_score = total_phase2_score / score_count if score_count > 0 else 0.0
            avg_quality_score = total_quality_score / score_count if score_count > 0 else 0.0
            avg_final_score = total_final_score / score_count if score_count > 0 else 0.0
            
            # æ›´æ–°overall_stats
            model_data['total_tests'] = total_tests
            model_data['overall_stats'] = {
                'total_tests': total_tests,
                'total_success': total_success,
                'total_partial': total_partial,
                'total_failed': total_failed,
                'success_rate': success_rate,
                'partial_rate': partial_rate,
                'failure_rate': failure_rate,
                'avg_execution_time': avg_execution_time,
                'avg_turns': avg_turns,
                'avg_workflow_score': avg_workflow_score,
                'avg_phase2_score': avg_phase2_score,
                'avg_quality_score': avg_quality_score,
                'avg_final_score': avg_final_score
            }
            
            print(f"    æ€»æµ‹è¯•: {total_tests}, æˆåŠŸ: {total_success}, æˆåŠŸç‡: {success_rate:.1%}")
            rebuilt_count += 1
        
        print(f"âœ… é‡å»ºäº† {rebuilt_count} ä¸ªæ¨¡å‹çš„overall_stats")
        return rebuilt_count
    
    def rebuild_summary_stats(self, db: Dict[str, Any]) -> bool:
        """é‡å»ºsummaryå…¨å±€ç»Ÿè®¡"""
        print("ğŸ”§ é‡å»ºsummaryå…¨å±€ç»Ÿè®¡...")
        
        total_tests = 0
        total_success = 0
        total_partial = 0
        total_failure = 0
        models_tested = []
        last_test_time = None
        
        models = db.get('models', {})
        for model_name, model_data in models.items():
            if not isinstance(model_data, dict):
                continue
            
            overall_stats = model_data.get('overall_stats', {})
            if overall_stats.get('total_tests', 0) > 0:
                total_tests += overall_stats.get('total_tests', 0)
                total_success += overall_stats.get('total_success', 0)
                total_partial += overall_stats.get('total_partial', 0)
                total_failure += overall_stats.get('total_failed', 0)
                models_tested.append(model_name)
            
            # æ›´æ–°æœ€åæµ‹è¯•æ—¶é—´
            model_last_time = model_data.get('last_test_time', '')
            if model_last_time:
                if not last_test_time or model_last_time > last_test_time:
                    last_test_time = model_last_time
        
        # æ›´æ–°summary
        if 'summary' not in db:
            db['summary'] = {}
        
        db['summary'].update({
            'total_tests': total_tests,
            'total_success': total_success,
            'total_partial': total_partial,
            'total_failure': total_failure,
            'models_tested': models_tested,
            'last_test_time': last_test_time
        })
        
        print(f"âœ… Summaryæ›´æ–°: {total_tests}ä¸ªæµ‹è¯•, {len(models_tested)}ä¸ªæ¨¡å‹")
        return True
    
    def add_missing_aggregation_fields(self, db: Dict[str, Any]) -> int:
        """æ·»åŠ ç¼ºå¤±çš„èšåˆå­—æ®µ"""
        print("ğŸ”§ æ·»åŠ ç¼ºå¤±çš„èšåˆå­—æ®µ...")
        added_count = 0
        
        models = db.get('models', {})
        for model_name, model_data in models.items():
            if not isinstance(model_data, dict):
                continue
            
            by_prompt = model_data.get('by_prompt_type', {})
            for prompt_type, prompt_data in by_prompt.items():
                by_rate = prompt_data.get('by_tool_success_rate', {})
                for rate, rate_data in by_rate.items():
                    by_diff = rate_data.get('by_difficulty', {})
                    for diff, diff_data in by_diff.items():
                        by_task = diff_data.get('by_task_type', {})
                        for task_type, task_data in by_task.items():
                            # ç¡®ä¿æ‰€æœ‰å¿…éœ€çš„å­—æ®µéƒ½å­˜åœ¨
                            required_fields = {
                                'total': 0,
                                'successful': 0,
                                'partial': 0, 
                                'failed': 0,
                                'success_rate': 0.0,
                                'partial_rate': 0.0,
                                'failure_rate': 0.0
                            }
                            
                            for field, default_value in required_fields.items():
                                if field not in task_data:
                                    task_data[field] = default_value
                                    added_count += 1
                            
                            # é‡æ–°è®¡ç®—æ¯”ä¾‹å­—æ®µ
                            total = task_data.get('total', 0)
                            if total > 0:
                                task_data['success_rate'] = task_data.get('successful', 0) / total
                                task_data['partial_rate'] = task_data.get('partial', 0) / total  
                                task_data['failure_rate'] = task_data.get('failed', 0) / total
        
        print(f"âœ… æ·»åŠ äº† {added_count} ä¸ªç¼ºå¤±å­—æ®µ")
        return added_count
    
    def validate_aggregation(self, db: Dict[str, Any]) -> Dict[str, Any]:
        """éªŒè¯èšåˆç»“æœ"""
        print("ğŸ” éªŒè¯èšåˆç»“æœ...")
        
        validation_results = {
            'models_with_data': 0,
            'models_without_data': [],
            'total_tests_sum': 0,
            'field_consistency': True,
            'issues': []
        }
        
        models = db.get('models', {})
        for model_name, model_data in models.items():
            if not isinstance(model_data, dict):
                continue
            
            overall_stats = model_data.get('overall_stats', {})
            total_tests = overall_stats.get('total_tests', 0)
            
            if total_tests > 0:
                validation_results['models_with_data'] += 1
                validation_results['total_tests_sum'] += total_tests
                print(f"  âœ… {model_name}: {total_tests} æµ‹è¯•")
            else:
                validation_results['models_without_data'].append(model_name)
                print(f"  âŒ {model_name}: æ— æ•°æ®")
        
        # æ£€æŸ¥summaryä¸€è‡´æ€§
        summary = db.get('summary', {})
        summary_total = summary.get('total_tests', 0)
        
        if summary_total != validation_results['total_tests_sum']:
            validation_results['field_consistency'] = False
            validation_results['issues'].append(
                f"Summaryæ€»æ•°ä¸åŒ¹é…: summary={summary_total}, å®é™…={validation_results['total_tests_sum']}"
            )
        
        print(f"âœ… éªŒè¯å®Œæˆ: {validation_results['models_with_data']} ä¸ªæ¨¡å‹æœ‰æ•°æ®")
        return validation_results
    
    def save_database(self, db: Dict[str, Any]):
        """ä¿å­˜æ•°æ®åº“"""
        with open(self.db_path, 'w') as f:
            json.dump(db, f, indent=2, ensure_ascii=False)
        print(f"âœ… æ•°æ®åº“å·²ä¿å­˜")
    
    def run_comprehensive_fix(self) -> Dict[str, Any]:
        """è¿è¡Œå®Œæ•´ä¿®å¤æµç¨‹"""
        print("ğŸš€ å¯åŠ¨å¹¶å‘æ•°æ®èšåˆä¿®å¤")
        print("=" * 50)
        
        # åŠ è½½æ•°æ®åº“
        db = self.load_database()
        if not db:
            return {'error': 'Cannot load database'}
        
        # å¤‡ä»½æ•°æ®åº“
        self.backup_database(db)
        
        # æ‰§è¡Œä¿®å¤
        results = {}
        
        # 1. ä¿®å¤å­—æ®µå‘½åä¸ä¸€è‡´
        results['field_naming_fixes'] = self.fix_field_naming_consistency(db)
        
        # 2. æ·»åŠ ç¼ºå¤±çš„èšåˆå­—æ®µ
        results['added_fields'] = self.add_missing_aggregation_fields(db)
        
        # 3. é‡å»ºoverall_stats
        results['rebuilt_overall_stats'] = self.rebuild_overall_stats(db)
        
        # 4. é‡å»ºsummaryç»Ÿè®¡
        results['rebuilt_summary'] = self.rebuild_summary_stats(db)
        
        # 5. éªŒè¯ç»“æœ
        results['validation'] = self.validate_aggregation(db)
        
        # ä¿å­˜ä¿®å¤åçš„æ•°æ®åº“
        self.save_database(db)
        
        print("\n" + "=" * 50)
        print("ğŸ“‹ ä¿®å¤å®Œæˆæ€»ç»“")
        print("=" * 50)
        print(f"å­—æ®µå‘½åä¿®å¤: {results['field_naming_fixes']} ä¸ª")
        print(f"æ·»åŠ ç¼ºå¤±å­—æ®µ: {results['added_fields']} ä¸ª")
        print(f"é‡å»ºoverall_stats: {results['rebuilt_overall_stats']} ä¸ªæ¨¡å‹")
        print(f"æœ‰æ•°æ®æ¨¡å‹: {results['validation']['models_with_data']} ä¸ª")
        print(f"æ€»æµ‹è¯•æ•°: {results['validation']['total_tests_sum']} ä¸ª")
        
        if results['validation']['issues']:
            print(f"âŒ å‘ç°é—®é¢˜:")
            for issue in results['validation']['issues']:
                print(f"  - {issue}")
        else:
            print(f"âœ… æ‰€æœ‰éªŒè¯é€šè¿‡")
        
        return results

def main():
    """ä¸»å‡½æ•°"""
    fixer = ConcurrentDataAggregationFixer()
    results = fixer.run_comprehensive_fix()
    
    # ä¿å­˜ä¿®å¤ç»“æœ
    output_file = Path(f"fix_concurrent_aggregation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ ä¿®å¤ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

if __name__ == "__main__":
    main()