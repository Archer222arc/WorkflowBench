# æ–‡ä»¶ï¼štool_and_task_generator.py
# ä½ç½®ï¼šç¬¬1-25è¡Œ
#!/usr/bin/env python3
"""
Tool and Task Generator for MCP Protocol
Generates diverse tools and multi-tool tasks for training
"""
from __future__ import annotations  # åœ¨æ–‡ä»¶é¡¶éƒ¨æ·»åŠ è¿™ä¸€è¡Œ
import json
import random
import uuid
from pathlib import Path
from typing import Dict, List, Optional, Set, Any, Tuple
from dataclasses import dataclass, field
from datetime import datetime
import xml.etree.ElementTree as ET
from collections import Counter  # æ·»åŠ Counterå¯¼å…¥
import logging  # æ·»åŠ loggingå¯¼å…¥
import networkx as nx  # æ·»åŠ networkxå¯¼å…¥
import concurrent.futures
from typing import Callable, Tuple
import time
import os
from tqdm import tqdm
from openai import OpenAI
# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


# æ–‡ä»¶ï¼štool_and_task_generator.py
# ä½ç½®ï¼šåœ¨ç°æœ‰wrapper functionsä¹‹åæ·»åŠ 
# æ³¨æ„ï¼šæ–°å¢å®Œæ•´çš„å¹¶è¡Œç”Ÿæˆæ¥å£

import concurrent.futures
from typing import Callable, Tuple
import time
from tqdm import tqdm

# ===========================
# Parallel Generation Interface
# ===========================

class ParallelGenerator:
    """å¹¶è¡Œç”Ÿæˆå™¨ï¼Œæ”¯æŒå¹¶è¡Œåˆ›å»ºå·¥å…·å’Œä»»åŠ¡"""
    
    def __init__(self, max_workers: int = None):
        """
        åˆå§‹åŒ–å¹¶è¡Œç”Ÿæˆå™¨
        
        Args:
            max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°ï¼ŒNoneè¡¨ç¤ºä½¿ç”¨CPUæ ¸å¿ƒæ•°
        """
        self.max_workers = max_workers or os.cpu_count()
        print(f"[ParallelGenerator] Initialized with {self.max_workers} workers")
    
    def parallel_generate(self,
                         num_tools: int = 50,
                         num_tasks: int = 200,
                         task_distribution: Dict[str, float] = None,
                         output_dir: str = "mcp_generated_library",
                         use_processes: bool = False,
                         show_progress: bool = True) -> Dict[str, Any]:
        """
        å¹¶è¡Œç”Ÿæˆå·¥å…·å’Œä»»åŠ¡
        
        Args:
            num_tools: ç”Ÿæˆå·¥å…·æ•°é‡
            num_tasks: ç”Ÿæˆä»»åŠ¡æ•°é‡
            task_distribution: ä»»åŠ¡ç±»å‹åˆ†å¸ƒ
            output_dir: è¾“å‡ºç›®å½•
            use_processes: æ˜¯å¦ä½¿ç”¨è¿›ç¨‹æ± ï¼ˆé»˜è®¤ä½¿ç”¨çº¿ç¨‹æ± ï¼‰
            show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
            
        Returns:
            åŒ…å«å·¥å…·å’Œä»»åŠ¡çš„å­—å…¸
        """
        start_time = time.time()
        results = {
            'tools': [],
            'tasks': [],
            'timings': {},
            'errors': []
        }
        
        # é€‰æ‹©æ‰§è¡Œå™¨
        ExecutorClass = concurrent.futures.ProcessPoolExecutor if use_processes else concurrent.futures.ThreadPoolExecutor
        
        with ExecutorClass(max_workers=self.max_workers) as executor:
            futures = []
            
            # æäº¤å·¥å…·ç”Ÿæˆä»»åŠ¡
            tool_future = executor.submit(
                self._generate_tools_batch,
                num_tools,
                output_dir
            )
            futures.append(('tools', tool_future))
            
            # æäº¤ä»»åŠ¡ç”Ÿæˆä»»åŠ¡ï¼ˆåˆ†æ‰¹å¹¶è¡Œï¼‰
            task_batches = self._split_task_generation(num_tasks, task_distribution)
            for i, (batch_size, batch_distribution) in enumerate(task_batches):
                task_future = executor.submit(
                    self._generate_tasks_batch,
                    batch_size,
                    batch_distribution,
                    output_dir,
                    batch_id=i
                )
                futures.append((f'tasks_batch_{i}', task_future))
            
            # æ”¶é›†ç»“æœ
            if show_progress:
                with tqdm(total=len(futures), desc="Parallel generation") as pbar:
                    for name, future in futures:
                        try:
                            result = future.result(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
                            if name == 'tools':
                                results['tools'] = result['tools']
                                results['timings']['tools'] = result['timing']
                            elif name.startswith('tasks_batch_'):
                                results['tasks'].extend(result['tasks'])
                                results['timings'][name] = result['timing']
                            pbar.update(1)
                        except Exception as e:
                            error_msg = f"Error in {name}: {str(e)}"
                            print(f"\n[ParallelGenerator] {error_msg}")
                            results['errors'].append(error_msg)
                            pbar.update(1)
            else:
                # æ— è¿›åº¦æ¡æ¨¡å¼
                for name, future in futures:
                    try:
                        result = future.result(timeout=300)
                        if name == 'tools':
                            results['tools'] = result['tools']
                            results['timings']['tools'] = result['timing']
                        elif name.startswith('tasks_batch_'):
                            results['tasks'].extend(result['tasks'])
                            results['timings'][name] = result['timing']
                    except Exception as e:
                        error_msg = f"Error in {name}: {str(e)}"
                        print(f"[ParallelGenerator] {error_msg}")
                        results['errors'].append(error_msg)
        
        # ä¿å­˜ç»“æœ
        self._save_results(results, output_dir)
        
        total_time = time.time() - start_time
        results['timings']['total'] = total_time
        
        print(f"\n[ParallelGenerator] Completed in {total_time:.2f} seconds")
        print(f"[ParallelGenerator] Generated {len(results['tools'])} tools and {len(results['tasks'])} tasks")
        if results['errors']:
            print(f"[ParallelGenerator] Encountered {len(results['errors'])} errors")
        
        return results
    
    def _generate_tools_batch(self, num_tools: int, output_dir: str) -> Dict[str, Any]:
        """ç”Ÿæˆå·¥å…·æ‰¹æ¬¡ï¼ˆåœ¨å·¥ä½œçº¿ç¨‹ä¸­æ‰§è¡Œï¼‰"""
        start_time = time.time()
        
        # ä½¿ç”¨ç°æœ‰çš„å·¥å…·ç”Ÿæˆå‡½æ•°
        tools = create_diverse_tool_library(num_tools=num_tools)
        
        return {
            'tools': tools,
            'timing': time.time() - start_time
        }
    
    def _generate_tasks_batch(self, 
                            num_tasks: int, 
                            task_distribution: Dict[str, float],
                            output_dir: str,
                            batch_id: int = 0) -> Dict[str, Any]:
        """ç”Ÿæˆä»»åŠ¡æ‰¹æ¬¡ï¼ˆåœ¨å·¥ä½œçº¿ç¨‹ä¸­æ‰§è¡Œï¼‰"""
        start_time = time.time()
        
        # é¦–å…ˆéœ€è¦åŠ è½½æˆ–ç”Ÿæˆå·¥å…·
        tool_registry_path = Path(output_dir) / "tool_registry.json"
        
        # ç­‰å¾…å·¥å…·æ–‡ä»¶ç”Ÿæˆï¼ˆæœ€å¤šç­‰å¾…60ç§’ï¼‰
        wait_time = 0
        while not tool_registry_path.exists() and wait_time < 60:
            time.sleep(1)
            wait_time += 1
        
        if not tool_registry_path.exists():
            # å¦‚æœå·¥å…·æ–‡ä»¶è¿˜ä¸å­˜åœ¨ï¼Œç”Ÿæˆä¸€ä¸ªä¸´æ—¶çš„
            temp_tools = create_diverse_tool_library(num_tools=30)
            tool_library = {tool.name: tool for tool in temp_tools}
        else:
            # åŠ è½½å·¥å…·
            with open(tool_registry_path, 'r') as f:
                tool_registry = json.load(f)
            
            # è½¬æ¢ä¸ºMCPToolå¯¹è±¡
            from tool_and_task_generator import MCPTool, ToolParameter, ToolReturn, ToolError
            tool_library = {}
            for tool_name, tool_data in tool_registry.items():
                # è½¬æ¢å‚æ•°
                parameters = []
                for param in tool_data.get('parameters', []):
                    parameters.append(ToolParameter(
                        name=param.get('name', ''),
                        type=param.get('type', 'string'),
                        description=param.get('description', ''),
                        required=param.get('required', True),
                        default=param.get('default', None),
                        constraints=param.get('constraints', {})
                    ))
                
                # è½¬æ¢è¿”å›å€¼
                returns = []
                for ret in tool_data.get('returns', []):
                    returns.append(ToolReturn(
                        name=ret.get('name', ''),
                        type=ret.get('type', 'string'),
                        description=ret.get('description', '')
                    ))
                
                # è½¬æ¢é”™è¯¯
                errors = []
                for err in tool_data.get('errors', []):
                    errors.append(ToolError(
                        code=err.get('code', ''),
                        description=err.get('description', '')
                    ))
                
                # åˆ›å»ºå·¥å…·å¯¹è±¡
                tool = MCPTool(
                    name=tool_name,
                    description=tool_data.get('description', ''),
                    parameters=parameters,
                    returns=returns,
                    errors=errors,
                    dependencies=tool_data.get('dependencies', []),
                    metadata=tool_data.get('metadata', {})
                )
                tool_library[tool.name] = tool
        
        # åŠ è½½å¢å¼ºçš„å·¥å…·æ³¨å†Œè¡¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        tool_registry_consolidated_path = Path(output_dir) / "tool_registry_consolidated.json"
        tool_registry_data = {}
        if tool_registry_consolidated_path.exists():
            with open(tool_registry_consolidated_path, 'r') as f:
                tool_registry_data = json.load(f)
        
        # åˆ›å»ºä»»åŠ¡ç”Ÿæˆå™¨
        task_generator = TaskGenerator(tool_library, tool_registry_data)
        
        # å¦‚æœéœ€è¦å¯ç”¨LLMç”Ÿæˆ
        if hasattr(task_generator, 'enable_llm_generation'):
            task_generator.enable_llm_generation(True)
        
        # ç”Ÿæˆä»»åŠ¡
        tasks = []
        if task_distribution:
            # æŒ‰åˆ†å¸ƒç”Ÿæˆä»»åŠ¡
            for task_type, proportion in task_distribution.items():
                type_num_tasks = int(num_tasks * proportion)
                type_templates = [t for t in task_generator.task_templates if t.task_type == task_type]
                
                for _ in range(type_num_tasks):
                    if type_templates:
                        template = random.choice(type_templates)
                        task = task_generator.generate_task_instance(template)
                        task.task_type = task_type  # ç¡®ä¿ç±»å‹æ­£ç¡®
                        task.metadata['batch_id'] = batch_id
                        tasks.append(task)
        else:
            # ä½¿ç”¨é»˜è®¤æ‰¹é‡ç”Ÿæˆ
            batch_tasks = task_generator._generate_task_batch(num_tasks)
            for task in batch_tasks:
                task.metadata['batch_id'] = batch_id
            tasks.extend(batch_tasks)
        
        return {
            'tasks': tasks,
            'timing': time.time() - start_time
        }
    
    def _split_task_generation(self, 
                             total_tasks: int, 
                             task_distribution: Dict[str, float] = None) -> List[Tuple[int, Dict[str, float]]]:
        """å°†ä»»åŠ¡ç”Ÿæˆåˆ†å‰²æˆå¤šä¸ªæ‰¹æ¬¡"""
        # æ¯ä¸ªæ‰¹æ¬¡çš„ä»»åŠ¡æ•°
        batch_size = max(50, total_tasks // self.max_workers)
        batches = []
        
        remaining_tasks = total_tasks
        while remaining_tasks > 0:
            current_batch_size = min(batch_size, remaining_tasks)
            batches.append((current_batch_size, task_distribution))
            remaining_tasks -= current_batch_size
        
        return batches
    
    def _save_results(self, results: Dict[str, Any], output_dir: str):
        """ä¿å­˜ç”Ÿæˆç»“æœ"""
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        # ä¿å­˜å·¥å…·
        if results['tools']:
            tool_registry = {}
            for tool in results['tools']:
                tool_registry[tool.name] = tool.to_mcp_json()
            
            with open(output_path / "tool_registry.json", 'w') as f:
                json.dump(tool_registry, f, indent=2)
        
        # ä¿å­˜ä»»åŠ¡
        if results['tasks']:
            task_data = []
            for task in results['tasks']:
                task_dict = {
                    "instance_id": task.instance_id,
                    "task_type": task.task_type,
                    "description": task.description,
                    "inputs": task.inputs,
                    "expected_outputs": task.expected_outputs,
                    "required_tools": task.required_tools,
                    "constraints": task.constraints,
                    "complexity": task.complexity,
                    "metadata": task.metadata
                }
                task_data.append(task_dict)
            
            task_library_data = {
                "tasks": task_data,
                "metadata": {
                    "generated_at": datetime.now().isoformat(),
                    "num_tasks": len(task_data),
                    "parallel_generation": True,
                    "max_workers": self.max_workers
                }
            }
            
            with open(output_path / "task_library.json", 'w') as f:
                json.dump(task_library_data, f, indent=2)
        
        # ä¿å­˜æ€§èƒ½æŠ¥å‘Š
        performance_report = {
            "timings": results['timings'],
            "errors": results['errors'],
            "summary": {
                "total_tools": len(results['tools']),
                "total_tasks": len(results['tasks']),
                "total_time": results['timings'].get('total', 0),
                "parallel_speedup": self._calculate_speedup(results['timings'])
            }
        }
        
        with open(output_path / "parallel_generation_report.json", 'w') as f:
            json.dump(performance_report, f, indent=2)
    
    def _calculate_speedup(self, timings: Dict[str, float]) -> float:
        """è®¡ç®—å¹¶è¡ŒåŠ é€Ÿæ¯”"""
        if 'total' not in timings:
            return 1.0
        
        # ä¼°ç®—ä¸²è¡Œæ—¶é—´ï¼ˆæ‰€æœ‰æ‰¹æ¬¡æ—¶é—´ä¹‹å’Œï¼‰
        serial_time = sum(t for k, t in timings.items() if k != 'total')
        parallel_time = timings['total']
        
        if parallel_time > 0:
            return serial_time / parallel_time
        return 1.0


# ä¾¿æ·å‡½æ•°
def parallel_generate_tools_and_tasks(
    num_tools: int = 50,
    num_tasks: int = 200,
    task_distribution: Dict[str, float] = None,
    max_workers: int = None,
    use_processes: bool = False,
    show_progress: bool = True,
    output_dir: str = "mcp_generated_library"
) -> Dict[str, Any]:
    """
    å¹¶è¡Œç”Ÿæˆå·¥å…·å’Œä»»åŠ¡çš„ä¾¿æ·æ¥å£
    
    Args:
        num_tools: ç”Ÿæˆå·¥å…·æ•°é‡
        num_tasks: ç”Ÿæˆä»»åŠ¡æ•°é‡  
        task_distribution: ä»»åŠ¡ç±»å‹åˆ†å¸ƒ
        max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°
        use_processes: æ˜¯å¦ä½¿ç”¨è¿›ç¨‹æ± 
        show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
        output_dir: è¾“å‡ºç›®å½•
        
    Returns:
        ç”Ÿæˆç»“æœå­—å…¸
        
    Example:
        # ä½¿ç”¨é»˜è®¤è®¾ç½®
        results = parallel_generate_tools_and_tasks()
        
        # è‡ªå®šä¹‰è®¾ç½®
        results = parallel_generate_tools_and_tasks(
            num_tools=100,
            num_tasks=500,
            task_distribution={
                'basic_task': 0.3,
                'simple_task': 0.3,
                'data_pipeline': 0.2,
                'api_integration': 0.1,
                'multi_stage_pipeline': 0.1
            },
            max_workers=8,
            show_progress=True
        )
    """
    generator = ParallelGenerator(max_workers=max_workers)
    return generator.parallel_generate(
        num_tools=num_tools,
        num_tasks=num_tasks,
        task_distribution=task_distribution,
        output_dir=output_dir,
        use_processes=use_processes,
        show_progress=show_progress
    )


# å¼‚æ­¥ç”Ÿæˆæ¥å£ï¼ˆç”¨äºé›†æˆåˆ°å¼‚æ­¥ç³»ç»Ÿï¼‰
async def async_parallel_generate(
    num_tools: int = 50,
    num_tasks: int = 200,
    task_distribution: Dict[str, float] = None,
    max_workers: int = None
) -> Dict[str, Any]:
    """
    å¼‚æ­¥å¹¶è¡Œç”Ÿæˆæ¥å£
    
    ä½¿ç”¨asyncioçš„run_in_executoræ¥æ‰§è¡Œå¹¶è¡Œç”Ÿæˆ
    """
    import asyncio
    
    loop = asyncio.get_event_loop()
    
    # åœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œ
    result = await loop.run_in_executor(
        None,  # ä½¿ç”¨é»˜è®¤executor
        parallel_generate_tools_and_tasks,
        num_tools,
        num_tasks,
        task_distribution,
        max_workers,
        False,  # use_processes
        False   # show_progress (åœ¨å¼‚æ­¥æ¨¡å¼ä¸‹ç¦ç”¨)
    )
    
    return result

# æ–‡ä»¶ï¼štool_and_task_generator.py
# ä½ç½®ï¼šåœ¨ ParallelGenerator ç±»ä¹‹åæ·»åŠ 
# æ³¨æ„ï¼šæ–°å¢ä¸“é—¨çš„å¹¶è¡Œä»»åŠ¡ç”Ÿæˆå‡½æ•°

def parallel_generate_tasks_from_existing_tools(
    tool_registry_path: str = "mcp_generated_library/tool_registry_consolidated.json",
    num_tasks: int = 500,
    task_distribution: Dict[str, float] = None,
    max_workers: int = None,
    use_llm: bool = True,
    output_dir: str = "mcp_generated_library",
    output_filename: str = None,
    show_progress: bool = True
) -> Dict[str, Any]:
    """
    åŸºäºç°æœ‰å·¥å…·åº“å¹¶è¡Œç”Ÿæˆä»»åŠ¡
    
    Args:
        tool_registry_path: å·¥å…·æ³¨å†Œè¡¨è·¯å¾„ï¼ˆtool_registry_consolidated.jsonï¼‰
        num_tasks: ç”Ÿæˆä»»åŠ¡æ•°é‡
        task_distribution: ä»»åŠ¡ç±»å‹åˆ†å¸ƒ
        max_workers: æœ€å¤§å¹¶è¡Œå·¥ä½œçº¿ç¨‹æ•°
        use_llm: æ˜¯å¦ä½¿ç”¨LLMå¢å¼ºä»»åŠ¡ç”Ÿæˆ
        output_dir: è¾“å‡ºç›®å½•
        output_filename: è¾“å‡ºæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦æ¡
        
    Returns:
        ç”Ÿæˆç»“æœå­—å…¸
    """
    print(f"\n{'='*60}")
    print(f"åŸºäºç°æœ‰å·¥å…·åº“å¹¶è¡Œç”Ÿæˆä»»åŠ¡")
    print(f"{'='*60}")
    print(f"å·¥å…·åº“è·¯å¾„: {tool_registry_path}")
    print(f"ä»»åŠ¡æ•°é‡: {num_tasks}")
    print(f"ä½¿ç”¨LLM: {use_llm}")
    print(f"å·¥ä½œçº¿ç¨‹: {max_workers or os.cpu_count()}")
    
    start_time = time.time()
    
    # æ£€æŸ¥å·¥å…·åº“æ˜¯å¦å­˜åœ¨
    if not Path(tool_registry_path).exists():
        raise FileNotFoundError(f"å·¥å…·åº“æ–‡ä»¶ä¸å­˜åœ¨: {tool_registry_path}")
    
    # åŠ è½½å·¥å…·åº“
    print("\nğŸ“‚ åŠ è½½å·¥å…·åº“...")
    with open(tool_registry_path, 'r') as f:
        tool_registry = json.load(f)
    print(f"âœ… å·²åŠ è½½ {len(tool_registry)} ä¸ªå·¥å…·")
    
    # è½¬æ¢ä¸ºMCPToolå¯¹è±¡
    tools = []
    tool_library = {}
    
    for tool_name, tool_data in tool_registry.items():
        # è½¬æ¢å‚æ•°
        parameters = []
        for param in tool_data.get('parameters', []):
            parameters.append(ToolParameter(
                name=param.get('name', ''),
                type=param.get('type', 'string'),
                description=param.get('description', ''),
                required=param.get('required', True),
                default=param.get('default', None),
                constraints=param.get('constraints', {})
            ))
        
        # è½¬æ¢è¿”å›å€¼
        returns = []
        for ret in tool_data.get('returns', []):
            returns.append(ToolReturn(
                name=ret.get('name', ''),
                type=ret.get('type', 'string'),
                description=ret.get('description', '')
            ))
        
        # è½¬æ¢é”™è¯¯
        errors = []
        for err in tool_data.get('errors', []):
            errors.append(ToolError(
                code=err.get('code', ''),
                description=err.get('description', '')
            ))
        
        # åˆ›å»ºå·¥å…·å¯¹è±¡
        tool = MCPTool(
            name=tool_name,
            description=tool_data.get('description', ''),
            parameters=parameters,
            returns=returns,
            errors=errors,
            dependencies=tool_data.get('dependencies', []),
            metadata=tool_data.get('metadata', {})
        )
        
        # è®¾ç½®ç±»åˆ«
        if 'category' not in tool.metadata and 'metadata' in tool_data:
            tool.metadata['category'] = tool_data['metadata'].get('category', 'general')
        
        tools.append(tool)
        tool_library[tool_name] = tool
    
    # ä½¿ç”¨é»˜è®¤ä»»åŠ¡åˆ†å¸ƒ
    if task_distribution is None:
        task_distribution = {
            'basic_task': 0.20,       # 20%
            'simple_task': 0.25,      # 25%
            'data_pipeline': 0.25,    # 25%
            'api_integration': 0.15,  # 15%
            'multi_stage_pipeline': 0.15  # 15%
        }
    
    # åˆ›å»ºå¹¶è¡Œç”Ÿæˆå™¨
    generator = ParallelTaskGenerator(
        tool_library=tool_library,
        tool_registry=tool_registry,
        max_workers=max_workers
    )
    
    # æ‰§è¡Œå¹¶è¡Œç”Ÿæˆ
    results = generator.generate_tasks_parallel(
        num_tasks=num_tasks,
        task_distribution=task_distribution,
        use_llm=use_llm,
        show_progress=show_progress
    )
    
    # ä¿å­˜ç»“æœ
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # ç¡®å®šè¾“å‡ºæ–‡ä»¶å
    if output_filename is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_filename = f"task_library_parallel_{timestamp}"
    
    # ä¿å­˜ä»»åŠ¡åº“
    task_data = []
    for task in results['tasks']:
        task_dict = {
            "instance_id": task.instance_id,
            "task_type": task.task_type,
            "description": task.description,
            "inputs": task.inputs,
            "expected_outputs": task.expected_outputs,
            "required_tools": task.required_tools,
            "constraints": task.constraints,
            "complexity": task.complexity,
            "metadata": task.metadata
        }
        task_data.append(task_dict)
    
    task_library_data = {
        "tasks": task_data,
        "metadata": {
            "generated_at": datetime.now().isoformat(),
            "num_tasks": len(task_data),
            "parallel_generation": True,
            "tool_registry_path": tool_registry_path,
            "llm_enhanced": use_llm,
            "task_distribution": task_distribution,
            "generation_time": results['timings']['total']
        }
    }
    
    task_file_path = output_path / f"{output_filename}.json"
    with open(task_file_path, 'w') as f:
        json.dump(task_library_data, f, indent=2)
    
    print(f"\nâœ… ä»»åŠ¡åº“å·²ä¿å­˜åˆ°: {task_file_path}")
    
    # ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
    report = {
        "summary": {
            "total_tasks": len(results['tasks']),
            "total_time": results['timings']['total'],
            "tasks_per_second": len(results['tasks']) / results['timings']['total'] if results['timings']['total'] > 0 else 0,
            "parallel_speedup": results.get('speedup', 1.0)
        },
        "task_distribution": {},
        "complexity_distribution": {"easy": 0, "medium": 0, "hard": 0},
        "tool_usage": {},
        "errors": results.get('errors', [])
    }
    
    # ç»Ÿè®¡ä»»åŠ¡ç±»å‹åˆ†å¸ƒ
    for task in results['tasks']:
        task_type = task.task_type
        if task_type not in report['task_distribution']:
            report['task_distribution'][task_type] = 0
        report['task_distribution'][task_type] += 1
        
        # ç»Ÿè®¡å¤æ‚åº¦
        report['complexity_distribution'][task.complexity] += 1
        
        # ç»Ÿè®¡å·¥å…·ä½¿ç”¨
        for tool in task.required_tools:
            if tool not in report['tool_usage']:
                report['tool_usage'][tool] = 0
            report['tool_usage'][tool] += 1
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = output_path / f"{output_filename}_report.json"
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2)
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"\nğŸ“Š ç”Ÿæˆç»Ÿè®¡:")
    print(f"æ€»ä»»åŠ¡æ•°: {report['summary']['total_tasks']}")
    print(f"æ€»è€—æ—¶: {report['summary']['total_time']:.2f} ç§’")
    print(f"ç”Ÿæˆé€Ÿåº¦: {report['summary']['tasks_per_second']:.2f} ä»»åŠ¡/ç§’")
    print(f"å¹¶è¡ŒåŠ é€Ÿ: {report['summary']['parallel_speedup']:.2f}x")
    
    print(f"\nä»»åŠ¡ç±»å‹åˆ†å¸ƒ:")
    for task_type, count in report['task_distribution'].items():
        percentage = (count / report['summary']['total_tasks']) * 100
        print(f"  - {task_type}: {count} ({percentage:.1f}%)")
    
    print(f"\nå¤æ‚åº¦åˆ†å¸ƒ:")
    for complexity, count in report['complexity_distribution'].items():
        percentage = (count / report['summary']['total_tasks']) * 100
        print(f"  - {complexity}: {count} ({percentage:.1f}%)")
    
    return {
        'task_file': str(task_file_path),
        'report_file': str(report_path),
        'tasks': results['tasks'],
        'summary': report
    }


class ParallelTaskGenerator:
    """ä¸“é—¨çš„å¹¶è¡Œä»»åŠ¡ç”Ÿæˆå™¨"""
    
    def __init__(self, 
                 tool_library: Dict[str, 'MCPTool'],  # ä½¿ç”¨å­—ç¬¦ä¸²å½¢å¼çš„å‰å‘å¼•ç”¨
                 tool_registry: Dict[str, Any],
                 max_workers: int = None):
        self.tool_library = tool_library
        self.tool_registry = tool_registry
        self.max_workers = max_workers or os.cpu_count()
        
        # é¢„åˆ›å»ºä»»åŠ¡ç”Ÿæˆå™¨ï¼ˆç”¨äºæ¨¡æ¿ï¼‰
        self.task_generator = TaskGenerator(tool_library, tool_registry)
        
    def generate_tasks_parallel(self,
                              num_tasks: int,
                              task_distribution: Dict[str, float],
                              use_llm: bool = True,
                              show_progress: bool = True) -> Dict[str, Any]:
        """å¹¶è¡Œç”Ÿæˆä»»åŠ¡"""
        start_time = time.time()
        
        # è®¡ç®—æ¯ç§ç±»å‹çš„ä»»åŠ¡æ•°é‡
        task_batches = []
        for task_type, proportion in task_distribution.items():
            type_num_tasks = int(num_tasks * proportion)
            if type_num_tasks > 0:
                # å°†æ¯ç§ç±»å‹åˆ†æˆå¤šä¸ªæ‰¹æ¬¡
                batch_size = max(10, type_num_tasks // self.max_workers)
                remaining = type_num_tasks
                
                while remaining > 0:
                    current_batch_size = min(batch_size, remaining)
                    task_batches.append({
                        'task_type': task_type,
                        'num_tasks': current_batch_size,
                        'use_llm': use_llm
                    })
                    remaining -= current_batch_size
        
        # éšæœºæ‰“ä¹±æ‰¹æ¬¡é¡ºåºä»¥å¹³è¡¡è´Ÿè½½
        random.shuffle(task_batches)
        
        print(f"\nğŸ”„ å¼€å§‹å¹¶è¡Œç”Ÿæˆ {len(task_batches)} ä¸ªæ‰¹æ¬¡...")
        
        results = {
            'tasks': [],
            'errors': [],
            'timings': {'batches': {}}
        }
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            # æäº¤æ‰€æœ‰æ‰¹æ¬¡
            future_to_batch = {
                executor.submit(self._generate_batch, batch): batch
                for batch in task_batches
            }
            
            # æ”¶é›†ç»“æœ
            if show_progress:
                with tqdm(total=len(task_batches), desc="ç”Ÿæˆä»»åŠ¡æ‰¹æ¬¡") as pbar:
                    for future in concurrent.futures.as_completed(future_to_batch):
                        batch = future_to_batch[future]
                        try:
                            batch_result = future.result(timeout=60)
                            results['tasks'].extend(batch_result['tasks'])
                            results['timings']['batches'][f"{batch['task_type']}_{len(results['timings']['batches'])}"] = batch_result['timing']
                            pbar.update(1)
                        except Exception as e:
                            error_msg = f"æ‰¹æ¬¡ç”Ÿæˆå¤±è´¥ ({batch['task_type']}): {str(e)}"
                            print(f"\nâŒ {error_msg}")
                            results['errors'].append(error_msg)
                            pbar.update(1)
            else:
                for future in concurrent.futures.as_completed(future_to_batch):
                    batch = future_to_batch[future]
                    try:
                        batch_result = future.result(timeout=60)
                        results['tasks'].extend(batch_result['tasks'])
                    except Exception as e:
                        results['errors'].append(f"æ‰¹æ¬¡å¤±è´¥: {str(e)}")
        
        # è®¡ç®—æ€»æ—¶é—´å’ŒåŠ é€Ÿæ¯”
        total_time = time.time() - start_time
        results['timings']['total'] = total_time
        
        # è®¡ç®—åŠ é€Ÿæ¯”
        batch_times = list(results['timings']['batches'].values())
        if batch_times:
            serial_time = sum(batch_times)
            results['speedup'] = serial_time / total_time if total_time > 0 else 1.0
        else:
            results['speedup'] = 1.0
        
        return results
        

    def _generate_batch(self, batch: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆå•ä¸ªæ‰¹æ¬¡çš„ä»»åŠ¡"""
        start_time = time.time()
        
        # åˆ›å»ºæœ¬æ‰¹æ¬¡çš„ä»»åŠ¡ç”Ÿæˆå™¨
        # æ³¨æ„ï¼šè¿™é‡Œä¹Ÿéœ€è¦ç¡®ä¿æ­£ç¡®åˆå§‹åŒ–
        
        # é¦–å…ˆç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        cache_dir = Path(".mcp_embedding_cache")
        cache_dir.mkdir(exist_ok=True)
        
        # ç¡®ä¿ç´¢å¼•æ–‡ä»¶å­˜åœ¨
        index_path = cache_dir / "tool_index.pkl"
        if not index_path.exists():
            # æ„å»ºç´¢å¼•
            from mcp_embedding_manager import get_embedding_manager
            temp_manager = get_embedding_manager()
            
            # æŸ¥æ‰¾å·¥å…·æ³¨å†Œè¡¨
            tool_registry_path = Path("mcp_generated_library/tool_registry_consolidated.json")
            if not tool_registry_path.exists():
                tool_registry_path = Path("mcp_generated_library/tool_registry.json")
            
            # æ„å»ºå¹¶ä¿å­˜ç´¢å¼•
            temp_manager.build_index(tool_registry_path, force_rebuild=True)
            print(f"[ParallelTaskGenerator] Built index for batch processing")
        
        # ç°åœ¨å¯ä»¥å®‰å…¨åˆ›å»ºä»»åŠ¡ç”Ÿæˆå™¨
        batch_generator = TaskGenerator(self.tool_library, self.tool_registry)
        
        # å¯ç”¨LLMç”Ÿæˆï¼ˆå¦‚æœé…ç½®ï¼‰
        batch_generator.use_llm_generation = batch.get('use_llm', True)
        
        # ç”Ÿæˆä»»åŠ¡
        tasks = []
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹ç”Ÿæˆ
        if batch['task_type'] == 'mixed':
            # ç”Ÿæˆæ··åˆç±»å‹ä»»åŠ¡
            for task_type in ['simple_task', 'data_pipeline', 'api_integration', 
                            'file_processing', 'multi_stage_pipeline']:
                type_tasks = batch_generator.generate_tasks_by_type(
                    task_type=task_type,
                    count=batch['count'] // 5,  # å¹³å‡åˆ†é…
                    use_llm=batch.get('use_llm', True)
                )
                tasks.extend(type_tasks)
        else:
            # ç”Ÿæˆç‰¹å®šç±»å‹ä»»åŠ¡
            tasks = batch_generator.generate_tasks_by_type(
                task_type=batch['task_type'],
                count=batch['count'],
                use_llm=batch.get('use_llm', True)
            )
        
        # ç»“æœ
        result = {
            'tasks': tasks,
            'timing': time.time() - start_time,
            'batch_info': batch
        }
        
        return result


    def _create_default_template(self, task_type: str) -> TaskTemplate:
        """åˆ›å»ºé»˜è®¤ä»»åŠ¡æ¨¡æ¿"""
        available_tools = list(self.tool_library.keys())
        selected_tools = random.sample(available_tools, min(3, len(available_tools)))
        
        return TaskTemplate(
            task_type=task_type,
            description=f"Default {task_type} task",
            required_tools=selected_tools,
            optional_tools=[],
            requirements=[
                TaskRequirement("input_data", "Input data to process")
            ],
            objectives=[
                TaskObjective(
                    f"Complete {task_type}",
                    [f"Use {tool}" for tool in selected_tools],
                    "output"
                )
            ],
            complexity="medium"
        )
# ===========================
# Data Classes
# ===========================

@dataclass
class ToolParameter:
    """Parameter for a tool"""
    name: str
    type: str
    description: str
    required: bool = True
    default: Optional[Any] = None
    constraints: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ToolReturn:
    """Return value from a tool"""
    name: str
    type: str
    description: str


@dataclass
class ToolError:
    """Possible error from a tool"""
    code: str
    description: str


@dataclass
class MCPTool:
    """Complete tool definition"""
    name: str
    description: str
    parameters: List[ToolParameter]
    returns: List[ToolReturn]
    errors: List[ToolError]
    dependencies: List[str] = field(default_factory=list)  # æ·»åŠ ä¾èµ–å­—æ®µ
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    # æ–‡ä»¶ï¼štool_and_task_generator.py
    # ä½ç½®ï¼šç¬¬51-76è¡Œï¼Œä¿®æ”¹to_mcp_jsonæ–¹æ³•
    def to_mcp_json(self) -> Dict[str, Any]:
        """Convert to MCP JSON format with dependency metadata"""
        return {
            "name": self.name,
            "description": self.description,
            "parameters": [
                {
                    "name": p.name,
                    "type": p.type,
                    "description": p.description,
                    "required": p.required,
                    "default": p.default,
                    "constraints": p.constraints
                }
                for p in self.parameters
            ],
            "returns": [
                {
                    "name": r.name,
                    "type": r.type,
                    "description": r.description
                }
                for r in self.returns
            ],
            "errors": [
                {
                    "code": e.code,
                    "description": e.description
                }
                for e in self.errors
            ],
            "dependencies": self.dependencies,
            "dependency_metadata": {
                "level": self.metadata.get('dependency_level', 0),
                "execution_order": self.metadata.get('execution_order', 0),
                "category": self.metadata.get('category', 'general')
            },
            "metadata": self.metadata
        }
    
    def to_mcp_xml(self) -> str:
        """Convert to MCP XML format"""
        tool = ET.Element("tool", name=self.name)
        ET.SubElement(tool, "description").text = self.description
        
        params = ET.SubElement(tool, "parameters")
        for p in self.parameters:
            param = ET.SubElement(params, "parameter", name=p.name, type=p.type)
            param.set("required", str(p.required).lower())
            ET.SubElement(param, "description").text = p.description
        
        returns = ET.SubElement(tool, "returns")
        for r in self.returns:
            ret = ET.SubElement(returns, "return", name=r.name, type=r.type)
            ET.SubElement(ret, "description").text = r.description
        
        errors = ET.SubElement(tool, "errors")
        for e in self.errors:
            err = ET.SubElement(errors, "error", code=e.code)
            ET.SubElement(err, "description").text = e.description
        
        return ET.tostring(tool, encoding='unicode')


@dataclass
class TaskRequirement:
    """Requirement for a task"""
    data_type: str
    description: str
    constraints: Dict[str, Any] = field(default_factory=dict)


@dataclass
class TaskObjective:
    """Objective for a task"""
    description: str
    success_criteria: List[str]
    output_type: str


@dataclass
class TaskTemplate:
    """Template for generating task instances"""
    task_type: str
    description: str
    required_tools: List[str]
    optional_tools: List[str]
    requirements: List[TaskRequirement]
    objectives: List[TaskObjective]
    complexity: str  # easy, medium, hard


@dataclass
class TaskInstance:
    """Concrete task instance"""
    instance_id: str
    task_type: str
    description: str
    inputs: Dict[str, Any]
    expected_outputs: Dict[str, Any]
    required_tools: List[str]
    constraints: Dict[str, Any]
    complexity: str
    metadata: Dict[str, Any] = field(default_factory=dict)
    tool_dependencies: Dict[str, List[str]] = field(default_factory=dict)  # æ–°å¢ï¼šä»»åŠ¡ç‰¹å®šçš„å·¥å…·ä¾èµ–




# ===========================
# Tool Generation
# ===========================

class ToolGenerator:
    """Generate diverse MCP tools"""
    
    def __init__(self):
        self.tool_templates = {
            "data_processing": [
                ("parser", "Parse and extract structured data"),
                ("transformer", "Transform data between formats"),
                ("validator", "Validate data against schemas"),
                ("aggregator", "Aggregate and summarize data"),
                ("filter", "Filter data based on criteria")
            ],
            "file_operations": [
                ("reader", "Read content from files"),
                ("writer", "Write content to files"),
                ("scanner", "Scan and analyze file contents"),
                ("compressor", "Compress and decompress files"),
                ("converter", "Convert between file formats")
            ],
            "network": [
                ("fetcher", "Fetch data from network sources"),
                ("poster", "Post data to endpoints"),
                ("monitor", "Monitor network connections"),
                ("validator", "Validate network responses"),
                ("router", "Route requests to appropriate handlers")
            ],
            "computation": [
                ("calculator", "Perform calculations"),
                ("analyzer", "Analyze numerical data"),
                ("optimizer", "Optimize parameters"),
                ("simulator", "Simulate processes"),
                ("predictor", "Make predictions based on data")
            ],
            "integration": [
                ("connector", "Connect to external services"),
                ("authenticator", "Handle authentication"),
                ("mapper", "Map between data schemas"),
                ("queue", "Queue operations for processing"),
                ("scheduler", "Schedule operations")
            ],
            "utility": [
                ("logger", "Log operations and results"),
                ("cache", "Cache frequently used data"),
                ("notifier", "Send notifications"),
                ("tracker", "Track operation progress"),
                ("helper", "Provide utility functions")
            ]
        }
        self.generated_tools = set()  # è¿½è¸ªå·²ç”Ÿæˆçš„å·¥å…·

    def to_training_view(self) -> Dict[str, Any]:
        """å®Œæ•´è§†å›¾ç”¨äºè®­ç»ƒï¼ŒåŒ…å«æ‰€æœ‰ä¾èµ–ä¿¡æ¯"""
        return self.to_mcp_json()  # åŒ…å«dependencies

    def to_testing_view(self) -> Dict[str, Any]:
        """æµ‹è¯•è§†å›¾ï¼Œéšè—ä¾èµ–ä¿¡æ¯"""
        data = self.to_mcp_json()
        # ç§»é™¤dependencieså­—æ®µ
        data.pop('dependencies', None)
        # å¯é€‰ï¼šæ··æ·†metadataä¸­çš„ä¾èµ–ç›¸å…³ä¿¡æ¯
        if 'metadata' in data:
            data['metadata'] = {k: v for k, v in data['metadata'].items() 
                            if 'depend' not in k.lower()}
        return data
        
    def generate_tool(self, category: str, operation: Optional[str] = None) -> MCPTool:
        """Generate a single tool"""
        if operation is None:
            operation = random.choice(self.tool_templates.get(category, [("generic", "Generic operation")]))[0]
        
        tool_name = f"{category}_{operation}"
        
        # Generate parameters based on category and operation
        parameters = self._generate_parameters(category, operation)
        
        # Generate returns
        returns = self._generate_returns(category, operation)
        
        # Generate errors
        errors = self._generate_errors(category, operation)
        
        # Generate dependencies based on operation type
        dependencies = self._generate_dependencies(category, operation)


        # Create tool
        return MCPTool(
            name=tool_name,
            description=f"{operation.capitalize()} for {category.replace('_', ' ')}",
            parameters=parameters,
            returns=returns,
            errors=errors,
            dependencies= dependencies,  # æ·»åŠ ä¾èµ–
            metadata={
                "category": category,
                "operation": operation,
                "version": "1.0.0",
                "created_at": datetime.now().isoformat()
            }
        )
    
    def _generate_parameters(self, category: str, operation: str) -> List[ToolParameter]:
        """Generate appropriate parameters for a tool"""
        params = []
        
        # Common parameters
        if operation in ["reader", "parser", "scanner", "fetcher"]:
            params.append(ToolParameter("source", "string", "Source location or identifier", True))
        
        if operation in ["writer", "poster", "notifier"]:
            params.append(ToolParameter("destination", "string", "Destination location or identifier", True))
            params.append(ToolParameter("data", "object", "Data to send", True))
        
        if operation in ["transformer", "converter", "mapper"]:
            params.append(ToolParameter("input_format", "string", "Input data format", True))
            params.append(ToolParameter("output_format", "string", "Desired output format", True))
        
        if operation in ["validator"]:
            params.append(ToolParameter("schema", "object", "Validation schema", True))
            params.append(ToolParameter("data", "object", "Data to validate", True))
        
        # Category-specific parameters
        if category == "computation":
            params.append(ToolParameter("precision", "number", "Calculation precision", False, 2))
        
        if category == "network":
            params.append(ToolParameter("timeout", "number", "Operation timeout in seconds", False, 30))
            params.append(ToolParameter("retry_count", "number", "Number of retries", False, 3))
        
        # Always add options parameter
        params.append(ToolParameter("options", "object", "Additional options", False, {}))
        
        return params
    

    
    def _generate_returns(self, category: str, operation: str) -> List[ToolReturn]:
        """Generate return values for a tool"""
        returns = []
        
        # Common returns
        returns.append(ToolReturn("success", "boolean", "Whether operation succeeded"))
        
        if operation in ["reader", "parser", "fetcher", "scanner"]:
            returns.append(ToolReturn("data", "object", "Retrieved or parsed data"))
        
        if operation in ["transformer", "converter"]:
            returns.append(ToolReturn("transformed_data", "object", "Transformed data"))
        
        if operation in ["validator"]:
            returns.append(ToolReturn("valid", "boolean", "Whether data is valid"))
            returns.append(ToolReturn("errors", "array", "Validation errors if any"))
        
        if operation in ["calculator", "analyzer", "predictor"]:
            returns.append(ToolReturn("result", "object", "Calculation or analysis result"))
        
        # Metadata return
        returns.append(ToolReturn("metadata", "object", "Operation metadata"))
        
        return returns
    
    def _generate_errors(self, category: str, operation: str) -> List[ToolError]:
        """Generate possible errors for a tool"""
        errors = [
            ToolError("INVALID_INPUT", "Input validation failed"),
            ToolError("OPERATION_FAILED", "Operation could not be completed"),
            ToolError("TIMEOUT", "Operation timed out")
        ]
        
        # Category-specific errors
        if category == "file_operations":
            errors.append(ToolError("FILE_NOT_FOUND", "Specified file not found"))
            errors.append(ToolError("PERMISSION_DENIED", "Insufficient permissions"))
        
        if category == "network":
            errors.append(ToolError("NETWORK_ERROR", "Network connection failed"))
            errors.append(ToolError("INVALID_RESPONSE", "Invalid response from server"))
        
        if category == "computation":
            errors.append(ToolError("CALCULATION_ERROR", "Calculation failed"))
            errors.append(ToolError("OVERFLOW", "Numerical overflow"))
        
        return errors
    
    # æ–‡ä»¶ï¼štool_and_task_generator.py
    # ä½ç½®ï¼šåœ¨_generate_errorsæ–¹æ³•åæ·»åŠ ï¼ˆçº¦ç¬¬350è¡Œï¼‰
    # æ–‡ä»¶ï¼štool_and_task_generator.py
    # ä½ç½®ï¼šç¬¬350è¡Œå¼€å§‹ï¼Œæ›¿æ¢åŸæœ‰çš„_generate_dependenciesæ–¹æ³•

    def _generate_dependencies(self, category: str, operation: str) -> List[str]:
        """Generate tool dependencies based on logical flow and data dependencies"""
        dependencies = []
        
        # å®šä¹‰æ“ä½œç±»å‹çš„è¯­ä¹‰åˆ†ç±»
        operation_types = {
            'sources': ['reader', 'fetcher', 'scanner', 'authenticator'],
            'processors': ['parser', 'transformer', 'validator', 'analyzer', 'calculator'],
            'aggregators': ['aggregator', 'combiner', 'merger'],
            'outputs': ['writer', 'poster', 'exporter', 'notifier'],
            'utilities': ['logger', 'cache', 'tracker', 'monitor']
        }
        
        # ç¡®å®šå½“å‰æ“ä½œçš„ç±»å‹
        current_type = None
        for op_type, ops in operation_types.items():
            if operation in ops:
                current_type = op_type
                break
        
        if not current_type:
            return dependencies
        
        # åŸºäºæ•°æ®æµçš„ä¾èµ–è§„åˆ™
        dependency_rules = {
            'processors': {
                'transformer': ['parser', 'reader'],
                'validator': ['parser', 'transformer'],
                'analyzer': ['parser', 'aggregator'],
                'calculator': ['parser', 'validator']
            },
            'aggregators': {
                'aggregator': ['parser', 'transformer'],
                'combiner': ['reader', 'parser'],
                'merger': ['transformer', 'validator']
            },
            'outputs': {
                'writer': ['transformer', 'validator'],
                'poster': ['transformer', 'authenticator'],
                'exporter': ['aggregator', 'validator'],
                'notifier': ['analyzer', 'monitor']
            }
        }
        
        # è·å–ç‰¹å®šæ“ä½œçš„ä¾èµ–
        if current_type in dependency_rules and operation in dependency_rules[current_type]:
            potential_deps = dependency_rules[current_type][operation]
            
            # æŸ¥æ‰¾å·²å­˜åœ¨çš„åŒ¹é…å·¥å…·
            for dep_op in potential_deps:
                # ä¼˜å…ˆæŸ¥æ‰¾åŒç±»åˆ«çš„å·¥å…·
                same_category_tool = f"{category}_{dep_op}"
                if same_category_tool in self.generated_tools:
                    dependencies.append(same_category_tool)
                    break
                
                # å…¶æ¬¡æŸ¥æ‰¾ä»»ä½•ç±»åˆ«çš„è¯¥æ“ä½œå·¥å…·
                for tool in self.generated_tools:
                    if dep_op in tool:
                        dependencies.append(tool)
                        break
        
        # ç‰¹æ®Šçš„è·¨ç±»åˆ«ä¾èµ–
        cross_category_deps = {
            ('network', 'poster'): 'integration_authenticator',
            ('network', 'fetcher'): 'integration_authenticator',
            ('file_operations', 'writer'): 'data_processing_validator',
            ('computation', 'optimizer'): 'computation_analyzer',
            ('integration', 'scheduler'): 'integration_queue'
        }
        
        key = (category, operation)
        if key in cross_category_deps and cross_category_deps[key] in self.generated_tools:
            dependencies.append(cross_category_deps[key])
        
        # å»é‡å¹¶é™åˆ¶ä¾èµ–æ•°é‡
        dependencies = list(dict.fromkeys(dependencies))[:2]  # æœ€å¤š2ä¸ªä¾èµ–
        
        return dependencies

    def _build_dependency_graph(self, tools: Dict[str, MCPTool]) -> Dict[str, Any]:
        """æ„å»ºå·¥å…·ä¾èµ–å›¾å¹¶æ£€æµ‹å¾ªç¯ä¾èµ–"""
        
        # åˆ›å»ºæœ‰å‘å›¾
        G = nx.DiGraph()
        
        # æ·»åŠ èŠ‚ç‚¹å’Œè¾¹
        for tool_name, tool in tools.items():
            G.add_node(tool_name)
            for dep in tool.dependencies:
                if dep in tools:  # ç¡®ä¿ä¾èµ–çš„å·¥å…·å­˜åœ¨
                    G.add_edge(dep, tool_name)
        
        # æ£€æµ‹å¾ªç¯ä¾èµ–
        cycles = list(nx.simple_cycles(G))
        if cycles:
            logger.warning(f"Detected circular dependencies: {cycles}")
            # ç§»é™¤å¾ªç¯ä¸­çš„æŸäº›è¾¹
            for cycle in cycles:
                # ç§»é™¤å¾ªç¯ä¸­çš„æœ€åä¸€æ¡è¾¹
                if len(cycle) > 1:
                    G.remove_edge(cycle[-1], cycle[0])
                    # æ›´æ–°å·¥å…·çš„ä¾èµ–åˆ—è¡¨
                    if cycle[0] in tools[cycle[-1]].dependencies:
                        tools[cycle[-1]].dependencies.remove(cycle[0])
        
        # è®¡ç®—æ‹“æ‰‘æ’åºï¼ˆæ‰§è¡Œé¡ºåºï¼‰
        try:
            topo_order = list(nx.topological_sort(G))
        except nx.NetworkXUnfeasible:
            logger.error("Failed to compute topological order - graph has cycles")
            topo_order = list(tools.keys())
        
        # è®¡ç®—æ¯ä¸ªå·¥å…·çš„å±‚çº§
        levels = {}
        for node in topo_order:
            if G.in_degree(node) == 0:
                levels[node] = 0
            else:
                levels[node] = max(levels[pred] for pred in G.predecessors(node)) + 1
        
        return {
            'graph': G,
            'topological_order': topo_order,
            'levels': levels,
            'has_cycles': len(cycles) > 0
        }

    def generate_tool_library(self, num_per_category: int = 5) -> Dict[str, MCPTool]:
        """Generate a library of tools with intelligent dependencies"""
        library = {}
        
        logger.info(f"Starting tool generation with {num_per_category} tools per category")
        
        # Phase 1: ç”ŸæˆåŸºç¡€å·¥å…·ï¼ˆæ— ä¾èµ–ï¼‰
        base_operations = ['reader', 'fetcher', 'scanner', 'authenticator', 'logger']
        for category in self.tool_templates:
            operations = self.tool_templates[category]
            # ä¼˜å…ˆç”ŸæˆåŸºç¡€æ“ä½œ
            for operation, desc in operations:
                if operation in base_operations and len([t for t in library.values() if t.metadata['category'] == category]) < num_per_category:
                    tool = self.generate_tool(category, operation)
                    tool.dependencies = []  # åŸºç¡€å·¥å…·æ— ä¾èµ–
                    library[tool.name] = tool
                    self.generated_tools.add(tool.name)
        
        logger.info(f"Phase 1 complete: Generated {len(library)} base tools")
        
        # Phase 2: ç”Ÿæˆä¸­é—´å±‚å·¥å…·ï¼ˆæœ‰å°‘é‡ä¾èµ–ï¼‰
        intermediate_operations = ['parser', 'transformer', 'validator', 'calculator']
        for category in self.tool_templates:
            operations = self.tool_templates[category]
            for operation, desc in operations:
                if operation in intermediate_operations and len([t for t in library.values() if t.metadata['category'] == category]) < num_per_category:
                    tool = self.generate_tool(category, operation)
                    library[tool.name] = tool
                    self.generated_tools.add(tool.name)
        
        logger.info(f"Phase 3 complete: Total {len(library)} tools generated")

        # Phase 3: ç”Ÿæˆé«˜çº§å·¥å…·ï¼ˆå¯èƒ½æœ‰å¤šä¸ªä¾èµ–ï¼‰
        for category in self.tool_templates:
            operations = self.tool_templates[category]
            remaining = num_per_category - len([t for t in library.values() if t.metadata['category'] == category])
            if remaining > 0:
                available_ops = [op for op, _ in operations if f"{category}_{op}" not in library]
                selected_ops = random.sample(available_ops, min(remaining, len(available_ops)))
                for operation in selected_ops:
                    tool = self.generate_tool(category, operation)
                    library[tool.name] = tool
                    self.generated_tools.add(tool.name)
        
        # Phase 4: æ„å»ºå’ŒéªŒè¯ä¾èµ–å›¾
        logger.info("Building dependency graph...")
        dep_graph_info = self._build_dependency_graph(library)
        
        # æ·»åŠ ä¾èµ–å›¾ä¿¡æ¯åˆ°metadata
        for tool_name, tool in library.items():
            tool.metadata['dependency_level'] = dep_graph_info['levels'].get(tool_name, 0)
            tool.metadata['execution_order'] = dep_graph_info['topological_order'].index(tool_name)
        
        logger.info(f"Generated {len(library)} tools with dependency graph")
        logger.info(f"Dependency levels: {dict(Counter(dep_graph_info['levels'].values()))}")
        
        # ä½¿ç”¨Counterå‰ç¡®ä¿å·²å¯¼å…¥

        level_counts = Counter(dep_graph_info['levels'].values())
        logger.info(f"Generated {len(library)} tools with dependency graph")
        logger.info(f"Dependency levels: {dict(level_counts)}")
        
        return library
    
    # æ–‡ä»¶ï¼štool_and_task_generator.py
    # ä½ç½®ï¼šåœ¨TaskGeneratorç±»ä¸­æ·»åŠ æ–°æ–¹æ³•ï¼ˆçº¦ç¬¬600è¡Œï¼‰

    def _create_dependency_aware_template(self, tool_chain: List[str]) -> TaskTemplate:
        """åŸºäºå·¥å…·ä¾èµ–é“¾åˆ›å»ºä»»åŠ¡æ¨¡æ¿"""
        # åˆ†æå·¥å…·é“¾çš„ç‰¹å¾
        categories = set()
        operations = []
        for tool in tool_chain:
            if tool in self.tool_library:
                tool_obj = self.tool_library[tool]
                categories.add(tool_obj.metadata.get('category', 'general'))
                operation = tool.name.split('_')[-1]
                operations.append(operation)
        
        # ç”Ÿæˆä»»åŠ¡æè¿°
        task_type = f"{'_'.join(categories)}_workflow" if len(categories) <= 2 else "multi_domain_workflow"
        description = f"Process data through {len(tool_chain)} stages involving {', '.join(categories)}"
        
        # åˆ›å»ºé˜¶æ®µæ€§ç›®æ ‡
        objectives = []
        for i, tool in enumerate(tool_chain):
            tool_obj = self.tool_library.get(tool)
            if tool_obj:
                stage_desc = tool_obj.description or f"Execute {tool}"
                objectives.append(TaskObjective(
                    f"Stage {i+1}: {stage_desc}",
                    [f"Execute {tool}", "Verify output"],
                    f"stage_{i+1}_output"
                ))
        
        return TaskTemplate(
            task_type=task_type,
            description=description,
            required_tools=tool_chain,
            optional_tools=[],
            requirements=[
                TaskRequirement("input_data", "Initial data to process"),
                TaskRequirement("stage_configs", f"Configuration for {len(tool_chain)} stages")
            ],
            objectives=objectives,
            complexity="medium" if len(tool_chain) <= 3 else "hard"
        )

    def _find_tool_chains(self) -> List[List[str]]:
        """Find tools that can be chained together based on dependencies"""
        chains = []
        
        # æ„å»ºä¾èµ–å›¾
        dep_graph_info = self.tool_library._build_dependency_graph(self.tool_library)
        G = dep_graph_info['graph']
        topo_order = dep_graph_info['topological_order']
        
        # æ–¹æ³•1ï¼šæ‰¾å‡ºæ‰€æœ‰ä»æºåˆ°æ±‡çš„è·¯å¾„
        sources = [n for n in G.nodes() if G.in_degree(n) == 0]
        sinks = [n for n in G.nodes() if G.out_degree(n) == 0]
        
        for source in sources[:3]:  # é™åˆ¶æºèŠ‚ç‚¹æ•°é‡
            for sink in sinks[:3]:  # é™åˆ¶æ±‡èŠ‚ç‚¹æ•°é‡
                try:
                    # æ‰¾æœ€çŸ­è·¯å¾„
                    path = nx.shortest_path(G, source, sink)
                    if 2 <= len(path) <= 5:  # åˆç†çš„é“¾é•¿åº¦
                        chains.append(path)
                except nx.NetworkXNoPath:
                    continue
        
        # æ–¹æ³•2ï¼šåŸºäºä¾èµ–å±‚çº§æ„å»ºé“¾
        levels = dep_graph_info['levels']
        max_level = max(levels.values()) if levels else 0
        
        for level in range(max_level):
            level_tools = [t for t, l in levels.items() if l == level]
            next_level_tools = [t for t, l in levels.items() if l == level + 1]
            
            if level_tools and next_level_tools:
                # åˆ›å»ºè·¨å±‚çº§çš„é“¾
                for start in random.sample(level_tools, min(2, len(level_tools))):
                    for end in random.sample(next_level_tools, min(2, len(next_level_tools))):
                        chain = [start, end]
                        # å¯é€‰ï¼šæ·»åŠ ä¸­é—´å·¥å…·
                        if level + 2 <= max_level:
                            mid_tools = [t for t, l in levels.items() if l == level + 2]
                            if mid_tools:
                                chain.append(random.choice(mid_tools))
                        chains.append(chain)
        
        # å»é‡å¹¶é™åˆ¶æ•°é‡
        unique_chains = []
        seen = set()
        for chain in chains:
            chain_key = tuple(sorted(chain))
            if chain_key not in seen:
                seen.add(chain_key)
                unique_chains.append(chain)
        
        return unique_chains[:10]  # æœ€å¤šè¿”å›10ä¸ªé“¾
    



# ===========================
# Task Generation
# ===========================

class TaskGenerator:
    """Generate diverse multi-tool tasks with semantic enhancement"""
        
    def __init__(self, tool_library: Dict[str, MCPTool], tool_registry: Dict[str, Any] = None):
            """åˆå§‹åŒ–ä»»åŠ¡ç”Ÿæˆå™¨"""
            self.tool_library = tool_library 
            self.tool_registry = tool_registry or {}
            
            # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
            cache_dir = Path(".mcp_embedding_cache")
            cache_dir.mkdir(exist_ok=True)
            
            # åˆå§‹åŒ–embeddingç®¡ç†å™¨
            self.embedding_manager = None
            self.operation_index = None
            self._initialize_semantic_components()
            
            # åˆ›å»ºä»»åŠ¡æ¨¡æ¿ - åœ¨è¯­ä¹‰ç»„ä»¶åˆå§‹åŒ–å
            self.task_templates = self._create_task_templates()
            
            api_key = os.getenv('OPENAI_API_KEY')
            self.llm_client = OpenAI(api_key=api_key)
            # åˆå§‹åŒ–å…¶ä»–å±æ€§
            self.use_llm_generation = True



    def _initialize_semantic_components(self):
        """åˆå§‹åŒ–è¯­ä¹‰ç»„ä»¶"""
        print("[TaskGenerator] Initializing semantic components...")
        
        # ä½¿ç”¨å•ä¾‹æ¨¡å¼è·å–MCPEmbeddingManager
        from mcp_embedding_manager import get_embedding_manager
        self.embedding_manager = get_embedding_manager()
        
        # ç¡®ä¿ç´¢å¼•å­˜åœ¨
        index_path = Path(".mcp_embedding_cache/tool_index.pkl")
        
        # æ£€æŸ¥ç´¢å¼•æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        index_exists = index_path.exists()
        print(f"[TaskGenerator] Index file exists: {index_exists}")
        
        # å¦‚æœç´¢å¼•ä¸å­˜åœ¨ï¼Œæ„å»ºå®ƒ
        if not index_exists:
            print("[TaskGenerator] Building embedding index...")
            
            # ç¡®å®šå·¥å…·æ³¨å†Œè¡¨è·¯å¾„
            tool_registry_path = Path("mcp_generated_library/tool_registry_consolidated.json")
            
            # å¦‚æœconsolidatedç‰ˆæœ¬ä¸å­˜åœ¨ï¼Œä½¿ç”¨æ™®é€šç‰ˆæœ¬
            if not tool_registry_path.exists():
                tool_registry_path = Path("mcp_generated_library/tool_registry.json")
            
            # æ„å»ºç´¢å¼•
            build_stats = self.embedding_manager.build_index(tool_registry_path, force_rebuild=True)
            print(f"[TaskGenerator] Built index with {build_stats['tools']} tools")
        else:
            # åŠ è½½å·²æœ‰ç´¢å¼•
            self.embedding_manager.load_index(index_path)
            print(f"[TaskGenerator] Loaded index with {len(self.embedding_manager.tool_embeddings)} tools")
        
        # åˆå§‹åŒ–æ“ä½œç´¢å¼•
        from operation_embedding_index import get_operation_index
        self.operation_index = get_operation_index()
        print(f"[TaskGenerator] Operation index initialized")
            
    def _create_task_templates(self) -> List[TaskTemplate]:
        """Create task templates based on available tools"""
        templates = []
        
        # Analyze available tools
        categories = self._analyze_tool_categories()
        
        # Simple single-tool tasks
        templates.extend(self._create_simple_templates(categories))
        
        # Data pipeline tasks
        if self._has_tools(["parser", "transformer", "writer"]):
            templates.append(self._create_data_pipeline_template())
        
        # API integration tasks
        if self._has_tools(["fetcher", "validator", "poster"]):
            templates.append(self._create_api_integration_template())
        
        # File processing tasks
        if self._has_tools(["reader", "scanner", "converter"]):
            templates.append(self._create_file_processing_template())
        
        # Multi-Stage Pipeline Tasks
        templates.append(self._create_multi_stage_template())
        
        # Basic tasks with guaranteed tools
        templates.extend(self._create_basic_templates())  # æ³¨æ„è¿™é‡Œæ˜¯ templates å¤æ•°å½¢å¼
        
        return templates
    
    def _generate_enhanced_description(self, template: TaskTemplate, instance_id: str) -> str:
        """ç”Ÿæˆå¢å¼ºçš„ä»»åŠ¡æè¿°ï¼Œåˆ©ç”¨key_differentiators"""
        base_description = template.description
        
        # æ”¶é›†æ‰€æœ‰å·¥å…·çš„key_differentiators
        tool_features = []
        for tool_name in template.required_tools:
            if tool_name in self.tool_registry:
                tool_data = self.tool_registry[tool_name]
                differentiation = tool_data.get('differentiation', {})
                
                # æ·»åŠ å…³é”®å·®å¼‚åŒ–ç‰¹å¾
                key_diffs = differentiation.get('key_differentiators', [])
                if key_diffs:
                    tool_features.extend(key_diffs[:2])  # æ¯ä¸ªå·¥å…·æœ€å¤š2ä¸ªç‰¹å¾
                
                # æ·»åŠ ä½¿ç”¨å…³é”®è¯
                usage_keywords = differentiation.get('usage_keywords', [])
                if usage_keywords:
                    tool_features.append(f"involving {usage_keywords[0]}")
        
        # æ„å»ºå¢å¼ºæè¿°
        if tool_features:
            feature_text = ", ".join(set(tool_features[:3]))  # æœ€å¤š3ä¸ªç‰¹å¾ï¼Œå»é‡
            enhanced_desc = f"{base_description} with {feature_text} - Instance {instance_id}"
        else:
            enhanced_desc = f"{base_description} - Instance {instance_id}"
        
        return enhanced_desc
    


    def _generate_semantic_tool_flow(self, task_type: str, num_tools: int) -> List[str]:
        """åŸºäºè¯­ä¹‰ç›¸ä¼¼åº¦ç”Ÿæˆå·¥å…·æµï¼Œå¹¶æŒ‰æ‹“æ‰‘é¡ºåºæ’åº"""
        if not self.embedding_manager or not self.embedding_manager.tool_embeddings:
            # Fallbackåˆ°éšæœºé€‰æ‹©
            selected_tools = random.sample(list(self.tool_library.keys()), min(num_tools, len(self.tool_library)))
            # å³ä½¿æ˜¯éšæœºé€‰æ‹©ï¼Œä¹Ÿè¦è¿›è¡Œæ‹“æ‰‘æ’åº
            return self._topological_sort_tools(selected_tools)
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹ç¡®å®šæ“ä½œåºåˆ—
        operation_sequences = {
            'data_pipeline': ['read', 'validate', 'transform', 'aggregate', 'write'],
            'api_integration': ['fetch', 'parse', 'validate', 'transform', 'post'],
            'file_processing': ['read', 'scan', 'filter', 'convert', 'write'],
            'multi_stage_pipeline': ['read', 'validate', 'transform', 'compute', 'aggregate', 'write'],
            'simple_task': ['read', 'process', 'output'],
            'basic_task': ['input', 'process']
        }
        
        operations = operation_sequences.get(task_type, ['process'])
        selected_tools = []
        used_tools = set()
        
        # ä¸ºæ¯ä¸ªæ“ä½œæ‰¾åˆ°æœ€åˆé€‚çš„å·¥å…·
        for operation in operations[:num_tools]:
            # æœç´¢è¯­ä¹‰ç›¸ä¼¼çš„å·¥å…·
            search_results = self.embedding_manager.search(
                query=operation,
                k=5,
                filter_tools=set(self.tool_library.keys()) - used_tools
            )
            
            if search_results:
                # é€‰æ‹©å¾—åˆ†æœ€é«˜ä¸”æœªä½¿ç”¨çš„å·¥å…·
                for result in search_results:
                    if result.tool_name not in used_tools:
                        selected_tools.append(result.tool_name)
                        used_tools.add(result.tool_name)
                        break
        
        # å¦‚æœå·¥å…·ä¸å¤Ÿï¼Œè¡¥å……éšæœºå·¥å…·
        remaining_tools = list(set(self.tool_library.keys()) - used_tools)
        while len(selected_tools) < num_tools and remaining_tools:
            tool = random.choice(remaining_tools)
            selected_tools.append(tool)
            remaining_tools.remove(tool)
        
        # å¯¹é€‰æ‹©çš„å·¥å…·è¿›è¡Œæ‹“æ‰‘æ’åº
        logger.debug(f" Before topological sort: {selected_tools}")
        sorted_tools = self._topological_sort_tools(selected_tools)
        logger.debug(f" After topological sort: {sorted_tools}")
        
        return sorted_tools

    def _topological_sort_tools(self, tools: List[str]) -> List[str]:
        """æ ¹æ®å·¥å…·ä¾èµ–å…³ç³»è¿›è¡Œæ‹“æ‰‘æ’åº"""
        if not tools or len(tools) <= 1:
            return tools
        
        # æ„å»ºæˆ–è·å–ä¾èµ–å›¾
        dep_graph_info = self._get_dependency_graph_info()
        if not dep_graph_info:
            print(f"[WARNING] No dependency graph available, returning original order")
            return tools
        
        dep_graph = dep_graph_info['graph']
        
        # åˆ›å»ºåŒ…å«é€‰å®šå·¥å…·çš„å­å›¾
        subgraph = dep_graph.subgraph(tools)
        
        # å°è¯•æ‹“æ‰‘æ’åº
        try:
            # è·å–å­å›¾çš„æ‹“æ‰‘æ’åº
            sorted_nodes = list(nx.topological_sort(subgraph))
            
            # å¤„ç†ä¸åœ¨ä¾èµ–å›¾ä¸­çš„å·¥å…·ï¼ˆä¿æŒç›¸å¯¹é¡ºåºï¼‰
            remaining_tools = [t for t in tools if t not in sorted_nodes]
            
            # åˆå¹¶æ’åºç»“æœ
            result = sorted_nodes + remaining_tools
            
            return result
            
        except nx.NetworkXUnfeasible:
            # å¦‚æœæœ‰å¾ªç¯ä¾èµ–ï¼Œå°è¯•åŸºäºexecution_orderæ’åº
            print(f"[WARNING] Circular dependency detected, using execution_order")
            return self._sort_by_execution_order(tools)
        except Exception as e:
            print(f"[ERROR] Topological sort failed: {e}")
            # é™çº§åˆ°åŸºäºexecution_orderçš„æ’åº
            return self._sort_by_execution_order(tools)

    def _get_dependency_graph_info(self) -> Optional[Dict[str, Any]]:
        """è·å–ä¾èµ–å›¾ä¿¡æ¯ï¼ˆä»å·¥å…·åº“æˆ–ç¼“å­˜ï¼‰"""
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰ç¼“å­˜çš„ä¾èµ–å›¾ä¿¡æ¯
        if hasattr(self, '_dependency_graph_info'):
            return self._dependency_graph_info
        
        # å°è¯•ä»å·¥å…·åº“æ„å»ºä¾èµ–å›¾
        if hasattr(self, 'tool_library') and self.tool_library:
            # æ£€æŸ¥å·¥å…·åº“æ˜¯å¦æ˜¯å­—å…¸
            if isinstance(self.tool_library, dict):
                # ä»å­—å…¸æ„å»ºä¾èµ–å›¾
                return self._build_dependency_graph_from_dict(self.tool_library)
            # æ£€æŸ¥å·¥å…·åº“æ˜¯å¦æœ‰_build_dependency_graphæ–¹æ³•
            elif hasattr(self.tool_library, '_build_dependency_graph'):
                self._dependency_graph_info = self.tool_library._build_dependency_graph(self.tool_library)
                return self._dependency_graph_info
        
        return None

    def _build_dependency_graph_from_dict(self, tools: Dict[str, Any]) -> Dict[str, Any]:
        """ä»å·¥å…·å­—å…¸æ„å»ºä¾èµ–å›¾"""
        import networkx as nx
        
        # åˆ›å»ºæœ‰å‘å›¾
        G = nx.DiGraph()
        
        # æ·»åŠ èŠ‚ç‚¹å’Œè¾¹
        for tool_name, tool in tools.items():
            G.add_node(tool_name)
            # è·å–ä¾èµ–åˆ—è¡¨
            dependencies = []
            if hasattr(tool, 'dependencies'):
                dependencies = tool.dependencies
            elif isinstance(tool, dict) and 'dependencies' in tool:
                dependencies = tool['dependencies']
            
            for dep in dependencies:
                if dep in tools:  # ç¡®ä¿ä¾èµ–çš„å·¥å…·å­˜åœ¨
                    G.add_edge(dep, tool_name)
        
        # è®¡ç®—æ‹“æ‰‘æ’åºï¼ˆæ‰§è¡Œé¡ºåºï¼‰
        try:
            topo_order = list(nx.topological_sort(G))
        except nx.NetworkXUnfeasible:
            print("[WARNING] Failed to compute topological order - graph has cycles")
            topo_order = list(tools.keys())
        
        # è®¡ç®—æ¯ä¸ªå·¥å…·çš„å±‚çº§
        levels = {}
        for node in topo_order:
            if G.in_degree(node) == 0:
                levels[node] = 0
            else:
                predecessors = list(G.predecessors(node))
                if predecessors:
                    levels[node] = max(levels.get(pred, 0) for pred in predecessors) + 1
                else:
                    levels[node] = 0
        
        # ç¼“å­˜ç»“æœ
        self._dependency_graph_info = {
            'graph': G,
            'topological_order': topo_order,
            'levels': levels,
            'has_cycles': False
        }
        
        return self._dependency_graph_info

    def _sort_by_execution_order(self, tools: List[str]) -> List[str]:
        """åŸºäºå·¥å…·çš„execution_orderè¿›è¡Œæ’åºï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰"""
        # è·å–æ¯ä¸ªå·¥å…·çš„execution_order
        tool_orders = []
        for tool in tools:
            order = float('inf')
            
            if tool in self.tool_library:
                tool_obj = self.tool_library[tool]
                # å°è¯•ä»ä¸åŒä½ç½®è·å–execution_order
                if hasattr(tool_obj, 'metadata') and isinstance(tool_obj.metadata, dict):
                    order = tool_obj.metadata.get('execution_order', float('inf'))
                elif isinstance(tool_obj, dict) and 'metadata' in tool_obj:
                    order = tool_obj['metadata'].get('execution_order', float('inf'))
                elif isinstance(tool_obj, dict) and 'execution_order' in tool_obj:
                    order = tool_obj['execution_order']
            
            tool_orders.append((tool, order))
        
        # æŒ‰execution_orderæ’åº
        tool_orders.sort(key=lambda x: x[1])
        
        return [tool for tool, _ in tool_orders]


    def _create_basic_templates(self, task_type: str = "basic_task", use_llm: bool = None) -> List[TaskTemplate]:
        """Create basic task templates using RAG+LLM when available"""
        templates = []
        
        # å¦‚æœæŒ‡å®šäº†ç‰¹å®šçš„task_typeä¸”ä¸æ˜¯basic_taskï¼Œåˆ›å»ºå¯¹åº”ç±»å‹çš„æ¨¡æ¿
        if task_type != "basic_task":
            # ä¸ºå…¶ä»–ç±»å‹åˆ›å»ºå•ä¸ªæ¨¡æ¿
            available_tools = list(self.tool_library.keys())
            if available_tools:
                num_tools = 3 if task_type in ['data_pipeline', 'api_integration'] else 4
                selected_tools = random.sample(available_tools, min(num_tools, len(available_tools)))
                
                templates.append(TaskTemplate(
                    task_type=task_type,
                    description=f"Generated {task_type} task",
                    required_tools=selected_tools,
                    optional_tools=[],
                    requirements=[
                        TaskRequirement("input_data", "Data to process"),
                        TaskRequirement("config", "Processing configuration")
                    ],
                    objectives=[
                        TaskObjective(
                            f"Complete {task_type}",
                            [f"Execute {tool}" for tool in selected_tools],
                            "output"
                        )
                    ],
                    complexity="medium" if task_type in ['data_pipeline', 'api_integration'] else "easy"
                ))
            
            return templates
        
        # åŸæœ‰çš„basic_taskæ¨¡æ¿ç”Ÿæˆé€»è¾‘
        available_tools = list(self.tool_library.keys())
        
        if len(available_tools) >= 2:
            # Create 2-3 basic templates with different tool combinations
            for i in range(min(3, len(available_tools) // 2)):
                num_tools = random.randint(2, min(3, len(available_tools)))
                selected_tools = random.sample(available_tools, num_tools)
                
                templates.append(TaskTemplate(
                    task_type="basic_task",
                    description=f"Basic data processing workflow #{i+1}",
                    required_tools=selected_tools,
                    optional_tools=[],
                    requirements=[
                        TaskRequirement("input_data", "Data to process"),
                        TaskRequirement("processing_config", "Configuration for processing")
                    ],
                    objectives=[
                        TaskObjective(
                            "Process data through selected tools",
                            [f"Execute {tool}" for tool in selected_tools],
                            "output"
                        )
                    ],
                    complexity="easy"
                ))
        
        # Fallback templates
        if not templates and available_tools:
            templates.append(TaskTemplate(
                task_type="basic_task",
                description="Basic single-tool processing task",
                required_tools=available_tools[:1],
                optional_tools=[],
                requirements=[
                    TaskRequirement("input_data", "Data to process")
                ],
                objectives=[
                    TaskObjective(
                        "Process data",
                        ["Execute processing"],
                        "output"
                    )
                ],
                complexity="easy"
            ))
        
        if not templates:
            print("[TaskGenerator] Warning: No tools available, creating minimal basic template")
            templates.append(TaskTemplate(
                task_type="basic_task",
                description="Minimal basic task",
                required_tools=["generic_processor"],
                optional_tools=[],
                requirements=[
                    TaskRequirement("input", "Input data")
                ],
                objectives=[
                    TaskObjective(
                        "Process input",
                        ["Execute basic processing"],
                        "output"
                    )
                ],
                complexity="easy"
            ))
        
        return templates



    def _create_simple_templates(self, categories: Dict[str, List[str]]) -> List[TaskTemplate]:
        """Create simple single/dual tool task templates"""
        templates = []
        
        # Single tool tasks
        for category, tools in categories.items():
            if tools:
                tool = random.choice(tools)
                templates.append(TaskTemplate(
                    task_type=f"simple_{category}_task",
                    description=f"Simple {category.replace('_', ' ')} task",
                    required_tools=[tool],
                    optional_tools=self._find_tools(["validator", "logger"])[:2],
                    requirements=[
                        TaskRequirement("input_data", f"Data for {category} processing")
                    ],
                    objectives=[
                        TaskObjective(
                            f"Process {category} data",
                            [f"Execute {tool}", "Validate results"],
                            "processed_data"
                        )
                    ],
                    complexity="easy"
                ))
        
        # Dual tool tasks
        if len(self.tool_library) >= 2:
            for _ in range(3):  # Create 3 dual-tool templates
                tools = random.sample(list(self.tool_library.keys()), 2)
                templates.append(TaskTemplate(
                    task_type="simple_task",
                    description="Simple two-step processing task",
                    required_tools=tools,
                    optional_tools=[],
                    requirements=[
                        TaskRequirement("data", "Input data"),
                        TaskRequirement("config", "Processing configuration")
                    ],
                    objectives=[
                        TaskObjective(
                            "Process data through two steps",
                            [f"Step 1: {tools[0]}", f"Step 2: {tools[1]}"],
                            "final_output"
                        )
                    ],
                    complexity="easy"
                ))
        
        return templates
    

    def _create_data_pipeline_template(self) -> TaskTemplate:
        """Create data pipeline task template with semantic-aware tool selection"""
        # ä½¿ç”¨è¯­ä¹‰åŒ–çš„å·¥å…·æµç”Ÿæˆï¼Œè€Œä¸æ˜¯ç®€å•çš„å­—ç¬¦ä¸²åŒ¹é…
        # å®šä¹‰data_pipelineçš„æ ‡å‡†æµç¨‹
        num_tools = 3  # data_pipelineé€šå¸¸éœ€è¦3ä¸ªæ ¸å¿ƒå·¥å…·
        
        # ç›´æ¥ä½¿ç”¨è¯­ä¹‰å·¥å…·æµç”Ÿæˆ
        semantic_tools = self._generate_semantic_tool_flow('data_pipeline', num_tools)
        
        # å¦‚æœè¯­ä¹‰ç”Ÿæˆå¤±è´¥æˆ–ç»“æœä¸åˆç†ï¼Œä½¿ç”¨æ”¹è¿›çš„æŸ¥æ‰¾é€»è¾‘
        if not semantic_tools or len(semantic_tools) < 3:
            # æŒ‰ç…§æ•°æ®å¤„ç†çš„é€»è¾‘é¡ºåºæŸ¥æ‰¾å·¥å…·
            # 1. é¦–å…ˆéœ€è¦è¯»å–å·¥å…·
            reader_tools = self._find_tools_by_operation(["reader", "loader", "fetcher"])
            # 2. ç„¶åéœ€è¦éªŒè¯/è§£æå·¥å…·
            validator_tools = self._find_tools_by_operation(["validator", "parser", "checker"])
            # 3. æ¥ç€éœ€è¦è½¬æ¢å·¥å…·
            transformer_tools = self._find_tools_by_operation(["transformer", "converter", "processor"])
            # 4. æœ€åéœ€è¦è¾“å‡ºå·¥å…·
            writer_tools = self._find_tools_by_operation(["writer", "exporter", "saver"])
            
            # æ„å»ºæœ‰åºçš„å·¥å…·åˆ—è¡¨
            required = []
            
            # æ·»åŠ è¯»å–å·¥å…·ï¼ˆä¼˜å…ˆfile_operations_readerï¼‰
            if reader_tools:
                for tool in reader_tools:
                    if "file_operations_reader" in tool:
                        required.append(tool)
                        break
                else:
                    required.append(reader_tools[0])
            
            # æ·»åŠ éªŒè¯å·¥å…·ï¼ˆä¼˜å…ˆdata_processing_validatorï¼‰
            if validator_tools:
                for tool in validator_tools:
                    if "data_processing_validator" in tool:
                        required.append(tool)
                        break
                else:
                    required.append(validator_tools[0])
            
            # æ·»åŠ è½¬æ¢å·¥å…·ï¼ˆä¼˜å…ˆdata_processing_transformerï¼‰
            if transformer_tools:
                for tool in transformer_tools:
                    if "data_processing_transformer" in tool:
                        required.append(tool)
                        break
                else:
                    required.append(transformer_tools[0])
            
            # æ·»åŠ è¾“å‡ºå·¥å…·ï¼ˆå¦‚æœè¿˜æœ‰ç©ºé—´ï¼‰
            if len(required) < 3 and writer_tools:
                for tool in writer_tools:
                    if "file_operations_writer" in tool:
                        required.append(tool)
                        break
                else:
                    required.append(writer_tools[0])
            
            # ç¡®ä¿è‡³å°‘æœ‰3ä¸ªå·¥å…·
            while len(required) < 3:
                # ä»å¯ç”¨å·¥å…·ä¸­é€‰æ‹©æœªä½¿ç”¨çš„
                available_tools = list(set(self.tool_library.keys()) - set(required))
                if available_tools:
                    # ä¼˜å…ˆé€‰æ‹©data_processingæˆ–file_operationsç±»åˆ«çš„å·¥å…·
                    for tool in available_tools:
                        if "data_processing" in tool or "file_operations" in tool:
                            required.append(tool)
                            break
                    else:
                        required.append(available_tools[0])
                else:
                    break
                    
            # ä½¿ç”¨æ‹“æ‰‘æ’åºç¡®ä¿ä¾èµ–é¡ºåºæ­£ç¡®
            required = self._topological_sort_tools(required[:3])
        else:
            required = semantic_tools
        
        # æ‰“å°è°ƒè¯•ä¿¡æ¯
        logger.debug(f" data_pipeline template - required_tools: {required}")
        
        return TaskTemplate(
            task_type="data_pipeline",
            description="Multi-stage data processing pipeline",
            required_tools=required,
            optional_tools=self._find_tools(["monitor", "logger", "cache"])[:3],
            requirements=[
                TaskRequirement("raw_data", "Raw data to process"),
                TaskRequirement("output_format", "Desired output format"),
                TaskRequirement("transformation_rules", "Data transformation rules")
            ],
            objectives=[
                TaskObjective(
                    "Parse raw data",
                    ["Extract structured data", "Validate format"],
                    "parsed_data"
                ),
                TaskObjective(
                    "Transform data",
                    ["Apply transformation rules", "Maintain data integrity"],
                    "transformed_data"
                ),
                TaskObjective(
                    "Write output",
                    ["Write to specified format", "Verify output"],
                    "output_file"
                )
            ],
            complexity="medium"
        )

    def _find_tools_by_operation(self, operations: List[str]) -> List[str]:
        """Find tools that match certain operations with better matching logic"""
        matching_tools = []
        operation_priority = {op: i for i, op in enumerate(operations)}
        
        # æ”¶é›†æ‰€æœ‰åŒ¹é…çš„å·¥å…·åŠå…¶ä¼˜å…ˆçº§
        tool_scores = []
        for tool_name in self.tool_library:
            for op_idx, op in enumerate(operations):
                if op in tool_name.lower():
                    # ç»™äºˆæ›´æ—©å‡ºç°çš„æ“ä½œæ›´é«˜çš„ä¼˜å…ˆçº§
                    score = len(operations) - op_idx
                    tool_scores.append((tool_name, score))
                    break
        
        # æŒ‰åˆ†æ•°æ’åº
        tool_scores.sort(key=lambda x: x[1], reverse=True)
        matching_tools = [tool for tool, _ in tool_scores]
        
        return matching_tools[:5]  # è¿”å›å‰5ä¸ªåŒ¹é…çš„å·¥å…·
    
    def _create_api_integration_template(self) -> TaskTemplate:
        """Create API integration task template"""
        fetcher_tools = self._find_tools(["fetcher"])
        validator_tools = self._find_tools(["validator"])
        poster_tools = self._find_tools(["poster"])
        
        required = []
        if fetcher_tools:
            required.append(fetcher_tools[0])
        if validator_tools:
            required.append(validator_tools[0])
        
        return TaskTemplate(
            task_type="api_integration",
            description="API data fetching and processing",
            required_tools=required if required else ["network_fetcher", "data_processing_validator"],
            optional_tools=poster_tools[:1] + self._find_tools(["cache", "logger"]),
            requirements=[
                TaskRequirement("api_endpoints", "API endpoints to interact with"),
                TaskRequirement("auth_credentials", "Authentication credentials"),
                TaskRequirement("validation_schema", "Response validation schema")
            ],
            objectives=[
                TaskObjective(
                    "Fetch API data",
                    ["Authenticate with API", "Retrieve data", "Handle errors"],
                    "api_response"
                ),
                TaskObjective(
                    "Validate response",
                    ["Check response format", "Validate against schema"],
                    "validated_data"
                )
            ],
            complexity="medium"
        )
    
    def _create_file_processing_template(self) -> TaskTemplate:
        """Create file processing task template"""
        reader_tools = self._find_tools(["reader"])
        scanner_tools = self._find_tools(["scanner"])
        converter_tools = self._find_tools(["converter"])
        
        required = []
        if reader_tools:
            required.extend(reader_tools[:1])
        if scanner_tools:
            required.extend(scanner_tools[:1])
        
        return TaskTemplate(
            task_type="file_processing",
            description="File reading and analysis task",
            required_tools=required if required else ["file_operations_reader", "file_operations_scanner"],
            optional_tools=converter_tools[:1] + self._find_tools(["compressor"]),
            requirements=[
                TaskRequirement("file_paths", "Files to process"),
                TaskRequirement("scan_criteria", "What to scan for")
            ],
            objectives=[
                TaskObjective(
                    "Read files",
                    ["Open and read file contents", "Handle different formats"],
                    "file_contents"
                ),
                TaskObjective(
                    "Analyze content",
                    ["Scan for patterns", "Extract metadata"],
                    "analysis_results"
                )
            ],
            complexity="easy"
        )
    
    def _create_multi_stage_template(self) -> TaskTemplate:
        """Create a complex multi-stage task template"""
        # Find tools that can be chained
        chains = self._find_tool_chains()
        
        if chains:
            chain = random.choice(chains)
            return TaskTemplate(
                task_type="multi_stage_pipeline",
                description="Complex multi-stage data processing pipeline",
                required_tools=chain,
                optional_tools=self._find_tools(["validator", "monitor"]),
                requirements=[
                    TaskRequirement("input_data", "Initial data to process"),
                    TaskRequirement("pipeline_config", "Pipeline configuration")
                ],
                objectives=[
                    TaskObjective(
                        "Execute complete pipeline",
                        [f"Stage {i+1}: {tool}" for i, tool in enumerate(chain)],
                        "pipeline_result"
                    )
                ],
                complexity="hard"
            )
        
        # Fallback: create a template with some guaranteed tools
        available_tools = list(self.tool_library.keys())
        if len(available_tools) >= 3:
            selected_tools = random.sample(available_tools, 3)
            return TaskTemplate(
                task_type="multi_stage_pipeline",
                description="Multi-stage processing pipeline",
                required_tools=selected_tools,
                optional_tools=[],
                requirements=[
                    TaskRequirement("input_data", "Initial data to process"),
                    TaskRequirement("stage_configs", "Configuration for each stage")
                ],
                objectives=[
                    TaskObjective(
                        "Execute pipeline stages",
                        [f"Stage {i+1}: {tool}" for i, tool in enumerate(selected_tools)],
                        "pipeline_output"
                    )
                ],
                complexity="hard"
            )
        
        # Last resort fallback with single tool
        return TaskTemplate(
            task_type="simple_task",
            description="Simple processing task",
            required_tools=[available_tools[0]] if available_tools else ["generic_processor"],
            optional_tools=[],
            requirements=[TaskRequirement("data", "Input data")],
            objectives=[TaskObjective("Process data", ["Process input"], "output")],
            complexity="easy"
        )
    

    # æ³¨æ„ï¼šé‡å†™generate_task_instanceæ–¹æ³•ï¼Œæ·»åŠ RAGå’ŒLLMæ”¯æŒ


    def generate_task_instance(self, template: TaskTemplate) -> TaskInstance:
        """Generate a specific task instance from template with RAG and LLM enhancement"""
        instance_id = f"task_{uuid.uuid4().hex[:8]}"
        
        print(f"[TaskGenerator] Generating task instance {instance_id} with RAG+LLM enhancement")
        
        # ä½¿ç”¨RAGå’ŒLLMç”Ÿæˆä»»åŠ¡
        # é€šè¿‡RAGæŸ¥è¯¢ç›¸å…³å·¥å…·
        relevant_tools = self._query_tools_with_rag(template.task_type, template.complexity)
        
        # ä½¿ç”¨LLMç”Ÿæˆå®Œæ•´çš„ä»»åŠ¡å®ä¾‹
        llm_result = self._generate_task_with_llm(
            task_type=template.task_type,
            complexity=template.complexity,
            relevant_tools=relevant_tools,
            template=template
        )
        
        # ä½¿ç”¨LLMç”Ÿæˆçš„å†…å®¹
        enhanced_description = llm_result['description']
        required_tools = llm_result['required_tools']
        inputs = llm_result.get('inputs', {})
        expected_outputs = llm_result.get('expected_outputs', {})
        constraints = llm_result.get('constraints', {})
        
        print(f"[TaskGenerator] LLM generated {len(required_tools)} required tools")
        print(f"[TaskGenerator] Required tools: {required_tools}")
        print(f"[TaskGenerator] Generated inputs: {list(inputs.keys())}")
        print(f"[TaskGenerator] Generated outputs: {list(expected_outputs.keys())}")
        
        # éªŒè¯å·¥å…·å­˜åœ¨æ€§
        validated_tools = []
        for tool in required_tools:
            if tool in self.tool_library or tool in self.tool_registry:
                validated_tools.append(tool)
            else:
                print(f"[TaskGenerator] Warning: Tool {tool} not found, skipping")
        
        if not validated_tools:
            print(f"[TaskGenerator] No valid tools found, falling back to template tools")
            validated_tools = template.required_tools
        
        # å¦‚æœLLMæ²¡æœ‰ç”Ÿæˆè¾“å…¥è¾“å‡ºï¼Œä½¿ç”¨åŸºäºå·¥å…·çš„é»˜è®¤å€¼
        if not inputs:
            inputs = self._generate_default_inputs(template.task_type, validated_tools)
        if not expected_outputs:
            expected_outputs = self._generate_default_outputs(template.task_type, validated_tools)
            

        
        # è·å–å·¥å…·ä¾èµ–
        tool_dependencies = {}
        for tool in validated_tools:
            if tool in self.tool_library:
                tool_dependencies[tool] = self.tool_library[tool].dependencies
        
        # åˆ›å»ºä»»åŠ¡å®ä¾‹
        return TaskInstance(
            instance_id=instance_id,
            task_type=template.task_type,
            description=enhanced_description,
            inputs=inputs,
            expected_outputs=expected_outputs,
            required_tools=validated_tools,
            constraints=constraints,
            complexity=template.complexity,
            tool_dependencies=tool_dependencies,
            metadata={
                "template": template.task_type,
                "generated_at": datetime.now().isoformat(),
                "timeout": constraints.get("timeout", 300),
                "semantic_generation": bool(self.embedding_manager),
                "llm_generated": hasattr(self, 'use_llm_generation') and self.use_llm_generation,
                "inputs_generated_from": "llm" if hasattr(self, 'use_llm_generation') and self.use_llm_generation else "template"
            }
        )


    def _query_tools_with_rag(self, task_type: str, complexity: str) -> List[Dict[str, Any]]:
        """ä½¿ç”¨RAGæŸ¥è¯¢ç›¸å…³å·¥å…·"""
        print(f"[TaskGenerator] Querying tools with RAG for {task_type} task")
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹æ„å»ºæŸ¥è¯¢
        query_mappings = {
            'basic_task': 'basic data processing read write',
            'simple_task': 'simple processing transform validate',
            'data_pipeline': 'data pipeline parse transform aggregate write',
            'api_integration': 'api fetch validate authenticate post',
            'multi_stage_pipeline': 'complex pipeline read validate transform compute aggregate write'
        }
        
        query = query_mappings.get(task_type, 'general data processing')
        
        # å¦‚æœæœ‰embedding managerï¼Œä½¿ç”¨è¯­ä¹‰æœç´¢
        relevant_tools = []
        
        if self.embedding_manager and hasattr(self.embedding_manager, 'search'):
            try:
                # æœç´¢ç›¸å…³å·¥å…·
                k = 10 if complexity == 'hard' else 7 if complexity == 'medium' else 5
                search_results = self.embedding_manager.search(
                    query=query,
                    k=k,
                    return_scores=True
                )
                
                print(f"[TaskGenerator] Found {len(search_results)} tools from RAG search")
                
                # è·å–å·¥å…·è¯¦ç»†ä¿¡æ¯
                for result in search_results:
                    if hasattr(result, 'tool_name'):
                        tool_name = result.tool_name
                        if tool_name in self.tool_registry:
                            tool_info = self.tool_registry[tool_name]
                            tool_info['rag_score'] = getattr(result, 'score', 0.0)
                            relevant_tools.append(tool_info)
                            print(f"[TaskGenerator] Added {tool_name} with score {tool_info['rag_score']:.3f}")
            
            except Exception as e:
                print(f"[TaskGenerator] RAG search failed: {e}")
        
        # å¦‚æœRAGå¤±è´¥æˆ–æ²¡æœ‰ç»“æœï¼Œä½¿ç”¨åŸºäºç±»åˆ«çš„fallback
        if not relevant_tools:
            print(f"[TaskGenerator] Using category-based fallback")
            categories = self._get_task_categories(task_type)
            for category in categories:
                for tool_name, tool_data in self.tool_registry.items():
                    if tool_data.get('metadata', {}).get('category') == category:
                        relevant_tools.append(tool_data)
                        if len(relevant_tools) >= 10:
                            break
        
        return relevant_tools


    def _generate_task_with_llm(self, task_type: str, complexity: str, 
                                relevant_tools: List[Dict[str, Any]], 
                                template: TaskTemplate = None) -> Dict[str, Any]:
        """ä½¿ç”¨LLMç”Ÿæˆä»»åŠ¡æè¿°ã€å·¥å…·æµã€è¾“å…¥å’Œè¾“å‡º"""
        print(f"[TaskGenerator] Generating {task_type} task with LLM")
        
        # å‡†å¤‡å·¥å…·ä¿¡æ¯ï¼ŒåŒ…å«å®Œæ•´çš„å‚æ•°å’Œè¿”å›å€¼ä¿¡æ¯
        tools_info = []
        for tool in relevant_tools[:15]:  # å¢åŠ åˆ°15ä¸ªå·¥å…·
            tool_desc = {
                'name': tool.get('name', ''),
                'description': tool.get('description', ''),
                'category': tool.get('metadata', {}).get('category', 'general'),
                'operation': tool.get('metadata', {}).get('operation', 'process'),
                'key_features': tool.get('differentiation', {}).get('key_differentiators', [])[:3],
                'rag_score': tool.get('rag_score', 0.0),
                # æ·»åŠ å®Œæ•´çš„å‚æ•°ä¿¡æ¯
                'parameters': [],
                'returns': []
            }
            
            # å¤„ç†å‚æ•°
            for param in tool.get('parameters', []):
                tool_desc['parameters'].append({
                    'name': param.get('name', ''),
                    'type': param.get('type', 'string'),
                    'description': param.get('description', ''),
                    'required': param.get('required', True),
                    'default': param.get('default')
                })
            
            # å¤„ç†è¿”å›å€¼
            for ret in tool.get('returns', []):
                tool_desc['returns'].append({
                    'name': ret.get('name', ''),
                    'type': ret.get('type', 'string'),
                    'description': ret.get('description', '')
                })
                
            tools_info.append(tool_desc)
        
        # æ ¹æ®ä»»åŠ¡ç±»å‹æ„å»ºæ›´å…·ä½“çš„prompt
        task_descriptions = {
            'basic_task': 'a simple data processing task with 1-3 tools',
            'simple_task': 'a straightforward task using 2-3 tools in sequence',
            'data_pipeline': 'a data processing pipeline that reads, transforms, and writes data',
            'api_integration': 'an API integration task that fetches, validates, and processes data',
            'multi_stage_pipeline': 'a complex multi-stage pipeline with 4-6 tools'
        }
        
        task_desc = task_descriptions.get(task_type, f'a {task_type} task')
        
        # æ„å»ºæ”¹è¿›çš„prompt
        prompt = f"""
    You are an expert workflow designer. Based on RAG search results, design {task_desc}.

    Available tools (sorted by relevance):
    {json.dumps(tools_info, indent=2)}

    Requirements:
    - Task type: {task_type}
    - Complexity: {complexity}
    - Select 3-6 most appropriate tools based on:
    1. RAG relevance scores
    2. Logical workflow sequence
    3. Tool categories and operations
    4. Input/output compatibility between tools

    Design a complete task instance with:
    1. A clear description of what the task accomplishes
    2. A logical sequence of tools that work together
    3. Realistic input data that matches the first tool's parameters
    4. Expected output that matches the final tool's returns

    IMPORTANT: 
    - Ensure inputs match the parameter types of the first tool
    - Ensure outputs match the return types of the last tool
    - Create a data flow where each tool's output can feed into the next tool's input

    Return ONLY a JSON object with this structure:
    {{
        "description": "Clear description of the task",
        "required_tools": ["tool1", "tool2", "tool3"],
        "inputs": {{
            "param_name": "value based on first tool's parameters",
            "another_param": "another value"
        }},
        "expected_outputs": {{
            "output_field": "expected value based on last tool's returns",
            "another_output": "another expected value"
        }},
        "constraints": {{
            "timeout": 300,
            "max_retries": 3
        }}
    }}
    """

        response = self.llm_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are a workflow design expert. Return only valid JSON."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=1000
        )
        
        response_text = response.choices[0].message.content
        logger.debug(f" LLM Response: {response_text[:200]}...")
        
        # è§£æJSONå“åº”
        result = json.loads(response_text)
        
        # éªŒè¯å’Œæ¸…ç†ç»“æœ
        required_tools = result.get('required_tools', [])
        if not required_tools:
            raise ValueError("LLM did not generate required_tools")
        
        # ç¡®ä¿inputså’Œoutputsä¸ä¸ºç©º
        if not result.get('inputs'):
            result['inputs'] = self._generate_default_inputs(task_type, required_tools)
        
        if not result.get('expected_outputs'):
            result['expected_outputs'] = self._generate_default_outputs(task_type, required_tools)
        
        # æ·»åŠ é»˜è®¤çº¦æŸ
        if not result.get('constraints'):
            result['constraints'] = {
                'timeout': 300,
                'max_retries': 3,
                'memory_limit': '1GB'
            }
        
        logger.debug(f" LLM generated {len(required_tools)} tools with inputs and outputs")
        
        return result
            
    def _generate_default_inputs(self, task_type: str, required_tools: List[str]) -> Dict[str, Any]:
        """åŸºäºå·¥å…·å‚æ•°ç”Ÿæˆé»˜è®¤è¾“å…¥"""
        # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå·¥å…·çš„å‚æ•°
        if required_tools and required_tools[0] in self.tool_registry:
            first_tool = self.tool_registry[required_tools[0]]
            params = first_tool.get('parameters', [])
            
            inputs = {}
            for param in params:
                if param.get('required', True):
                    param_name = param.get('name', 'input')
                    param_type = param.get('type', 'string')
                    
                    # æ ¹æ®å‚æ•°ç±»å‹ç”Ÿæˆåˆç†çš„é»˜è®¤å€¼
                    if param_type == 'string':
                        if 'source' in param_name.lower():
                            inputs[param_name] = f"data/input_{task_type}.json"
                        elif 'format' in param_name.lower():
                            inputs[param_name] = "json"
                        else:
                            inputs[param_name] = f"sample_{param_name}"
                    elif param_type == 'number':
                        inputs[param_name] = 100
                    elif param_type == 'boolean':
                        inputs[param_name] = True
                    elif param_type == 'object':
                        inputs[param_name] = {"key": "value"}
                    elif param_type == 'array':
                        inputs[param_name] = ["item1", "item2"]
                    else:
                        inputs[param_name] = f"default_{param_name}"
            
            return inputs
        
        # å¦‚æœæ— æ³•è·å–å·¥å…·ä¿¡æ¯ï¼Œè¿”å›é»˜è®¤è¾“å…¥
        return {
            "data": "sample_input",
            "format": "json",
            "config": {"mode": "batch"}
        }
    

    def _generate_default_outputs(self, task_type: str, required_tools: List[str]) -> Dict[str, Any]:
        """åŸºäºå·¥å…·è¿”å›å€¼ç”Ÿæˆé»˜è®¤è¾“å‡º"""
        # æŸ¥æ‰¾æœ€åä¸€ä¸ªå·¥å…·çš„è¿”å›å€¼
        if required_tools and required_tools[-1] in self.tool_registry:
            last_tool = self.tool_registry[required_tools[-1]]
            returns = last_tool.get('returns', [])
            
            outputs = {}
            for ret in returns:
                ret_name = ret.get('name', 'result')
                ret_type = ret.get('type', 'string')
                
                # æ ¹æ®è¿”å›ç±»å‹ç”Ÿæˆåˆç†çš„æœŸæœ›å€¼
                if ret_name == 'success':
                    outputs[ret_name] = True
                elif ret_type == 'string':
                    outputs[ret_name] = f"processed_{ret_name}"
                elif ret_type == 'number':
                    outputs[ret_name] = 42
                elif ret_type == 'boolean':
                    outputs[ret_name] = True
                elif ret_type == 'object':
                    if 'data' in ret_name.lower():
                        outputs[ret_name] = {"processed": True, "records": 100}
                    else:
                        outputs[ret_name] = {"status": "completed"}
                elif ret_type == 'array':
                    outputs[ret_name] = ["result1", "result2"]
                else:
                    outputs[ret_name] = f"expected_{ret_name}"
            
            return outputs
        
        # å¦‚æœæ— æ³•è·å–å·¥å…·ä¿¡æ¯ï¼Œè¿”å›é»˜è®¤è¾“å‡º
        return {
            "status": "success",
            "data": "processed",
            "records_processed": 100
        }



    def _get_task_categories(self, task_type: str) -> List[str]:
        """è·å–ä»»åŠ¡ç±»å‹éœ€è¦çš„å·¥å…·ç±»åˆ«"""
        category_mapping = {
            'basic_task': ['file_operations', 'data_processing'],
            'simple_task': ['data_processing', 'utility'],
            'data_pipeline': ['data_processing', 'file_operations', 'utility'],
            'api_integration': ['network', 'data_processing', 'integration'],
            'multi_stage_pipeline': ['data_processing', 'file_operations', 'network', 'utility']
        }
        return category_mapping.get(task_type, ['data_processing'])

    def _get_tool_action(self, tool_data: Dict[str, Any]) -> str:
        """æ ¹æ®å·¥å…·ä¿¡æ¯ç”ŸæˆåŠ¨ä½œæè¿°"""
        operation = tool_data.get('metadata', {}).get('operation', 'process')
        category = tool_data.get('metadata', {}).get('category', 'general')
        
        action_templates = {
            'parser': 'parse and extract structured data',
            'transformer': 'transform data to the required format',
            'validator': 'validate data against schema',
            'writer': 'write processed data to output',
            'reader': 'read input data from source',
            'fetcher': 'fetch data from external source',
            'aggregator': 'aggregate and combine data',
            'converter': 'convert data format',
            'scanner': 'scan and analyze content',
            'poster': 'post data to destination'
        }
        
        return action_templates.get(operation, f'{operation} {category} data')

    def enable_llm_generation(self, enabled: bool = True):
        """å¯ç”¨æˆ–ç¦ç”¨LLMç”Ÿæˆæ¨¡å¼"""
        if enabled and not self.llm_client:
            print("[WARNING] Cannot enable LLM generation - OpenAI client not initialized")
            print("[WARNING] Please set OPENAI_API_KEY environment variable")
            self.use_llm_generation = False
        else:
            self.use_llm_generation = enabled
            print(f"[TaskGenerator] LLM generation {'enabled' if enabled else 'disabled'}")



    def _generate_task_batch(self, batch: Dict[str, Any]) -> Dict[str, Any]:
        """Generate a batch of tasks with proper LLM support"""
        start_time = time.time()
        
        logger.debug(f" Generating batch: type={batch['task_type']}, use_llm={batch['use_llm']}")
        
        # é‡å»ºæœ¬åœ°ç´¢å¼•ï¼ˆå¦‚æœéœ€è¦ï¼‰
        index_path = Path("mcp_generated_library/embeddings/tool_embeddings_index.pkl")
        if not index_path.exists() and batch['use_llm']:
            print(f"[ParallelTaskGenerator] Building index for RAG...")
            
            # åˆ›å»ºä¸´æ—¶çš„EmbeddingManager
            from tool_semantic_embedding import EmbeddingManager
            temp_manager = EmbeddingManager()
            
            # æ„å»ºåŸºç¡€ç´¢å¼•
            tool_data = []
            for tool_name, tool in self.tool_library.items():
                tool_data.append({
                    'name': tool_name,
                    'description': getattr(tool, 'description', ''),
                    'parameters': getattr(tool, 'parameters', []),
                    'returns': getattr(tool, 'returns', [])
                })
            
            # åˆ›å»ºä¸´æ—¶æ³¨å†Œè¡¨æ–‡ä»¶
            import tempfile
            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                json.dump(tool_data, f)
                temp_file = f.name
            
            # æ„å»ºç´¢å¼•
            temp_manager.build_index(temp_file, force_rebuild=True)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            Path(temp_file).unlink()
            
            print(f"[ParallelTaskGenerator] Built index for batch processing")
        
        # åˆ›å»ºä»»åŠ¡ç”Ÿæˆå™¨
        batch_generator = TaskGenerator(self.tool_library, self.tool_registry)
        
        # å¯ç”¨LLMç”Ÿæˆï¼ˆå¦‚æœéœ€è¦ï¼‰
        if batch['use_llm']:
            batch_generator.enable_llm_generation(True)
            logger.debug(f" LLM generation enabled for batch generator")
        
        # æŸ¥æ‰¾è¯¥ç±»å‹çš„æ¨¡æ¿
        task_type = batch['task_type']
        type_templates = [t for t in batch_generator.task_templates if t.task_type == task_type]
        
        if not type_templates:
            # åˆ›å»ºæ¨¡æ¿æ—¶ä¼ é€’use_llmå‚æ•°
            print(f"[WARNING] No template found for {task_type}, creating with LLM={batch['use_llm']}")
            
            # æ ¹æ®ä»»åŠ¡ç±»å‹åˆ›å»ºæ¨¡æ¿
            if hasattr(batch_generator, '_create_basic_templates'):
                templates = batch_generator._create_basic_templates()
                template = templates[0] if templates else None
            else:
                template = self._create_default_template(task_type)
            
            if template:
                type_templates = [template]
            else:
                print(f"[ERROR] Failed to create template for {task_type}")
                return {'tasks': [], 'timing': time.time() - start_time}
        
        # ç”Ÿæˆä»»åŠ¡
        tasks = []
        for i in range(batch['num_tasks']):
            if type_templates:
                template = random.choice(type_templates)
                
                # å¦‚æœå¯ç”¨äº†LLMï¼Œæ¯ä¸ªä»»åŠ¡å®ä¾‹éƒ½ä½¿ç”¨RAG+LLM
                if batch['use_llm'] and batch_generator.use_llm_generation:
                    logger.debug(f" Generating task {i+1}/{batch['num_tasks']} with RAG+LLM")
                
                task = batch_generator.generate_task_instance(template)
                # ç¡®ä¿ä»»åŠ¡ç±»å‹æ­£ç¡®
                task.task_type = task_type
                tasks.append(task)
        
        logger.debug(f" Generated {len(tasks)} {task_type} tasks in {time.time() - start_time:.2f}s")
        
        return {
            'tasks': tasks,
            'timing': time.time() - start_time
        }


    def _analyze_tool_categories(self) -> Dict[str, List[str]]:
        """Analyze available tools by category"""
        categories = {}
        for tool_name, tool in self.tool_library.items():
            category = tool.metadata.get("category", "general")
            if category not in categories:
                categories[category] = []
            categories[category].append(tool_name)
        return categories
    
    def _has_tools(self, operations: List[str]) -> bool:
        """Check if tools with certain operations exist"""
        for tool_name in self.tool_library:
            for op in operations:
                if op in tool_name:
                    return True
        return False
    
    def _find_tools(self, operations: List[str]) -> List[str]:
        """Find tools that match certain operations"""
        matching_tools = []
        for tool_name in self.tool_library:
            for op in operations:
                if op in tool_name:
                    matching_tools.append(tool_name)
                    break
        return matching_tools[:3]  # Limit to 3 tools
    
    def _find_tool_chains(self) -> List[List[str]]:
        """Find tools that can be chained together"""
        chains = []
        
        # Look for natural chains
        parser_tools = self._find_tools(["parser", "reader"])
        transformer_tools = self._find_tools(["transformer", "converter"])
        writer_tools = self._find_tools(["writer", "poster"])
        
        if parser_tools and transformer_tools and writer_tools:
            chains.append([parser_tools[0], transformer_tools[0], writer_tools[0]])
        
        # Look for computation chains
        fetcher_tools = self._find_tools(["fetcher"])
        calculator_tools = self._find_tools(["calculator", "analyzer"])
        
        if fetcher_tools and calculator_tools:
            chains.append([fetcher_tools[0], calculator_tools[0]])
        
        return chains
    
    def _generate_input_data(self, data_type: str) -> Any:
        """Generate sample input data"""
        generators = {
            "raw_data": lambda: {"records": [{"id": i, "value": random.random()} for i in range(10)]},
            "file_paths": lambda: [f"/data/file_{i}.txt" for i in range(3)],
            "api_endpoints": lambda: ["https://api.example.com/data", "https://api.example.com/submit"],
            "auth_credentials": lambda: {"api_key": "sample_key_123", "secret": "sample_secret"},
            "validation_schema": lambda: {"type": "object", "required": ["id", "value"]},
            "transformation_rules": lambda: {"normalize": True, "format": "json"},
            "output_format": lambda: random.choice(["json", "csv", "xml"]),
            "scan_criteria": lambda: {"patterns": ["error", "warning"], "threshold": 0.8},
            "input_data": lambda: {"data": [random.random() for _ in range(5)]},
            "data": lambda: {"values": [random.randint(1, 100) for _ in range(5)]},
            "config": lambda: {"mode": "standard", "options": {}},
            "pipeline_config": lambda: {"stages": 3, "parallel": False},
            "stage_configs": lambda: [{"stage": i, "timeout": 30} for i in range(3)]
        }
        
        generator = generators.get(data_type, lambda: {"placeholder": "data"})
        return generator()
    
    def _generate_expected_output(self, output_type: str) -> Any:
        """Generate expected output structure"""
        generators = {
            "parsed_data": lambda: {"structured": True, "record_count": 10},
            "transformed_data": lambda: {"format": "normalized", "records": []},
            "output_file": lambda: {"path": "/output/result.json", "size": 1024},
            "api_response": lambda: {"status": 200, "data": {}},
            "validated_data": lambda: {"valid": True, "errors": []},
            "file_contents": lambda: {"content": "sample", "encoding": "utf-8"},
            "analysis_results": lambda: {"patterns_found": 3, "metadata": {}},
            "pipeline_result": lambda: {"stages_completed": 3, "final_output": {}},
            "processed_data": lambda: {"processed": True, "result": {}},
            "final_output": lambda: {"success": True, "data": {}},
            "output": lambda: {"result": "completed"},
            "processed_output": lambda: {"status": "complete", "data": {}},
            "pipeline_output": lambda: {"stages": 3, "results": []}
        }
        
        generator = generators.get(output_type, lambda: {"placeholder": "output"})
        return generator()
    
    def _generate_task_constraints(self, complexity: str) -> Dict[str, Any]:
        """Generate task constraints based on complexity"""
        base_constraints = {
            "easy": {"timeout": 60, "max_retries": 3, "required_confidence": 0.8},
            "medium": {"timeout": 300, "max_retries": 2, "required_confidence": 0.9},
            "hard": {"timeout": 600, "max_retries": 1, "required_confidence": 0.95}
        }
        
        constraints = base_constraints.get(complexity, base_constraints["medium"]).copy()
        
        # Add random specific constraints
        if random.random() < 0.3:
            constraints["max_cost"] = random.uniform(0.1, 10.0)
        if random.random() < 0.2:
            constraints["priority"] = random.choice(["low", "medium", "high"])
        
        return constraints


# ===========================
# Main Generator Class
# ===========================

class MCPToolTaskGenerator:
    """Complete MCP tool and task generation system with semantic enhancement"""
    
    def __init__(self, output_dir: str = "mcp_generated_library"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)
        self.tool_generator = ToolGenerator()
        self.tool_library = {}
        self.task_generator = None
        self.tool_registry_path = self.output_dir / "tool_registry_consolidated.json"
    
    def generate_complete_library(self, 
                                num_tools_per_category: int = 5,
                                num_tasks: int = 100) -> Dict[str, Any]:
        """Generate complete tool and task library with semantic enhancement"""
        
        # Generate tools
        print(f"Generating {num_tools_per_category} tools per category...")
        self.tool_library = self.tool_generator.generate_tool_library(num_tools_per_category)
        print(f"Generated {len(self.tool_library)} tools")
        
        # Save tools
        self._save_tools()
        
        # Load or create tool registry with differentiation
        tool_registry = {}
        if self.tool_registry_path.exists():
            with open(self.tool_registry_path, 'r') as f:
                tool_registry = json.load(f)
            print(f"Loaded existing tool registry with differentiation info")
        else:
            # Create basic registry
            for tool_name, tool in self.tool_library.items():
                tool_registry[tool_name] = tool.to_mcp_json()
            print("Created basic tool registry")
        
        # Generate tasks with semantic enhancement
        print(f"\nGenerating {num_tasks} tasks with semantic enhancement...")
        self.task_generator = TaskGenerator(self.tool_library, tool_registry)
        print("TaskGenerator initialized with semantic components")
        tasks = self.task_generator._generate_task_batch(num_tasks)
        print(f"Generated {len(tasks)} tasks")
        
        # Save tasks
        print("Saving tasks...")
        self._save_tasks(tasks)
        print("Tasks saved successfully")
        
        # Generate summary
        summary = self._generate_summary(tasks)
        self._save_summary(summary)
        
        return {
            "tools": self.tool_library,
            "tasks": tasks,
            "summary": summary
        }
    
    def _save_tools(self):
        """Save generated tools in MCP format"""
        # Create directories
        xml_dir = self.output_dir / "tools_xml"
        json_dir = self.output_dir / "tools_json"
        xml_dir.mkdir(exist_ok=True)
        json_dir.mkdir(exist_ok=True)
        
        # Save each tool
        for tool_name, tool in self.tool_library.items():
            # Save XML version
            xml_path = xml_dir / f"{tool_name}.xml"
            with open(xml_path, 'w') as f:
                f.write(tool.to_mcp_xml())
            
            # Save JSON version
            json_path = json_dir / f"{tool_name}.json"
            with open(json_path, 'w') as f:
                json.dump(tool.to_mcp_json(), f, indent=2)
        
        # Save complete registry
        registry = {
            tool_name: tool.to_mcp_json()
            for tool_name, tool in self.tool_library.items()
        }
        with open(self.output_dir / "tool_registry.json", 'w') as f:
            json.dump(registry, f, indent=2)
    
    def _save_tasks(self, tasks: List[TaskInstance]):
        """Save generated tasks with all required fields"""
        tasks_data = []
        
        for task in tasks:
            # ç¡®ä¿æ‰€æœ‰å¿…éœ€å­—æ®µéƒ½å­˜åœ¨
            task_data = {
                # ä½¿ç”¨instance_idä½œä¸ºä¸»é”®ï¼Œä½†ä¹Ÿæä¾›idå­—æ®µä»¥ä¿æŒå…¼å®¹æ€§
                "instance_id": task.instance_id,
                "id": task.instance_id,  # å…¼å®¹æ€§å­—æ®µ
                "task_type": task.task_type,
                "description": task.description,
                
                # è¾“å…¥è¾“å‡ºå­—æ®µ - ç¡®ä¿å…¼å®¹æ€§
                "inputs": task.inputs,
                "test_input": task.inputs,  # å…¼å®¹æ€§å­—æ®µ
                "expected_outputs": task.expected_outputs,
                "expected_output": task.expected_outputs,  # å…¼å®¹æ€§å­—æ®µ
                
                # å·¥å…·å’Œçº¦æŸ
                "required_tools": task.required_tools,
                "constraints": task.constraints,
                "complexity": task.complexity,
                
                # ä¾èµ–ä¿¡æ¯ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                "tool_dependencies": getattr(task, 'tool_dependencies', {}),
                
                # å…ƒæ•°æ®
                "metadata": {
                    **task.metadata,
                    "llm_generated_inputs": task.metadata.get('inputs_generated_from', 'unknown') == 'llm',
                    "llm_generated": task.metadata.get('llm_generated', False),
                    "semantic_generation": task.metadata.get('semantic_generation', False),
                    "generated_at": task.metadata.get('generated_at', datetime.now().isoformat()),
                    "timeout": task.constraints.get('timeout', 300)
                }
            }
            
            tasks_data.append(task_data)
        
        # ä¿å­˜æ‰€æœ‰ä»»åŠ¡
        with open(self.output_dir / "task_library.json", 'w') as f:
            json.dump(tasks_data, f, indent=2)
        
        # ä¿å­˜ä»»åŠ¡ç±»å‹ç´¢å¼•
        tasks_by_type = {}
        for task in tasks:
            if task.task_type not in tasks_by_type:
                tasks_by_type[task.task_type] = []
            tasks_by_type[task.task_type].append({
                "id": task.instance_id,
                "complexity": task.complexity,
                "num_tools": len(task.required_tools),
                "has_llm_inputs": task.metadata.get('inputs_generated_from') == 'llm'
            })
        
        with open(self.output_dir / "tasks_by_type.json", 'w') as f:
            json.dump(tasks_by_type, f, indent=2)
        
        # ä¿å­˜è¾“å…¥è¾“å‡ºç¤ºä¾‹ï¼ˆç”¨äºéªŒè¯ï¼‰
        io_examples = {
            "task_type_examples": {}
        }
        
        # ä¸ºæ¯ç§ä»»åŠ¡ç±»å‹ä¿å­˜å‡ ä¸ªè¾“å…¥è¾“å‡ºç¤ºä¾‹
        for task_type, task_list in tasks_by_type.items():
            examples = []
            # è·å–è¯¥ç±»å‹çš„å‰3ä¸ªä»»åŠ¡ä½œä¸ºç¤ºä¾‹
            example_tasks = [t for t in tasks if t.task_type == task_type][:3]
            
            for task in example_tasks:
                examples.append({
                    "task_id": task.instance_id,
                    "description": task.description[:100] + "..." if len(task.description) > 100 else task.description,
                    "required_tools": task.required_tools,
                    "input_keys": list(task.inputs.keys()),
                    "output_keys": list(task.expected_outputs.keys()),
                    "sample_input": {k: str(v)[:50] + "..." if len(str(v)) > 50 else v 
                                for k, v in list(task.inputs.items())[:3]},
                    "sample_output": {k: str(v)[:50] + "..." if len(str(v)) > 50 else v 
                                    for k, v in list(task.expected_outputs.items())[:3]}
                })
            
            io_examples["task_type_examples"][task_type] = examples
        
        with open(self.output_dir / "io_examples.json", 'w') as f:
            json.dump(io_examples, f, indent=2)
        
        print(f"\nğŸ“ ä»»åŠ¡ä¿å­˜å®Œæˆ:")
        print(f"  - ä¸»ä»»åŠ¡åº“: {self.output_dir / 'task_library.json'}")
        print(f"  - ç±»å‹ç´¢å¼•: {self.output_dir / 'tasks_by_type.json'}")
        print(f"  - IOç¤ºä¾‹: {self.output_dir / 'io_examples.json'}")
    
    def _generate_summary(self, tasks: List[TaskInstance]) -> Dict[str, Any]:
        """Generate summary statistics"""
        # Tool statistics
        tool_stats = {
            "total_tools": len(self.tool_library),
            "tools_by_category": {}
        }
        
        for tool in self.tool_library.values():
            category = tool.metadata.get("category", "general")
            if category not in tool_stats["tools_by_category"]:
                tool_stats["tools_by_category"][category] = 0
            tool_stats["tools_by_category"][category] += 1
        
        # Task statistics
        task_stats = {
            "total_tasks": len(tasks),
            "tasks_by_type": {},
            "tasks_by_complexity": {"easy": 0, "medium": 0, "hard": 0},
            "tool_usage": {}
        }
        
        for task in tasks:
            # By type
            if task.task_type not in task_stats["tasks_by_type"]:
                task_stats["tasks_by_type"][task.task_type] = 0
            task_stats["tasks_by_type"][task.task_type] += 1
            
            # By complexity
            task_stats["tasks_by_complexity"][task.complexity] += 1
            
            # Tool usage
            for tool in task.required_tools:
                if tool not in task_stats["tool_usage"]:
                    task_stats["tool_usage"][tool] = 0
                task_stats["tool_usage"][tool] += 1
        
        return {
            "generation_timestamp": datetime.now().isoformat(),
            "tool_statistics": tool_stats,
            "task_statistics": task_stats
        }
    
    def _save_summary(self, summary: Dict[str, Any]):
        """Save generation summary"""
        with open(self.output_dir / "generation_summary.json", 'w') as f:
            json.dump(summary, f, indent=2)
        
        # Also create human-readable report
        report = [
            "MCP Tool and Task Generation Report",
            "=" * 40,
            f"Generated: {summary['generation_timestamp']}",
            "",
            "Tool Statistics:",
            f"  Total tools: {summary['tool_statistics']['total_tools']}",
            "  By category:"
        ]
        
        for category, count in summary['tool_statistics']['tools_by_category'].items():
            report.append(f"    - {category}: {count}")
        
        report.extend([
            "",
            "Task Statistics:",
            f"  Total tasks: {summary['task_statistics']['total_tasks']}",
            "  By type:"
        ])
        
        for task_type, count in summary['task_statistics']['tasks_by_type'].items():
            report.append(f"    - {task_type}: {count}")
        
        report.extend([
            "  By complexity:",
            f"    - Easy: {summary['task_statistics']['tasks_by_complexity']['easy']}",
            f"    - Medium: {summary['task_statistics']['tasks_by_complexity']['medium']}",
            f"    - Hard: {summary['task_statistics']['tasks_by_complexity']['hard']}",
            "",
            "Top 10 Most Used Tools:"
        ])
        
        tool_usage = sorted(
            summary['task_statistics']['tool_usage'].items(),
            key=lambda x: x[1],
            reverse=True
        )[:10]
        for tool, count in tool_usage:
            report.append(f"  - {tool}: {count} tasks")
        
        with open(self.output_dir / "generation_report.txt", 'w') as f:
            f.write("\n".join(report))


# ===========================
# Wrapper Functions for Compatibility
# ===========================


def create_diverse_tool_library(num_tools: int = 30, tool_registry_path: str = None) -> List[MCPTool]:
    """
    Wrapper function for compatibility with main.py
    Creates a diverse tool library and returns list of tools
    
    Args:
        num_tools: Number of tools to generate
        tool_registry_path: Path to consolidated tool registry with differentiation info
    """
    # Calculate tools per category to reach total
    num_categories = 6  # We have 6 categories
    tools_per_category = max(1, num_tools // num_categories)
    
    logger.info(f"Creating diverse tool library with {num_tools} tools")
    
    # Create generator and generate tools
    generator = ToolGenerator()
    
    # ç¡®ä¿è°ƒç”¨å‰æ¸…ç©ºgenerated_tools
    generator.generated_tools = set()
    
    tool_library = generator.generate_tool_library(tools_per_category)
    
    # Convert to list and ensure we have exactly num_tools
    tools_list = list(tool_library.values())
    
    logger.info(f"Generated {len(tools_list)} tools, adjusting to {num_tools}")
    
    # ä¿®å¤ï¼šé¿å…æ— é™å¾ªç¯
    max_attempts = 100  # é˜²æ­¢æ— é™å¾ªç¯
    attempts = 0
    
    # Add more tools if needed
    while len(tools_list) < num_tools and attempts < max_attempts:
        attempts += 1
        category = random.choice(list(generator.tool_templates.keys()))
        operation = random.choice([op for op, _ in generator.tool_templates[category]])
        
        # ç”Ÿæˆå”¯ä¸€çš„å·¥å…·å
        tool_name = f"{category}_{operation}_{attempts}"
        
        # ç¡®ä¿å·¥å…·åå”¯ä¸€
        if tool_name not in [t.name for t in tools_list]:
            tool = generator.generate_tool(category, operation)
            tool.name = tool_name  # ä½¿ç”¨å”¯ä¸€åç§°
            tools_list.append(tool)
            
            # æ¯ç”Ÿæˆ10ä¸ªå·¥å…·è®°å½•ä¸€æ¬¡
            if len(tools_list) % 10 == 0:
                logger.debug(f"Progress: {len(tools_list)}/{num_tools} tools")
    
    if len(tools_list) < num_tools:
        logger.warning(f"Could only generate {len(tools_list)} tools out of requested {num_tools}")
    
    # Trim if we have too many
    return tools_list[:num_tools]


def generate_multi_tool_tasks(tools: List[MCPTool], num_tasks: int = 200, 
                             task_type: str = None, complexity: str = None,
                             tool_registry_path: str = "mcp_generated_library/tool_registry_consolidated.json") -> List[TaskInstance]:
    """
    Wrapper function for compatibility with main.py
    Generates tasks that use the provided tools with semantic enhancement
    
    Args:
        tools: List of MCPTool objects
        num_tasks: Number of tasks to generate
        task_type: Specific task type to generate
        complexity: Specific complexity level
        tool_registry_path: Path to consolidated tool registry with differentiation info
    """
    logger.info(f"Generating {num_tasks} tasks of type {task_type} with complexity {complexity}")
    
    # Convert tools list to dictionary
    tool_library = {tool.name: tool for tool in tools}
    
    # Load tool registry if available
    tool_registry = {}
    if tool_registry_path and Path(tool_registry_path).exists():
        with open(tool_registry_path, 'r') as f:
            tool_registry = json.load(f)
        logger.info(f"Loaded tool registry with {len(tool_registry)} tools")
    
    # Create task generator with registry
    task_generator = TaskGenerator(tool_library, tool_registry)
    
    # å¦‚æœæŒ‡å®šäº†task_typeï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
    if task_type:
        logger.info(f"Generating specific task type: {task_type}")
        # ä¸´æ—¶å‡å°‘ä»»åŠ¡æ•°é‡ç”¨äºè°ƒè¯•
        if num_tasks > 20:
            logger.warning(f"Reducing tasks from {num_tasks} to 20 for debugging")
            num_tasks = 20
    
    tasks = task_generator._generate_task_batch(num_tasks)
    
    # å¦‚æœæŒ‡å®šäº†task_typeï¼Œéœ€è¦ä¿®æ”¹ä»»åŠ¡ç±»å‹
    if task_type:
        for task in tasks:
            task.task_type = task_type
            if complexity:
                task.complexity = complexity
    
    logger.info(f"Generated {len(tasks)} tasks")
    return tasks

def tool_task_generator(
    num_tasks: int = 500,
    output_filename: str = None,
    task_distribution: Dict[str, float] = None,
    num_tools: int = 50,
    output_dir: str = "mcp_generated_library",
    generation_mode: str = "both"
) -> Dict[str, Any]:
    """
    ç»Ÿä¸€çš„å·¥å…·å’Œä»»åŠ¡ç”Ÿæˆå™¨æ¥å£
    
    Args:
        num_tasks: ç”Ÿæˆä»»åŠ¡æ€»æ•°ï¼Œé»˜è®¤500
        output_filename: è¾“å‡ºæ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰ï¼Œé»˜è®¤ä¸º task_library_{timestamp}
        task_distribution: å„ä»»åŠ¡ç±»å‹çš„æ¯”ä¾‹ï¼Œé»˜è®¤å‡è¡¡åˆ†å¸ƒ
            ä¾‹å¦‚: {
                'basic_task': 0.2,      # 20%
                'simple_task': 0.2,     # 20%
                'data_pipeline': 0.2,   # 20%
                'api_integration': 0.2, # 20%
                'multi_stage_pipeline': 0.2  # 20%
            }
        num_tools: ç”Ÿæˆå·¥å…·æ•°é‡ï¼Œé»˜è®¤50
        output_dir: è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ mcp_generated_library
        generation_mode: ç”Ÿæˆæ¨¡å¼ï¼Œå¯é€‰å€¼ï¼š
            - "both": åŒæ—¶ç”Ÿæˆå·¥å…·å’Œä»»åŠ¡ï¼ˆé»˜è®¤ï¼‰
            - "tools_only": ä»…ç”Ÿæˆå·¥å…·åº“
            - "tasks_only": ä»…ç”Ÿæˆä»»åŠ¡åº“ï¼ˆä½¿ç”¨ç°æœ‰å·¥å…·ï¼‰
    
    Returns:
        DictåŒ…å«:
            - tools_file: å·¥å…·æ³¨å†Œè¡¨æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœç”Ÿæˆäº†å·¥å…·ï¼‰
            - tasks_file: ä»»åŠ¡åº“æ–‡ä»¶è·¯å¾„ï¼ˆå¦‚æœç”Ÿæˆäº†ä»»åŠ¡ï¼‰
            - summary: ç”Ÿæˆæ‘˜è¦ç»Ÿè®¡
    
    Example:
        # é»˜è®¤ç”Ÿæˆå·¥å…·å’Œä»»åŠ¡
        result = tool_task_generator()
        
        # ä»…ç”Ÿæˆå·¥å…·åº“
        result = tool_task_generator(
            generation_mode="tools_only",
            num_tools=100
        )
        
        # ä»…ç”Ÿæˆä»»åŠ¡åº“ï¼ˆä½¿ç”¨ç°æœ‰å·¥å…·ï¼‰
        result = tool_task_generator(
            generation_mode="tasks_only",
            num_tasks=1000,
            task_distribution={
                'basic_task': 0.3,      # 30%
                'simple_task': 0.3,     # 30%
                'data_pipeline': 0.2,   # 20%
                'api_integration': 0.1, # 10%
                'multi_stage_pipeline': 0.1  # 10%
            }
        )
    """
    from pathlib import Path
    import json
    import time
    from datetime import datetime
    
    # éªŒè¯ç”Ÿæˆæ¨¡å¼
    valid_modes = ["both", "tools_only", "tasks_only"]
    if generation_mode not in valid_modes:
        raise ValueError(f"æ— æ•ˆçš„ç”Ÿæˆæ¨¡å¼: {generation_mode}ã€‚æœ‰æ•ˆå€¼: {valid_modes}")
    
    # è®¾ç½®è¾“å‡ºç›®å½•
    output_path = Path(output_dir)
    output_path.mkdir(exist_ok=True)
    
    # è®¾ç½®é»˜è®¤çš„ä»»åŠ¡åˆ†å¸ƒï¼ˆå‡è¡¡ï¼‰
    if task_distribution is None:
        task_distribution = {
            'basic_task': 0.2,
            'simple_task': 0.2,
            'data_pipeline': 0.2,
            'api_integration': 0.2,
            'multi_stage_pipeline': 0.2
        }
    
    # éªŒè¯ä»»åŠ¡åˆ†å¸ƒ
    total_ratio = sum(task_distribution.values())
    if abs(total_ratio - 1.0) > 0.01:
        print(f"è­¦å‘Š: ä»»åŠ¡åˆ†å¸ƒæ€»å’Œä¸º {total_ratio}ï¼Œå°†è¿›è¡Œå½’ä¸€åŒ–å¤„ç†")
        # å½’ä¸€åŒ–
        task_distribution = {k: v/total_ratio for k, v in task_distribution.items()}
    
    # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    if output_filename is None:
        task_filename = f"task_library_{timestamp}.json"
        tool_filename = f"tool_registry_{timestamp}.json"
    else:
        task_filename = f"{output_filename}_tasks.json"
        tool_filename = f"{output_filename}_tools.json"
    
    task_file_path = output_path / task_filename
    tool_file_path = output_path / tool_filename
    
    print(f"ğŸ”§ å¼€å§‹ç”Ÿæˆ...")
    print(f"ğŸ“‹ ç”Ÿæˆæ¨¡å¼: {generation_mode}")
    
    # åˆå§‹åŒ–ç»“æœ
    result = {
        "summary": {
            "generated_at": datetime.now().isoformat(),
            "generation_mode": generation_mode,
            "configuration": {}
        }
    }
    
    tools = []
    tool_library = {}
    
    # 1. ç”Ÿæˆæˆ–åŠ è½½å·¥å…·
    if generation_mode in ["both", "tools_only"]:
        # ç”Ÿæˆæ–°å·¥å…·
        print(f"\nğŸ“¦ ç”Ÿæˆ {num_tools} ä¸ªå·¥å…·...")
        from tool_and_task_generator import create_diverse_tool_library
        tools = create_diverse_tool_library(num_tools=num_tools)
        
        # ä¿å­˜å·¥å…·
        tool_registry = {}
        for tool in tools:
            tool_registry[tool.name] = tool.to_mcp_json()
        
        with open(tool_file_path, 'w') as f:
            json.dump(tool_registry, f, indent=2)
        print(f"âœ… å·¥å…·å·²ä¿å­˜åˆ°: {tool_file_path}")
        
        result["tools_file"] = str(tool_file_path)
        result["summary"]["configuration"]["num_tools"] = num_tools
        
        # è½¬æ¢ä¸ºå­—å…¸ä¾›ä»»åŠ¡ç”Ÿæˆä½¿ç”¨
        tool_library = {tool.name: tool for tool in tools}
    
    elif generation_mode == "tasks_only":
        # åŠ è½½ç°æœ‰å·¥å…·
        print(f"\nğŸ“‚ åŠ è½½ç°æœ‰å·¥å…·...")
        existing_tool_file = output_path / "tool_registry.json"
        
        if not existing_tool_file.exists():
            raise FileNotFoundError(
                f"ä»»åŠ¡ç”Ÿæˆæ¨¡å¼éœ€è¦ç°æœ‰çš„å·¥å…·æ–‡ä»¶: {existing_tool_file}\n"
                "è¯·å…ˆä½¿ç”¨ generation_mode='tools_only' ç”Ÿæˆå·¥å…·ï¼Œæˆ–ä½¿ç”¨ 'both' æ¨¡å¼"
            )
        
        with open(existing_tool_file, 'r') as f:
            tool_registry = json.load(f)
        
        print(f"âœ… å·²åŠ è½½ {len(tool_registry)} ä¸ªå·¥å…·")
        
        # è½¬æ¢ä¸º MCPTool å¯¹è±¡
        from tool_and_task_generator import MCPTool, ToolParameter, ToolReturn, ToolError
        for tool_name, tool_data in tool_registry.items():
            # è½¬æ¢å‚æ•°
            parameters = []
            for param in tool_data.get('parameters', []):
                parameters.append(ToolParameter(
                    name=param.get('name', ''),
                    type=param.get('type', 'string'),
                    description=param.get('description', ''),
                    required=param.get('required', True),
                    default=param.get('default', None),
                    constraints=param.get('constraints', {})
                ))
            
            # è½¬æ¢è¿”å›å€¼
            returns = []
            for ret in tool_data.get('returns', []):
                returns.append(ToolReturn(
                    name=ret.get('name', ''),
                    type=ret.get('type', 'string'),
                    description=ret.get('description', '')
                ))
            
            # è½¬æ¢é”™è¯¯
            errors = []
            for err in tool_data.get('errors', []):
                errors.append(ToolError(
                    code=err.get('code', ''),
                    description=err.get('description', '')
                ))
            
            # åˆ›å»ºå·¥å…·å¯¹è±¡
            tool = MCPTool(
                name=tool_name,
                description=tool_data.get('description', ''),
                parameters=parameters,
                returns=returns,
                errors=errors,
                dependencies=tool_data.get('dependencies', []),
                metadata=tool_data.get('metadata', {})
            )
            # è®¾ç½®ç±»åˆ«ï¼ˆå¦‚æœä¸åœ¨metadataä¸­ï¼‰
            if 'category' not in tool.metadata and 'metadata' in tool_data:
                tool.metadata['category'] = tool_data['metadata'].get('category', 'general')
            tools.append(tool)
            tool_library[tool_name] = tool
    
    # 2. ä¿®æ”¹ TaskGenerator ä»¥æ”¯æŒè‡ªå®šä¹‰åˆ†å¸ƒ
    # è½¬æ¢å·¥å…·åˆ—è¡¨ä¸ºå­—å…¸
    tool_library = {tool.name: tool for tool in tools}
    
    # åŠ è½½å·¥å…·æ³¨å†Œè¡¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    tool_registry_consolidated_path = output_path / "tool_registry_consolidated.json"
    tool_registry_data = {}
    if tool_registry_consolidated_path.exists():
        with open(tool_registry_consolidated_path, 'r') as f:
            tool_registry_data = json.load(f)
    
    # åˆ›å»ºä»»åŠ¡ç”Ÿæˆå™¨
    print(f"\nğŸ“ ç”Ÿæˆ {num_tasks} ä¸ªä»»åŠ¡...")
    task_generator = TaskGenerator(tool_library, tool_registry_data)
    
    # ä¸´æ—¶ä¿®æ”¹ä»»åŠ¡ç”Ÿæˆå™¨çš„åˆ†å¸ƒï¼ˆé€šè¿‡çŒ´å­è¡¥ä¸ï¼‰
    original_generate_task_batch = task_generator._generate_task_batch
    
    def generate_task_batch_with_distribution(self, num_tasks_param: int = 100) -> List[TaskInstance]:
        """ä½¿ç”¨è‡ªå®šä¹‰åˆ†å¸ƒç”Ÿæˆä»»åŠ¡æ‰¹æ¬¡"""
        tasks = []
        
        # ä½¿ç”¨è‡ªå®šä¹‰çš„ä»»åŠ¡åˆ†å¸ƒ
        for task_type, proportion in task_distribution.items():
            num_tasks_of_type = int(num_tasks_param * proportion)
            
            # æŸ¥æ‰¾è¯¥ç±»å‹çš„æ¨¡æ¿
            type_templates = [t for t in self.task_templates if t.task_type == task_type]
            
            if not type_templates:
                # å¦‚æœæ²¡æœ‰ç²¾ç¡®åŒ¹é…çš„æ¨¡æ¿ï¼Œå°è¯•åˆ›å»ºä¸€ä¸ª
                print(f"è­¦å‘Š: æœªæ‰¾åˆ° {task_type} çš„æ¨¡æ¿ï¼Œå°è¯•ç”Ÿæˆ...")
                if task_type == 'basic_task':
                    templates = self._create_basic_templates()
                    template = templates[0] if templates else None
                elif task_type == 'simple_task':
                    templates = self._create_simple_templates(self._analyze_tool_categories())
                    template = templates[0] if templates else None
                elif task_type == 'data_pipeline':
                    template = self._create_data_pipeline_template()
                elif task_type == 'api_integration':
                    template = self._create_api_integration_template()
                elif task_type == 'multi_stage_pipeline':
                    template = self._create_multi_stage_pipeline_template()
                else:
                    print(f"é”™è¯¯: æ— æ³•ç”Ÿæˆ {task_type} ç±»å‹çš„ä»»åŠ¡")
                    continue
                
                if template:
                    type_templates = [template]
            
            # ç”Ÿæˆè¯¥ç±»å‹çš„ä»»åŠ¡
            for i in range(num_tasks_of_type):
                if type_templates:
                    template = random.choice(type_templates)
                    task = self.generate_task_instance(template)
                    # ç¡®ä¿ä»»åŠ¡ç±»å‹æ­£ç¡®
                    task.task_type = task_type
                    tasks.append(task)
                else:
                    print(f"è·³è¿‡ {task_type} ä»»åŠ¡ç”Ÿæˆ")
        
        # å¦‚æœä»»åŠ¡æ•°ä¸è¶³ï¼Œéšæœºè¡¥å……
        while len(tasks) < num_tasks_param:
            task_type = random.choice(list(task_distribution.keys()))
            templates = self._create_basic_templates()  # è¿”å›åˆ—è¡¨
            if templates:
                template = templates[0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡æ¿
                task = self.generate_task_instance(template)
                task.task_type = task_type
                tasks.append(task)
        
        # è£å‰ªå¤šä½™çš„ä»»åŠ¡
        tasks = tasks[:num_tasks_param]
        
        # æ‰“ä¹±é¡ºåº
        random.shuffle(tasks)
        
        return tasks
    
    # åº”ç”¨çŒ´å­è¡¥ä¸
    import types
    task_generator._generate_task_batch = types.MethodType(generate_task_batch_with_distribution, task_generator)
    
    # ç”Ÿæˆä»»åŠ¡
    tasks = task_generator._generate_task_batch(num_tasks)
    
    # 3. ä¿å­˜ä»»åŠ¡
    task_data = []
    for task in tasks:
        task_dict = {
            "id": task.instance_id,
            "task_type": task.task_type,
            "complexity": task.complexity,
            "description": task.description,
            "test_input": task.inputs,
            "expected_output": task.expected_outputs,
            "required_tools": task.required_tools,
            "metadata": task.metadata
        }
        task_data.append(task_dict)
    
    task_library_data = {
        "tasks": task_data,
        "metadata": {
            "generated_at": datetime.now().isoformat(),
            "num_tasks": len(tasks),
            "task_distribution": task_distribution,
            "generator_version": "2.0"
        }
    }
    
    with open(task_file_path, 'w') as f:
        json.dump(task_library_data, f, indent=2)
    print(f"âœ… ä»»åŠ¡å·²ä¿å­˜åˆ°: {task_file_path}")
    
    # 3. ç”Ÿæˆç»Ÿè®¡æ‘˜è¦
    if generation_mode in ["both", "tools_only"] and tools:
        # å·¥å…·ç»Ÿè®¡
        tool_categories = {}
        for tool in tools:
            category = tool.category if hasattr(tool, 'category') else 'general'
            if category not in tool_categories:
                tool_categories[category] = 0
            tool_categories[category] += 1
        
        result["summary"]["results"] = result["summary"].get("results", {})
        result["summary"]["results"]["tools"] = {
            "total": len(tools),
            "by_category": tool_categories,
            "file": str(tool_file_path) if "tools_file" in result else None
        }
    
    if generation_mode in ["both", "tasks_only"] and tasks:
        # ä»»åŠ¡ç»Ÿè®¡
        task_stats = {
            "total": len(tasks),
            "by_type": {},
            "by_complexity": {"easy": 0, "medium": 0, "hard": 0},
            "tool_usage": {}
        }
        
        for task in tasks:
            # æŒ‰ç±»å‹ç»Ÿè®¡
            if task.task_type not in task_stats["by_type"]:
                task_stats["by_type"][task.task_type] = 0
            task_stats["by_type"][task.task_type] += 1
            
            # æŒ‰å¤æ‚åº¦ç»Ÿè®¡
            task_stats["by_complexity"][task.complexity] += 1
            
            # å·¥å…·ä½¿ç”¨ç»Ÿè®¡
            for tool in task.required_tools:
                if tool not in task_stats["tool_usage"]:
                    task_stats["tool_usage"][tool] = 0
                task_stats["tool_usage"][tool] += 1
        
        result["summary"]["results"] = result["summary"].get("results", {})
        result["summary"]["results"]["tasks"] = task_stats
        result["summary"]["results"]["tasks"]["file"] = str(task_file_path) if "tasks_file" in result else None
    
    # ä¿å­˜æ‘˜è¦
    summary_file = output_path / f"generation_summary_{timestamp}.json"
    with open(summary_file, 'w') as f:
        json.dump(result["summary"], f, indent=2)
    
    # æ‰“å°æ‘˜è¦
    print(f"\nğŸ“Š ç”Ÿæˆå®Œæˆ!")
    
    if generation_mode in ["both", "tools_only"] and tools:
        print(f"å·¥å…·æ•°é‡: {len(tools)}")
        if "results" in result["summary"] and "tools" in result["summary"]["results"]:
            print("å·¥å…·ç±»åˆ«åˆ†å¸ƒ:")
            for category, count in result["summary"]["results"]["tools"]["by_category"].items():
                print(f"  - {category}: {count}")
    
    if generation_mode in ["both", "tasks_only"] and tasks:
        print(f"\nä»»åŠ¡æ•°é‡: {len(tasks)}")
        print("ä»»åŠ¡ç±»å‹åˆ†å¸ƒ:")
        for task_type, count in task_stats["by_type"].items():
            percentage = (count / len(tasks)) * 100
            print(f"  - {task_type}: {count} ({percentage:.1f}%)")
        print("\nå¤æ‚åº¦åˆ†å¸ƒ:")
        for complexity, count in task_stats["by_complexity"].items():
            percentage = (count / len(tasks)) * 100
            print(f"  - {complexity}: {count} ({percentage:.1f}%)")
    
    return result




# ===========================
# Example Usage
# ===========================

if __name__ == "__main__":
    # Create generator
    generator = MCPToolTaskGenerator(output_dir="mcp_generated_library")
    
    # Generate complete library
    print("Starting tool and task generation...")
    result = generator.generate_complete_library(
        num_tools_per_category=5,
        num_tasks=200
    )
    
    print("\nGeneration complete!")
    print(f"Generated {len(result['tools'])} tools")
    print(f"Generated {len(result['tasks'])} tasks")
    print(f"\nFiles saved to: mcp_generated_library/")
    print("\nFiles created:")
    print("  - tool_registry.json: Complete tool registry")
    print("  - task_library.json: All task instances")
    print("  - tools_xml/: Individual tool definitions in XML")
    print("  - tools_json/: Individual tool definitions in JSON")
    print("  - generation_summary.json: Detailed statistics")
    print("  - generation_report.txt: Human-readable report")