#!/usr/bin/env python3
"""
éªŒè¯æ•°æ®ä¿å­˜ä¿®å¤
"""

import os
import json
from pathlib import Path
from datetime import datetime

def test_data_save():
    """æµ‹è¯•æ•°æ®ä¿å­˜åŠŸèƒ½"""
    print("="*60)
    print("éªŒè¯æ•°æ®ä¿å­˜ä¿®å¤")
    print("="*60)
    
    # è®°å½•æµ‹è¯•å‰çŠ¶æ€
    db_path = Path("pilot_bench_cumulative_results/master_database.json")
    with open(db_path) as f:
        db_before = json.load(f)
    before_total = db_before['summary'].get('total_tests', 0)
    print(f"æµ‹è¯•å‰: total_tests = {before_total}")
    
    # å¯¼å…¥ä¿®å¤åçš„manager
    from cumulative_test_manager import CumulativeTestManager, TestRecord
    
    manager = CumulativeTestManager()
    
    # åˆ›å»ºæµ‹è¯•è®°å½•ï¼ˆä½¿ç”¨æ­£ç¡®çš„å‚æ•°åï¼‰
    record = TestRecord(
        model="test-fix-verify",
        task_type="simple_task",
        prompt_type="baseline",
        difficulty="easy",
        tool_reliability=0.8,  # ä½¿ç”¨æ­£ç¡®çš„å‚æ•°å
        success=True,
        execution_time=2.5,
        turns=5,
        tool_calls=3
    )
    
    print(f"\næ·»åŠ æµ‹è¯•è®°å½•: {record.model}")
    
    # æ·»åŠ è®°å½•
    try:
        success = manager.add_test_result_with_classification(record)
        print(f"add_test_result_with_classificationè¿”å›: {success}")
    except AttributeError as e:
        print(f"âŒ AttributeError: {e}")
        # å¦‚æœæ–¹æ³•ä¸å­˜åœ¨ï¼Œä½¿ç”¨åŸºç¡€æ–¹æ³•
        success = manager.add_test_result(record)
        print(f"add_test_resultè¿”å›: {success}")
    
    # æ‰‹åŠ¨ä¿å­˜
    manager.save_database()
    print("è°ƒç”¨save_database()å®Œæˆ")
    
    # æ£€æŸ¥ç»“æœ
    with open(db_path) as f:
        db_after = json.load(f)
    after_total = db_after['summary'].get('total_tests', 0)
    print(f"\næµ‹è¯•å: total_tests = {after_total}")
    
    if after_total > before_total:
        print(f"âœ… æ•°æ®å·²ä¿å­˜! æ–°å¢ {after_total - before_total} ä¸ªæµ‹è¯•")
        return True
    else:
        print("âŒ æ•°æ®æœªä¿å­˜!")
        
        # æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨æ•°æ®åº“ä¸­
        if 'test-fix-verify' in db_after.get('models', {}):
            print("  ä½†test-fix-verifyå·²æ·»åŠ åˆ°modelsä¸­")
            model_data = db_after['models']['test-fix-verify']
            print(f"  model_dataç±»å‹: {type(model_data)}")
            if isinstance(model_data, dict):
                print(f"  total_tests: {model_data.get('total_tests', 0)}")
        
        return False

def test_flawed_prompt():
    """æµ‹è¯•flawed promptä¿å­˜"""
    print("\n" + "="*60)
    print("æµ‹è¯•flawed promptæ•°æ®ä¿å­˜")
    print("="*60)
    
    # è®°å½•å‰çŠ¶æ€
    db_path = Path("pilot_bench_cumulative_results/master_database.json")
    with open(db_path) as f:
        db_before = json.load(f)
    
    # è¿è¡Œæµ‹è¯•
    import subprocess
    result = subprocess.run([
        'python', 'smart_batch_runner.py',
        '--model', 'gpt-4o-mini',
        '--prompt-types', 'flawed_redundant_steps',
        '--difficulty', 'easy',
        '--task-types', 'simple_task',
        '--num-instances', '1',
        '--tool-success-rate', '0.8',
        '--max-workers', '5',
        '--no-adaptive',
        '--qps', '5',
        '--batch-commit',
        '--checkpoint-interval', '1',
        '--no-save-logs',
        '--silent'
    ], capture_output=True, text=True, timeout=60)
    
    if result.returncode == 0:
        print("âœ… æµ‹è¯•è¿è¡ŒæˆåŠŸ")
    else:
        print(f"âŒ æµ‹è¯•è¿è¡Œå¤±è´¥ (é€€å‡ºç : {result.returncode})")
        if result.stderr:
            print(f"é”™è¯¯: {result.stderr[-500:]}")
    
    # æ£€æŸ¥ç»“æœ
    with open(db_path) as f:
        db_after = json.load(f)
    
    after_total = db_after['summary'].get('total_tests', 0)
    before_total = db_before['summary'].get('total_tests', 0)
    
    if after_total > before_total:
        print(f"âœ… æ•°æ®å·²ä¿å­˜! æ–°å¢ {after_total - before_total} ä¸ªæµ‹è¯•")
        
        # æ£€æŸ¥flawedæ•°æ®
        if 'gpt-4o-mini' in db_after['models']:
            model_data = db_after['models']['gpt-4o-mini']
            if 'by_prompt_type' in model_data:
                flawed_types = [k for k in model_data['by_prompt_type'].keys() if 'flawed' in k]
                if flawed_types:
                    print(f"\nflawed promptç±»å‹:")
                    for ft in flawed_types:
                        data = model_data['by_prompt_type'][ft]
                        total = data.get('total_tests', 0)
                        if total > 0:
                            print(f"  âœ… {ft}: {total} tests")
                        else:
                            print(f"  âš ï¸ {ft}: 0 tests (é”®å­˜åœ¨ä½†æ— æ•°æ®)")
        return True
    else:
        print("âŒ æ•°æ®æœªæ›´æ–°!")
        return False

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ”§ éªŒè¯æ•°æ®ä¿å­˜ä¿®å¤")
    print(f"æ—¶é—´: {datetime.now()}")
    print(f"STORAGE_FORMAT: {os.environ.get('STORAGE_FORMAT', 'json')}")
    
    # æµ‹è¯•1ï¼šåŸºç¡€åŠŸèƒ½
    test1_success = test_data_save()
    
    # æµ‹è¯•2ï¼šflawed prompt
    test2_success = test_flawed_prompt()
    
    print("\n" + "="*60)
    print("éªŒè¯ç»“æœæ±‡æ€»")
    print("="*60)
    print(f"åŸºç¡€ä¿å­˜æµ‹è¯•: {'âœ… é€šè¿‡' if test1_success else 'âŒ å¤±è´¥'}")
    print(f"Flawedæµ‹è¯•: {'âœ… é€šè¿‡' if test2_success else 'âŒ å¤±è´¥'}")
    
    if test1_success and test2_success:
        print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ•°æ®ä¿å­˜åŠŸèƒ½å·²ä¿®å¤ã€‚")
    else:
        print("\nâš ï¸ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥ã€‚")

if __name__ == "__main__":
    main()