#!/usr/bin/env python3
"""
åˆå¹¶Parquetå¢é‡æ•°æ®åˆ°ä¸»æ–‡ä»¶
"""

import os
from pathlib import Path
from datetime import datetime
import pandas as pd

print("=" * 70)
print("              Parquetæ•°æ®åˆå¹¶å·¥å…·")
print("=" * 70)

# è®¾ç½®è·¯å¾„
data_dir = Path("pilot_bench_parquet_data")
incremental_dir = data_dir / "incremental"
test_results_path = data_dir / "test_results.parquet"

# 1. æ£€æŸ¥å½“å‰çŠ¶æ€
print("\nğŸ“Š å½“å‰çŠ¶æ€ï¼š")
incremental_files = list(incremental_dir.glob("increment_*.parquet"))
print(f"   å¢é‡æ–‡ä»¶æ•°: {len(incremental_files)}")

if test_results_path.exists():
    main_df = pd.read_parquet(test_results_path)
    print(f"   ä¸»æ–‡ä»¶è®°å½•æ•°: {len(main_df)}")
    if 'timestamp' in main_df.columns and len(main_df) > 0:
        last_update = main_df['timestamp'].max()
        print(f"   ä¸»æ–‡ä»¶æœ€åæ›´æ–°: {last_update}")
else:
    print("   ä¸»æ–‡ä»¶ä¸å­˜åœ¨")
    main_df = pd.DataFrame()

# 2. è¯»å–å¢é‡æ–‡ä»¶
if incremental_files:
    print(f"\nğŸ“‚ è¯»å– {len(incremental_files)} ä¸ªå¢é‡æ–‡ä»¶...")
    
    dfs = []
    total_new_records = 0
    
    for file in incremental_files:
        try:
            df = pd.read_parquet(file)
            dfs.append(df)
            total_new_records += len(df)
            print(f"   âœ… {file.name}: {len(df)} æ¡è®°å½•")
        except Exception as e:
            print(f"   âŒ è¯»å–å¤±è´¥ {file.name}: {e}")
    
    if dfs:
        print(f"\n   æ€»è®¡: {total_new_records} æ¡æ–°è®°å½•")
        
        # 3. åˆå¹¶æ•°æ®
        print("\nğŸ”„ åˆå¹¶æ•°æ®...")
        incremental_df = pd.concat(dfs, ignore_index=True)
        
        if not main_df.empty:
            combined_df = pd.concat([main_df, incremental_df], ignore_index=True)
            print(f"   åˆå¹¶å‰: {len(main_df)} + {len(incremental_df)} = {len(combined_df)} æ¡")
        else:
            combined_df = incremental_df
            print(f"   åˆ›å»ºæ–°ä¸»æ–‡ä»¶: {len(combined_df)} æ¡")
        
        # å»é‡ï¼ˆåŸºäºtest_idï¼‰
        if 'test_id' in combined_df.columns:
            before_dedup = len(combined_df)
            combined_df = combined_df.drop_duplicates(subset=['test_id'], keep='last')
            after_dedup = len(combined_df)
            if before_dedup != after_dedup:
                print(f"   å»é‡: {before_dedup} -> {after_dedup} æ¡")
        
        # 4. å¤‡ä»½ä¸»æ–‡ä»¶
        if test_results_path.exists():
            backup_path = test_results_path.with_suffix('.parquet.backup')
            print(f"\nğŸ’¾ å¤‡ä»½ä¸»æ–‡ä»¶åˆ°: {backup_path.name}")
            import shutil
            shutil.copy2(test_results_path, backup_path)
        
        # 5. ä¿å­˜åˆå¹¶åçš„æ•°æ®
        print(f"\nğŸ’¾ ä¿å­˜åˆå¹¶åçš„æ•°æ®...")
        combined_df.to_parquet(test_results_path, index=False)
        print(f"   âœ… å·²ä¿å­˜ {len(combined_df)} æ¡è®°å½•åˆ°ä¸»æ–‡ä»¶")
        
        # 6. æ¸…ç†å¢é‡æ–‡ä»¶
        print(f"\nğŸ—‘ï¸ æ¸…ç†å¢é‡æ–‡ä»¶...")
        cleaned = 0
        for file in incremental_files:
            try:
                file.unlink()
                cleaned += 1
            except Exception as e:
                print(f"   âš ï¸ æ— æ³•åˆ é™¤ {file.name}: {e}")
        print(f"   âœ… å·²æ¸…ç† {cleaned}/{len(incremental_files)} ä¸ªæ–‡ä»¶")
        
        # 7. éªŒè¯ç»“æœ
        print(f"\nâœ… åˆå¹¶å®Œæˆï¼")
        final_df = pd.read_parquet(test_results_path)
        print(f"   æœ€ç»ˆè®°å½•æ•°: {len(final_df)}")
        if 'timestamp' in final_df.columns and len(final_df) > 0:
            print(f"   æœ€æ–°è®°å½•æ—¶é—´: {final_df['timestamp'].max()}")
            
        # æ˜¾ç¤ºä¸€äº›ç»Ÿè®¡
        if 'model' in final_df.columns:
            model_counts = final_df['model'].value_counts()
            print(f"\n   æ¨¡å‹åˆ†å¸ƒ:")
            for model, count in model_counts.head(5).items():
                print(f"     - {model}: {count} æ¡")
    else:
        print("\nâš ï¸ æ²¡æœ‰æˆåŠŸè¯»å–ä»»ä½•å¢é‡æ–‡ä»¶")
else:
    print("\nâœ… æ²¡æœ‰éœ€è¦åˆå¹¶çš„å¢é‡æ–‡ä»¶")

print("\n" + "=" * 70)
print("æç¤ºï¼š")
print("  - åˆå¹¶æ“ä½œä¼šå°†æ‰€æœ‰å¢é‡æ–‡ä»¶çš„æ•°æ®æ·»åŠ åˆ°ä¸»æ–‡ä»¶")
print("  - ä¸»æ–‡ä»¶è·¯å¾„: pilot_bench_parquet_data/test_results.parquet")
print("  - å¯ä»¥å®šæœŸè¿è¡Œæ­¤è„šæœ¬ä»¥ä¿æŒä¸»æ–‡ä»¶æœ€æ–°")
print("  - ä¹Ÿå¯ä»¥åœ¨ smart_batch_runner ä¸­è°ƒç”¨ finalize() æ¥è§¦å‘åˆå¹¶")
print("=" * 70)