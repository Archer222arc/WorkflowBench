#!/usr/bin/env python3
"""
å¢å¼ºçš„å¤±è´¥æµ‹è¯•è‡ªåŠ¨ç®¡ç†å™¨
- è‡ªåŠ¨æ£€æµ‹å’Œè®°å½•æµ‹è¯•å¤±è´¥
- ç»´æŠ¤å¤±è´¥è®°å½•çŠ¶æ€
- æ”¯æŒè¿›åº¦åŒæ­¥å’ŒçŠ¶æ€æ›´æ–°
"""

import json
import os
import time
import subprocess
import threading
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
from contextlib import contextmanager

@dataclass
class FailedTest:
    """å¤±è´¥æµ‹è¯•è®°å½•"""
    model: str
    group_name: str
    prompt_types: str
    test_type: str
    failure_reason: str
    failure_time: str
    retry_count: int = 0
    max_retries: int = 3
    status: str = "failed"  # failed, retrying, completed, abandoned
    
    def to_dict(self):
        return asdict(self)

@dataclass 
class TestProgress:
    """æµ‹è¯•è¿›åº¦è®°å½•"""
    step: str
    model_index: int
    substep: str
    current_model: str = ""
    current_group: str = ""
    total_models: int = 0
    completed_models: int = 0
    failed_models: List[str] = None
    
    def __post_init__(self):
        if self.failed_models is None:
            self.failed_models = []

class EnhancedFailedTestsManager:
    """å¢å¼ºçš„å¤±è´¥æµ‹è¯•ç®¡ç†å™¨"""
    
    def __init__(self, config_file=None, progress_file=None, model_type=None):
        # æ ¹æ®æ¨¡å‹ç±»å‹é€‰æ‹©é…ç½®æ–‡ä»¶
        if config_file is None:
            # ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡ä¸­çš„æ¨¡å‹ç±»å‹
            if model_type is None:
                model_type = os.environ.get('MODEL_TYPE', 'opensource')
            
            if model_type == 'closed_source':
                config_file = "failed_tests_config_closed_source.json"
            else:
                config_file = "failed_tests_config_opensource.json"
        
        if progress_file is None:
            if model_type is None:
                model_type = os.environ.get('MODEL_TYPE', 'opensource')
            
            if model_type == 'closed_source':
                progress_file = "test_progress_closed_source.txt"
            else:
                progress_file = "test_progress_opensource.txt"
        
        self.config_file = Path(config_file)
        self.progress_file = Path(progress_file)
        self.model_type = model_type or os.environ.get('MODEL_TYPE', 'opensource')
        self.lock = threading.Lock()
        self._load_config()
        
    def _load_config(self):
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if self.config_file.exists():
            try:
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    self.config = json.load(f)
            except Exception as e:
                print(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
                self._create_empty_config()
        else:
            self._create_empty_config()
    
    def _create_empty_config(self):
        """åˆ›å»ºç©ºçš„é…ç½®æ–‡ä»¶ç»“æ„"""
        self.config = {
            "version": "2.0",
            "created_at": datetime.now().isoformat(),
            "last_updated": datetime.now().isoformat(),
            "active_session": {
                "session_id": datetime.now().strftime("%Y%m%d_%H%M%S"),
                "start_time": datetime.now().isoformat(),
                "status": "active",  # active, completed, paused
                "total_failed_tests": 0,
                "failed_tests": {},  # model -> [FailedTest]
                "completed_retests": {},
                "abandoned_tests": {}
            },
            "progress_tracking": {
                "current_step": "",
                "current_model": "",
                "completed_steps": [],
                "failed_steps": [],
                "retry_queue": []
            },
            "statistics": {
                "total_tests_run": 0,
                "total_failures": 0,
                "total_retries": 0,
                "total_recoveries": 0,
                "failure_rate": 0.0,
                "recovery_rate": 0.0
            }
        }
    
    def _save_config(self):
        """ä¿å­˜é…ç½®æ–‡ä»¶"""
        with self.lock:
            self.config["last_updated"] = datetime.now().isoformat()
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(self.config, f, indent=2, ensure_ascii=False)
    
    def record_test_failure(self, model: str, group_name: str, prompt_types: str, 
                          test_type: str, failure_reason: str, context: Dict = None):
        """è®°å½•æµ‹è¯•å¤±è´¥"""
        failed_test = FailedTest(
            model=model,
            group_name=group_name,
            prompt_types=prompt_types,
            test_type=test_type,
            failure_reason=failure_reason,
            failure_time=datetime.now().isoformat()
        )
        
        session = self.config["active_session"]
        if model not in session["failed_tests"]:
            session["failed_tests"][model] = []
        
        session["failed_tests"][model].append(failed_test.to_dict())
        session["total_failed_tests"] += 1
        
        # æ›´æ–°ç»Ÿè®¡
        stats = self.config["statistics"]
        stats["total_failures"] += 1
        stats["total_tests_run"] = stats.get("total_tests_run", 0) + 1
        if stats["total_tests_run"] > 0:
            stats["failure_rate"] = stats["total_failures"] / stats["total_tests_run"]
        
        self._save_config()
        
        print(f"ğŸ”´ è®°å½•å¤±è´¥: {model} - {group_name}")
        print(f"   åŸå› : {failure_reason}")
        print(f"   æ—¶é—´: {failed_test.failure_time}")
        
        return failed_test
    
    def record_test_success(self, model: str, group_name: str, was_retry: bool = False):
        """è®°å½•æµ‹è¯•æˆåŠŸï¼ˆç‰¹åˆ«æ˜¯é‡æµ‹æˆåŠŸï¼‰"""
        if was_retry:
            session = self.config["active_session"]
            if model not in session["completed_retests"]:
                session["completed_retests"][model] = []
            
            session["completed_retests"][model].append({
                "group_name": group_name,
                "completed_time": datetime.now().isoformat(),
                "retry_success": True
            })
            
            # æ›´æ–°ç»Ÿè®¡
            stats = self.config["statistics"]
            stats["total_recoveries"] += 1
            if stats["total_failures"] > 0:
                stats["recovery_rate"] = stats["total_recoveries"] / stats["total_failures"]
            
            print(f"ğŸŸ¢ é‡æµ‹æˆåŠŸ: {model} - {group_name}")
        
        self._save_config()
    
    def get_failed_tests_for_model(self, model: str) -> List[Dict]:
        """è·å–æŸä¸ªæ¨¡å‹çš„å¤±è´¥æµ‹è¯•"""
        session = self.config["active_session"]
        return session["failed_tests"].get(model, [])
    
    def get_all_failed_models(self) -> List[str]:
        """è·å–æ‰€æœ‰æœ‰å¤±è´¥æµ‹è¯•çš„æ¨¡å‹"""
        session = self.config["active_session"]
        return list(session["failed_tests"].keys())
    
    def has_failed_tests(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰å¤±è´¥çš„æµ‹è¯•"""
        session = self.config["active_session"]
        return session["total_failed_tests"] > 0
    
    def get_retry_queue(self) -> List[Dict]:
        """è·å–é‡è¯•é˜Ÿåˆ—"""
        retry_queue = []
        session = self.config["active_session"]
        
        for model, failed_tests in session["failed_tests"].items():
            for test in failed_tests:
                # åŒ…å«çŠ¶æ€ä¸º "failed" æˆ– "retrying" çš„æµ‹è¯•ï¼Œä¸”æœªè¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°
                if test["status"] in ["failed", "retrying"] and test["retry_count"] < test["max_retries"]:
                    retry_queue.append({
                        "model": model,
                        "test": test,
                        "priority": test["retry_count"]  # é‡è¯•æ¬¡æ•°å°‘çš„ä¼˜å…ˆ
                    })
        
        # æŒ‰ä¼˜å…ˆçº§æ’åº
        retry_queue.sort(key=lambda x: x["priority"])
        return retry_queue
    
    def mark_test_for_retry(self, model: str, group_name: str):
        """æ ‡è®°æµ‹è¯•ä¸ºé‡è¯•çŠ¶æ€"""
        session = self.config["active_session"]
        if model in session["failed_tests"]:
            for test in session["failed_tests"][model]:
                if test["group_name"] == group_name:
                    test["status"] = "retrying"
                    test["retry_count"] += 1
                    test["last_retry_time"] = datetime.now().isoformat()
                    break
        
        # æ›´æ–°ç»Ÿè®¡
        stats = self.config["statistics"]
        stats["total_retries"] += 1
        
        self._save_config()
    
    def abandon_test(self, model: str, group_name: str, reason: str):
        """æ”¾å¼ƒæµ‹è¯•ï¼ˆè¶…è¿‡æœ€å¤§é‡è¯•æ¬¡æ•°ï¼‰"""
        session = self.config["active_session"]
        if model in session["failed_tests"]:
            for test in session["failed_tests"][model]:
                if test["group_name"] == group_name:
                    test["status"] = "abandoned"
                    test["abandon_reason"] = reason
                    test["abandon_time"] = datetime.now().isoformat()
                    
                    # ç§»åŠ¨åˆ°æ”¾å¼ƒåˆ—è¡¨
                    if model not in session["abandoned_tests"]:
                        session["abandoned_tests"][model] = []
                    session["abandoned_tests"][model].append(test)
                    break
        
        self._save_config()
        print(f"ğŸš« æ”¾å¼ƒæµ‹è¯•: {model} - {group_name} ({reason})")
    
    def update_progress(self, step: str, model_index: int, substep: str, 
                       current_model: str = "", current_group: str = ""):
        """æ›´æ–°æµ‹è¯•è¿›åº¦"""
        progress = self.config["progress_tracking"]
        progress.update({
            "current_step": step,
            "current_model": current_model,
            "current_group": current_group,
            "model_index": model_index,
            "substep": substep,
            "last_update": datetime.now().isoformat()
        })
        
        self._save_config()
    
    def generate_retry_script(self, output_file: str = "auto_retry_failed_tests.sh"):
        """ç”Ÿæˆè‡ªåŠ¨é‡è¯•è„šæœ¬"""
        retry_queue = self.get_retry_queue()
        
        if not retry_queue:
            print("æ²¡æœ‰éœ€è¦é‡è¯•çš„æµ‹è¯•")
            return
        
        script_content = f'''#!/bin/bash

# è‡ªåŠ¨ç”Ÿæˆçš„å¤±è´¥æµ‹è¯•é‡è¯•è„šæœ¬
# ç”Ÿæˆæ—¶é—´: {datetime.now().isoformat()}
# å¾…é‡è¯•æµ‹è¯•æ•°: {len(retry_queue)}

set -e

echo "å¼€å§‹è‡ªåŠ¨é‡è¯•å¤±è´¥çš„æµ‹è¯•..."
echo "æ€»å…± {len(retry_queue)} ä¸ªæµ‹è¯•éœ€è¦é‡è¯•"
echo ""

'''
        
        for i, item in enumerate(retry_queue, 1):
            model = item["model"]
            test = item["test"]
            
            script_content += f'''
echo "=== é‡è¯• {i}/{len(retry_queue)}: {model} - {test["group_name"]} ==="
echo "é‡è¯•åŸå› : {test["failure_reason"]}"
echo "é‡è¯•æ¬¡æ•°: {test["retry_count"] + 1}/{test["max_retries"]}"

# æ ‡è®°ä¸ºé‡è¯•çŠ¶æ€
python3 -c "
from enhanced_failed_tests_manager import EnhancedFailedTestsManager
manager = EnhancedFailedTestsManager()
manager.mark_test_for_retry('{model}', '{test["group_name"]}')
"

# æ‰§è¡Œé‡è¯•
if python ultra_parallel_runner.py \\
    --model "{model}" \\
    --prompt-types "{test["prompt_types"]}" \\
    --difficulty easy \\
    --task-types all \\
    --num-instances 20; then
    
    echo "âœ… é‡è¯•æˆåŠŸ: {model} - {test["group_name"]}"
    
    # æ ‡è®°ä¸ºæˆåŠŸ
    python3 -c "
from enhanced_failed_tests_manager import EnhancedFailedTestsManager
manager = EnhancedFailedTestsManager()
manager.record_test_success('{model}', '{test["group_name"]}', was_retry=True)
"
else
    echo "âŒ é‡è¯•å¤±è´¥: {model} - {test["group_name"]}"
    
    # æ£€æŸ¥æ˜¯å¦éœ€è¦æ”¾å¼ƒ
    python3 -c "
from enhanced_failed_tests_manager import EnhancedFailedTestsManager
manager = EnhancedFailedTestsManager()
test = manager.get_failed_tests_for_model('{model}')
for t in test:
    if t['group_name'] == '{test["group_name"]}' and t['retry_count'] >= t['max_retries']:
        manager.abandon_test('{model}', '{test["group_name"]}', 'Max retries exceeded')
        break
"
fi

echo ""
'''
        
        script_content += '''
echo "è‡ªåŠ¨é‡è¯•å®Œæˆ"
echo ""
echo "æŸ¥çœ‹æœ€ç»ˆçŠ¶æ€:"
python3 -c "
from enhanced_failed_tests_manager import EnhancedFailedTestsManager
manager = EnhancedFailedTestsManager()
print(f'æ€»å¤±è´¥æ•°: {manager.config[\"statistics\"][\"total_failures\"]}')
print(f'æ€»æ¢å¤æ•°: {manager.config[\"statistics\"][\"total_recoveries\"]}')
print(f'æ¢å¤ç‡: {manager.config[\"statistics\"][\"recovery_rate\"]:.1%}')
"
'''
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(script_content)
        
        os.chmod(output_file, 0o755)
        print(f"âœ… ç”Ÿæˆé‡è¯•è„šæœ¬: {output_file}")
        return output_file
    
    def show_status_report(self):
        """æ˜¾ç¤ºçŠ¶æ€æŠ¥å‘Š"""
        print("=" * 50)
        print("ğŸ“Š å¤±è´¥æµ‹è¯•ç®¡ç†å™¨çŠ¶æ€æŠ¥å‘Š")
        print("=" * 50)
        
        session = self.config["active_session"]
        stats = self.config["statistics"]
        
        print(f"ä¼šè¯ID: {session['session_id']}")
        print(f"ä¼šè¯çŠ¶æ€: {session['status']}")
        print(f"å¼€å§‹æ—¶é—´: {session['start_time']}")
        print()
        
        print("ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
        print(f"  æ€»æµ‹è¯•æ•°: {stats['total_tests_run']}")
        print(f"  æ€»å¤±è´¥æ•°: {stats['total_failures']}")
        print(f"  æ€»é‡è¯•æ•°: {stats['total_retries']}")
        print(f"  æ€»æ¢å¤æ•°: {stats['total_recoveries']}")
        print(f"  å¤±è´¥ç‡: {stats['failure_rate']:.1%}")
        print(f"  æ¢å¤ç‡: {stats['recovery_rate']:.1%}")
        print()
        
        if session["failed_tests"]:
            print("ğŸ”´ å¤±è´¥çš„æ¨¡å‹å’Œæµ‹è¯•:")
            for model, tests in session["failed_tests"].items():
                active_failures = [t for t in tests if t["status"] == "failed"]
                if active_failures:
                    print(f"  {model}: {len(active_failures)} ä¸ªå¤±è´¥æµ‹è¯•")
                    for test in active_failures:
                        print(f"    - {test['group_name']} ({test['failure_reason']})")
        
        retry_queue = self.get_retry_queue()
        if retry_queue:
            print(f"\nâ³ å¾…é‡è¯•é˜Ÿåˆ—: {len(retry_queue)} ä¸ªæµ‹è¯•")
        
        if session.get("completed_retests"):
            print(f"\nğŸŸ¢ æˆåŠŸé‡æµ‹: {len(session['completed_retests'])} ä¸ªæ¨¡å‹")
        
        if session.get("abandoned_tests"):
            print(f"\nğŸš« å·²æ”¾å¼ƒ: {len(session['abandoned_tests'])} ä¸ªæ¨¡å‹")

def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    import sys
    
    manager = EnhancedFailedTestsManager()
    
    if len(sys.argv) < 2:
        manager.show_status_report()
        return
    
    command = sys.argv[1]
    
    if command == "status":
        manager.show_status_report()
    elif command == "retry":
        script_file = manager.generate_retry_script()
        print(f"è¿è¡Œé‡è¯•è„šæœ¬: ./{script_file}")
    elif command == "queue":
        retry_queue = manager.get_retry_queue()
        print(f"é‡è¯•é˜Ÿåˆ—: {len(retry_queue)} ä¸ªæµ‹è¯•")
        for item in retry_queue:
            test = item["test"]
            print(f"  {item['model']} - {test['group_name']} (é‡è¯• {test['retry_count']}/{test['max_retries']})")
    else:
        print(f"æœªçŸ¥å‘½ä»¤: {command}")
        print("å¯ç”¨å‘½ä»¤: status, retry, queue")

if __name__ == "__main__":
    main()