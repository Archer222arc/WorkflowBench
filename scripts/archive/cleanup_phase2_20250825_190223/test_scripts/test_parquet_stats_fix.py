#!/usr/bin/env python3
"""
æµ‹è¯•Parquetç»Ÿè®¡ä¿®å¤æ•ˆæœ
éªŒè¯Parquetç°åœ¨ä½¿ç”¨ä¸enhancedç›¸åŒçš„ç»Ÿè®¡æ–¹æ³•
"""

import os
import sys
from pathlib import Path

# è®¾ç½®Parquetå­˜å‚¨æ ¼å¼
os.environ['STORAGE_FORMAT'] = 'parquet'

def test_parquet_statistics():
    """æµ‹è¯•Parquetç»Ÿè®¡è®¡ç®—æ˜¯å¦ä¸enhancedä¸€è‡´"""
    
    print("=" * 60)
    print("æµ‹è¯•Parquetç»Ÿè®¡ä¿®å¤æ•ˆæœ")
    print("=" * 60)
    
    # å¯¼å…¥ç®¡ç†å™¨
    from parquet_cumulative_manager import ParquetCumulativeManager
    from cumulative_test_manager import TestRecord
    
    # åˆ›å»ºç®¡ç†å™¨
    manager = ParquetCumulativeManager()
    print("âœ… åˆ›å»ºParquetCumulativeManageræˆåŠŸ")
    
    # åˆ›å»ºæµ‹è¯•è®°å½•
    test_records = []
    
    # 1. å®Œå…¨æˆåŠŸçš„è®°å½•
    record1 = TestRecord(
        model='test-model-stats',
        task_type='simple_task',
        prompt_type='baseline',
        difficulty='easy'
    )
    record1.tool_success_rate = 0.8
    record1.success = True
    record1.success_level = 'full_success'
    record1.execution_time = 2.5
    record1.turns = 5
    record1.tool_calls = 3
    record1.required_tools = ['tool1', 'tool2', 'tool3']
    record1.executed_tools = ['tool1', 'tool2', 'tool3']
    record1.tool_coverage_rate = 1.0
    record1.workflow_score = 0.9
    record1.phase2_score = 0.85
    record1.quality_score = 0.88
    record1.final_score = 0.87
    record1.error_message = None
    test_records.append(record1)
    
    # 2. éƒ¨åˆ†æˆåŠŸçš„è®°å½•ï¼ˆæœ‰format errorï¼‰
    record2 = TestRecord(
        model='test-model-stats',
        task_type='simple_task',
        prompt_type='baseline',
        difficulty='easy'
    )
    record2.tool_success_rate = 0.8
    record2.success = True
    record2.success_level = 'partial_success'
    record2.partial_success = True
    record2.execution_time = 4.0
    record2.turns = 8
    record2.tool_calls = 2
    record2.required_tools = ['tool1', 'tool2', 'tool3']
    record2.executed_tools = ['tool1', 'tool2']
    record2.tool_coverage_rate = 0.67
    record2.workflow_score = 0.6
    record2.phase2_score = 0.5
    record2.quality_score = 0.55
    record2.final_score = 0.55
    record2.format_error_count = 2  # æœ‰è¾…åŠ©
    record2.error_message = "Format errors detected"
    record2.ai_error_category = 'format'  # AIåˆ†ç±»ç»“æœ
    test_records.append(record2)
    
    # 3. å¤±è´¥çš„è®°å½•ï¼ˆtimeoutï¼‰
    record3 = TestRecord(
        model='test-model-stats',
        task_type='simple_task',
        prompt_type='baseline',
        difficulty='easy'
    )
    record3.tool_success_rate = 0.8
    record3.success = False
    record3.partial_success = False  # æ˜ç¡®è®¾ç½®ä¸ºFalse
    record3.success_level = 'failure'  # æ˜ç¡®è®¾ç½®å¤±è´¥çº§åˆ«
    record3.execution_time = 10.0
    record3.turns = 10
    record3.tool_calls = 1
    record3.required_tools = ['tool1', 'tool2', 'tool3']
    record3.executed_tools = ['tool1']
    record3.tool_coverage_rate = 0.33
    record3.workflow_score = 0.2
    record3.phase2_score = 0.1
    record3.quality_score = 0.15
    record3.final_score = 0.15
    record3.error_message = "Test timeout after 10 minutes"
    record3.ai_error_category = 'timeout'
    test_records.append(record3)
    
    # æ·»åŠ æ‰€æœ‰æµ‹è¯•è®°å½•
    print("\næ·»åŠ æµ‹è¯•è®°å½•...")
    for i, record in enumerate(test_records, 1):
        success = manager.add_test_result_with_classification(record)
        print(f"  è®°å½• {i}: {'âœ…' if success else 'âŒ'}")
    
    # åˆ·æ–°ç¼“å†²åŒº
    manager._flush_buffer()
    print("\nâœ… åˆ·æ–°ç¼“å†²åŒºæˆåŠŸ")
    
    # æ£€æŸ¥ç»Ÿè®¡ç»“æœ
    print("\néªŒè¯ç»Ÿè®¡è®¡ç®—...")
    
    # ä»ç¼“å­˜æˆ–æ•°æ®åº“è·å–ç»Ÿè®¡
    from parquet_data_manager import ParquetDataManager
    pdm = ParquetDataManager()
    df = pdm.query_model_stats(model_name='test-model-stats')
    
    if not df.empty:
        print(f"\næ‰¾åˆ° {len(df)} æ¡æ±‡æ€»è®°å½•")
        
        # æ£€æŸ¥æœ€æ–°çš„æ±‡æ€»
        latest = df.iloc[-1]
        
        # éªŒè¯å…³é”®ç»Ÿè®¡
        checks = []
        
        # 1. æˆåŠŸç»Ÿè®¡
        total = latest.get('total', 0)
        success = latest.get('success', 0)
        full_success = latest.get('full_success', 0)
        partial_success = latest.get('partial_success', 0)
        failed = latest.get('failed', 0)
        
        checks.append(("æ€»æ•°", total == 3, f"{total} (æœŸæœ›: 3)"))
        checks.append(("æˆåŠŸæ•°", success == 2, f"{success} (æœŸæœ›: 2)"))
        checks.append(("å®Œå…¨æˆåŠŸ", full_success == 1, f"{full_success} (æœŸæœ›: 1)"))
        checks.append(("éƒ¨åˆ†æˆåŠŸ", partial_success == 1, f"{partial_success} (æœŸæœ›: 1)"))
        checks.append(("å¤±è´¥æ•°", failed == 1, f"{failed} (æœŸæœ›: 1)"))
        
        # 2. æˆåŠŸç‡
        success_rate = latest.get('success_rate', 0)
        weighted_score = latest.get('weighted_success_score', 0)
        
        checks.append(("æˆåŠŸç‡", abs(success_rate - 0.667) < 0.01, f"{success_rate:.3f} (æœŸæœ›: 0.667)"))
        checks.append(("åŠ æƒæˆåŠŸåˆ†", abs(weighted_score - 0.5) < 0.01, f"{weighted_score:.3f} (æœŸæœ›: 0.5)"))
        
        # 3. æ‰§è¡Œç»Ÿè®¡ï¼ˆå¹³å‡å€¼ï¼‰
        avg_time = latest.get('avg_execution_time', 0)
        avg_turns = latest.get('avg_turns', 0)
        avg_calls = latest.get('avg_tool_calls', 0)
        tool_coverage = latest.get('tool_coverage_rate', 0)
        
        checks.append(("å¹³å‡æ‰§è¡Œæ—¶é—´", abs(avg_time - 5.5) < 0.1, f"{avg_time:.1f} (æœŸæœ›: 5.5)"))
        checks.append(("å¹³å‡è½®æ•°", abs(avg_turns - 7.67) < 0.1, f"{avg_turns:.1f} (æœŸæœ›: 7.67)"))
        checks.append(("å¹³å‡å·¥å…·è°ƒç”¨", abs(avg_calls - 2.0) < 0.1, f"{avg_calls:.1f} (æœŸæœ›: 2.0)"))
        checks.append(("å·¥å…·è¦†ç›–ç‡", abs(tool_coverage - 0.667) < 0.01, f"{tool_coverage:.3f} (æœŸæœ›: 0.667)"))
        
        # 4. è´¨é‡åˆ†æ•°
        avg_workflow = latest.get('avg_workflow_score', 0)
        avg_phase2 = latest.get('avg_phase2_score', 0)
        avg_quality = latest.get('avg_quality_score', 0)
        avg_final = latest.get('avg_final_score', 0)
        
        checks.append(("workflowåˆ†æ•°", abs(avg_workflow - 0.567) < 0.01, f"{avg_workflow:.3f} (æœŸæœ›: 0.567)"))
        checks.append(("phase2åˆ†æ•°", abs(avg_phase2 - 0.483) < 0.01, f"{avg_phase2:.3f} (æœŸæœ›: 0.483)"))
        checks.append(("qualityåˆ†æ•°", abs(avg_quality - 0.527) < 0.01, f"{avg_quality:.3f} (æœŸæœ›: 0.527)"))
        checks.append(("finalåˆ†æ•°", abs(avg_final - 0.523) < 0.01, f"{avg_final:.3f} (æœŸæœ›: 0.523)"))
        
        # 5. é”™è¯¯ç»Ÿè®¡
        total_errors = latest.get('total_errors', 0)
        format_errors = latest.get('tool_call_format_errors', 0)
        timeout_errors = latest.get('timeout_errors', 0)
        
        checks.append(("æ€»é”™è¯¯æ•°", total_errors == 2, f"{total_errors} (æœŸæœ›: 2)"))
        checks.append(("æ ¼å¼é”™è¯¯", format_errors == 1, f"{format_errors} (æœŸæœ›: 1)"))
        checks.append(("è¶…æ—¶é”™è¯¯", timeout_errors == 1, f"{timeout_errors} (æœŸæœ›: 1)"))
        
        # 6. é”™è¯¯ç‡ï¼ˆåŸºäºæ€»é”™è¯¯ï¼‰
        format_rate = latest.get('format_error_rate', 0)
        timeout_rate = latest.get('timeout_error_rate', 0)
        
        checks.append(("æ ¼å¼é”™è¯¯ç‡", abs(format_rate - 0.5) < 0.01, f"{format_rate:.3f} (æœŸæœ›: 0.5)"))
        checks.append(("è¶…æ—¶é”™è¯¯ç‡", abs(timeout_rate - 0.5) < 0.01, f"{timeout_rate:.3f} (æœŸæœ›: 0.5)"))
        
        # 7. è¾…åŠ©ç»Ÿè®¡
        tests_with_assist = latest.get('tests_with_assistance', 0)
        assisted_success = latest.get('assisted_success', 0)
        assisted_turns = latest.get('total_assisted_turns', 0)
        assistance_rate = latest.get('assistance_rate', 0)
        
        checks.append(("è¾…åŠ©æµ‹è¯•æ•°", tests_with_assist == 1, f"{tests_with_assist} (æœŸæœ›: 1)"))
        checks.append(("è¾…åŠ©æˆåŠŸæ•°", assisted_success == 1, f"{assisted_success} (æœŸæœ›: 1)"))
        checks.append(("è¾…åŠ©è½®æ•°", assisted_turns == 2, f"{assisted_turns} (æœŸæœ›: 2)"))
        checks.append(("è¾…åŠ©ç‡", abs(assistance_rate - 0.333) < 0.01, f"{assistance_rate:.3f} (æœŸæœ›: 0.333)"))
        
        # æ˜¾ç¤ºæµ‹è¯•ç»“æœ
        print("\næµ‹è¯•ç»“æœ:")
        print("-" * 50)
        
        all_passed = True
        for check_name, passed, value in checks:
            status = "âœ…" if passed else "âŒ"
            print(f"  {status} {check_name}: {value}")
            if not passed:
                all_passed = False
        
        print("-" * 50)
        if all_passed:
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼Parquetç»Ÿè®¡è®¡ç®—ä¸enhancedå®Œå…¨ä¸€è‡´")
        else:
            print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç»Ÿè®¡è®¡ç®—é€»è¾‘")
        
        # æ˜¾ç¤ºå®Œæ•´çš„æ±‡æ€»æ•°æ®ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        print("\nå®Œæ•´æ±‡æ€»æ•°æ®ï¼ˆè°ƒè¯•ç”¨ï¼‰:")
        important_fields = [
            'total', 'success', 'full_success', 'partial_success', 'failed',
            'success_rate', 'weighted_success_score',
            'avg_execution_time', 'avg_turns', 'avg_tool_calls', 'tool_coverage_rate',
            'avg_workflow_score', 'avg_phase2_score', 'avg_quality_score', 'avg_final_score',
            'total_errors', 'tool_call_format_errors', 'timeout_errors',
            'format_error_rate', 'timeout_error_rate',
            'tests_with_assistance', 'assisted_success', 'total_assisted_turns', 'assistance_rate'
        ]
        
        for field in important_fields:
            if field in latest:
                value = latest[field]
                if isinstance(value, float):
                    print(f"  {field}: {value:.3f}")
                else:
                    print(f"  {field}: {value}")
        
        return all_passed
    else:
        print("âŒ æœªæ‰¾åˆ°æµ‹è¯•æ•°æ®")
        return False

if __name__ == "__main__":
    success = test_parquet_statistics()
    sys.exit(0 if success else 1)