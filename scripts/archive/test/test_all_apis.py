#!/usr/bin/env python3
"""
å…¨é¢APIè¿æ¥æµ‹è¯•è„šæœ¬
å®Œå…¨æ¨¡æ‹Ÿbashè„šæœ¬çš„è°ƒç”¨é“¾
"""

import time
import json
from api_client_manager import get_client_for_model, get_api_model_name

# ä»bashè„šæœ¬ä¸­çš„æ¨¡å‹åˆ—è¡¨
OPENSOURCE_MODELS = [
    "DeepSeek-V3-0324",
    "DeepSeek-R1-0528",
    "qwen2.5-72b-instruct",
    "qwen2.5-32b-instruct",
    "qwen2.5-14b-instruct",
    "qwen2.5-7b-instruct",
    "qwen2.5-3b-instruct",
    "Llama-3.3-70B-Instruct"
]

CLOSED_SOURCE_MODELS = [
    "gpt-4o-mini",
    "gpt-5-mini",
    "o3-0416-global",
    "gemini-2.5-flash-06-17",
    "kimi-k2",
    "claude_sonnet4"
]

def test_model_api(model_name: str, prompt_type: str = "baseline") -> dict:
    """æµ‹è¯•å•ä¸ªæ¨¡å‹çš„APIè¿æ¥ - å®Œå…¨æ¨¡æ‹ŸInteractiveExecutorçš„è°ƒç”¨æ–¹å¼"""
    result = {
        "model": model_name,
        "prompt_type": prompt_type,
        "status": "unknown",
        "response_time": None,
        "error": None,
        "warning": None
    }
    
    start = time.time()
    try:
        # å®Œå…¨æ¨¡æ‹ŸInteractiveExecutorçš„è°ƒç”¨æ–¹å¼
        # 1. è·å–å®¢æˆ·ç«¯ï¼ˆä¼ é€’prompt_typeç”¨äºIdealLab keyé€‰æ‹©ï¼‰
        client = get_client_for_model(model_name, prompt_type)
        
        # 2. è·å–APIæ¨¡å‹å
        api_model_name = get_api_model_name(model_name)
        
        # 3. æ„å»ºè¯·æ±‚å‚æ•° - å®Œå…¨æ¨¡æ‹ŸInteractiveExecutorç¬¬1189è¡Œçš„è°ƒç”¨
        # InteractiveExecutorä¸è®¾ç½®max_tokenså’Œtemperature
        request_params = {
            "model": api_model_name,
            "messages": [{"role": "user", "content": "Reply with OK"}]
        }
        
        # 4. å‘é€æµ‹è¯•è¯·æ±‚ï¼ˆå¸¦60ç§’è¶…æ—¶ï¼Œä¸InteractiveExecutorä¸€è‡´ï¼‰
        response = client.chat.completions.create(**request_params, timeout=60)
        
        elapsed = time.time() - start
        result["status"] = "success"
        result["response_time"] = round(elapsed, 2)
        
        # æ£€æŸ¥å“åº”å†…å®¹
        content = response.choices[0].message.content
        if not content or content.strip() == "":
            result["warning"] = "Empty response content"
            
    except Exception as e:
        elapsed = time.time() - start
        result["status"] = "failed"
        result["response_time"] = round(elapsed, 2)
        result["error"] = str(e)
        
        # ç‰¹æ®Šé”™è¯¯æ£€æŸ¥
        if "max_tokens" in str(e):
            result["warning"] = "Model doesn't support max_tokens parameter"
        elif "temperature" in str(e):
            result["warning"] = "Model doesn't support temperature parameter"
    
    return result

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=" * 60)
    print("å…¨é¢APIè¿æ¥æµ‹è¯•ï¼ˆæ¨¡æ‹Ÿå®é™…æ‰¹æµ‹è¯•è°ƒç”¨ï¼‰")
    print("=" * 60)
    
    # æµ‹è¯•å¼€æºæ¨¡å‹
    print("\nğŸ“Š å¼€æºæ¨¡å‹æµ‹è¯•")
    print("-" * 40)
    opensource_results = []
    for model in OPENSOURCE_MODELS:
        print(f"æµ‹è¯• {model}...", end=" ")
        # ä½¿ç”¨baseline prompt_typeæµ‹è¯•ï¼ˆæ¨¡æ‹Ÿæ‰¹æµ‹è¯•çš„é»˜è®¤è¡Œä¸ºï¼‰
        result = test_model_api(model, prompt_type="baseline")
        opensource_results.append(result)
        
        if result["status"] == "success":
            print(f"âœ… æˆåŠŸ ({result['response_time']}s)")
            if result["warning"]:
                print(f"  âš ï¸  {result['warning']}")
        else:
            print(f"âŒ å¤±è´¥")
            print(f"  é”™è¯¯: {result['error']}")
    
    # æµ‹è¯•é—­æºæ¨¡å‹
    print("\nğŸ“Š é—­æºæ¨¡å‹æµ‹è¯•")
    print("-" * 40)
    closed_results = []
    for model in CLOSED_SOURCE_MODELS:
        print(f"æµ‹è¯• {model}...", end=" ")
        # é—­æºæ¨¡å‹ä¹Ÿä½¿ç”¨baseline
        result = test_model_api(model, prompt_type="baseline")
        closed_results.append(result)
        
        if result["status"] == "success":
            print(f"âœ… æˆåŠŸ ({result['response_time']}s)")
            if result["warning"]:
                print(f"  âš ï¸  {result['warning']}")
        else:
            print(f"âŒ å¤±è´¥")
            print(f"  é”™è¯¯: {result['error']}")
    
    # ç»Ÿè®¡ç»“æœ
    print("\n" + "=" * 60)
    print("ğŸ“ˆ æµ‹è¯•ç»Ÿè®¡")
    print("-" * 40)
    
    # å¼€æºæ¨¡å‹ç»Ÿè®¡
    opensource_success = sum(1 for r in opensource_results if r["status"] == "success")
    print(f"å¼€æºæ¨¡å‹: {opensource_success}/{len(OPENSOURCE_MODELS)} æˆåŠŸ")
    if opensource_success < len(OPENSOURCE_MODELS):
        failed = [r["model"] for r in opensource_results if r["status"] == "failed"]
        print(f"  å¤±è´¥: {', '.join(failed)}")
    
    # é—­æºæ¨¡å‹ç»Ÿè®¡
    closed_success = sum(1 for r in closed_results if r["status"] == "success")
    print(f"é—­æºæ¨¡å‹: {closed_success}/{len(CLOSED_SOURCE_MODELS)} æˆåŠŸ")
    if closed_success < len(CLOSED_SOURCE_MODELS):
        failed = [r["model"] for r in closed_results if r["status"] == "failed"]
        print(f"  å¤±è´¥: {', '.join(failed)}")
    
    # æ€§èƒ½ç»Ÿè®¡
    all_results = opensource_results + closed_results
    success_results = [r for r in all_results if r["status"] == "success"]
    if success_results:
        avg_time = sum(r["response_time"] for r in success_results) / len(success_results)
        max_time = max(r["response_time"] for r in success_results)
        min_time = min(r["response_time"] for r in success_results)
        
        print(f"\nå“åº”æ—¶é—´ç»Ÿè®¡:")
        print(f"  å¹³å‡: {avg_time:.2f}s")
        print(f"  æœ€å¿«: {min_time:.2f}s")
        print(f"  æœ€æ…¢: {max_time:.2f}s")
        
        # æ‰¾å‡ºæœ€æ…¢çš„æ¨¡å‹
        slowest = max(success_results, key=lambda x: x["response_time"])
        if slowest["response_time"] > 5:
            print(f"  âš ï¸  {slowest['model']} å“åº”è¾ƒæ…¢ ({slowest['response_time']}s)")
    
    # è­¦å‘Šæ±‡æ€»
    warnings = [r for r in all_results if r.get("warning")]
    if warnings:
        print(f"\nâš ï¸  è­¦å‘Šæ±‡æ€»:")
        for r in warnings:
            print(f"  {r['model']}: {r['warning']}")
    
    print("\n" + "=" * 60)
    
    # è¿”å›æ˜¯å¦æ‰€æœ‰æµ‹è¯•éƒ½æˆåŠŸ
    total_success = opensource_success + closed_success
    total_models = len(OPENSOURCE_MODELS) + len(CLOSED_SOURCE_MODELS)
    
    if total_success == total_models:
        print("âœ… æ‰€æœ‰APIæµ‹è¯•é€šè¿‡ï¼")
        return 0
    else:
        print(f"âš ï¸  {total_models - total_success} ä¸ªæ¨¡å‹APIæµ‹è¯•å¤±è´¥")
        return 1

if __name__ == "__main__":
    exit(main())