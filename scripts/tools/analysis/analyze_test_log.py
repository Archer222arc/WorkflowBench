#!/usr/bin/env python3
"""
æµ‹è¯•æ—¥å¿—åˆ†æå·¥å…· - ç”¨äºè°ƒè¯•å•ä¸ªæµ‹è¯•å¤±è´¥
ä½¿ç”¨æ–¹æ³•ï¼š
    python analyze_test_log.py [æ—¥å¿—æ–‡ä»¶è·¯å¾„]
    python analyze_test_log.py --latest DeepSeek  # æŸ¥çœ‹æœ€æ–°çš„DeepSeekæµ‹è¯•
    python analyze_test_log.py --list  # åˆ—å‡ºæœ€æ–°çš„10ä¸ªæµ‹è¯•æ—¥å¿—
"""

import json
import sys
import argparse
from pathlib import Path
from datetime import datetime

def analyze_log(log_file_path):
    """åˆ†æå•ä¸ªæµ‹è¯•æ—¥å¿—æ–‡ä»¶"""
    try:
        with open(log_file_path, 'r') as f:
            log_data = json.load(f)
    except Exception as e:
        print(f"âŒ æ— æ³•è¯»å–æ—¥å¿—æ–‡ä»¶: {e}")
        return
    
    print("\n" + "="*80)
    print("ğŸ“Š æµ‹è¯•æ—¥å¿—åˆ†ææŠ¥å‘Š")
    print("="*80)
    
    # åŸºæœ¬ä¿¡æ¯
    print(f"\nğŸ“ åŸºæœ¬ä¿¡æ¯:")
    print(f"  æµ‹è¯•ID: {log_data.get('test_id', 'N/A')}")
    print(f"  ä»»åŠ¡ç±»å‹: {log_data.get('task_type', 'N/A')}")
    print(f"  æç¤ºç±»å‹: {log_data.get('prompt_type', 'N/A')}")
    print(f"  æ—¶é—´æˆ³: {log_data.get('timestamp', 'N/A')}")
    print(f"  æ˜¯å¦æœ‰ç¼ºé™·: {log_data.get('is_flawed', False)}")
    if log_data.get('flaw_type'):
        print(f"  ç¼ºé™·ç±»å‹: {log_data['flaw_type']}")
    
    # ä»»åŠ¡å®ä¾‹
    task_instance = log_data.get('task_instance', {})
    if task_instance:
        print(f"\nğŸ“‹ ä»»åŠ¡å®ä¾‹:")
        print(f"  ä»»åŠ¡æè¿°: {task_instance.get('description', 'N/A')[:100]}...")
        required_tools = task_instance.get('required_tools', [])
        if required_tools:
            print(f"  éœ€è¦çš„å·¥å…·: {', '.join(required_tools)}")
    
    # å¯¹è¯å†å²åˆ†æ
    conversation = log_data.get('conversation_history', [])
    print(f"\nğŸ’¬ å¯¹è¯å†å²: å…±{len(conversation)}æ¡æ¶ˆæ¯")
    
    for i, msg in enumerate(conversation[:6]):  # åªæ˜¾ç¤ºå‰6æ¡
        role = msg.get('role', '')
        content = msg.get('content', '')
        
        if role == 'user':
            print(f"\n  [{i+1}] ğŸ‘¤ ç”¨æˆ·:")
            # æå–å…³é”®ä¿¡æ¯
            if 'Tool Search Results:' in content:
                print(f"    [å·¥å…·æœç´¢ç»“æœ]")
            elif 'Tool Execution Result:' in content:
                print(f"    [å·¥å…·æ‰§è¡Œç»“æœ]")
            else:
                print(f"    {content[:150]}...")
                
        elif role == 'assistant':
            print(f"\n  [{i+1}] ğŸ¤– åŠ©æ‰‹:")
            # åˆ†æå·¥å…·è°ƒç”¨
            if '<tool_call>' in content:
                import re
                tool_calls = re.findall(r'<tool_call>(.*?)</tool_call>', content)
                print(f"    âœ… æ­£ç¡®æ ¼å¼å·¥å…·è°ƒç”¨: {', '.join(tool_calls)}")
            elif '<tool_search>' in content:
                import re
                tool_searches = re.findall(r'<tool_search>(.*?)</tool_search>', content)
                print(f"    âš ï¸ é”™è¯¯æ ¼å¼(tool_search): {', '.join(tool_searches)}")
            elif '<tool_info>' in content:
                import re
                tool_infos = re.findall(r'<tool_info>(.*?)</tool_info>', content)
                print(f"    â„¹ï¸ å·¥å…·ä¿¡æ¯æŸ¥è¯¢: {', '.join(tool_infos)}")
            else:
                # æ˜¾ç¤ºå‰200å­—ç¬¦
                print(f"    {content[:200]}...")
    
    if len(conversation) > 6:
        print(f"\n  ... è¿˜æœ‰ {len(conversation)-6} æ¡æ¶ˆæ¯æœªæ˜¾ç¤º")
    
    # æ‰§è¡Œå†å²
    exec_history = log_data.get('execution_history', [])
    print(f"\nâš™ï¸ æ‰§è¡Œå†å²: å…±{len(exec_history)}ä¸ªå·¥å…·è°ƒç”¨")
    
    if exec_history:
        success_count = sum(1 for h in exec_history if h.get('success', False))
        print(f"  æˆåŠŸ: {success_count}/{len(exec_history)}")
        
        for i, h in enumerate(exec_history[:5]):
            tool = h.get('tool', 'unknown')
            success = h.get('success', False)
            error = h.get('error')
            
            status = "âœ…" if success else "âŒ"
            print(f"  [{i+1}] {status} {tool}")
            if error:
                print(f"      é”™è¯¯: {error[:100]}...")
    
    # æå–çš„å·¥å…·è°ƒç”¨
    extracted_calls = log_data.get('extracted_tool_calls', [])
    if extracted_calls:
        print(f"\nğŸ”§ æå–çš„å·¥å…·è°ƒç”¨: {', '.join(extracted_calls)}")
    
    # APIé—®é¢˜
    api_issues = log_data.get('api_issues', [])
    if api_issues:
        print(f"\nâš ï¸ APIé—®é¢˜: {len(api_issues)}ä¸ª")
        for issue in api_issues[:3]:
            print(f"  - {issue}")
    
    print("\n" + "="*80)

def list_latest_logs(pattern=None, limit=10):
    """åˆ—å‡ºæœ€æ–°çš„æµ‹è¯•æ—¥å¿—"""
    log_dir = Path('workflow_quality_results/test_logs')
    if not log_dir.exists():
        print("âŒ æ—¥å¿—ç›®å½•ä¸å­˜åœ¨")
        return
    
    # è·å–æ‰€æœ‰JSONæ—¥å¿—æ–‡ä»¶
    if pattern:
        log_files = list(log_dir.glob(f'*{pattern}*.json'))
    else:
        log_files = list(log_dir.glob('*.json'))
    
    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
    log_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
    
    print(f"\nğŸ“ æœ€æ–°çš„{min(limit, len(log_files))}ä¸ªæµ‹è¯•æ—¥å¿—:")
    print("-"*80)
    
    for i, log_file in enumerate(log_files[:limit]):
        mtime = datetime.fromtimestamp(log_file.stat().st_mtime)
        size = log_file.stat().st_size / 1024  # KB
        
        # è§£ææ–‡ä»¶åè·å–ä¿¡æ¯
        name_parts = log_file.stem.split('_')
        model = name_parts[0] if name_parts else "unknown"
        
        print(f"{i+1:2}. {log_file.name}")
        print(f"    æ¨¡å‹: {model}")
        print(f"    æ—¶é—´: {mtime.strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"    å¤§å°: {size:.1f} KB")
        
        # å¿«é€Ÿè¯»å–æµ‹è¯•ç»“æœ
        try:
            with open(log_file, 'r') as f:
                data = json.load(f)
                task_type = data.get('task_type', 'N/A')
                prompt_type = data.get('prompt_type', 'N/A')
                exec_history = data.get('execution_history', [])
                success = any(h.get('success') for h in exec_history) if exec_history else False
                
                print(f"    ä»»åŠ¡: {task_type} | æç¤º: {prompt_type} | ç»“æœ: {'âœ…æˆåŠŸ' if success else 'âŒå¤±è´¥'}")
        except:
            pass
        
        print()

def main():
    parser = argparse.ArgumentParser(description='åˆ†ææµ‹è¯•æ—¥å¿—æ–‡ä»¶')
    parser.add_argument('log_file', nargs='?', help='æ—¥å¿—æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--latest', help='æŸ¥çœ‹åŒ…å«æŒ‡å®šæ¨¡å¼çš„æœ€æ–°æ—¥å¿—')
    parser.add_argument('--list', action='store_true', help='åˆ—å‡ºæœ€æ–°çš„æ—¥å¿—æ–‡ä»¶')
    parser.add_argument('--limit', type=int, default=10, help='åˆ—å‡ºæ—¥å¿—çš„æ•°é‡é™åˆ¶')
    
    args = parser.parse_args()
    
    if args.list:
        list_latest_logs(limit=args.limit)
    elif args.latest:
        # æ‰¾åˆ°æœ€æ–°çš„åŒ¹é…æ–‡ä»¶
        log_dir = Path('workflow_quality_results/test_logs')
        matching_files = list(log_dir.glob(f'*{args.latest}*.json'))
        if matching_files:
            matching_files.sort(key=lambda x: x.stat().st_mtime, reverse=True)
            latest = matching_files[0]
            print(f"ğŸ“‚ åˆ†ææœ€æ–°çš„ {args.latest} æ—¥å¿—: {latest.name}")
            analyze_log(latest)
        else:
            print(f"âŒ æ²¡æœ‰æ‰¾åˆ°åŒ…å« '{args.latest}' çš„æ—¥å¿—æ–‡ä»¶")
    elif args.log_file:
        log_path = Path(args.log_file)
        if log_path.exists():
            analyze_log(log_path)
        else:
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {args.log_file}")
    else:
        parser.print_help()

if __name__ == "__main__":
    main()