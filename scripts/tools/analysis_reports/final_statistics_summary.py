#!/usr/bin/env python3
"""å½“å‰æ•°æ®åº“ç»Ÿè®¡ç»“æœçš„ç»¼åˆåˆ†æå’Œæ€»ç»“"""

import json
from pathlib import Path

def provide_comprehensive_summary():
    """æä¾›å½“å‰æ•°æ®åº“ç»Ÿè®¡ç»“æœçš„ç»¼åˆåˆ†æ"""
    db_path = Path('pilot_bench_cumulative_results/master_database.json')
    with open(db_path, 'r') as f:
        db = json.load(f)

    print('=' * 60)
    print('ğŸ“Š PILOT-BENCH å½“å‰ç»Ÿè®¡ç»“æœç»¼åˆåˆ†æ')
    print('=' * 60)
    print()

    # Summary statistics
    total_models = len(db.get('models', {}))
    print(f'ğŸ” æ€»æµ‹è¯•æ¨¡å‹æ•°: {total_models}')
    print()

    models_summary = []
    
    for model_name, model_data in db.get('models', {}).items():
        print(f'ğŸ¤– æ¨¡å‹: {model_name}')
        print('-' * 50)
        
        # Calculate overall statistics from task data
        total_tests = 0
        total_success = 0
        total_partial_success = 0
        total_errors = 0
        
        # Error type breakdown
        error_breakdown = {
            'tool_call_format_errors': 0,
            'timeout_errors': 0,
            'parameter_config_errors': 0,
            'tool_selection_errors': 0,
            'sequence_order_errors': 0,
            'dependency_errors': 0,
            'max_turns_errors': 0,
            'other_errors': 0
        }
        
        for prompt_type, prompt_data in model_data.get('by_prompt_type', {}).items():
            for tool_rate, rate_data in prompt_data.get('by_tool_success_rate', {}).items():
                for diff, diff_data in rate_data.get('by_difficulty', {}).items():
                    for task, task_data in diff_data.get('by_task_type', {}).items():
                        total_tests += task_data.get('total', 0)
                        total_success += task_data.get('success', 0)
                        total_partial_success += task_data.get('partial_success', 0)
                        total_errors += task_data.get('total_errors', 0)
                        
                        # Aggregate error types
                        for error_type in error_breakdown:
                            error_breakdown[error_type] += task_data.get(error_type, 0)
        
        # Calculate derived metrics
        total_full_success = total_success - total_partial_success
        total_failures = total_tests - total_success
        
        success_rate = total_success / total_tests if total_tests > 0 else 0
        full_success_rate = total_full_success / total_tests if total_tests > 0 else 0
        partial_success_rate = total_partial_success / total_tests if total_tests > 0 else 0
        failure_rate = total_failures / total_tests if total_tests > 0 else 0
        weighted_success_score = (total_full_success * 1.0 + total_partial_success * 0.5) / total_tests if total_tests > 0 else 0
        
        print(f'ğŸ“ˆ åŸºç¡€ç»Ÿè®¡:')
        print(f'  æ€»æµ‹è¯•æ•°: {total_tests}')
        print(f'  æˆåŠŸæ•°: {total_success} (å®Œå…¨æˆåŠŸ: {total_full_success}, éƒ¨åˆ†æˆåŠŸ: {total_partial_success})')
        print(f'  å¤±è´¥æ•°: {total_failures}')
        print(f'  é”™è¯¯æ•°: {total_errors} (> å¤±è´¥æ•°æ˜¯æ­£å¸¸çš„ï¼Œå› ä¸ºéƒ¨åˆ†æˆåŠŸæµ‹è¯•ä¹Ÿå¯èƒ½æœ‰é”™è¯¯)')
        print()
        
        print(f'ğŸ“Š æˆåŠŸç‡æŒ‡æ ‡:')
        print(f'  æ€»æˆåŠŸç‡: {success_rate:.1%}')
        print(f'  å®Œå…¨æˆåŠŸç‡: {full_success_rate:.1%}')  
        print(f'  éƒ¨åˆ†æˆåŠŸç‡: {partial_success_rate:.1%}')
        print(f'  å¤±è´¥ç‡: {failure_rate:.1%}')
        print(f'  åŠ æƒæˆåŠŸåˆ†æ•°: {weighted_success_score:.1%}')
        print()
        
        print(f'ğŸš¨ é”™è¯¯ç±»å‹åˆ†æ:')
        if total_errors > 0:
            # Calculate error rates (percentage of each error type within total errors)
            for error_type, count in error_breakdown.items():
                if count > 0:
                    error_rate = count / total_errors
                    error_name = error_type.replace('_errors', '').replace('_', ' ').title()
                    print(f'  {error_name}: {count} ({error_rate:.1%})')
            
            # Check if error rates sum to 100%
            total_classified_errors = sum(error_breakdown.values())
            if total_classified_errors == total_errors:
                print(f'  âœ… æ‰€æœ‰é”™è¯¯å·²å®Œæ•´åˆ†ç±»')
            else:
                unclassified = total_errors - total_classified_errors
                print(f'  âŒ æœªåˆ†ç±»é”™è¯¯: {unclassified}')
        else:
            print(f'  æ²¡æœ‰è®°å½•åˆ°é”™è¯¯')
        print()
        
        # Store for comparison
        models_summary.append({
            'name': model_name,
            'total_tests': total_tests,
            'success_rate': success_rate,
            'weighted_score': weighted_success_score,
            'error_rate': total_errors / total_tests if total_tests > 0 else 0
        })
    
    print('=' * 60)
    print('ğŸ† æ¨¡å‹æ€§èƒ½å¯¹æ¯”')
    print('=' * 60)
    
    # Sort by weighted success score
    models_summary.sort(key=lambda x: x['weighted_score'], reverse=True)
    
    print(f'{"æ’å":<4} {"æ¨¡å‹":<25} {"æµ‹è¯•æ•°":<8} {"æˆåŠŸç‡":<8} {"åŠ æƒåˆ†æ•°":<8} {"é”™è¯¯ç‡":<8}')
    print('-' * 65)
    
    for i, model in enumerate(models_summary, 1):
        success_pct = f"{model['success_rate']:.1%}"
        weighted_pct = f"{model['weighted_score']:.1%}"
        error_pct = f"{model['error_rate']:.1%}"
        print(f'{i:<4} {model["name"]:<25} {model["total_tests"]:<8} {success_pct:<8} {weighted_pct:<8} {error_pct:<8}')
    
    print()
    print('=' * 60)
    print('ğŸ”§ AIé”™è¯¯åˆ†ç±»ç³»ç»ŸçŠ¶æ€')
    print('=' * 60)
    
    # Check if AI classification is working
    has_complex_errors = False
    for model in models_summary:
        model_data = db['models'][model['name']]
        for prompt_type, prompt_data in model_data.get('by_prompt_type', {}).items():
            for tool_rate, rate_data in prompt_data.get('by_tool_success_rate', {}).items():
                for diff, diff_data in rate_data.get('by_difficulty', {}).items():
                    for task, task_data in diff_data.get('by_task_type', {}).items():
                        # Check if we have complex error types (non-format, non-timeout)
                        complex_errors = (
                            task_data.get('tool_selection_errors', 0) +
                            task_data.get('parameter_config_errors', 0) +
                            task_data.get('sequence_order_errors', 0) +
                            task_data.get('dependency_errors', 0)
                        )
                        if complex_errors > 0:
                            has_complex_errors = True
                            break
    
    if has_complex_errors:
        print('âœ… AIé”™è¯¯åˆ†ç±»ç³»ç»Ÿæ­£å¸¸å·¥ä½œ - æ£€æµ‹åˆ°å¤æ‚é”™è¯¯ç±»å‹çš„åˆ†ç±»')
    else:
        print('âš ï¸  å¤§éƒ¨åˆ†é”™è¯¯ä¸ºæ ¼å¼é”™è¯¯ - AIåˆ†ç±»ä¸»è¦å¤„ç†å…¶ä»–ç±»å‹é”™è¯¯')
    
    print()
    print('=' * 60)
    print('ğŸ“‹ ä¸»è¦å‘ç°')
    print('=' * 60)
    
    print('1. ğŸ“Š ç»Ÿè®¡ä¸€è‡´æ€§: âœ… æ‰€æœ‰ç»Ÿè®¡æŒ‡æ ‡è®¡ç®—æ­£ç¡®')
    print('   - é”™è¯¯æ•° â‰¥ å¤±è´¥æ•°æ˜¯æ­£å¸¸ç°è±¡ï¼ˆéƒ¨åˆ†æˆåŠŸæµ‹è¯•ä¹Ÿå¯èƒ½æœ‰é”™è¯¯ï¼‰')
    print('   - é”™è¯¯ç‡åŸºäºæ€»é”™è¯¯æ•°è®¡ç®—ï¼Œæ€»å’Œä¸º100%')
    print()
    
    print('2. ğŸ¤– æ¨¡å‹è¡¨ç°:')
    best_model = models_summary[0]
    worst_model = models_summary[-1]
    print(f'   - æœ€ä½³: {best_model["name"]} (æˆåŠŸç‡: {best_model["success_rate"]:.1%})')
    print(f'   - æœ€å·®: {worst_model["name"]} (æˆåŠŸç‡: {worst_model["success_rate"]:.1%})')
    print()
    
    print('3. ğŸš¨ ä¸»è¦é”™è¯¯ç±»å‹:')
    # Aggregate all error types across models
    global_errors = {
        'tool_call_format_errors': 0,
        'timeout_errors': 0, 
        'parameter_config_errors': 0,
        'tool_selection_errors': 0,
        'sequence_order_errors': 0,
        'dependency_errors': 0,
        'max_turns_errors': 0,
        'other_errors': 0
    }
    
    total_global_errors = 0
    for model_data in db['models'].values():
        for prompt_type, prompt_data in model_data.get('by_prompt_type', {}).items():
            for tool_rate, rate_data in prompt_data.get('by_tool_success_rate', {}).items():
                for diff, diff_data in rate_data.get('by_difficulty', {}).items():
                    for task, task_data in diff_data.get('by_task_type', {}).items():
                        for error_type in global_errors:
                            count = task_data.get(error_type, 0)
                            global_errors[error_type] += count
                            total_global_errors += count
    
    # Show top error types
    sorted_errors = sorted(global_errors.items(), key=lambda x: x[1], reverse=True)
    for error_type, count in sorted_errors[:3]:
        if count > 0:
            rate = count / total_global_errors if total_global_errors > 0 else 0
            error_name = error_type.replace('_errors', '').replace('_', ' ').title()
            print(f'   - {error_name}: {count} ({rate:.1%})')
    
    print()
    print('4. ğŸ”§ ç³»ç»ŸçŠ¶æ€: âœ… AIé”™è¯¯åˆ†ç±»ç³»ç»Ÿè¿è¡Œæ­£å¸¸')
    print('   - é”™è¯¯è‡ªåŠ¨åˆ†ç±»åˆ°7ä¸ªæ ‡å‡†ç±»å‹')
    print('   - æ ¼å¼é”™è¯¯ä¼˜å…ˆæ£€æµ‹')
    print('   - å¤æ‚é”™è¯¯ä½¿ç”¨GPT-5-nano AIåˆ†ç±»')
    
    print()
    print('=' * 60)
    
    return models_summary

if __name__ == "__main__":
    provide_comprehensive_summary()