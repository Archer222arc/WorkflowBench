#!/usr/bin/env python3
"""
å¢å¼ºç‰ˆæµ‹è¯•ç»Ÿè®¡æŸ¥çœ‹å™¨ - åŒ…å«è¯¦ç»†çš„scoreç»Ÿè®¡
"""

import argparse
import json
from pathlib import Path
from batch_test_runner import BatchTestRunner
from cumulative_data_structure import ModelStatistics

def view_enhanced_statistics(model: str = "qwen2.5-3b-instruct", detailed: bool = False):
    """æŸ¥çœ‹å¢å¼ºæµ‹è¯•ç»Ÿè®¡ï¼ŒåŒ…å«scores"""
    
    print("\n" + "=" * 80)
    print("WorkflowBench å¢å¼ºç»Ÿè®¡æŠ¥å‘Š")
    print("=" * 80)
    
    # åˆå§‹åŒ–runneræ¥è®¿é—®ç´¯ç§¯æ•°æ®
    runner = BatchTestRunner(debug=False, silent=True)
    runner._lazy_init()
    
    # ç›´æ¥è®¿é—®æ•°æ®åº“
    if model not in runner.manager.database.get('models', {}):
        print(f"\nâŒ æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹ {model} çš„æµ‹è¯•æ•°æ®")
        print("\nå¯ç”¨çš„æ¨¡å‹:")
        for m in runner.manager.database.get('models', {}).keys():
            print(f"  - {m}")
        return
    
    model_stats = runner.manager.database['models'][model]
    
    # å¦‚æœæ˜¯å­—å…¸ï¼Œéœ€è¦è½¬æ¢ä¸ºModelStatisticså¯¹è±¡
    if isinstance(model_stats, dict):
        # è¿™æ˜¯æ—§æ ¼å¼ï¼Œéœ€è¦å…¼å®¹
        print("\nâš ï¸ æ£€æµ‹åˆ°æ—§æ ¼å¼æ•°æ®ï¼Œæ˜¾ç¤ºåŸºæœ¬ç»Ÿè®¡")
        total_tests = model_stats.get('total_tests', 0)
        total_success = model_stats.get('total_success', 0)
        success_rate = (total_success / total_tests * 100) if total_tests > 0 else 0
        
        print(f"\nğŸ“Š æ¨¡å‹: {model}")
        print(f"æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"æˆåŠŸæ•°: {total_success}")
        print(f"æˆåŠŸç‡: {success_rate:.1f}%")
        return
    
    # æ–°æ ¼å¼ - ModelStatisticså¯¹è±¡
    print(f"\nğŸ“Š æ¨¡å‹: {model}")
    print(f"é¦–æ¬¡æµ‹è¯•: {model_stats.first_test_time}")
    print(f"æœ€åæµ‹è¯•: {model_stats.last_test_time}")
    
    # æˆåŠŸç‡ç»Ÿè®¡
    print("\nğŸ“ˆ æˆåŠŸç‡ç»Ÿè®¡:")
    print("-" * 60)
    print(f"æ€»æµ‹è¯•æ•°: {model_stats.overall_success.total_tests}")
    print(f"å®Œå…¨æˆåŠŸ: {model_stats.overall_success.full_success} ({model_stats.overall_success.full_success_rate*100:.1f}%)")
    print(f"éƒ¨åˆ†æˆåŠŸ: {model_stats.overall_success.partial_success} ({model_stats.overall_success.partial_success_rate*100:.1f}%)")
    print(f"å¤±è´¥: {model_stats.overall_success.failure} ({model_stats.overall_success.failure_rate*100:.1f}%)")
    print(f"æ€»æˆåŠŸç‡: {model_stats.overall_success.success_rate*100:.1f}%")
    print(f"åŠ æƒæˆåŠŸåˆ†æ•°: {model_stats.overall_success.weighted_success_score:.3f}")
    
    # Scoreç»Ÿè®¡
    print("\nğŸ¯ Scoreç»Ÿè®¡:")
    print("-" * 60)
    
    # Workflow Score
    ws = model_stats.overall_scores.workflow_scores
    if ws.count > 0:
        print(f"Workflow Score:")
        print(f"  å¹³å‡: {ws.mean:.3f}")
        print(f"  æœ€å°: {ws.min:.3f}")
        print(f"  æœ€å¤§: {ws.max:.3f}")
        print(f"  æ ·æœ¬æ•°: {ws.count}")
    else:
        print("Workflow Score: æ— æ•°æ®")
    
    # Phase2 Score
    ps = model_stats.overall_scores.phase2_scores
    if ps.count > 0:
        print(f"Phase2 Score:")
        print(f"  å¹³å‡: {ps.mean:.3f}")
        print(f"  æœ€å°: {ps.min:.3f}")
        print(f"  æœ€å¤§: {ps.max:.3f}")
        print(f"  æ ·æœ¬æ•°: {ps.count}")
    else:
        print("Phase2 Score: æ— æ•°æ®")
    
    # Quality Score
    qs = model_stats.overall_scores.quality_scores
    if qs.count > 0:
        print(f"Quality Score:")
        print(f"  å¹³å‡: {qs.mean:.3f}")
        print(f"  æœ€å°: {qs.min:.3f}")
        print(f"  æœ€å¤§: {qs.max:.3f}")
        print(f"  æ ·æœ¬æ•°: {qs.count}")
    else:
        print("Quality Score: æ— æ•°æ®")
    
    # Final Score
    fs = model_stats.overall_scores.final_scores
    if fs.count > 0:
        print(f"Final Score:")
        print(f"  å¹³å‡: {fs.mean:.3f}")
        print(f"  æœ€å°: {fs.min:.3f}")
        print(f"  æœ€å¤§: {fs.max:.3f}")
        print(f"  æ ·æœ¬æ•°: {fs.count}")
    else:
        print("Final Score: æ— æ•°æ®")
    
    # æ‰§è¡Œç»Ÿè®¡
    print("\nâš¡ æ‰§è¡Œç»Ÿè®¡:")
    print("-" * 60)
    et = model_stats.overall_execution.execution_times
    if et.count > 0:
        print(f"æ‰§è¡Œæ—¶é—´: å¹³å‡ {et.mean:.2f}s (æœ€å° {et.min:.2f}s, æœ€å¤§ {et.max:.2f}s)")
    
    turns = model_stats.overall_execution.turns_used
    if turns.count > 0:
        print(f"æ‰§è¡Œè½®æ•°: å¹³å‡ {turns.mean:.1f} (æœ€å° {int(turns.min)}, æœ€å¤§ {int(turns.max)})")
    
    tc = model_stats.overall_execution.tool_calls
    if tc.count > 0:
        print(f"å·¥å…·è°ƒç”¨: å¹³å‡ {tc.mean:.1f} (æœ€å° {int(tc.min)}, æœ€å¤§ {int(tc.max)})")
    
    print(f"ç‹¬ç‰¹å·¥å…·æ•°: {model_stats.overall_execution.unique_tools_count}")
    print(f"æ€»å·¥å…·è°ƒç”¨: {model_stats.overall_execution.total_tool_invocations}")
    
    # æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡
    if model_stats.by_task_type:
        print("\nğŸ“‹ æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡:")
        print("-" * 60)
        print(f"{'ä»»åŠ¡ç±»å‹':<25} {'æµ‹è¯•æ•°':>8} {'æˆåŠŸç‡':>10} {'Avg Score':>10}")
        print("-" * 60)
        for task_type, stats in sorted(model_stats.by_task_type.items()):
            success_rate = stats.success_metrics.success_rate * 100
            avg_score = stats.score_metrics.final_scores.mean if stats.score_metrics.final_scores.count > 0 else 0
            print(f"{task_type:<25} {stats.success_metrics.total_tests:>8} {success_rate:>9.1f}% {avg_score:>10.3f}")
    
    # æŒ‰Promptç±»å‹ç»Ÿè®¡
    if model_stats.by_prompt_type:
        print("\nğŸ¯ æŒ‰Promptç­–ç•¥ç»Ÿè®¡:")
        print("-" * 60)
        
        # åŸºæœ¬prompt
        print("åŸºæœ¬Promptç­–ç•¥:")
        print(f"{'Promptç±»å‹':<25} {'æµ‹è¯•æ•°':>8} {'æˆåŠŸç‡':>10} {'Avg Score':>10}")
        print("-" * 60)
        for prompt_type in ['baseline', 'optimal', 'cot']:
            if prompt_type in model_stats.by_prompt_type:
                stats = model_stats.by_prompt_type[prompt_type]
                success_rate = stats.success_metrics.success_rate * 100
                avg_score = stats.score_metrics.final_scores.mean if stats.score_metrics.final_scores.count > 0 else 0
                print(f"{prompt_type:<25} {stats.success_metrics.total_tests:>8} {success_rate:>9.1f}% {avg_score:>10.3f}")
        
        # Flawed prompts (å¦‚æœæœ‰)
        if 'flawed' in model_stats.by_prompt_type:
            print("\nFlawed Promptç­–ç•¥:")
            stats = model_stats.by_prompt_type['flawed']
            success_rate = stats.success_metrics.success_rate * 100
            avg_score = stats.score_metrics.final_scores.mean if stats.score_metrics.final_scores.count > 0 else 0
            print(f"{'flawed (all)':<25} {stats.success_metrics.total_tests:>8} {success_rate:>9.1f}% {avg_score:>10.3f}")
    
    # æŒ‰ç¼ºé™·ç±»å‹ç»Ÿè®¡
    if model_stats.by_flaw_type:
        print("\nğŸ”§ æŒ‰ç¼ºé™·ç±»å‹ç»Ÿè®¡:")
        print("-" * 60)
        print(f"{'ç¼ºé™·ç±»å‹':<25} {'æµ‹è¯•æ•°':>8} {'æˆåŠŸç‡':>10} {'Avg Score':>10}")
        print("-" * 60)
        flaw_types = [
            "sequence_disorder", "tool_misuse", "parameter_error",
            "missing_step", "redundant_operations", 
            "logical_inconsistency", "semantic_drift"
        ]
        for flaw_type in flaw_types:
            if flaw_type in model_stats.by_flaw_type:
                stats = model_stats.by_flaw_type[flaw_type]
                success_rate = stats.success_metrics.success_rate * 100
                avg_score = stats.score_metrics.final_scores.mean if stats.score_metrics.final_scores.count > 0 else 0
                print(f"{flaw_type:<25} {stats.success_metrics.total_tests:>8} {success_rate:>9.1f}% {avg_score:>10.3f}")
    
    # æµ‹è¯•å‡è¡¡æ€§åˆ†æ
    print("\nğŸ“Š æµ‹è¯•å‡è¡¡æ€§åˆ†æ:")
    print("-" * 60)
    
    # æ”¶é›†æ‰€æœ‰æµ‹è¯•æ•°é‡
    all_counts = []
    
    # ä»prompt typeæ”¶é›†
    for pt in ['baseline', 'optimal', 'cot']:
        if pt in model_stats.by_prompt_type:
            all_counts.append(model_stats.by_prompt_type[pt].success_metrics.total_tests)
    
    # ä»flaw typeæ”¶é›†
    for ft in model_stats.by_flaw_type.keys():
        all_counts.append(model_stats.by_flaw_type[ft].success_metrics.total_tests)
    
    if all_counts:
        min_count = min(all_counts)
        max_count = max(all_counts)
        avg_count = sum(all_counts) / len(all_counts)
        
        print(f"ç­–ç•¥æ•°é‡: {len(all_counts)}")
        print(f"æœ€å°‘æµ‹è¯•: {min_count}")
        print(f"æœ€å¤šæµ‹è¯•: {max_count}")
        print(f"å¹³å‡æµ‹è¯•: {avg_count:.1f}")
        print(f"å·®å¼‚: {max_count - min_count}")
        
        if max_count - min_count <= avg_count * 0.1:  # 10%ä»¥å†…
            print("âœ… åˆ†é…éå¸¸å‡è¡¡")
        elif max_count - min_count <= avg_count * 0.2:  # 20%ä»¥å†…
            print("âœ“ åˆ†é…è¾ƒä¸ºå‡è¡¡")
        else:
            print("âš ï¸ åˆ†é…ä¸å¤Ÿå‡è¡¡")
    
    # å·¥å…·å¯é æ€§æ•æ„Ÿåº¦ï¼ˆå¦‚æœæœ‰æ•°æ®ï¼‰
    if model_stats.by_tool_reliability:
        print("\nğŸ”§ å·¥å…·å¯é æ€§æ•æ„Ÿåº¦:")
        print("-" * 60)
        print(f"{'å¯é æ€§':<10} {'æµ‹è¯•æ•°':>8} {'æˆåŠŸç‡':>10}")
        print("-" * 60)
        for reliability, metrics in sorted(model_stats.by_tool_reliability.items()):
            success_rate = metrics.success_rate * 100
            print(f"{reliability:<10.1f} {metrics.total_tests:>8} {success_rate:>9.1f}%")
    
    if detailed:
        # æ˜¾ç¤ºæ›´è¯¦ç»†çš„ä¿¡æ¯
        print("\nğŸ“ è¯¦ç»†å·¥å…·ä½¿ç”¨ç»Ÿè®¡:")
        print("-" * 60)
        if model_stats.overall_execution.tools_used:
            sorted_tools = sorted(model_stats.overall_execution.tools_used.items(), 
                                key=lambda x: x[1], reverse=True)[:10]
            print("Top 10 ä½¿ç”¨æœ€å¤šçš„å·¥å…·:")
            for tool, count in sorted_tools:
                print(f"  {tool}: {count}æ¬¡")
    
    print("\n" + "=" * 80)
    print("æŠ¥å‘Šç”Ÿæˆå®Œæ¯•")
    print("=" * 80)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='æŸ¥çœ‹å¢å¼ºæ‰¹æµ‹è¯•ç»Ÿè®¡')
    parser.add_argument('--model', type=str, default='qwen2.5-3b-instruct',
                       help='æ¨¡å‹åç§°')
    parser.add_argument('--detailed', action='store_true',
                       help='æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯')
    args = parser.parse_args()
    
    view_enhanced_statistics(args.model, args.detailed)