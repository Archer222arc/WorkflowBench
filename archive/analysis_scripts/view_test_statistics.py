#!/usr/bin/env python3
"""
æŸ¥çœ‹æ‰¹æµ‹è¯•çš„è¯¦ç»†ç»Ÿè®¡ç»“æœ
"""

import argparse
from batch_test_runner import BatchTestRunner

def view_statistics(model: str = "qwen2.5-3b-instruct"):
    """æŸ¥çœ‹æµ‹è¯•ç»Ÿè®¡"""
    
    print("\n" + "=" * 80)
    print("WorkflowBench æ‰¹æµ‹è¯•ç»Ÿè®¡æŠ¥å‘Š")
    print("=" * 80)
    
    # åˆå§‹åŒ–runneræ¥è®¿é—®ç´¯ç§¯æ•°æ®
    runner = BatchTestRunner(debug=False, silent=True)
    runner._lazy_init()
    
    # è·å–è¿›åº¦æŠ¥å‘Š
    progress = runner.manager.get_progress_report(model)
    
    if model not in progress.get('models', {}):
        print(f"\nâŒ æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹ {model} çš„æµ‹è¯•æ•°æ®")
        print("\nå¯ç”¨çš„æ¨¡å‹:")
        for m in progress.get('models', {}).keys():
            print(f"  - {m}")
        return
    
    model_data = progress['models'][model]
    total_tests = model_data['total_tests']
    total_success = model_data['total_success']
    success_rate = (total_success / total_tests * 100) if total_tests > 0 else 0
    
    print(f"\nğŸ“Š æ¨¡å‹: {model}")
    print(f"æ€»æµ‹è¯•æ•°: {total_tests}")
    print(f"æˆåŠŸæ•°: {total_success}")
    print(f"æˆåŠŸç‡: {success_rate:.1f}%")
    
    # æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡
    print("\nğŸ“‹ æŒ‰ä»»åŠ¡ç±»å‹:")
    print("-" * 60)
    print(f"{'ä»»åŠ¡ç±»å‹':<25} {'æµ‹è¯•æ•°':>10} {'æˆåŠŸæ•°':>10} {'æˆåŠŸç‡':>10}")
    print("-" * 60)
    for task_type, stats in sorted(model_data.get('by_task_type', {}).items()):
        rate = (stats['success'] / stats['total'] * 100) if stats['total'] > 0 else 0
        print(f"{task_type:<25} {stats['total']:>10} {stats['success']:>10} {rate:>9.1f}%")
    
    # æŒ‰Promptç±»å‹ç»Ÿè®¡  
    print("\nğŸ¯ æŒ‰Promptç­–ç•¥ (3ç§åŸºæœ¬):")
    print("-" * 60)
    print(f"{'Promptç±»å‹':<25} {'æµ‹è¯•æ•°':>10} {'æˆåŠŸæ•°':>10} {'æˆåŠŸç‡':>10}")
    print("-" * 60)
    for prompt_type in ['baseline', 'optimal', 'cot']:
        stats = model_data.get('by_prompt_type', {}).get(prompt_type, {'total': 0, 'success': 0})
        if stats['total'] > 0:
            rate = (stats['success'] / stats['total'] * 100)
            print(f"{prompt_type:<25} {stats['total']:>10} {stats['success']:>10} {rate:>9.1f}%")
    
    # æŒ‰ç¼ºé™·ç±»å‹ç»Ÿè®¡
    print("\nğŸ”§ æŒ‰ç¼ºé™·ç±»å‹ (7ç§ç¼ºé™·):")
    print("-" * 60)
    print(f"{'ç¼ºé™·ç±»å‹':<25} {'æµ‹è¯•æ•°':>10} {'æˆåŠŸæ•°':>10} {'æˆåŠŸç‡':>10}")
    print("-" * 60)
    flaw_types = [
        "sequence_disorder", "tool_misuse", "parameter_error",
        "missing_step", "redundant_operations", 
        "logical_inconsistency", "semantic_drift"
    ]
    for flaw_type in flaw_types:
        stats = model_data.get('by_flaw_type', {}).get(flaw_type, {'total': 0, 'success': 0})
        if stats['total'] > 0:
            rate = (stats['success'] / stats['total'] * 100)
            print(f"{flaw_type:<25} {stats['total']:>10} {stats['success']:>10} {rate:>9.1f}%")
    
    # å®éªŒè®¡åˆ’åˆè§„æ€§æ£€æŸ¥
    print("\nâœ… å®éªŒè®¡åˆ’åˆè§„æ€§:")
    print("-" * 60)
    
    # æ£€æŸ¥10ç§ç­–ç•¥çš„è¦†ç›–
    strategies_covered = 0
    print("3ç§åŸºæœ¬Prompt:")
    for pt in ['baseline', 'optimal', 'cot']:
        count = model_data.get('by_prompt_type', {}).get(pt, {'total': 0})['total']
        status = "âœ“" if count > 0 else "âœ—"
        print(f"  {status} {pt}: {count} ä¸ªæµ‹è¯•")
        if count > 0:
            strategies_covered += 1
    
    print("\n7ç§ç¼ºé™·Prompt:")
    for ft in flaw_types:
        count = model_data.get('by_flaw_type', {}).get(ft, {'total': 0})['total']
        status = "âœ“" if count > 0 else "âœ—"
        print(f"  {status} flawed_{ft}: {count} ä¸ªæµ‹è¯•")
        if count > 0:
            strategies_covered += 1
    
    print(f"\nç­–ç•¥è¦†ç›–: {strategies_covered}/10")
    if strategies_covered == 10:
        print("ğŸ‰ å®Œå…¨ç¬¦åˆå®éªŒè®¡åˆ’è¦æ±‚ï¼")
    else:
        print(f"âš ï¸ è¿˜éœ€è¦è¦†ç›– {10 - strategies_covered} ç§ç­–ç•¥")
    
    # å‡è¡¡æ€§åˆ†æ
    print("\nğŸ“Š åˆ†é…å‡è¡¡æ€§åˆ†æ:")
    print("-" * 60)
    
    all_counts = []
    # æ”¶é›†æ‰€æœ‰ç­–ç•¥çš„æµ‹è¯•æ•°
    for pt in ['baseline', 'optimal', 'cot']:
        count = model_data.get('by_prompt_type', {}).get(pt, {'total': 0})['total']
        if count > 0:
            all_counts.append(count)
    
    for ft in flaw_types:
        count = model_data.get('by_flaw_type', {}).get(ft, {'total': 0})['total']
        if count > 0:
            all_counts.append(count)
    
    if all_counts:
        min_count = min(all_counts)
        max_count = max(all_counts)
        avg_count = sum(all_counts) / len(all_counts)
        
        print(f"æœ€å°‘æµ‹è¯•æ•°: {min_count}")
        print(f"æœ€å¤šæµ‹è¯•æ•°: {max_count}")
        print(f"å¹³å‡æµ‹è¯•æ•°: {avg_count:.1f}")
        print(f"åˆ†é…å·®å¼‚: {max_count - min_count}")
        
        if max_count - min_count <= 1:
            print("âœ… åˆ†é…éå¸¸å‡è¡¡")
        elif max_count - min_count <= 3:
            print("âœ“ åˆ†é…è¾ƒä¸ºå‡è¡¡")
        else:
            print("âš ï¸ åˆ†é…ä¸å¤Ÿå‡è¡¡ï¼Œå»ºè®®è¿è¡Œæ›´å¤šæµ‹è¯•")
    
    print("\n" + "=" * 80)
    print("æŠ¥å‘Šç”Ÿæˆå®Œæ¯•")
    print("=" * 80)

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='æŸ¥çœ‹æ‰¹æµ‹è¯•ç»Ÿè®¡')
    parser.add_argument('--model', type=str, default='qwen2.5-3b-instruct',
                       help='æ¨¡å‹åç§°')
    args = parser.parse_args()
    
    view_statistics(args.model)