# è°ƒè¯•çŸ¥è¯†åº“ (Debug Knowledge Base)

## ğŸ“‹ ç›®å½•
1. [å¸¸è§é”™è¯¯æ¨¡å¼](#å¸¸è§é”™è¯¯æ¨¡å¼)
2. [å·²è§£å†³çš„Bugè®°å½•](#å·²è§£å†³çš„bugè®°å½•)
3. [è°ƒè¯•æ£€æŸ¥æ¸…å•](#è°ƒè¯•æ£€æŸ¥æ¸…å•)
4. [æ€§èƒ½ä¼˜åŒ–ç‚¹](#æ€§èƒ½ä¼˜åŒ–ç‚¹)
5. [æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥](#æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥)

---

## å¸¸è§é”™è¯¯æ¨¡å¼

### 1. AttributeError: 'XXX' object has no attribute 'YYY'

#### æ¨¡å¼1: ExecutionStateç¼ºå°‘å±æ€§
```python
# é—®é¢˜: state.format_error_count ä¸å­˜åœ¨
# åŸå› : ExecutionStateç±»å®šä¹‰ä¸­ç¼ºå°‘è¯¥å±æ€§

# è§£å†³æ–¹æ¡ˆ:
# 1. ä½¿ç”¨ hasattr() æ£€æŸ¥
if hasattr(state, 'format_error_count'):
    count = state.format_error_count

# 2. ä½¿ç”¨ getattr() å¸¦é»˜è®¤å€¼
count = getattr(state, 'format_error_count', 0)
```

#### æ¨¡å¼2: TestRecordå­—æ®µç¼ºå¤±
```python
# é—®é¢˜: record.execution_status ä¸å­˜åœ¨
# åŸå› : TestRecordåˆ›å»ºæ—¶æœªè®¾ç½®è¯¥å­—æ®µ

# è§£å†³æ–¹æ¡ˆ: åœ¨åˆ›å»ºTestRecordåç«‹å³è®¾ç½®
record.execution_status = result.get('success_level', 'failure')
record.format_error_count = result.get('format_error_count', 0)
```

### 2. KeyError: å­—å…¸é”®ä¸å­˜åœ¨

#### æ¨¡å¼1: test_recordå­—å…¸ç¼ºå°‘é”®
```python
# é—®é¢˜: test_record['executed_tools'] KeyError
# åŸå› : æ—§ç‰ˆæœ¬æ•°æ®æ²¡æœ‰è¯¥å­—æ®µ

# è§£å†³æ–¹æ¡ˆ: ä½¿ç”¨ get() æ–¹æ³•
executed_tools = test_record.get('executed_tools', [])
# æˆ–è€…ä½¿ç”¨å›é€€
executed_tools = test_record.get('executed_tools', test_record.get('tool_calls', []))
```

### 3. TypeError: ç±»å‹ä¸åŒ¹é…

#### æ¨¡å¼1: Noneå€¼è¿ç®—
```python
# é—®é¢˜: TypeError: unsupported operand type(s) for +: 'NoneType' and 'int'
# åŸå› : scoreå¯èƒ½ä¸ºNone

# è§£å†³æ–¹æ¡ˆ: æ·»åŠ Noneæ£€æŸ¥
if score is not None:
    total += score
```

#### æ¨¡å¼2: åˆ—è¡¨/å­—å…¸æ··æ·†
```python
# é—®é¢˜: 'list' object has no attribute 'get'
# åŸå› : tool_callså¯èƒ½æ˜¯åˆ—è¡¨æˆ–å­—å…¸

# è§£å†³æ–¹æ¡ˆ: ç±»å‹æ£€æŸ¥
if isinstance(tool_calls, list):
    for tool in tool_calls:
        # å¤„ç†åˆ—è¡¨
elif isinstance(tool_calls, dict):
    # å¤„ç†å­—å…¸
```

---

## å·²è§£å†³çš„Bugè®°å½•

### Bug #1: é”™è¯¯åˆ†ç±»ä¸æ­£ç¡® (2025-01-07)
**ç—‡çŠ¶**: æ‰€æœ‰é”™è¯¯ç‡æ˜¾ç¤ºä¸º0
**åŸå› **: 
1. APIé”™è¯¯è¢«é”™è¯¯åœ°åˆ†ç±»ä¸ºå·¥ä½œæµé”™è¯¯
2. _generate_intelligent_error_messageè¿”å›é€šç”¨æ¶ˆæ¯è¦†ç›–äº†åŸå§‹é”™è¯¯

**è§£å†³æ–¹æ¡ˆ**:
```python
# interactive_executor.py
# ä¿ç•™åŸå§‹é”™è¯¯æ¶ˆæ¯
if 'timeout' in error_lower:
    return f"{error} (tool: {tool_name})"  # ä¿ç•™åŸå§‹é”™è¯¯
```

### Bug #2: tool_coverage_rateå§‹ç»ˆä¸º0 (2025-01-07)
**ç—‡çŠ¶**: tool_coverage_rate = 0.0
**åŸå› **: ä½¿ç”¨tool_callsè€Œä¸æ˜¯executed_tools

**è§£å†³æ–¹æ¡ˆ**:
```python
# cumulative_data_structure.py
# ä¼˜å…ˆä½¿ç”¨executed_tools
tools_to_track = test_record.get('executed_tools', test_record.get('tool_calls', []))
```

### Bug #3: assisted_attemptç»Ÿè®¡æ··ä¹± (2025-01-08)
**ç—‡çŠ¶**: assistedç»Ÿè®¡å½±å“äº†åŸæœ‰çš„success/failureç»Ÿè®¡
**åŸå› **: ä½¿ç”¨äº†æ¡ä»¶åˆ†æ”¯å¯¼è‡´æŸäº›æµ‹è¯•ä¸è®¡å…¥total_tests

**è§£å†³æ–¹æ¡ˆ**:
```python
# æ”¹ä¸ºå¹¶è¡Œç»Ÿè®¡
self.overall_success.total_tests += 1  # å§‹ç»ˆè®¡å…¥
if had_assistance:  # é¢å¤–ç»Ÿè®¡
    if success:
        self.overall_success.assisted_success += 1
    else:
        self.overall_success.assisted_failure += 1
```

### Bug #4: å¹¶å‘å†™å…¥æ•°æ®åº“å†²çª (å·²è§£å†³)
**ç—‡çŠ¶**: å¤šä¸ªè¿›ç¨‹åŒæ—¶å†™å…¥å¯¼è‡´æ•°æ®ä¸¢å¤±
**åŸå› **: ä¸´æ—¶æ–‡ä»¶åå†²çª

**è§£å†³æ–¹æ¡ˆ**:
```python
# cumulative_test_manager.py
import uuid
temp_file = self.db_file.parent / f"{self.db_file.stem}_{uuid.uuid4().hex}.tmp"
```

---

## è°ƒè¯•æ£€æŸ¥æ¸…å•

### å¯åŠ¨æ–°æµ‹è¯•å‰æ£€æŸ¥
- [ ] æ£€æŸ¥ `master_database.json` æ˜¯å¦å­˜åœ¨
- [ ] ç¡®è®¤æ¨¡å‹åç§°åœ¨ `SUPPORTED_MODELS` åˆ—è¡¨ä¸­
- [ ] éªŒè¯ API å¯†é’¥é…ç½®æ­£ç¡®
- [ ] ç¡®è®¤ `mcp_generated_library/task_library.json` å­˜åœ¨

### æµ‹è¯•å¤±è´¥æ—¶æ£€æŸ¥
- [ ] æŸ¥çœ‹å®Œæ•´é”™è¯¯å †æ ˆ
- [ ] æ£€æŸ¥ `test_record` æ˜¯å¦åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µ
- [ ] éªŒè¯ `success_level` å€¼æ˜¯å¦åˆæ³•
- [ ] ç¡®è®¤ `format_error_count` æ˜¯å¦æ­£ç¡®ä¼ é€’

### ç»Ÿè®¡å¼‚å¸¸æ—¶æ£€æŸ¥
- [ ] æ£€æŸ¥ `ModelStatistics.update_from_test()` æ¥æ”¶çš„å­—å…¸
- [ ] éªŒè¯ `had_assistance` è®¡ç®—é€»è¾‘
- [ ] ç¡®è®¤ç»Ÿè®¡æ›´æ–°æ²¡æœ‰æ¡ä»¶åˆ†æ”¯å½±å“ `total_tests`
- [ ] æ£€æŸ¥æ˜¯å¦æ‰€æœ‰ç»Ÿè®¡å±‚çº§éƒ½æ›´æ–°äº†

---

## æ€§èƒ½ä¼˜åŒ–ç‚¹

### 1. æ‰¹é‡æ•°æ®åº“å†™å…¥
```python
# ä¸è¦æ¯ä¸ªæµ‹è¯•éƒ½å†™æ•°æ®åº“
local_records = []
for task in tasks:
    result = run_test(task)
    local_records.append(create_record(result))

# æ‰¹é‡å†™å…¥
for record in local_records:
    manager.add_test_result(record)
manager.save_database()  # ä¸€æ¬¡æ€§ä¿å­˜
```

### 2. LLMè°ƒç”¨ä¼˜åŒ–
```python
# é‡è¯•æœºåˆ¶ä¼˜åŒ–
max_retries = 5  # è€Œä¸æ˜¯3
base_wait = random.uniform(0.5, 1.5)  # å‡å°‘åŸºç¡€ç­‰å¾…
wait_time = min(base_wait * (1.5 ** attempt), 10)  # ä¸Šé™10ç§’
```

### 3. å¹¶å‘æ§åˆ¶
```python
# QPSé™åˆ¶
min_interval = 1.0 / qps
with self._request_lock:
    time_since_last = now - self._last_request_time
    if time_since_last < min_interval:
        time.sleep(min_interval - time_since_last)
```

---

## æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥

### 1. éªŒè¯ç»Ÿè®¡æ€»å’Œ
```python
def validate_stats(stats: SuccessMetrics):
    # åŸºæœ¬ä¸‰å…ƒç»„å¿…é¡»ç­‰äºæ€»æ•°
    assert stats.total_tests == (stats.full_success + 
                                 stats.partial_success + 
                                 stats.failure)
    
    # assistedç»Ÿè®¡æ˜¯é¢å¤–çš„
    assert stats.total_assisted_tests == (stats.assisted_success + 
                                          stats.assisted_failure)
    
    # assistedä¸èƒ½è¶…è¿‡æ€»æµ‹è¯•æ•°
    assert stats.tests_with_assistance <= stats.total_tests
```

### 2. å­—æ®µå®Œæ•´æ€§æ£€æŸ¥
```python
REQUIRED_FIELDS = [
    'model', 'task_type', 'prompt_type', 'success', 
    'success_level', 'execution_time', 'timestamp'
]

def validate_record(record: Dict):
    for field in REQUIRED_FIELDS:
        assert field in record, f"Missing required field: {field}"
```

### 3. åˆ†æ•°èŒƒå›´æ£€æŸ¥
```python
def validate_scores(record: Dict):
    score_fields = ['workflow_score', 'phase2_score', 
                   'quality_score', 'final_score']
    for field in score_fields:
        if field in record and record[field] is not None:
            assert 0 <= record[field] <= 1, f"{field} out of range"
```

---

## å¸¸ç”¨è°ƒè¯•å‘½ä»¤

### 1. æŸ¥çœ‹æ•°æ®åº“å†…å®¹
```python
import json
with open('pilot_bench_cumulative_results/master_database.json') as f:
    db = json.load(f)
    print(f"Models: {list(db['models'].keys())}")
    for model, stats in db['models'].items():
        print(f"{model}: {stats.get('overall_success', {}).get('total_tests', 0)} tests")
```

### 2. æ¸…ç†æµ‹è¯•æ•°æ®
```python
from cumulative_test_manager import CumulativeTestManager
manager = CumulativeTestManager()
manager.clear_database()  # ä¼šè‡ªåŠ¨å¤‡ä»½
```

### 3. éªŒè¯å•ä¸ªæµ‹è¯•
```python
# è¿è¡Œå•ä¸ªæµ‹è¯•ç”¨äºè°ƒè¯•
python batch_test_runner.py --model gpt-4o-mini --count 1 --debug
```

### 4. æ£€æŸ¥é”™è¯¯åˆ†ç±»
```python
# æŸ¥çœ‹é”™è¯¯åˆ†ç±»ç»Ÿè®¡
stats = db['models']['gpt-4o-mini']['overall_errors']
print(f"Format errors: {stats.get('tool_call_format_errors', 0)}")
print(f"Timeout errors: {stats.get('timeout_errors', 0)}")
```

---

## è°ƒè¯•è¾“å‡ºç‚¹

### å…³é”®è°ƒè¯•ä½ç½®
1. `interactive_executor.py:_get_llm_response()` - APIè°ƒç”¨
2. `cumulative_data_structure.py:update_from_test()` - ç»Ÿè®¡æ›´æ–°
3. `batch_test_runner.py:run_single_test()` - æµ‹è¯•æ‰§è¡Œ
4. `cumulative_test_manager.py:add_test_result()` - æ•°æ®ä¿å­˜

### è°ƒè¯•è¾“å‡ºç¤ºä¾‹
```python
# æ·»åŠ è°ƒè¯•è¾“å‡º
print(f"[DEBUG] format_error_count: {format_error_count}")
print(f"[DEBUG] had_assistance: {had_assistance}")
print(f"[DEBUG] success_level: {success_level}")
print(f"[DEBUG] Updating stats - before: {stats.total_tests}")
```

---

**æ–‡æ¡£åˆ›å»ºæ—¶é—´**: 2025-01-08
**æœ€åæ›´æ–°**: 2025-01-08
**ç‰ˆæœ¬**: 1.0