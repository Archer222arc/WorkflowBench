#!/usr/bin/env python3
"""
PILOT-Bench ç´¯ç§¯æµ‹è¯•ç®¡ç†ç³»ç»Ÿ
æ”¯æŒä»»æ„æ–¹å¼çš„æµ‹è¯•ï¼Œæ‰€æœ‰ç»“æœè‡ªåŠ¨ç´¯ç§¯
åªä¿å­˜ç»Ÿè®¡æ•°æ®ï¼Œä¸ä¿å­˜å®ä¾‹ç»†èŠ‚
"""
import json
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any
from old_data_structures.cumulative_data_structure import ModelStatistics

from dataclasses import dataclass, asdict, field
import hashlib
from threading import Lock
try:
    from file_lock_manager import get_file_lock
    FILE_LOCK_AVAILABLE = True
except ImportError:
    FILE_LOCK_AVAILABLE = False

# ä½¿ç”¨çº¯å­—å…¸ç»“æ„ï¼Œä¸å†å¯¼å…¥æ•°æ®ç»“æ„ç±»
# V3ç‰ˆæœ¬ï¼šæ‰€æœ‰æ•°æ®éƒ½æ˜¯å­—å…¸æ ¼å¼ï¼Œç›´æ¥æ“ä½œ

# ç´¯ç§¯ç»“æœå­˜å‚¨è·¯å¾„
CUMULATIVE_DB_PATH = Path("pilot_bench_cumulative_results")
CUMULATIVE_DB_PATH.mkdir(exist_ok=True)

def normalize_model_name(model_name: str) -> str:
    """
    è§„èŒƒåŒ–æ¨¡å‹åç§°ï¼Œå°†åŒä¸€æ¨¡å‹çš„ä¸åŒå®ä¾‹æ˜ å°„åˆ°ä¸»åç§°
    ä¾‹å¦‚ï¼šdeepseek-v3-0324-2 -> DeepSeek-V3-0324
    åŒ…æ‹¬å¤„ç†å¹¶è¡Œå®ä¾‹ï¼ˆ-2, -3ç­‰åç¼€ï¼‰
    """
    import re
    model_name_lower = model_name.lower()
    
    # é¦–å…ˆå¤„ç†å¹¶è¡Œå®ä¾‹åç¼€ï¼ˆ-2, -3ç­‰ï¼‰
    # åªå¯¹DeepSeekã€Llamaã€Grokç­‰å·²çŸ¥ä½¿ç”¨å¹¶è¡Œå®ä¾‹çš„æ¨¡å‹å»é™¤åç¼€
    if any(base in model_name_lower for base in ['deepseek', 'llama', 'grok']):
        # ç§»é™¤ -æ•°å­— åç¼€
        model_name_cleaned = re.sub(r'-\d+$', '', model_name)
        model_name_lower = model_name_cleaned.lower()
    else:
        model_name_cleaned = model_name
    
    # DeepSeek V3 ç³»åˆ—
    if 'deepseek-v3' in model_name_lower or 'deepseek_v3' in model_name_lower:
        return 'DeepSeek-V3-0324'
    
    # DeepSeek R1 ç³»åˆ—
    if 'deepseek-r1' in model_name_lower or 'deepseek_r1' in model_name_lower:
        return 'DeepSeek-R1-0528'
    
    # Llama 3.3 ç³»åˆ—
    if 'llama-3.3' in model_name_lower or 'llama_3.3' in model_name_lower:
        return 'Llama-3.3-70B-Instruct'
    
    # Grok ç³»åˆ—
    if 'grok-3' in model_name_lower or 'grok_3' in model_name_lower:
        return 'grok-3'
    
    # Qwen ç³»åˆ— - æ ¹æ®å‚æ•°è§„æ¨¡ç¡®å®šå…·ä½“æ¨¡å‹ï¼ˆä¸å»é™¤åç¼€ï¼‰
    if 'qwen' in model_name_lower:
        if '72b' in model_name_lower:
            return 'qwen2.5-72b-instruct'
        elif '32b' in model_name_lower:
            return 'qwen2.5-32b-instruct'
        elif '14b' in model_name_lower:
            return 'qwen2.5-14b-instruct'
        elif '7b' in model_name_lower:
            return 'qwen2.5-7b-instruct'
        elif '3b' in model_name_lower:
            return 'qwen2.5-3b-instruct'
    
    # å…¶ä»–æ¨¡å‹è¿”å›æ¸…ç†åçš„åç§°
    return model_name_cleaned

@dataclass
class TestRecord:
    """å•ä¸ªæµ‹è¯•è®°å½•"""
    # åŸºæœ¬ä¿¡æ¯
    model: str
    task_type: str
    prompt_type: str
    difficulty: str = "easy"
    
    # æµ‹è¯•ç»“æœ
    success: bool = False
    partial_success: bool = False
    execution_time: float = 0.0
    error_message: Optional[str] = None
    
    # é”™è¯¯åˆ†ç±»ä¿¡æ¯
    format_recognition_errors: int = 0
    instruction_following_errors: int = 0
    tool_selection_errors: int = 0
    parameter_config_errors: int = 0
    sequence_order_errors: int = 0
    dependency_errors: int = 0
    
    # ç¼ºé™·ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
    is_flawed: bool = False
    flaw_type: Optional[str] = None
    
    # å…ƒæ•°æ®
    timestamp: str = ""
    task_id: Optional[str] = None
    session_id: Optional[str] = None
    test_instance: int = 0  # ç¬¬å‡ æ¬¡æµ‹è¯•
    
    # æ‰§è¡Œç»†èŠ‚ï¼ˆæ–°å¢ï¼‰
    turns: int = 0  # æ‰§è¡Œè½®æ•°
    tool_calls: List = field(default_factory=list)  # å·¥å…·è°ƒç”¨åˆ—è¡¨
    success_level: str = "failure"  # full_success, partial_success, failure
    execution_status: str = "failure"  # ä¸success_levelç›¸åŒï¼Œç”¨äºå…¼å®¹æ€§
    
    # æ–°å¢çš„é‡è¦å­—æ®µ
    format_error_count: int = 0  # æ ¼å¼é”™è¯¯è®¡æ•°
    api_issues: List = field(default_factory=list)  # APIå±‚é¢çš„é—®é¢˜
    executed_tools: List = field(default_factory=list)  # å®é™…æ‰§è¡Œçš„å·¥å…·
    required_tools: List = field(default_factory=list)  # ä»»åŠ¡è¦æ±‚çš„å·¥å…·
    execution_history: List = field(default_factory=list)  # å·¥å…·æ‰§è¡Œå†å²
    
    # åˆ†æ•°æŒ‡æ ‡ï¼ˆå¦‚æœæœ‰ï¼‰
    workflow_score: Optional[float] = None
    phase2_score: Optional[float] = None
    quality_score: Optional[float] = None
    final_score: Optional[float] = None
    tool_reliability: float = 0.8  # å·¥å…·å¯é æ€§è®¾ç½®
    
    def get_key(self) -> str:
        """ç”Ÿæˆå”¯ä¸€é”®"""
        parts = [self.model, self.task_type, self.prompt_type, self.difficulty]
        if self.is_flawed and self.flaw_type:
            parts.extend(["flawed", self.flaw_type])
        return "_".join(parts)
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return asdict(self)

class CumulativeTestManager:
    """ç´¯ç§¯æµ‹è¯•ç®¡ç†å™¨"""
    
    def __init__(self, db_suffix=''):
        # æ”¯æŒè‡ªå®šä¹‰æ•°æ®åº“æ–‡ä»¶åç¼€ï¼ˆç”¨äºåŒºåˆ†é—­æº/å¼€æºæ¨¡å‹ï¼‰
        if db_suffix:
            self.db_file = CUMULATIVE_DB_PATH / f"master_database{db_suffix}.json"
        else:
            self.db_file = CUMULATIVE_DB_PATH / "master_database.json"
        self.lock = Lock()  # çº¿ç¨‹é”
        
        # æ·»åŠ æ–‡ä»¶é”ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if FILE_LOCK_AVAILABLE:
            self.file_lock = get_file_lock(self.db_file)
        else:
            self.file_lock = None
        self.database = self._load_database()
        
    def _load_database(self) -> Dict:
        """åŠ è½½æ•°æ®åº“"""
        if self.db_file.exists():
            try:
                with open(self.db_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                # V3ç‰ˆæœ¬ï¼šä¸å†ååºåˆ—åŒ–modelsï¼Œä¿æŒå­—å…¸æ ¼å¼
                # æ³¨é‡Šæ‰ååºåˆ—åŒ–ä»¥é¿å…åˆ›å»ºModelStatisticså¯¹è±¡
                # if "models" in data:
                #     self._deserialize_models(data)
                return data
            except Exception as e:
                # å¦‚æœåŠ è½½å¤±è´¥ï¼Œåˆ›å»ºå¤‡ä»½
                backup_file = self.db_file.with_suffix('.backup')
                self.db_file.rename(backup_file)
                print(f"æ•°æ®åº“æŸåï¼Œå·²å¤‡ä»½åˆ°: {backup_file}ï¼Œé”™è¯¯: {e}")
                return self._create_empty_database()
        else:
            return self._create_empty_database()
    
    # V3ç‰ˆæœ¬ï¼šå·²ç§»é™¤æ‰€æœ‰ååºåˆ—åŒ–æ–¹æ³•
    # æ•°æ®ç›´æ¥ä»¥å­—å…¸æ ¼å¼å­˜å‚¨å’Œä½¿ç”¨ï¼Œä¸å†è½¬æ¢ä¸ºå¯¹è±¡
    
    def _create_empty_database(self) -> Dict:
        """åˆ›å»ºç©ºæ•°æ®åº“ - V3çº¯å­—å…¸ç»“æ„"""
        return {
            "version": "3.0",
            "created_at": datetime.now().isoformat(),
            "last_updated": datetime.now().isoformat(),
            "test_groups": {},  # ä¿ç•™ç”¨äºå…¼å®¹
            "models": {},  # çº¯å­—å…¸æ ¼å¼çš„æ¨¡å‹ç»Ÿè®¡
            "summary": {
                "total_tests": 0,
                "total_success": 0,
                "total_partial": 0,
                "total_failure": 0,
                "models_tested": [],
                "last_test_time": None
            }
        }
    
    def save_database(self):
        """ä¿å­˜æ•°æ®åº“ï¼ˆåŸå­æ“ä½œï¼Œæ”¯æŒæ–‡ä»¶é”ï¼‰"""
        self.database["last_updated"] = datetime.now().isoformat()
        
        if self.file_lock:
            # ä½¿ç”¨æ–‡ä»¶é”è¿›è¡Œå¤šè¿›ç¨‹å®‰å…¨å†™å…¥
            def update_func(current_data):
                # åˆå¹¶å½“å‰ç£ç›˜æ•°æ®å’Œå†…å­˜æ•°æ®ï¼Œé¿å…è¦†ç›–å…¶ä»–è¿›ç¨‹çš„æ›´æ–°
                if current_data and isinstance(current_data, dict):
                    # åˆå¹¶modelsæ•°æ®
                    if "models" in current_data:
                        for model_name, model_data in current_data["models"].items():
                            # å¦‚æœè¿™ä¸ªæ¨¡å‹ä¸åœ¨å†…å­˜ä¸­ï¼Œä¿ç•™ç£ç›˜ä¸Šçš„æ•°æ®
                            if model_name not in self.database.get("models", {}):
                                self.database["models"][model_name] = model_data
                    
                    # åˆå¹¶test_groups
                    if "test_groups" in current_data:
                        for group_id, group_data in current_data["test_groups"].items():
                            if group_id not in self.database.get("test_groups", {}):
                                self.database["test_groups"][group_id] = group_data
                
                return self._serialize_database()
            
            success = self.file_lock.update_json_safe(update_func)
            if not success:
                print("[è­¦å‘Š] ä½¿ç”¨æ–‡ä»¶é”ä¿å­˜å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šä¿å­˜")
                self._save_database_fallback()
        else:
            # ğŸ”§ æ•°æ®ä¿æŠ¤ä¿®å¤ï¼šå³ä½¿æ²¡æœ‰æ–‡ä»¶é”ä¹Ÿè¿›è¡Œæ™ºèƒ½åˆå¹¶
            self._save_database_with_merge()
    
    def _save_database_fallback(self):
        """å›é€€çš„ä¿å­˜æ–¹æ³•ï¼ˆæ— æ–‡ä»¶é”ï¼‰"""
        with self.lock:
            # å…ˆå†™å…¥ä¸´æ—¶æ–‡ä»¶
            temp_file = self.db_file.with_suffix('.tmp')
            with open(temp_file, 'w', encoding='utf-8') as f:
                # ä½¿ç”¨è‡ªå®šä¹‰åºåˆ—åŒ–å™¨å¤„ç†ModelStatisticså¯¹è±¡
                json.dump(self._serialize_database(), f, indent=2, ensure_ascii=False)
            
            # åŸå­æ›¿æ¢
            temp_file.replace(self.db_file)
    
    def _save_database_with_merge(self):
        """å¸¦æ™ºèƒ½åˆå¹¶çš„ä¿å­˜æ–¹æ³•ï¼ˆæ•°æ®ä¿æŠ¤å¢å¼ºç‰ˆï¼‰"""
        with self.lock:
            # ğŸ”§ æ•°æ®ä¿æŠ¤ä¿®å¤ï¼šä¿å­˜å‰å…ˆåˆå¹¶ç£ç›˜ä¸Šçš„æœ€æ–°æ•°æ®
            try:
                if self.db_file.exists():
                    # è¯»å–å½“å‰ç£ç›˜æ•°æ®
                    with open(self.db_file, 'r', encoding='utf-8') as f:
                        disk_data = json.load(f)
                    
                    # æ™ºèƒ½åˆå¹¶modelsæ•°æ®
                    if "models" in disk_data:
                        for model_name, disk_model_data in disk_data["models"].items():
                            if model_name not in self.database.get("models", {}):
                                # ç£ç›˜ä¸Šæœ‰ä½†å†…å­˜ä¸­æ²¡æœ‰çš„æ¨¡å‹ï¼Œä¿ç•™ç£ç›˜æ•°æ®
                                self.database["models"][model_name] = disk_model_data
                                print(f"[SAVE_PROTECTION] ä¿ç•™ç£ç›˜æ¨¡å‹æ•°æ®: {model_name}")
                            else:
                                # ä¸¤è¾¹éƒ½æœ‰çš„æ¨¡å‹ï¼Œæ™ºèƒ½åˆå¹¶prompt_typeæ•°æ®
                                memory_model = self.database["models"][model_name]
                                disk_prompts = disk_model_data.get("by_prompt_type", {})
                                memory_prompts = memory_model.get("by_prompt_type", {})
                                
                                # åˆå¹¶promptç±»å‹ï¼Œä¿ç•™æ‰€æœ‰ç±»å‹
                                for prompt_type, prompt_data in disk_prompts.items():
                                    if prompt_type not in memory_prompts:
                                        memory_prompts[prompt_type] = prompt_data
                                        print(f"[SAVE_PROTECTION] ä¿ç•™ç£ç›˜promptæ•°æ®: {model_name}/{prompt_type}")
                    
                    # åˆå¹¶test_groups
                    if "test_groups" in disk_data:
                        for group_id, group_data in disk_data["test_groups"].items():
                            if group_id not in self.database.get("test_groups", {}):
                                self.database["test_groups"][group_id] = group_data
                
                print("[SAVE_PROTECTION] å®Œæˆæ™ºèƒ½æ•°æ®åˆå¹¶")
            except Exception as e:
                print(f"[SAVE_PROTECTION] åˆå¹¶å¤±è´¥ï¼Œä½¿ç”¨çº¯å†…å­˜æ•°æ®: {e}")
            
            # ä½¿ç”¨åŸæœ‰çš„å®‰å…¨ä¿å­˜é€»è¾‘
            temp_file = self.db_file.with_suffix('.tmp')
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(self._serialize_database(), f, indent=2, ensure_ascii=False)
            
            # åŸå­æ›¿æ¢
            temp_file.replace(self.db_file)
    
    def clear_database(self):
        """æ¸…é™¤æ•°æ®åº“ä¸­çš„æ‰€æœ‰è®°å½•"""
        # å¤‡ä»½å½“å‰æ•°æ®åº“ï¼ˆåœ¨é”å¤–è¿›è¡Œï¼‰
        if self.db_file.exists():
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_file = self.db_file.parent / f"master_database_backup_{timestamp}.json"
            import shutil
            shutil.copy(self.db_file, backup_file)
            print(f"[INFO] å·²å¤‡ä»½å½“å‰æ•°æ®åº“åˆ°: {backup_file}")
        
        # åˆ›å»ºæ–°çš„ç©ºæ•°æ®åº“
        with self.lock:
            self.database = self._create_empty_database()
        
        # ä¿å­˜æ•°æ®åº“ï¼ˆsave_databaseä¼šè‡ªå·±è·å–é”ï¼‰
        self.save_database()
        print(f"[INFO] å·²æ¸…é™¤ç´¯ç§¯è®°å½•æ•°æ®åº“")
    
    def add_test_result(self, record: TestRecord) -> bool:
        """æ·»åŠ æµ‹è¯•ç»“æœ - åªä¿å­˜ç»Ÿè®¡ï¼Œä¸ä¿å­˜å®ä¾‹"""
        with self.lock:
            # æ·»åŠ æ—¶é—´æˆ³
            if not record.timestamp:
                record.timestamp = datetime.now().isoformat()
            
            # åˆå§‹åŒ–æ¨¡å‹ç»Ÿè®¡ï¼ˆå¦‚æœéœ€è¦ï¼‰- å†…å±‚æ•°æ®ä¿æŠ¤ä¿®å¤
            model = normalize_model_name(record.model)  # è§„èŒƒåŒ–æ¨¡å‹åç§°
            if model not in self.database["models"]:
                # ğŸ”§ æ•°æ®ä¿æŠ¤ä¿®å¤ï¼šå…ˆæ£€æŸ¥ç£ç›˜ä¸Šæ˜¯å¦æœ‰æœ€æ–°æ•°æ®
                try:
                    if self.db_file.exists():
                        # é‡æ–°åŠ è½½ç£ç›˜æ•°æ®ï¼Œæ£€æŸ¥å…¶ä»–è¿›ç¨‹æ˜¯å¦å·²åˆ›å»ºæ­¤æ¨¡å‹
                        with open(self.db_file, 'r', encoding='utf-8') as f:
                            latest_disk_data = json.load(f)
                        
                        if model in latest_disk_data.get("models", {}):
                            # å…¶ä»–è¿›ç¨‹å·²åˆ›å»ºï¼Œåˆå¹¶ç£ç›˜æ•°æ®é¿å…è¦†ç›–
                            self.database["models"][model] = latest_disk_data["models"][model]
                            print(f"[DATA_PROTECTION] åˆå¹¶æ¥è‡ªç£ç›˜çš„æ¨¡å‹æ•°æ®: {model}")
                        else:
                            # çœŸæ­£çš„æ–°æ¨¡å‹ï¼Œåˆ›å»ºç©ºç»“æ„
                            self.database["models"][model] = {
                                "model_name": model,
                                "first_test_time": datetime.now().isoformat(),
                                "last_test_time": datetime.now().isoformat(),
                                "total_tests": 0,
                                "overall_stats": {},
                                "by_prompt_type": {}
                            }
                            print(f"[DATA_PROTECTION] åˆ›å»ºæ–°æ¨¡å‹ç»“æ„: {model}")
                    else:
                        # æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°æ¨¡å‹
                        self.database["models"][model] = {
                            "model_name": model,
                            "first_test_time": datetime.now().isoformat(),
                            "last_test_time": datetime.now().isoformat(),
                            "total_tests": 0,
                            "overall_stats": {},
                            "by_prompt_type": {}
                        }
                        print(f"[DATA_PROTECTION] åˆ›å»ºé¦–ä¸ªæ¨¡å‹ç»“æ„: {model}")
                except Exception as e:
                    print(f"[DATA_PROTECTION] ç£ç›˜æ•°æ®åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨å†…å­˜æ•°æ®: {e}")
                    # å›é€€åˆ°åŸå§‹é€»è¾‘
                    self.database["models"][model] = {
                        "model_name": model,
                        "first_test_time": datetime.now().isoformat(),
                        "last_test_time": datetime.now().isoformat(),
                        "total_tests": 0,
                        "overall_stats": {},
                        "by_prompt_type": {}
                    }
                
                # æ›´æ–°å·²æµ‹è¯•æ¨¡å‹åˆ—è¡¨
                if model not in self.database["summary"]["models_tested"]:
                    self.database["summary"]["models_tested"].append(model)
            
            # æ„å»ºæµ‹è¯•è®°å½•å­—å…¸
            test_dict = {
                "model": model,  # ä½¿ç”¨è§„èŒƒåŒ–åçš„æ¨¡å‹åç§°
                "task_type": record.task_type,
                "prompt_type": record.prompt_type,
                "difficulty": record.difficulty,
                "success": record.success,
                "success_level": self._determine_success_level(record),
                "execution_time": record.execution_time,
                "error_message": record.error_message,
                "is_flawed": record.is_flawed,
                "flaw_type": record.flaw_type,
                "timestamp": record.timestamp,
                "turns": record.turns,
                "tool_calls": record.tool_calls,
                # Scores (if available)
                "workflow_score": record.workflow_score,
                "phase2_score": record.phase2_score,
                "quality_score": record.quality_score,
                "final_score": record.final_score,
                # Tool reliability (default 0.8) - é‡è¦ï¼šç”¨äºæ–°å±‚æ¬¡ç»“æ„
                "tool_reliability": record.tool_reliability,
                # æ–°å¢ï¼šé‡è¦çš„å­—æ®µ
                "format_error_count": record.format_error_count,
                "api_issues": record.api_issues,
                "executed_tools": record.executed_tools,
                "required_tools": record.required_tools,
                "tool_coverage_rate": getattr(record, 'tool_coverage_rate', 0.0),  # æ·»åŠ tool_coverage_rate
            }
            
            # æ›´æ–°æ¨¡å‹ç»Ÿè®¡ï¼ˆä¿æŒå­—å…¸æ ¼å¼ï¼‰
            model_data = self.database["models"][model]
            if isinstance(model_data, dict):
                # V3æ ¼å¼ - ç›´æ¥æ›´æ–°å­—å…¸
                model_data["last_test_time"] = datetime.now().isoformat()
                # æ³¨æ„ï¼šè¿™é‡Œä¸æ›´æ–°å…¶ä»–å­—æ®µï¼Œç”±enhanced_cumulative_managerå¤„ç†
            else:
                # æ—§æ ¼å¼ - ModelStatisticså¯¹è±¡
                model_data.update_from_test(test_dict)
            
            # æ›´æ–°å…¨å±€æ‘˜è¦
            self._update_global_summary_v2()
            
            # ä¿æŒå‘åå…¼å®¹æ€§ - æ›´æ–°æ—§æ ¼å¼çš„test_groupsï¼ˆä½†ä¸å­˜å‚¨å®ä¾‹ï¼‰
            # å¦‚æœæ²¡æœ‰test_groupså­—æ®µï¼Œè·³è¿‡è¿™éƒ¨åˆ†
            if "test_groups" not in self.database:
                # v2.1ç‰ˆæœ¬ä¸éœ€è¦test_groups
                pass
            else:
                key = record.get_key()
                if key not in self.database["test_groups"]:
                    self.database["test_groups"][key] = {
                    "model": model,  # ä½¿ç”¨è§„èŒƒåŒ–åçš„æ¨¡å‹åç§°
                    "task_type": record.task_type,
                    "prompt_type": record.prompt_type,
                    "difficulty": record.difficulty,
                    "is_flawed": record.is_flawed,
                    "flaw_type": record.flaw_type,
                    "statistics": {
                        "total": 0,
                        "success": 0,
                        "partial_success": 0,
                        "failure": 0,
                        "success_rate": 0.0,
                        "avg_execution_time": 0.0
                    },
                    "instances": []  # ç©ºåˆ—è¡¨ - ä¸å­˜å‚¨å®ä¾‹
                }
            
                # åªæ›´æ–°ç»Ÿè®¡
                group = self.database["test_groups"][key]
                stats = group["statistics"]
                stats["total"] += 1
                if record.success:
                    stats["success"] += 1
                elif record.partial_success:
                    stats["partial_success"] += 1
                else:
                    stats["failure"] = stats["total"] - stats["success"] - stats["partial_success"]
                
                stats["success_rate"] = (stats["success"] / stats["total"] * 100) if stats["total"] > 0 else 0
                
                # æ›´æ–°å¹³å‡æ‰§è¡Œæ—¶é—´
                if record.execution_time > 0:
                    # ä½¿ç”¨å¢é‡å¹³å‡å€¼è®¡ç®—
                    n = stats["total"]
                    stats["avg_execution_time"] = ((n - 1) * stats["avg_execution_time"] + record.execution_time) / n
            
            # è‡ªåŠ¨ä¿å­˜ - ç°åœ¨ä½¿ç”¨ç»Ÿä¸€çš„save_databaseæ–¹æ³•ï¼ˆæ”¯æŒæ–‡ä»¶é”ï¼‰
            # éœ€è¦åœ¨é”å¤–è°ƒç”¨save_databaseï¼Œå› ä¸ºsave_databaseå†…éƒ¨ä¼šè·å–é”
            pass  # é€€å‡ºé”åå†ä¿å­˜
        
        # åœ¨é”å¤–ä¿å­˜ï¼Œé¿å…æ­»é”
        self.save_database()
        return True
    
    def _determine_success_level(self, record: TestRecord) -> str:
        """ç¡®å®šæˆåŠŸçº§åˆ«ã€‚å¦‚æœæœ‰execution_statusï¼Œä¼˜å…ˆä½¿ç”¨ã€‚"""
        # ä¼˜å…ˆä½¿ç”¨execution_statusï¼ˆæ¥è‡ªInteractiveExecutorï¼‰
        if record.execution_status and record.execution_status != "failure":
            return record.execution_status
        
        # é€€å›åˆ°æ—§çš„é€»è¾‘
        if record.success:
            return "full_success"
        elif record.partial_success:
            return "partial_success"
        else:
            return "failure"
    
    def _serialize_database(self) -> Dict:
        """åºåˆ—åŒ–æ•°æ®åº“ï¼Œå°†æ•°æ®ç±»è½¬æ¢ä¸ºå­—å…¸"""
        result = dict(self.database)
        
        # åºåˆ—åŒ–modelséƒ¨åˆ†
        if "models" in result:
            serialized_models = {}
            for model_name, model_stats in result["models"].items():
                if isinstance(model_stats, dict):
                    # V3ç‰ˆæœ¬ï¼šå·²ç»æ˜¯å­—å…¸æ ¼å¼
                    serialized_models[model_name] = model_stats
                elif isinstance(model_stats, dict):
                    # å¦‚æœæ˜¯V2å­—å…¸ï¼Œç¡®ä¿å®ƒæœ‰æ­£ç¡®çš„overall_statsç»“æ„
                    if 'overall_stats' not in model_stats:
                        # ä»V2å­—å…¸æ„å»ºoverall_stats
                        model_stats['overall_stats'] = {
                            'total_success': model_stats.get('total_success', 0),
                            'total_partial': 0,  # V2å¯èƒ½æ²¡æœ‰è¿™ä¸ªå­—æ®µ
                            'total_full': 0,  # V2å¯èƒ½æ²¡æœ‰è¿™ä¸ªå­—æ®µ
                            'total_failure': model_stats.get('total_failure', 0),
                            'success_rate': model_stats.get('overall_success_rate', 0.0),
                            'weighted_success_score': 0.0,  # V2å¯èƒ½æ²¡æœ‰è¿™ä¸ªå­—æ®µ
                            'avg_execution_time': model_stats.get('avg_execution_time', 0.0),
                            'avg_turns': 0.0,  # V2å¯èƒ½æ²¡æœ‰è¿™ä¸ªå­—æ®µ
                            'tool_coverage_rate': model_stats.get('tool_coverage_rate', 0.0)  # ä»V2è·å–
                        }
                    elif 'tool_coverage_rate' not in model_stats['overall_stats']:
                        # å¦‚æœoverall_statså­˜åœ¨ä½†ç¼ºå°‘tool_coverage_rateï¼Œæ·»åŠ å®ƒ
                        model_stats['overall_stats']['tool_coverage_rate'] = model_stats.get('tool_coverage_rate', 0.0)
                    serialized_models[model_name] = model_stats
                else:
                    serialized_models[model_name] = model_stats
            result["models"] = serialized_models
        
        return result
    
    # V3ç‰ˆæœ¬ï¼šå·²ç§»é™¤æ‰€æœ‰åºåˆ—åŒ–æ–¹æ³•
    # æ•°æ®ç›´æ¥ä»¥å­—å…¸æ ¼å¼å­˜å‚¨å’Œä½¿ç”¨
    def _serialize_model_stats_v3(self, stats, use_v2_if_available: bool = True) -> Dict:
        """åºåˆ—åŒ–æ¨¡å‹ç»Ÿè®¡å¯¹è±¡ - V3ç‰ˆæœ¬ï¼Œæ”¯æŒæ–°å±‚æ¬¡ç»“æ„"""
        result = {
            "model_name": stats.model_name,
            "first_test_time": stats.first_test_time,
            "last_test_time": stats.last_test_time,
            "total_tests": stats.overall_success.total_tests,
            "overall_stats": {
                "total_success": stats.overall_success.total_success,
                "total_partial": stats.overall_success.partial_success,
                "total_full": stats.overall_success.full_success,
                "total_failure": stats.overall_success.failure,
                "success_rate": stats.overall_success.success_rate,
                "weighted_success_score": stats.overall_success.weighted_success_score,
                "avg_execution_time": stats.overall_execution.avg_execution_time,
                "avg_turns": stats.overall_execution.avg_turns,
                "tool_coverage_rate": stats.overall_execution.tool_coverage_rate
            },
            "by_prompt_type": {}
        }
        
        # æ„å»ºæ–°çš„å±‚æ¬¡ç»“æ„ï¼šprompt_type -> tool_success_rate -> difficulty -> task_type
        # ä»ç°æœ‰æ•°æ®ä¸­é‡ç»„ - è¿™é‡Œæ­£ç¡®å¤„ç†å¤šç§å·¥å…·æˆåŠŸç‡
        by_prompt_type = {}
        
        # ä¿å­˜æ‰€æœ‰é‡åˆ°çš„tool_reliabilityå€¼
        if not hasattr(stats, '_tool_rates_by_prompt'):
            stats._tool_rates_by_prompt = {}
        
        # å¤„ç†prompt_typeç»Ÿè®¡
        for prompt_type, prompt_stats in stats.by_prompt_type.items():
            if prompt_type not in by_prompt_type:
                by_prompt_type[prompt_type] = {
                    "by_tool_success_rate": {},
                    "summary": self._serialize_prompt_stats({prompt_type: prompt_stats})[prompt_type]
                }
        
        # å¤„ç†task_typeç»Ÿè®¡
        for task_type, task_stats in stats.by_task_type.items():
            # é»˜è®¤æ”¾åœ¨baseline -> 0.8 -> easy -> task_type
            prompt_type = "baseline"
            tool_rate = "0.8"  # é»˜è®¤å€¼
            
            if prompt_type not in by_prompt_type:
                by_prompt_type[prompt_type] = {
                    "by_tool_success_rate": {},
                    "summary": {}
                }
            
            if tool_rate not in by_prompt_type[prompt_type]["by_tool_success_rate"]:
                by_prompt_type[prompt_type]["by_tool_success_rate"][tool_rate] = {
                    "by_difficulty": {
                        "easy": {
                            "by_task_type": {}
                        }
                    }
                }
            
            location = by_prompt_type[prompt_type]["by_tool_success_rate"][tool_rate]["by_difficulty"]["easy"]["by_task_type"]
            location[task_type] = self._serialize_task_stats({task_type: task_stats})[task_type]
        
        # å¤„ç†flaw_typeç»Ÿè®¡ï¼ˆå°†å…¶ä½œä¸ºflawed_xxxçš„prompt_typeï¼‰
        for flaw_type, flaw_stats in stats.by_flaw_type.items():
            prompt_type = f"flawed_{flaw_type}"
            tool_rate = "0.8"  # é»˜è®¤å€¼
            
            if prompt_type not in by_prompt_type:
                by_prompt_type[prompt_type] = {
                    "by_tool_success_rate": {},
                    "summary": self._serialize_flaw_stats({flaw_type: flaw_stats})[flaw_type]
                }
            
            if tool_rate not in by_prompt_type[prompt_type]["by_tool_success_rate"]:
                by_prompt_type[prompt_type]["by_tool_success_rate"][tool_rate] = {
                    "by_difficulty": {
                        "easy": {
                            "by_task_type": {}
                        }
                    }
                }
        
        result["by_prompt_type"] = by_prompt_type
        
        # å¦‚æœæœ‰V2æ•°æ®ï¼Œä¼˜å…ˆä½¿ç”¨V2æ•°æ®
        if use_v2_if_available and hasattr(stats, '_v2_data'):
            v2_data = stats._v2_data
            if 'by_prompt_type' in v2_data:
                result['by_prompt_type'] = v2_data['by_prompt_type']
        
        return result
    
    def _serialize_task_stats(self, task_stats: Dict) -> Dict:
        """åºåˆ—åŒ–ä»»åŠ¡ç±»å‹ç»Ÿè®¡"""
        result = {}
        for task_type, stats in task_stats.items():
            result[task_type] = {
                "total": stats.success_metrics.total_tests,
                "success": stats.success_metrics.total_success,
                "success_rate": stats.success_metrics.success_rate,
                "weighted_success_score": stats.success_metrics.weighted_success_score,
                "full_success_rate": stats.success_metrics.full_success_rate,
                "partial_success_rate": stats.success_metrics.partial_success_rate,
                "failure_rate": stats.success_metrics.failure_rate,
                # Execution metrics
                "avg_execution_time": stats.execution_metrics.avg_execution_time,
                "avg_turns": stats.execution_metrics.avg_turns,
                "avg_tool_calls": stats.execution_metrics.avg_tool_calls,
                "tool_coverage_rate": stats.execution_metrics.tool_coverage_rate,
                # Score metrics
                "avg_workflow_score": stats.score_metrics.workflow_scores.mean,
                "avg_phase2_score": stats.score_metrics.phase2_scores.mean,
                "avg_quality_score": stats.score_metrics.quality_scores.mean,
                "avg_final_score": stats.score_metrics.final_scores.mean,
                # å®Œæ•´çš„é”™è¯¯ç»Ÿè®¡
                "total_errors": stats.error_metrics.total_errors,
                "tool_call_format_errors": stats.error_metrics.tool_call_format_errors,
                "timeout_errors": stats.error_metrics.timeout_errors,
                "dependency_errors": stats.error_metrics.dependency_errors,
                "parameter_config_errors": stats.error_metrics.parameter_config_errors,
                "tool_selection_errors": stats.error_metrics.tool_selection_errors,
                "sequence_order_errors": stats.error_metrics.sequence_order_errors,
                "max_turns_errors": stats.error_metrics.max_turns_errors,
                
                # å®Œæ•´çš„é”™è¯¯ç‡
                "tool_selection_error_rate": stats.error_metrics.tool_selection_error_rate,
                "parameter_error_rate": stats.error_metrics.parameter_error_rate,
                "sequence_error_rate": stats.error_metrics.sequence_error_rate,
                "dependency_error_rate": stats.error_metrics.dependency_error_rate,
                "timeout_error_rate": stats.error_metrics.timeout_error_rate,
                "format_error_rate": stats.error_metrics.format_error_rate,
                "max_turns_error_rate": stats.error_metrics.max_turns_error_rate,
                
                # Assistedç»Ÿè®¡
                "assisted_failure": stats.success_metrics.assisted_failure,
                "assisted_success": stats.success_metrics.assisted_success,
                "total_assisted_turns": stats.success_metrics.total_assisted_turns,
                "tests_with_assistance": stats.success_metrics.tests_with_assistance,
                "avg_assisted_turns": stats.success_metrics.avg_assisted_turns,
                "assisted_success_rate": stats.success_metrics.assisted_success_rate,
                "assistance_rate": stats.success_metrics.assistance_rate
            }
        return result
    
    def _serialize_prompt_stats(self, prompt_stats: Dict) -> Dict:
        """åºåˆ—åŒ–æç¤ºç±»å‹ç»Ÿè®¡"""
        result = {}
        for prompt_type, stats in prompt_stats.items():
            result[prompt_type] = {
                "total": stats.success_metrics.total_tests,
                "success": stats.success_metrics.total_success,
                "success_rate": stats.success_metrics.success_rate,
                "weighted_success_score": stats.success_metrics.weighted_success_score,
                "full_success_rate": stats.success_metrics.full_success_rate,
                "partial_success_rate": stats.success_metrics.partial_success_rate,
                "failure_rate": stats.success_metrics.failure_rate,
                
                # Assistedç»Ÿè®¡
                "assisted_failure": stats.success_metrics.assisted_failure,
                "assisted_success": stats.success_metrics.assisted_success,
                "total_assisted_turns": stats.success_metrics.total_assisted_turns,
                "tests_with_assistance": stats.success_metrics.tests_with_assistance,
                "avg_assisted_turns": stats.success_metrics.avg_assisted_turns,
                "assisted_success_rate": stats.success_metrics.assisted_success_rate,
                "assistance_rate": stats.success_metrics.assistance_rate,
                
                # Execution metrics
                "avg_execution_time": stats.execution_metrics.avg_execution_time,
                "avg_turns": stats.execution_metrics.avg_turns,
                "avg_tool_calls": stats.execution_metrics.avg_tool_calls,
                "tool_coverage_rate": stats.execution_metrics.tool_coverage_rate,
                # Score metrics
                "avg_workflow_score": stats.score_metrics.workflow_scores.mean,
                "avg_phase2_score": stats.score_metrics.phase2_scores.mean,
                "avg_quality_score": stats.score_metrics.quality_scores.mean,
                "avg_final_score": stats.score_metrics.final_scores.mean,
                
                # å®Œæ•´çš„é”™è¯¯ç»Ÿè®¡ï¼ˆæ‰€æœ‰7ç§ç±»å‹ï¼‰
                "total_errors": stats.error_metrics.total_errors,
                "tool_call_format_errors": stats.error_metrics.tool_call_format_errors,
                "timeout_errors": stats.error_metrics.timeout_errors,
                "dependency_errors": stats.error_metrics.dependency_errors,
                "parameter_config_errors": stats.error_metrics.parameter_config_errors,
                "tool_selection_errors": stats.error_metrics.tool_selection_errors,
                "sequence_order_errors": stats.error_metrics.sequence_order_errors,
                "max_turns_errors": stats.error_metrics.max_turns_errors,
                
                # å®Œæ•´çš„é”™è¯¯ç‡
                "tool_selection_error_rate": stats.error_metrics.tool_selection_error_rate,
                "parameter_error_rate": stats.error_metrics.parameter_error_rate,
                "sequence_error_rate": stats.error_metrics.sequence_error_rate,
                "dependency_error_rate": stats.error_metrics.dependency_error_rate,
                "timeout_error_rate": stats.error_metrics.timeout_error_rate,
                "format_error_rate": stats.error_metrics.format_error_rate,
                "max_turns_error_rate": stats.error_metrics.max_turns_error_rate
            }
        return result
    
    def _serialize_flaw_stats(self, flaw_stats: Dict) -> Dict:
        """åºåˆ—åŒ–ç¼ºé™·ç±»å‹ç»Ÿè®¡"""
        result = {}
        for flaw_type, stats in flaw_stats.items():
            result[flaw_type] = {
                "total": stats.success_metrics.total_tests,
                "success": stats.success_metrics.total_success,
                "success_rate": stats.success_metrics.success_rate,
                "robustness_score": stats.robustness_score,
                "weighted_success_score": stats.success_metrics.weighted_success_score,
                "full_success_rate": stats.success_metrics.full_success_rate,
                "partial_success_rate": stats.success_metrics.partial_success_rate,
                "failure_rate": stats.success_metrics.failure_rate,
                # Execution metrics
                "avg_execution_time": stats.execution_metrics.avg_execution_time,
                "avg_turns": stats.execution_metrics.avg_turns,
                "avg_tool_calls": stats.execution_metrics.avg_tool_calls,
                "tool_coverage_rate": stats.execution_metrics.tool_coverage_rate,
                # Score metrics
                "avg_workflow_score": stats.score_metrics.workflow_scores.mean,
                "avg_phase2_score": stats.score_metrics.phase2_scores.mean,
                "avg_quality_score": stats.score_metrics.quality_scores.mean,
                "avg_final_score": stats.score_metrics.final_scores.mean,
                # å®Œæ•´çš„é”™è¯¯ç»Ÿè®¡
                "total_errors": stats.error_metrics.total_errors,
                "tool_call_format_errors": stats.error_metrics.tool_call_format_errors,
                "timeout_errors": stats.error_metrics.timeout_errors,
                "dependency_errors": stats.error_metrics.dependency_errors,
                "parameter_config_errors": stats.error_metrics.parameter_config_errors,
                "tool_selection_errors": stats.error_metrics.tool_selection_errors,
                "sequence_order_errors": stats.error_metrics.sequence_order_errors,
                "max_turns_errors": stats.error_metrics.max_turns_errors,
                
                # å®Œæ•´çš„é”™è¯¯ç‡
                "tool_selection_error_rate": stats.error_metrics.tool_selection_error_rate,
                "parameter_error_rate": stats.error_metrics.parameter_error_rate,
                "sequence_error_rate": stats.error_metrics.sequence_error_rate,
                "dependency_error_rate": stats.error_metrics.dependency_error_rate,
                "timeout_error_rate": stats.error_metrics.timeout_error_rate,
                "format_error_rate": stats.error_metrics.format_error_rate,
                "max_turns_error_rate": stats.error_metrics.max_turns_error_rate,
                
                # Assistedç»Ÿè®¡
                "assisted_failure": stats.success_metrics.assisted_failure,
                "assisted_success": stats.success_metrics.assisted_success,
                "total_assisted_turns": stats.success_metrics.total_assisted_turns,
                "tests_with_assistance": stats.success_metrics.tests_with_assistance,
                "avg_assisted_turns": stats.success_metrics.avg_assisted_turns,
                "assisted_success_rate": stats.success_metrics.assisted_success_rate,
                "assistance_rate": stats.success_metrics.assistance_rate
            }
        return result
    def _serialize_difficulty_stats(self, diff_stats: Dict) -> Dict:
        """åºåˆ—åŒ–éš¾åº¦ç»Ÿè®¡"""
        result = {}
        for difficulty, stats in diff_stats.items():
            result[difficulty] = {
                "total": stats.success_metrics.total_tests,
                "success": stats.success_metrics.total_success,
                "success_rate": stats.success_metrics.success_rate
            }
        return result
    
    def _update_group_statistics(self, key: str):
        """æ›´æ–°ç»„ç»Ÿè®¡"""
        group = self.database["test_groups"][key]
        instances = group["instances"]
        
        if not instances:
            return
        
        # è®¡ç®—ç»Ÿè®¡
        total = len(instances)
        success = sum(1 for inst in instances if inst["success"])
        partial = sum(1 for inst in instances if inst["partial_success"])
        failure = total - success - partial
        
        # è®¡ç®—å¹³å‡æ‰§è¡Œæ—¶é—´
        times = [inst["execution_time"] for inst in instances if inst["execution_time"] > 0]
        avg_time = sum(times) / len(times) if times else 0
        
        # æ›´æ–°ç»Ÿè®¡
        group["statistics"] = {
            "total": total,
            "success": success,
            "partial_success": partial,
            "failure": failure,
            "success_rate": success / total * 100 if total > 0 else 0,
            "partial_rate": partial / total * 100 if total > 0 else 0,
            "failure_rate": failure / total * 100 if total > 0 else 0,
            "avg_execution_time": avg_time
        }
    
    def _update_global_summary(self):
        """æ›´æ–°å…¨å±€æ‘˜è¦ - æ—§ç‰ˆæœ¬ï¼Œä¿æŒå‘åå…¼å®¹"""
        total_tests = 0
        total_success = 0
        total_partial = 0
        total_failure = 0
        models = set()
        
        for group in self.database["test_groups"].values():
            stats = group["statistics"]
            total_tests += stats["total"]
            total_success += stats["success"]
            total_partial += stats["partial_success"]
            total_failure += stats["failure"]
            models.add(group["model"])
        
        self.database["summary"]["total_tests"] = total_tests
        self.database["summary"]["total_success"] = total_success
        self.database["summary"]["total_partial"] = total_partial
        self.database["summary"]["total_failure"] = total_failure
        self.database["summary"]["overall_success_rate"] = total_success / total_tests * 100 if total_tests > 0 else 0
        self.database["summary"]["models_tested"] = sorted(list(models))
        self.database["summary"]["last_test_time"] = datetime.now().isoformat()
    
    def _update_global_summary_v2(self):
        """æ›´æ–°å…¨å±€æ‘˜è¦ - ä½¿ç”¨æ–°çš„ç»Ÿè®¡ç»“æ„"""
        total_tests = 0
        total_success = 0
        total_partial = 0
        total_failure = 0
        
        # ä»æ–°çš„modelsç»“æ„ä¸­è®¡ç®—
        for model_name, model_stats in self.database["models"].items():
            if isinstance(model_stats, dict):
                # V3: ä»å­—å…¸æ ¼å¼ä¸­è·å–ç»Ÿè®¡
                total_tests += model_stats.get("total_tests", 0)
                if "overall_stats" in model_stats:
                    total_success += model_stats["overall_stats"].get("total_success", 0)
                    total_partial += model_stats["overall_stats"].get("total_partial", 0)
                    total_failure += model_stats["overall_stats"].get("total_failure", 0)
        
        # åªåœ¨æœ‰æ–°æ•°æ®æ—¶æ‰æ›´æ–°ï¼Œé¿å…æ¸…é›¶
        if total_tests > 0 or self.database["summary"]["total_tests"] == 0:
            self.database["summary"]["total_tests"] = total_tests
            self.database["summary"]["total_success"] = total_success
            self.database["summary"]["total_partial"] = total_partial
            self.database["summary"]["total_failure"] = total_failure
            self.database["summary"]["overall_success_rate"] = (total_success + total_partial) / total_tests * 100 if total_tests > 0 else 0
        
        self.database["summary"]["last_test_time"] = datetime.now().isoformat()
    
    def get_test_count(self, model: str, task_type: str, prompt_type: str, 
                       difficulty: str = "easy", flaw_type: Optional[str] = None) -> int:
        """è·å–ç‰¹å®šç»„åˆçš„æµ‹è¯•æ¬¡æ•°"""
        parts = [model, task_type, prompt_type, difficulty]
        if flaw_type:
            parts.extend(["flawed", flaw_type])
        key = "_".join(parts)
        
        if key in self.database["test_groups"]:
            return self.database["test_groups"][key]["statistics"]["total"]
        return 0
    
    def needs_more_tests(self, model: str, task_type: str, prompt_type: str,
                         difficulty: str = "easy", flaw_type: Optional[str] = None,
                         target_count: int = 100) -> bool:
        """æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´å¤šæµ‹è¯•"""
        current_count = self.get_test_count(model, task_type, prompt_type, difficulty, flaw_type)
        return current_count < target_count
    
    def get_remaining_tests(self, model: str, target_count: int = 100) -> List[Dict]:
        """è·å–å‰©ä½™éœ€è¦æµ‹è¯•çš„ç»„åˆ"""
        remaining = []
        
        # å®šä¹‰æ‰€æœ‰å¯èƒ½çš„ç»„åˆ
        task_types = ["simple_task", "basic_task", "data_pipeline", "api_integration", "multi_stage_pipeline"]
        prompt_types = ["baseline", "optimal", "cot"]
        flaw_types = ["sequence_disorder", "tool_misuse", "parameter_error", "missing_step",
                      "redundant_operations", "logical_inconsistency", "semantic_drift"]
        
        # æ£€æŸ¥æ­£å¸¸æµ‹è¯•
        for task_type in task_types:
            for prompt_type in prompt_types:
                count = self.get_test_count(model, task_type, prompt_type)
                if count < target_count:
                    remaining.append({
                        "task_type": task_type,
                        "prompt_type": prompt_type,
                        "is_flawed": False,
                        "flaw_type": None,
                        "current_count": count,
                        "needed": target_count - count
                    })
        
        # æ£€æŸ¥ç¼ºé™·æµ‹è¯•
        for task_type in task_types:
            for flaw_type in flaw_types:
                count = self.get_test_count(model, task_type, "baseline", "easy", flaw_type)
                if count < target_count:
                    remaining.append({
                        "task_type": task_type,
                        "prompt_type": "baseline",
                        "is_flawed": True,
                        "flaw_type": flaw_type,
                        "current_count": count,
                        "needed": target_count - count
                    })
        
        return remaining
    
    def get_progress_report(self, model: Optional[str] = None) -> Dict:
        """ç”Ÿæˆè¿›åº¦æŠ¥å‘Š - ä¼˜å…ˆä½¿ç”¨æ–°çš„modelsç»“æ„"""
        report = {
            "summary": self.database["summary"].copy(),
            "models": {}
        }
        
        # ä¼˜å…ˆä»æ–°çš„modelsç»“æ„è·å–æ•°æ®
        if "models" in self.database and self.database["models"]:
            for model_name, model_stats in self.database["models"].items():
                if model and model_name != model:
                    continue
                
                if False:  # V3: no ModelStatistics
                    # ä»ModelStatisticså¯¹è±¡æå–ç»Ÿè®¡
                    report["models"][model_name] = {
                        "total_tests": model_stats.overall_success.total_tests,
                        "total_success": model_stats.overall_success.total_success,
                        "by_task_type": self._serialize_task_stats(model_stats.by_task_type),
                        "by_prompt_type": self._serialize_prompt_stats(model_stats.by_prompt_type),
                        "by_flaw_type": self._serialize_flaw_stats(model_stats.by_flaw_type)
                    }
                elif isinstance(model_stats, dict):
                    # å·²åºåˆ—åŒ–çš„æ•°æ®
                    report["models"][model_name] = {
                        "total_tests": model_stats.get("total_tests", 0),
                        "total_success": model_stats.get("total_success", 0),
                        "by_task_type": model_stats.get("by_task_type", {}),
                        "by_prompt_type": model_stats.get("by_prompt_type", {}),
                        "by_flaw_type": model_stats.get("by_flaw_type", {})
                    }
        
        # å¦‚æœæ–°ç»“æ„æ²¡æœ‰æ•°æ®ï¼Œå›é€€åˆ°æ—§çš„test_groups
        if not report["models"] and "test_groups" in self.database:
            for key, group in self.database["test_groups"].items():
                if model and group["model"] != model:
                    continue
                
                model_name = group["model"]
                if model_name not in report["models"]:
                    report["models"][model_name] = {
                        "total_tests": 0,
                        "total_success": 0,
                        "by_task_type": {},
                        "by_prompt_type": {},
                        "by_flaw_type": {}
                    }
                
                model_report = report["models"][model_name]
                stats = group["statistics"]
                
                # æ›´æ–°æ€»è®¡
                model_report["total_tests"] += stats["total"]
                model_report["total_success"] += stats["success"]
                
                # æŒ‰ä»»åŠ¡ç±»å‹ç»Ÿè®¡
                task_type = group["task_type"]
                if task_type not in model_report["by_task_type"]:
                    model_report["by_task_type"][task_type] = {"total": 0, "success": 0}
                model_report["by_task_type"][task_type]["total"] += stats["total"]
                model_report["by_task_type"][task_type]["success"] += stats["success"]
                
                # æŒ‰æç¤ºç±»å‹ç»Ÿè®¡
                if not group["is_flawed"]:
                    prompt_type = group["prompt_type"]
                    if prompt_type not in model_report["by_prompt_type"]:
                        model_report["by_prompt_type"][prompt_type] = {"total": 0, "success": 0}
                    model_report["by_prompt_type"][prompt_type]["total"] += stats["total"]
                    model_report["by_prompt_type"][prompt_type]["success"] += stats["success"]
                
                # æŒ‰ç¼ºé™·ç±»å‹ç»Ÿè®¡
                if group["is_flawed"] and group["flaw_type"]:
                    flaw_type = group["flaw_type"]
                    if flaw_type not in model_report["by_flaw_type"]:
                        model_report["by_flaw_type"][flaw_type] = {"total": 0, "success": 0}
                    model_report["by_flaw_type"][flaw_type]["total"] += stats["total"]
                    model_report["by_flaw_type"][flaw_type]["success"] += stats["success"]
        
        return report
    
    def export_for_report_generation(self) -> Dict:
        """å¯¼å‡ºç”¨äºæŠ¥å‘Šç”Ÿæˆçš„æ•°æ®"""
        export_data = {
            "metadata": {
                "export_time": datetime.now().isoformat(),
                "database_version": self.database.get("version", "1.0"),
                "total_tests": self.database["summary"]["total_tests"]
            },
            "results": {}
        }
        
        # è½¬æ¢ä¸ºæŠ¥å‘Šç”Ÿæˆå™¨æœŸæœ›çš„æ ¼å¼
        for key, group in self.database["test_groups"].items():
            export_data["results"][key] = {
                "total": group["statistics"]["total"],
                "success": group["statistics"]["success"],
                "partial_success": group["statistics"]["partial_success"],
                "model": group["model"],
                "task_type": group["task_type"],
                "prompt_type": group["prompt_type"],
                "difficulty": group["difficulty"],
                "is_flawed": group["is_flawed"],
                "flaw_type": group["flaw_type"],
                "success_rate": group["statistics"]["success_rate"],
                "avg_execution_time": group["statistics"]["avg_execution_time"]
            }
        
        return export_data

# ä¾¿æ·å‡½æ•°
def add_test_result(model: str, task_type: str, prompt_type: str,
                   success: bool, execution_time: float = 0.0,
                   difficulty: str = "easy", is_flawed: bool = False,
                   flaw_type: Optional[str] = None, error_message: Optional[str] = None,
                   partial_success: bool = False,
                   format_recognition_errors: int = 0,
                   instruction_following_errors: int = 0,
                   tool_selection_errors: int = 0,
                   parameter_config_errors: int = 0,
                   sequence_order_errors: int = 0,
                   dependency_errors: int = 0) -> bool:
    """å¿«é€Ÿæ·»åŠ æµ‹è¯•ç»“æœ"""
    manager = CumulativeTestManager()
    
    record = TestRecord(
        model=model,
        task_type=task_type,
        prompt_type=prompt_type,
        difficulty=difficulty,
        success=success,
        partial_success=partial_success,
        execution_time=execution_time,
        error_message=error_message,
        is_flawed=is_flawed,
        flaw_type=flaw_type,
        format_recognition_errors=format_recognition_errors,
        instruction_following_errors=instruction_following_errors,
        tool_selection_errors=tool_selection_errors,
        parameter_config_errors=parameter_config_errors,
        sequence_order_errors=sequence_order_errors,
        dependency_errors=dependency_errors
    )
    
    return manager.add_test_result(record)

def check_progress(model: str, target_count: int = 100) -> Dict:
    """æ£€æŸ¥æ¨¡å‹çš„æµ‹è¯•è¿›åº¦"""
    manager = CumulativeTestManager()
    remaining = manager.get_remaining_tests(model, target_count)
    
    total_needed = sum(item["needed"] for item in remaining)
    total_completed = target_count * len(remaining) - total_needed if remaining else 0
    
    return {
        "model": model,
        "target_per_combination": target_count,
        "total_combinations": len(remaining),
        "total_needed": total_needed,
        "total_completed": total_completed,
        "completion_rate": total_completed / (total_completed + total_needed) * 100 if total_needed > 0 else 100,
        "remaining_tests": remaining
    }

def get_model_report(model: str) -> Dict:
    """è·å–æ¨¡å‹çš„è¯¦ç»†æŠ¥å‘Š"""
    manager = CumulativeTestManager()
    return manager.get_progress_report(model)

class EnhancedCumulativeManager(CumulativeTestManager):
    """å¢å¼ºç‰ˆç´¯ç§¯æµ‹è¯•ç®¡ç†å™¨ - æ”¯æŒæ–°çš„å±‚æ¬¡ç»“æ„"""
    
    def __init__(self, buffer_size: int = 10):
        super().__init__()
        self.buffer_size = buffer_size
        self.buffer = []  # æµ‹è¯•ç»“æœç¼“å†²
        self.v2_models = {}  # å­˜å‚¨V2æ¨¡å‹æ•°æ®
        
        # æ£€æŸ¥å¹¶å‡çº§æ•°æ®åº“ç‰ˆæœ¬
        if self.database.get("version", "2.0") < "3.0":
            self._upgrade_to_v3()
    
    def _upgrade_to_v3(self):
        """å‡çº§æ•°æ®åº“åˆ°V3ç‰ˆæœ¬"""
        self.database["version"] = "3.0"
        # åˆå§‹åŒ–V2æ¨¡å‹æ•°æ®
        for model_name in self.database.get("models", {}):
            if model_name not in self.v2_models:
                self.v2_models[model_name] = {}  # V3: dict format
    
    def add_test_result_with_classification(self, record: TestRecord) -> bool:
        """æ·»åŠ å¸¦åˆ†ç±»çš„æµ‹è¯•ç»“æœï¼ˆæ”¯æŒæ–°å±‚æ¬¡ï¼‰"""
        # æ›´æ–°V2æ¨¡å‹æ•°æ®
        model = record.model
        if model not in self.v2_models:
            try:
                from old_data_structures.cumulative_data_structure import ModelStatistics
                self.v2_models[model] = ModelStatistics(model_name=model)
            except ImportError:
                self.v2_models[model] = {}  # fallback to dict
        
        # æ„å»ºæµ‹è¯•å­—å…¸
        test_dict = {
            "model": record.model,
            "task_type": record.task_type,
            "prompt_type": record.prompt_type,
            "difficulty": record.difficulty,
            "tool_reliability": record.tool_reliability,  # å…³é”®ï¼šä¼ é€’tool_reliability
            "success_level": record.success_level if hasattr(record, 'success_level') else 
                           ("full_success" if record.success else "failure"),
            "execution_time": record.execution_time,
            "turns": record.turns,
            "tool_calls": record.tool_calls,
            "required_tools": getattr(record, 'required_tools', []),  # æ·»åŠ required_tools
            "executed_tools": getattr(record, 'executed_tools', getattr(record, 'tool_calls', [])),  # æ·»åŠ executed_tools
            "tool_coverage_rate": getattr(record, 'tool_coverage_rate', 0.0),  # æ·»åŠ tool_coverage_rate
            "workflow_score": getattr(record, 'workflow_score', None),
            "phase2_score": getattr(record, 'phase2_score', None),
            "quality_score": getattr(record, 'quality_score', None),
            "final_score": getattr(record, 'final_score', None),
            "format_error_count": getattr(record, 'format_error_count', 0),
            "is_flawed": record.is_flawed,
            "flaw_type": record.flaw_type,
            "timestamp": record.timestamp or datetime.now().isoformat()
        }
        
        # ä½¿ç”¨V2æ¨¡å‹æ›´æ–°
        if hasattr(self.v2_models[model], 'update_from_test'):
            self.v2_models[model].update_from_test(test_dict)
        else:
            # v2_modelsæ˜¯å­—å…¸ï¼Œè·³è¿‡æ›´æ–°
            pass
        
        # æ·»åŠ åˆ°ç¼“å†²
        self.buffer.append(record)
        
        # å¦‚æœç¼“å†²æ»¡äº†ï¼Œåˆ·æ–°åˆ°æ•°æ®åº“
        if len(self.buffer) >= self.buffer_size:
            self._flush_buffer()
        
        # è°ƒç”¨çˆ¶ç±»æ–¹æ³•ä¿å­˜ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
        return super().add_test_result(record)
    
    def _flush_buffer(self):
        """åˆ·æ–°ç¼“å†²åŒº"""
        if not self.buffer:
            return
        
        # æ¸…ç©ºç¼“å†²
        self.buffer.clear()
        
        # ä¿å­˜V2æ¨¡å‹æ•°æ®åˆ°æ•°æ®åº“
        self._save_v2_models()
        
        # ä¿å­˜æ•°æ®åº“
        self.save_database()
    
    def _save_v2_models(self):
        """ä¿å­˜V2æ¨¡å‹æ•°æ®åˆ°æ•°æ®åº“"""
        for model_name, v2_model in self.v2_models.items():
            # å°†V2æ¨¡å‹åºåˆ—åŒ–å¹¶ä¿å­˜åˆ°æ•°æ®åº“
            v2_dict = v2_model.to_dict()
            # è°ƒè¯•è¾“å‡º
            if model_name == 'gpt-4o-mini':
                print(f"[DEBUG] Saving V2 model {model_name}: tool_coverage_rate = {v2_dict.get('tool_coverage_rate', 'NOT FOUND')}")
                if 'overall_stats' in v2_dict:
                    print(f"[DEBUG] V2 overall_stats.tool_coverage_rate = {v2_dict['overall_stats'].get('tool_coverage_rate', 'NOT FOUND')}")
            self.database["models"][model_name] = v2_dict
    
    def finalize(self):
        """å®Œæˆå¹¶åˆ·æ–°æ‰€æœ‰ç¼“å†²æ•°æ®"""
        self._flush_buffer()
        # ç¡®ä¿V2æ•°æ®å·²ä¿å­˜
        if self.v2_models:
            self._save_v2_models()
            self.save_database()
    
    def get_statistics_by_hierarchy(self, model: str, prompt_type: str = None,
                                   tool_rate: float = None, difficulty: str = None,
                                   task_type: str = None) -> Dict:
        """æŒ‰å±‚æ¬¡è·å–ç»Ÿè®¡æ•°æ®"""
        if model not in self.database["models"]:
            return {}
        
        model_data = self.database["models"][model]
        
        # å¦‚æœæ˜¯ModelStatisticså¯¹è±¡ï¼Œåºåˆ—åŒ–å®ƒ
        if False:  # V3: no ModelStatistics
            model_data = self._serialize_model_stats_v3(model_data)
        
        # å¦‚æœåªæŒ‡å®šæ¨¡å‹ï¼Œè¿”å›æ¨¡å‹çº§åˆ«ç»Ÿè®¡
        if not prompt_type:
            return model_data
        
        # å¯¼èˆªåˆ°promptç±»å‹
        if "by_prompt_type" not in model_data or prompt_type not in model_data["by_prompt_type"]:
            return {}
        prompt_data = model_data["by_prompt_type"][prompt_type]
        
        # å¦‚æœåªæŒ‡å®šåˆ°promptç±»å‹
        if tool_rate is None:
            return prompt_data
        
        # å¯¼èˆªåˆ°å·¥å…·æˆåŠŸç‡
        rate_key = str(tool_rate)
        if "by_tool_success_rate" not in prompt_data or rate_key not in prompt_data["by_tool_success_rate"]:
            return {}
        rate_data = prompt_data["by_tool_success_rate"][rate_key]
        
        # å¦‚æœåªæŒ‡å®šåˆ°å·¥å…·æˆåŠŸç‡
        if not difficulty:
            return rate_data
        
        # å¯¼èˆªåˆ°éš¾åº¦
        if "by_difficulty" not in rate_data or difficulty not in rate_data["by_difficulty"]:
            return {}
        diff_data = rate_data["by_difficulty"][difficulty]
        
        # å¦‚æœåªæŒ‡å®šåˆ°éš¾åº¦
        if not task_type:
            return diff_data
        
        # å¯¼èˆªåˆ°ä»»åŠ¡ç±»å‹
        if "by_task_type" not in diff_data or task_type not in diff_data["by_task_type"]:
            return {}
        
        return diff_data["by_task_type"][task_type]

if __name__ == "__main__":
    # ç¤ºä¾‹ä½¿ç”¨
    print("PILOT-Bench ç´¯ç§¯æµ‹è¯•ç®¡ç†ç³»ç»Ÿ")
    print("="*60)
    
    # æ·»åŠ ä¸€äº›æµ‹è¯•ç»“æœç¤ºä¾‹
    add_test_result(
        model="qwen2.5-3b-instruct",
        task_type="simple_task",
        prompt_type="baseline",
        success=True,
        execution_time=2.5
    )
    
    # æ£€æŸ¥è¿›åº¦
    progress = check_progress("qwen2.5-3b-instruct", target_count=100)
    print(f"\næ¨¡å‹è¿›åº¦: {progress['model']}")
    print(f"å®Œæˆç‡: {progress['completion_rate']:.1f}%")
    print(f"å‰©ä½™æµ‹è¯•: {progress['total_needed']}ä¸ª")
