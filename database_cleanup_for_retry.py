#!/usr/bin/env python3
"""
æ•°æ®åº“æ¸…ç†å·¥å…· - é‡æµ‹å‰æ¸…ç†æŒ‡å®šæ¨¡å‹çš„ç‰¹å®šé…ç½®æ•°æ®
ç¡®ä¿é‡æµ‹æ—¶è¦†ç›–è€Œä¸æ˜¯ç´¯åŠ æ•°æ®
"""

import json
import os
from pathlib import Path
from typing import List, Dict, Any
from datetime import datetime

class DatabaseCleanupForRetry:
    """é‡æµ‹å‰çš„æ•°æ®åº“æ¸…ç†å·¥å…·"""
    
    def __init__(self, db_path: str = "pilot_bench_cumulative_results/master_database.json"):
        self.db_path = Path(db_path)
        self.backup_created = False
    
    def create_backup(self):
        """åˆ›å»ºæ•°æ®åº“å¤‡ä»½"""
        if not self.backup_created and self.db_path.exists():
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_path = self.db_path.parent / f"master_database_backup_before_retry_cleanup_{timestamp}.json"
            
            import shutil
            shutil.copy2(self.db_path, backup_path)
            print(f"âœ… å·²åˆ›å»ºå¤‡ä»½: {backup_path}")
            self.backup_created = True
    
    def clean_model_prompt_data(self, model: str, prompt_types: List[str], 
                               difficulty: str = "easy", task_types: List[str] = None,
                               clean_timeouts: bool = True):
        """
        æ¸…ç†æŒ‡å®šæ¨¡å‹çš„ç‰¹å®špromptç±»å‹æ•°æ®
        
        Args:
            model: æ¨¡å‹åç§°
            prompt_types: è¦æ¸…ç†çš„promptç±»å‹åˆ—è¡¨
            difficulty: éš¾åº¦çº§åˆ« (é»˜è®¤: "easy") 
            task_types: ä»»åŠ¡ç±»å‹åˆ—è¡¨ (é»˜è®¤: ["simple_task", "basic_task", "data_pipeline", "api_integration", "multi_stage_pipeline"])
            clean_timeouts: æ˜¯å¦åŒæ—¶æ¸…ç†æ˜æ˜¾çš„è¶…æ—¶é”™è¯¯æ•°æ® (é»˜è®¤: True)
        """
        if not self.db_path.exists():
            print(f"âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {self.db_path}")
            return False
        
        # åˆ›å»ºå¤‡ä»½
        self.create_backup()
        
        # é»˜è®¤ä»»åŠ¡ç±»å‹
        if task_types is None:
            task_types = ["simple_task", "basic_task", "data_pipeline", "api_integration", "multi_stage_pipeline"]
        
        try:
            # è¯»å–æ•°æ®åº“
            with open(self.db_path, 'r', encoding='utf-8') as f:
                db = json.load(f)
            
            if model not in db.get('models', {}):
                print(f"âš ï¸  æ¨¡å‹ {model} åœ¨æ•°æ®åº“ä¸­ä¸å­˜åœ¨")
                return True
            
            model_data = db['models'][model]
            by_prompt_type = model_data.get('by_prompt_type', {})
            
            cleaned_configs = []
            total_cleaned_tests = 0
            
            # æ¸…ç†æŒ‡å®šçš„promptç±»å‹æ•°æ®
            for prompt_type in prompt_types:
                if prompt_type in by_prompt_type:
                    prompt_data = by_prompt_type[prompt_type]
                    
                    # æ¸…ç† by_tool_success_rate/0.8/difficulty/task_type å±‚æ¬¡
                    by_rate = prompt_data.get('by_tool_success_rate', {})
                    if '0.8' in by_rate:
                        rate_data = by_rate['0.8']
                        by_diff = rate_data.get('by_difficulty', {})
                        
                        if difficulty in by_diff:
                            diff_data = by_diff[difficulty]
                            by_task = diff_data.get('by_task_type', {})
                            
                            # æ¸…ç†æŒ‡å®šä»»åŠ¡ç±»å‹çš„æ•°æ®
                            for task_type in task_types:
                                if task_type in by_task:
                                    task_data = by_task[task_type]
                                    total_tests = task_data.get('total', 0)
                                    
                                    if total_tests > 0:
                                        cleaned_configs.append(f"{prompt_type}/0.8/{difficulty}/{task_type}")
                                        total_cleaned_tests += total_tests
                                        
                                        # åˆ é™¤è¯¥é…ç½®çš„æ‰€æœ‰æ•°æ®
                                        del by_task[task_type]
                            
                            # å¦‚æœby_task_typeä¸ºç©ºï¼Œåˆ é™¤difficultyå±‚
                            if not by_task:
                                del by_diff[difficulty]
                        
                        # å¦‚æœby_difficultyä¸ºç©ºï¼Œåˆ é™¤rateå±‚
                        if not by_diff:
                            del by_rate['0.8']
                    
                    # å¦‚æœby_tool_success_rateä¸ºç©ºï¼Œåˆ é™¤prompt_typeå±‚
                    if not by_rate:
                        del by_prompt_type[prompt_type]
            
            # é‡æ–°è®¡ç®—æ¨¡å‹çš„æ€»æµ‹è¯•æ•°
            new_total_tests = self._recalculate_model_total(model_data)
            model_data['total_tests'] = new_total_tests
            
            # æ¸…ç†overall_statsï¼ˆé‡æ–°è®¡ç®—ï¼‰
            model_data['overall_stats'] = {}
            
            # æ›´æ–°æ•°æ®åº“æ—¶é—´æˆ³
            db['last_updated'] = datetime.now().isoformat()
            
            # ä¿å­˜æ•°æ®åº“
            with open(self.db_path, 'w', encoding='utf-8') as f:
                json.dump(db, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… å·²æ¸…ç†æ¨¡å‹ {model} çš„æ•°æ®:")
            print(f"   æ¸…ç†çš„é…ç½®: {len(cleaned_configs)} ä¸ª")
            print(f"   æ¸…ç†çš„æµ‹è¯•æ•°: {total_cleaned_tests} ä¸ª")
            for config in cleaned_configs:
                print(f"   - {config}")
            print(f"   æ¨¡å‹æ–°çš„æ€»æµ‹è¯•æ•°: {new_total_tests}")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ¸…ç†å¤±è´¥: {e}")
            return False
    
    def _recalculate_model_total(self, model_data: Dict[str, Any]) -> int:
        """é‡æ–°è®¡ç®—æ¨¡å‹çš„æ€»æµ‹è¯•æ•°"""
        total = 0
        by_prompt_type = model_data.get('by_prompt_type', {})
        
        for prompt_type, prompt_data in by_prompt_type.items():
            by_rate = prompt_data.get('by_tool_success_rate', {})
            for rate, rate_data in by_rate.items():
                by_diff = rate_data.get('by_difficulty', {})
                for diff, diff_data in by_diff.items():
                    by_task = diff_data.get('by_task_type', {})
                    for task, task_data in by_task.items():
                        total += task_data.get('total', 0)
        
        return total
    
    def clean_failed_test_configs(self, failed_tests_config: Dict[str, Any]):
        """
        æ ¹æ®failed_tests_config.jsonæ¸…ç†å¯¹åº”çš„æ•°æ®åº“é…ç½®
        
        Args:
            failed_tests_config: å¤±è´¥æµ‹è¯•é…ç½®å­—å…¸
        """
        if 'active_session' not in failed_tests_config:
            print("âŒ æ— æ•ˆçš„å¤±è´¥æµ‹è¯•é…ç½®æ ¼å¼")
            return False
        
        active_session = failed_tests_config['active_session']
        failed_tests = active_session.get('failed_tests', {})
        
        total_cleaned = 0
        
        for model, test_list in failed_tests.items():
            for test in test_list:
                if test.get('status') == 'retrying':
                    prompt_types_str = test.get('prompt_types', '')
                    prompt_types = [pt.strip() for pt in prompt_types_str.split(',') if pt.strip()]
                    
                    if prompt_types:
                        print(f"\nğŸ§¹ æ¸…ç†æ¨¡å‹ {model} çš„é‡æµ‹æ•°æ®...")
                        success = self.clean_model_prompt_data(
                            model=model,
                            prompt_types=prompt_types,
                            difficulty="easy",  # ä»é…ç½®æ¨æ–­éš¾åº¦ä¸ºeasy
                            task_types=["simple_task", "basic_task", "data_pipeline", "api_integration", "multi_stage_pipeline"]
                        )
                        if success:
                            total_cleaned += 1
        
        print(f"\nâœ… å®Œæˆæ¸…ç†ï¼Œå¤„ç†äº† {total_cleaned} ä¸ªæ¨¡å‹çš„é‡æµ‹æ•°æ®")
        return total_cleaned > 0
    
    def clean_timeout_errors(self, timeout_threshold: float = 600.0):
        """
        æ¸…ç†æ•°æ®åº“ä¸­æ˜æ˜¾çš„è¶…æ—¶é”™è¯¯æ•°æ®
        
        Args:
            timeout_threshold: è¶…æ—¶é˜ˆå€¼ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤600ç§’
        """
        if not self.db_path.exists():
            print(f"âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {self.db_path}")
            return False
        
        # åˆ›å»ºå¤‡ä»½
        self.create_backup()
        
        try:
            # è¯»å–æ•°æ®åº“
            with open(self.db_path, 'r', encoding='utf-8') as f:
                db = json.load(f)
            
            total_cleaned = 0
            timeout_configs = []
            
            for model_name, model_data in db.get('models', {}).items():
                by_prompt_type = model_data.get('by_prompt_type', {})
                
                for prompt_type, prompt_data in by_prompt_type.items():
                    by_rate = prompt_data.get('by_tool_success_rate', {})
                    
                    for rate, rate_data in by_rate.items():
                        by_diff = rate_data.get('by_difficulty', {})
                        
                        for diff, diff_data in by_diff.items():
                            by_task = diff_data.get('by_task_type', {})
                            
                            configs_to_remove = []
                            for task_type, task_data in by_task.items():
                                avg_execution_time = task_data.get('avg_execution_time', 0)
                                total_tests = task_data.get('total', 0)
                                success_rate = task_data.get('success_rate', 0)
                                
                                # æ£€æµ‹æ˜æ˜¾çš„è¶…æ—¶é”™è¯¯ï¼š
                                # 1. å¹³å‡æ‰§è¡Œæ—¶é—´è¾¾åˆ°æˆ–æ¥è¿‘è¶…æ—¶é˜ˆå€¼
                                # 2. æˆåŠŸç‡ä¸º0ï¼ˆå®Œå…¨å¤±è´¥ï¼‰
                                # 3. æœ‰æµ‹è¯•æ•°æ®
                                if (avg_execution_time >= timeout_threshold and 
                                    success_rate == 0 and 
                                    total_tests > 0):
                                    
                                    config_id = f"{model_name}/{prompt_type}/{rate}/{diff}/{task_type}"
                                    timeout_configs.append({
                                        "config": config_id,
                                        "total_tests": total_tests,
                                        "avg_execution_time": avg_execution_time
                                    })
                                    configs_to_remove.append(task_type)
                                    total_cleaned += total_tests
                            
                            # åˆ é™¤è¶…æ—¶é…ç½®
                            for task_type in configs_to_remove:
                                del by_task[task_type]
                            
                            # æ¸…ç†ç©ºçš„å±‚çº§
                            if not by_task:
                                del by_diff[diff]
                        
                        if not by_diff:
                            del by_rate[rate]
                    
                    if not by_rate:
                        del by_prompt_type[prompt_type]
                
                # é‡æ–°è®¡ç®—æ¨¡å‹çš„æ€»æµ‹è¯•æ•°
                new_total_tests = self._recalculate_model_total(model_data)
                model_data['total_tests'] = new_total_tests
                
                # æ¸…ç†overall_statsï¼ˆé‡æ–°è®¡ç®—ï¼‰
                model_data['overall_stats'] = {}
            
            # æ›´æ–°æ•°æ®åº“æ—¶é—´æˆ³
            db['last_updated'] = datetime.now().isoformat()
            
            # ä¿å­˜æ•°æ®åº“
            with open(self.db_path, 'w', encoding='utf-8') as f:
                json.dump(db, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… å·²æ¸…ç†æ˜æ˜¾çš„è¶…æ—¶é”™è¯¯æ•°æ®:")
            print(f"   æ¸…ç†çš„é…ç½®: {len(timeout_configs)} ä¸ª")
            print(f"   æ¸…ç†çš„æµ‹è¯•æ•°: {total_cleaned} ä¸ª")
            print(f"   è¶…æ—¶é˜ˆå€¼: {timeout_threshold}ç§’")
            
            if timeout_configs:
                print("\næ¸…ç†çš„è¶…æ—¶é…ç½®:")
                for config in timeout_configs:
                    print(f"   - {config['config']}: {config['total_tests']}ä¸ªæµ‹è¯•, å¹³å‡æ‰§è¡Œæ—¶é—´{config['avg_execution_time']}ç§’")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ¸…ç†è¶…æ—¶é”™è¯¯æ•°æ®å¤±è´¥: {e}")
            return False

def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    import sys
    
    cleaner = DatabaseCleanupForRetry()
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•:")
        print("  python database_cleanup_for_retry.py clean_failed  # æ ¹æ®failed_tests_config.jsonæ¸…ç†")
        print("  python database_cleanup_for_retry.py clean_model <model> <prompt_types>  # æ¸…ç†æŒ‡å®šæ¨¡å‹")
        print("  python database_cleanup_for_retry.py clean_timeouts  # æ¸…ç†æ˜æ˜¾çš„è¶…æ—¶é”™è¯¯æ•°æ®")
        return
    
    command = sys.argv[1]
    
    if command == "clean_failed":
        # æ ¹æ®failed_tests_config.jsonæ¸…ç†
        config_path = Path("failed_tests_config.json")
        if not config_path.exists():
            print(f"âŒ æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶: {config_path}")
            return
        
        with open(config_path, 'r', encoding='utf-8') as f:
            failed_config = json.load(f)
        
        cleaner.clean_failed_test_configs(failed_config)
        
    elif command == "clean_model" and len(sys.argv) >= 4:
        # æ¸…ç†æŒ‡å®šæ¨¡å‹
        model = sys.argv[2]
        prompt_types = sys.argv[3].split(',')
        
        cleaner.clean_model_prompt_data(
            model=model,
            prompt_types=prompt_types,
            difficulty="easy"
        )
        
    elif command == "clean_timeouts":
        # æ¸…ç†æ˜æ˜¾çš„è¶…æ—¶é”™è¯¯æ•°æ®
        cleaner.clean_timeout_errors()
        
    else:
        print("âŒ æ— æ•ˆçš„å‘½ä»¤æˆ–å‚æ•°")

if __name__ == "__main__":
    main()