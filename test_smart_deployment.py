#!/usr/bin/env python3
"""
æµ‹è¯•æ™ºèƒ½éƒ¨ç½²ç®¡ç†å™¨åŠŸèƒ½
===================
éªŒè¯å¤šéƒ¨ç½²è´Ÿè½½å‡è¡¡å’Œ429é”™è¯¯å¤„ç†
"""

import sys
import time
import logging
from smart_deployment_manager import SmartDeploymentManager, get_deployment_manager
from api_client_manager import APIClientManager

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_deployment_manager():
    """æµ‹è¯•æ™ºèƒ½éƒ¨ç½²ç®¡ç†å™¨åŸºæœ¬åŠŸèƒ½"""
    print("ğŸš€ æµ‹è¯•æ™ºèƒ½éƒ¨ç½²ç®¡ç†å™¨")
    print("=" * 50)
    
    # åˆ›å»ºç®¡ç†å™¨å®ä¾‹
    manager = SmartDeploymentManager()
    
    # æ˜¾ç¤ºå½“å‰çŠ¶æ€
    manager.print_status()
    
    # æµ‹è¯•è·å–æœ€ä½³éƒ¨ç½²
    print(f"\nğŸ¯ æµ‹è¯•éƒ¨ç½²é€‰æ‹©:")
    for i in range(5):
        llama_deployment = manager.get_best_deployment("Llama-3.3-70B-Instruct")
        deepseek_deployment = manager.get_best_deployment("DeepSeek-V3-0324")
        print(f"  è½®æ¬¡{i+1}: Llama -> {llama_deployment}, DeepSeek -> {deepseek_deployment}")
        time.sleep(0.1)
    
    # æµ‹è¯•æ•…éšœæ ‡è®°
    print(f"\nâš ï¸  æµ‹è¯•æ•…éšœæ ‡è®°:")
    manager.mark_deployment_failed("Llama-3.3-70B-Instruct", "429")
    manager.mark_deployment_failed("Llama-3.3-70B-Instruct-2", "timeout")
    
    print(f"æ ‡è®°å¤±è´¥åçš„é€‰æ‹©:")
    for i in range(3):
        deployment = manager.get_best_deployment("Llama-3.3-70B-Instruct")
        print(f"  è½®æ¬¡{i+1}: {deployment}")
        time.sleep(0.1)
    
    # æ˜¾ç¤ºæ›´æ–°åçš„çŠ¶æ€
    print(f"\nğŸ“Š æ›´æ–°åçš„çŠ¶æ€:")
    manager.print_status()

def test_api_integration():
    """æµ‹è¯•APIå®¢æˆ·ç«¯ç®¡ç†å™¨é›†æˆ"""
    print(f"\nğŸ”Œ æµ‹è¯•APIå®¢æˆ·ç«¯ç®¡ç†å™¨é›†æˆ")
    print("=" * 50)
    
    try:
        # åˆ›å»ºAPIå®¢æˆ·ç«¯ç®¡ç†å™¨
        api_manager = APIClientManager()
        
        # æµ‹è¯•è·å–Llamaå®¢æˆ·ç«¯
        print("ğŸ“¡ æµ‹è¯•è·å–Llama-3.3-70B-Instructå®¢æˆ·ç«¯...")
        client = api_manager._get_user_azure_client("Llama-3.3-70B-Instruct")
        
        if hasattr(client, 'deployment_name'):
            print(f"âœ… æˆåŠŸåˆ›å»ºå®¢æˆ·ç«¯ï¼Œéƒ¨ç½²å: {client.deployment_name}")
            if hasattr(client, 'current_deployment'):
                print(f"âœ… å½“å‰éƒ¨ç½²: {client.current_deployment}")
        else:
            print("âŒ å®¢æˆ·ç«¯ç¼ºå°‘éƒ¨ç½²ä¿¡æ¯")
            
    except Exception as e:
        print(f"âŒ APIé›†æˆæµ‹è¯•å¤±è´¥: {e}")

def test_mock_429_scenario():
    """æ¨¡æ‹Ÿ429é”™è¯¯åœºæ™¯"""
    print(f"\nğŸš¨ æ¨¡æ‹Ÿ429é”™è¯¯å¤„ç†åœºæ™¯")
    print("=" * 50)
    
    manager = get_deployment_manager()
    
    # æ¨¡æ‹Ÿå¤šä¸ª429é”™è¯¯
    print("ğŸ“¡ æ¨¡æ‹ŸLlama-3.3-70B-Instructçš„3ä¸ªéƒ¨ç½²éƒ½é‡åˆ°429é”™è¯¯...")
    
    deployments = ["Llama-3.3-70B-Instruct", "Llama-3.3-70B-Instruct-2", "Llama-3.3-70B-Instruct-3"]
    
    for i, deployment in enumerate(deployments):
        print(f"  {i+1}. æ ‡è®° {deployment} ä¸º429é”™è¯¯")
        manager.mark_deployment_failed(deployment, "429")
        
        # å°è¯•è·å–æœ€ä½³éƒ¨ç½²
        best = manager.get_best_deployment("Llama-3.3-70B-Instruct")
        print(f"     -> å½“å‰æœ€ä½³é€‰æ‹©: {best}")
    
    # æ˜¾ç¤ºæœ€ç»ˆçŠ¶æ€
    print(f"\nğŸ“Š æ‰€æœ‰éƒ¨ç½²æ ‡è®°å¤±è´¥åçš„çŠ¶æ€:")
    manager.print_status()
    
    # æµ‹è¯•æ¢å¤æœºåˆ¶
    print(f"\nğŸ”„ æµ‹è¯•æ¢å¤æœºåˆ¶ - æ ‡è®°ä¸€ä¸ªéƒ¨ç½²æ¢å¤æˆåŠŸ:")
    manager.mark_deployment_success("Llama-3.3-70B-Instruct-2")
    best = manager.get_best_deployment("Llama-3.3-70B-Instruct")
    print(f"æ¢å¤åçš„æœ€ä½³é€‰æ‹©: {best}")

def test_configuration_validation():
    """éªŒè¯é…ç½®æ­£ç¡®æ€§"""
    print(f"\nâš™ï¸  éªŒè¯é…ç½®æ­£ç¡®æ€§")
    print("=" * 50)
    
    try:
        manager = SmartDeploymentManager()
        
        # æ£€æŸ¥é…ç½®ä¸­çš„å¹¶è¡Œéƒ¨ç½²
        parallel_deployments = manager.parallel_deployments
        
        print("ğŸ“‹ é…ç½®çš„å¹¶è¡Œéƒ¨ç½²:")
        for model, deployments in parallel_deployments.items():
            print(f"  {model}:")
            for deployment in deployments:
                print(f"    â€¢ {deployment}")
        
        # éªŒè¯æ¯ä¸ªéƒ¨ç½²æ˜¯å¦æœ‰å¯¹åº”çš„é…ç½®
        print(f"\nğŸ” éªŒè¯éƒ¨ç½²é…ç½®:")
        config = manager.config
        model_configs = config.get('model_configs', {})
        
        for model, deployments in parallel_deployments.items():
            print(f"  {model}:")
            for deployment in deployments:
                if deployment in model_configs:
                    provider = model_configs[deployment].get('provider', 'unknown')
                    endpoint = model_configs[deployment].get('azure_endpoint', 'N/A')
                    print(f"    âœ… {deployment}: {provider} - {endpoint}")
                else:
                    print(f"    âŒ {deployment}: é…ç½®ç¼ºå¤±")
                    
    except Exception as e:
        print(f"âŒ é…ç½®éªŒè¯å¤±è´¥: {e}")

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸ§ª æ™ºèƒ½éƒ¨ç½²ç®¡ç†å™¨æµ‹è¯•å¥—ä»¶")
    print("=" * 60)
    
    try:
        # æµ‹è¯•1: åŸºæœ¬åŠŸèƒ½
        test_deployment_manager()
        
        # æµ‹è¯•2: APIé›†æˆ
        test_api_integration()
        
        # æµ‹è¯•3: 429é”™è¯¯åœºæ™¯
        test_mock_429_scenario()
        
        # æµ‹è¯•4: é…ç½®éªŒè¯
        test_configuration_validation()
        
        print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("ğŸ’¡ å»ºè®®: åœ¨å®é™…ä½¿ç”¨ä¸­ç›‘æ§æ—¥å¿—ä»¥è§‚å¯Ÿæ•…éšœè½¬ç§»è¡Œä¸º")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()