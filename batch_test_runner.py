#!/usr/bin/env python3
"""
PILOT-Bench Batch Test Runner with Concurrent Execution Support
æ”¯æŒå¹¶å‘æ‰§è¡Œã€ç´¯ç§¯æµ‹è¯•ã€æ™ºèƒ½é€‰æ‹©çš„æ‰¹é‡æµ‹è¯•è¿è¡Œå™¨
"""

import argparse
import json
import logging
import random
import time
import os
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from threading import Lock, Semaphore
import traceback

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

# å¯¼å…¥æ ¸å¿ƒç»„ä»¶
from mdp_workflow_generator import MDPWorkflowGenerator
from interactive_executor import InteractiveExecutor
from flawed_workflow_generator import FlawedWorkflowGenerator

# æ”¯æŒå­˜å‚¨æ ¼å¼é€‰æ‹©
storage_format = os.environ.get('STORAGE_FORMAT', 'json').lower()
if storage_format == 'parquet':
    try:
        from parquet_cumulative_manager import ParquetCumulativeManager as CumulativeTestManager
        from parquet_cumulative_manager import ParquetCumulativeManager as EnhancedCumulativeManager
        from cumulative_test_manager import TestRecord  # TestRecordä»ä»åŸæ¨¡å—å¯¼å…¥
        print(f"[INFO] ä½¿ç”¨Parquetå­˜å‚¨æ ¼å¼")
    except ImportError:
        from cumulative_test_manager import CumulativeTestManager, TestRecord
        from enhanced_cumulative_manager import EnhancedCumulativeManager
        print(f"[INFO] Parquetä¸å¯ç”¨ï¼Œä½¿ç”¨JSONå­˜å‚¨æ ¼å¼")
else:
    from cumulative_test_manager import CumulativeTestManager, TestRecord
    from enhanced_cumulative_manager import EnhancedCumulativeManager
    print(f"[INFO] ä½¿ç”¨JSONå­˜å‚¨æ ¼å¼")
from workflow_quality_test_flawed import WorkflowQualityTester
from adaptive_rate_limiter import AdaptiveRateLimiter
from storage_adapter import create_storage_adapter

# è®¾ç½®é»˜è®¤æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

@dataclass
class TestTask:
    """æµ‹è¯•ä»»åŠ¡"""
    model: str  # ç»Ÿè®¡ç”¨çš„åŸºç¡€æ¨¡å‹åï¼ˆå°å†™ï¼‰
    task_type: str
    prompt_type: str
    difficulty: str = "easy"
    deployment: Optional[str] = None  # APIè°ƒç”¨ç”¨çš„éƒ¨ç½²å®ä¾‹åï¼ˆä¿æŒåŸå§‹å¤§å°å†™ï¼‰
    is_flawed: bool = False
    flaw_type: Optional[str] = None
    task_instance: Optional[Dict] = None
    timeout: int = 600  # é»˜è®¤è¶…æ—¶æ—¶é—´ï¼ˆ10åˆ†é’Ÿï¼Œç¡®ä¿ä»»åŠ¡èƒ½æŒ‰æ—¶ç»“æŸï¼‰
    tool_success_rate: float = 0.8  # å·¥å…·æˆåŠŸç‡
    required_tools: Optional[List[str]] = None  # å¿…éœ€å·¥å…·åˆ—è¡¨
    
    def __post_init__(self):
        """æ ¹æ®æ¨¡å‹åŠ¨æ€è°ƒæ•´è¶…æ—¶æ—¶é—´ï¼ˆç»Ÿä¸€é™åˆ¶åœ¨10åˆ†é’Ÿå†…ï¼‰"""
        # æ‰€æœ‰æ¨¡å‹ç»Ÿä¸€ä½¿ç”¨10åˆ†é’Ÿè¶…æ—¶ï¼Œç¡®ä¿ä»»åŠ¡èƒ½æŒ‰æ—¶ç»“æŸ
        # ä¸å†æ ¹æ®æ¨¡å‹ç±»å‹åŒºåˆ†ï¼Œé¿å…æŸäº›æ¨¡å‹å ç”¨è¿‡é•¿æ—¶é—´
        self.timeout = 600  # ç»Ÿä¸€10åˆ†é’Ÿè¶…æ—¶


class BatchTestRunner:
    """æ‰¹é‡æµ‹è¯•è¿è¡Œå™¨ - æ”¯æŒå¹¶å‘å’Œç´¯ç§¯æµ‹è¯•"""
    
    def __init__(self, debug: bool = False, silent: bool = False, use_adaptive: bool = False, 
                 save_logs: bool = False, enable_database_updates: bool = True, 
                 use_ai_classification: bool = True, checkpoint_interval: int = 0,
                 idealab_key_index: Optional[int] = None):
        """åˆå§‹åŒ–æµ‹è¯•è¿è¡Œå™¨"""
        self.debug = debug
        self.silent = silent
        self.use_adaptive = use_adaptive
        self.save_logs = save_logs  # æ˜¯å¦ä¿å­˜è¯¦ç»†äº¤äº’æ—¥å¿—
        self.enable_database_updates = enable_database_updates  # æ˜¯å¦å¯ç”¨å®æ—¶æ•°æ®åº“æ›´æ–°
        self.use_ai_classification = use_ai_classification  # æ˜¯å¦å¯ç”¨AIé”™è¯¯åˆ†ç±»
        self.checkpoint_interval = checkpoint_interval  # ä¸­é—´ä¿å­˜é—´éš”ï¼ˆæ¯Nä¸ªæµ‹è¯•ä¿å­˜ä¸€æ¬¡ï¼‰
        self.pending_results = []  # å¾…ä¿å­˜çš„ç»“æœç¼“å­˜
        self.idealab_key_index = idealab_key_index  # IdealLab API keyç´¢å¼•
        
        # åˆå§‹åŒ–å­˜å‚¨é€‚é…å™¨ï¼ˆç¨ååˆ›å»ºï¼Œéœ€è¦managerï¼‰
        self.storage_adapter = None
        
        # åˆ›å»ºlogsç›®å½• (å¿…é¡»å…ˆåˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ)
        log_dir = Path("logs")
        log_dir.mkdir(exist_ok=True)
        
        # åˆ›å»ºæ—¥å¿—æ–‡ä»¶å
        self.log_filename = log_dir / f"batch_test_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        # è·å–logger (å¿…é¡»åœ¨ä½¿ç”¨loggerä¹‹å‰åˆå§‹åŒ–)
        self.logger = logging.getLogger(__name__)
        
        # åˆå§‹åŒ–AIåˆ†ç±»å™¨
        self.ai_classifier = None
        if use_ai_classification:
            try:
                from txt_based_ai_classifier import TxtBasedAIClassifier
                self.ai_classifier = TxtBasedAIClassifier(model_name="gpt-5-nano")
                print(f"[AI_DEBUG] AIåˆ†ç±»å™¨åˆå§‹åŒ–æˆåŠŸ: {self.ai_classifier}")
                self.logger.info("åŸºäºTXTæ–‡ä»¶çš„AIé”™è¯¯åˆ†ç±»ç³»ç»Ÿå·²å¯ç”¨ (ä½¿ç”¨gpt-5-nano)")
            except Exception as e:
                self.logger.error(f"Failed to initialize TXT-based AI classifier: {e}")
                self.use_ai_classification = False
        
        print(f"[DEBUG] BatchTestRunner initialized with save_logs={save_logs}, enable_database_updates={enable_database_updates}, use_ai_classification={use_ai_classification}, checkpoint_interval={checkpoint_interval}")
        
        # æ¸…é™¤ç°æœ‰çš„handlersï¼ˆé¿å…é‡å¤ï¼‰
        self.logger.handlers.clear()
        
        # æ·»åŠ æ§åˆ¶å°handler
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
        if silent:
            console_handler.setLevel(logging.WARNING)  # silentæ¨¡å¼ä¸‹æ§åˆ¶å°åªæ˜¾ç¤ºè­¦å‘Š
        else:
            console_handler.setLevel(logging.DEBUG if debug else logging.INFO)
        self.logger.addHandler(console_handler)
        
        # æ·»åŠ æ–‡ä»¶handlerï¼ˆå§‹ç»ˆè®°å½•æ‰€æœ‰çº§åˆ«ï¼‰
        file_handler = logging.FileHandler(self.log_filename, encoding='utf-8')
        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s'))
        file_handler.setLevel(logging.DEBUG)  # æ–‡ä»¶å§‹ç»ˆè®°å½•DEBUGçº§åˆ«
        self.logger.addHandler(file_handler)
        
        # è®¾ç½®loggerçº§åˆ«
        self.logger.setLevel(logging.DEBUG)
        self.logger.propagate = False  # é¿å…æ—¥å¿—ä¼ æ’­åˆ°çˆ¶logger
        
        # è®°å½•è¿è¡Œé…ç½®
        self.logger.info("="*60)
        self.logger.info(f"Batch test runner initialized")
        self.logger.info(f"Configuration: debug={debug}, silent={silent}, adaptive={use_adaptive}")
        self.logger.info(f"Log file: {self.log_filename}")
        self.logger.info("="*60)
        
        # å»¶è¿Ÿåˆå§‹åŒ–ï¼ˆä½¿ç”¨lazy loadingé¿å…å¯åŠ¨æ—¶åŠ è½½ï¼‰
        self.generator = None
        self.flawed_generator = None
        self.manager = None
        self.quality_tester = None  # WorkflowQualityTester å®ä¾‹ï¼ˆç”¨äºç»§æ‰¿promptæ–¹æ³•ï¼‰
        
        # çº¿ç¨‹å®‰å…¨
        self._init_lock = Lock()
        self._initialized = False
        
        # ç»Ÿè®¡
        self._test_counter = 0
        self._success_counter = 0
        self._lock = Lock()
        
        # QPSæ§åˆ¶
        self._last_request_time = 0
        self._request_lock = Lock()
    
    def _generate_txt_log_content(self, task: TestTask, result: Dict, log_data: Dict = None) -> str:
        """ç”ŸæˆTXTæ ¼å¼çš„æ—¥å¿—å†…å®¹ï¼ˆä¸é™åˆ¶tokenæ•°ï¼‰
        
        Returns:
            str: TXTæ ¼å¼çš„æ—¥å¿—å†…å®¹å­—ç¬¦ä¸²
        """
        # å‡†å¤‡å®Œæ•´çš„æ—¥å¿—æ•°æ®
        if not log_data:
            # å¦‚æœæ²¡æœ‰ä¼ å…¥log_dataï¼Œåˆ›å»ºåŸºæœ¬ç»“æ„ï¼ŒåŒ…å«æ¨¡å‹ä¿¡æ¯
            model_safe = task.model.replace('-', '_').replace('.', '_')  # å®‰å…¨çš„æ¨¡å‹å
            # ç”Ÿæˆtest_idï¼Œæ¨¡å‹åæ”¾åœ¨å‰é¢æ›´æ˜¾çœ¼
            if task.is_flawed and task.flaw_type:
                test_id = f"{model_safe}_{task.task_type}_inst0_test{random.randint(0, 999)}_flawed_{task.flaw_type}_{task.prompt_type}"
            else:
                test_id = f"{model_safe}_{task.task_type}_inst0_test{random.randint(0, 999)}_{task.prompt_type}"
            
            log_data = {
                'test_id': test_id,
                'task_type': task.task_type,
                'prompt_type': task.prompt_type,
                'timestamp': datetime.now().isoformat(),
                'task_instance': {
                    'required_tools': task.required_tools or [],
                    'description': 'Task description not available',
                    'task_type': task.task_type
                },
                'prompt': 'Prompt not captured',
                'llm_response': json.dumps(result, indent=2),
                'extracted_tool_calls': result.get('tool_calls', []),
                'conversation_history': result.get('conversation_history', []),
                'execution_history': result.get('execution_history', []),
                'is_flawed': task.is_flawed,
                'flaw_type': task.flaw_type
            }
        
        # æ·»åŠ ç»“æœä¿¡æ¯
        if result:
            log_data['result'] = {
                'success': result.get('success', False),
                'final_score': result.get('final_score', 0),
                'execution_time': result.get('execution_time', 0),
                'workflow_score': result.get('workflow_score', 0),
                'phase2_score': result.get('phase2_score', 0),
                'quality_score': result.get('quality_score', 0),
                'tool_calls': result.get('tool_calls', []),
                'error': result.get('error'),
                'error_type': result.get('error_type')
            }
        
        test_id = log_data.get('test_id', 'unknown')
        
        # ç”ŸæˆTXTæ ¼å¼å†…å®¹ï¼ˆä¸é™åˆ¶é•¿åº¦ï¼‰
        txt_lines = []
        txt_lines.append(f"Test Log: {test_id}")
        txt_lines.append("=" * 80 + "\n")
        
        txt_lines.append(f"Task Type: {log_data.get('task_type', 'unknown')}")
        txt_lines.append(f"Prompt Type: {log_data.get('prompt_type', 'unknown')}")
        txt_lines.append(f"Timestamp: {log_data.get('timestamp', datetime.now().isoformat())}\n")
        
        txt_lines.append("Task Instance:")
        txt_lines.append("-" * 40)
        if log_data.get('task_instance'):
            txt_lines.append(f"Required Tools: {log_data['task_instance'].get('required_tools', [])}")
            txt_lines.append(f"Description: {log_data['task_instance'].get('description', 'N/A')}")
        txt_lines.append("")
        
        txt_lines.append("Prompt:")
        txt_lines.append("-" * 40)
        txt_lines.append(str(log_data.get('prompt', 'Not captured')))
        txt_lines.append("")
        
        txt_lines.append("LLM Response:")
        txt_lines.append("-" * 40)
        txt_lines.append(str(log_data.get('llm_response', 'Not captured')))
        txt_lines.append("")
        
        # æ·»åŠ å®Œæ•´çš„å¯¹è¯å†å²ï¼ˆæŒ‰turnåˆ†ç»„ï¼‰
        txt_lines.append("Conversation History:")
        txt_lines.append("-" * 40)
        conversation = log_data.get('conversation_history', [])
        
        if conversation:
            # æŒ‰turnå·åˆ†ç»„å¯¹è¯
            turns_dict = {}
            
            for msg in conversation:
                if isinstance(msg, dict):
                    # è·å–turnå·ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç´¢å¼•
                    turn_num = msg.get('turn', len(turns_dict) + 1)
                    
                    if turn_num not in turns_dict:
                        turns_dict[turn_num] = {'assistant': [], 'user': []}
                    
                    role = msg.get('role', '')
                    content = msg.get('content', 'N/A')
                    
                    if role == 'assistant':
                        turns_dict[turn_num]['assistant'].append(content)
                    elif role == 'user':
                        turns_dict[turn_num]['user'].append(content)
                    elif 'human' in msg and 'assistant' in msg:
                        # æ—§æ ¼å¼
                        turns_dict[turn_num]['user'].append(msg.get('human', 'N/A'))
                        turns_dict[turn_num]['assistant'].append(msg.get('assistant', 'N/A'))
            
            # æŒ‰turnå·æ’åºè¾“å‡º
            for turn_num in sorted(turns_dict.keys()):
                txt_lines.append(f"\nTurn {turn_num}:")
                
                # è¾“å‡ºassistantæ¶ˆæ¯
                if turns_dict[turn_num]['assistant']:
                    for i, content in enumerate(turns_dict[turn_num]['assistant']):
                        if i == 0:
                            txt_lines.append(f"  Assistant: {content}")
                        else:
                            # å¦‚æœåŒä¸€turnæœ‰å¤šä¸ªassistantæ¶ˆæ¯ï¼Œç¼©è¿›æ˜¾ç¤º
                            txt_lines.append(f"            {content}")
                
                # è¾“å‡ºuseræ¶ˆæ¯
                if turns_dict[turn_num]['user']:
                    for i, content in enumerate(turns_dict[turn_num]['user']):
                        if i == 0:
                            txt_lines.append(f"  User: {content}")
                        else:
                            # å¦‚æœåŒä¸€turnæœ‰å¤šä¸ªuseræ¶ˆæ¯ï¼Œç¼©è¿›æ˜¾ç¤º
                            txt_lines.append(f"        {content}")
                
                # å¦‚æœæŸä¸ªè§’è‰²æ²¡æœ‰æ¶ˆæ¯ï¼Œæ˜¾ç¤ºN/A
                if not turns_dict[turn_num]['assistant']:
                    txt_lines.append(f"  Assistant: N/A")
                if not turns_dict[turn_num]['user']:
                    txt_lines.append(f"  User: N/A")
        else:
            # æ²¡æœ‰å¯¹è¯å†å²
            txt_lines.append("\n(No conversation history recorded)")
        txt_lines.append("")
        
        txt_lines.append("Extracted Tool Calls:")
        txt_lines.append("-" * 40)
        txt_lines.append(str(log_data.get('extracted_tool_calls', [])))
        txt_lines.append("")
        
        # æ·»åŠ APIé—®é¢˜ä¿¡æ¯ï¼ˆå¦‚æœæœ‰ï¼‰
        if log_data.get('api_issues'):
            txt_lines.append("API Issues:")
            txt_lines.append("-" * 40)
            for issue in log_data.get('api_issues', []):
                txt_lines.append(f"Turn {issue.get('turn', 'N/A')}: {issue.get('issue', 'Unknown issue')}")
                if issue.get('timestamp'):
                    txt_lines.append(f"  Timestamp: {issue['timestamp']}")
            txt_lines.append("")
        
        # æ·»åŠ æ‰§è¡Œå†å²
        txt_lines.append("Execution History:")
        txt_lines.append("-" * 40)
        execution = log_data.get('execution_history', [])
        for i, step in enumerate(execution):
            txt_lines.append(f"Step {i+1}: {step}")
        txt_lines.append("")
        
        if log_data.get('result'):
            txt_lines.append("Execution Results:")
            txt_lines.append("-" * 40)
            txt_lines.append(f"Success: {log_data['result'].get('success', False)}")
            txt_lines.append(f"Final Score: {log_data['result'].get('final_score', 0):.3f}")
            txt_lines.append(f"Execution Time: {log_data['result'].get('execution_time', 0):.2f}s")
            txt_lines.append(f"Workflow Score: {log_data['result'].get('workflow_score', 0):.3f}")
            txt_lines.append(f"Phase2 Score: {log_data['result'].get('phase2_score', 0):.3f}")
            if log_data['result'].get('error'):
                txt_lines.append(f"Error: {log_data['result']['error']}")
                txt_lines.append(f"Error Type: {log_data['result'].get('error_type', 'Unknown')}")
        
        return "\n".join(txt_lines)
    
    def _ai_classify_with_txt_content(self, task: TestTask, result: Dict, txt_content: str = None) -> Tuple[str, str, float]:
        """åŸºäºTXTå†…å®¹å­—ç¬¦ä¸²è¿›è¡ŒAIé”™è¯¯åˆ†ç±»ï¼ˆä¸éœ€è¦æ–‡ä»¶ï¼‰"""
        # AIåˆ†ç±»è°ƒè¯•ä¿¡æ¯
        print(f"[AI_DEBUG] _ai_classify_with_txt_content called:")
        print(f"  - use_ai_classification={self.use_ai_classification}")
        print(f"  - ai_classifier={self.ai_classifier is not None}")
        print(f"  - txt_content_len={len(txt_content) if txt_content else 0}")
        print(f"  - task_model={task.model}")
        
        if not (self.use_ai_classification and self.ai_classifier and txt_content):
            return None, None, 0.0
        
        try:
            # ä½¿ç”¨åŸºäºTXTå†…å®¹çš„åˆ†ç±»å™¨
            category, reason, confidence = self.ai_classifier.classify_from_txt_content(txt_content)
            
            # Check if category is not None before accessing .value
            if category is None:
                self.logger.warning("AI classifier returned None category")
                return None, None, 0.0
            
            if self.debug:
                self.logger.info(f"AIåŸºäºTXTå†…å®¹åˆ†ç±»ç»“æœ: {category.value} (ç½®ä¿¡åº¦: {confidence:.2f}) - {reason[:100] if reason else 'No reason'}")
            
            return category.value, reason, confidence
            
        except Exception as e:
            self.logger.error(f"AIåŸºäºTXTå†…å®¹é”™è¯¯åˆ†ç±»å¤±è´¥: {e}")
            return None, None, 0.0
    
    def _save_interaction_log(self, task: TestTask, result: Dict, log_data: Dict = None) -> Optional[Path]:
        """ä¿å­˜äº¤äº’æ—¥å¿—åˆ°æ–‡ä»¶ï¼ˆä½¿ç”¨WorkflowQualityTesterçš„æ ¼å¼ï¼‰
        
        Returns:
            Path: ä¿å­˜çš„TXTæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¿å­˜å¤±è´¥åˆ™è¿”å›None
        """
        try:
            # åˆ›å»ºæ—¥å¿—ç›®å½•
            log_dir = Path("workflow_quality_results/test_logs")
            log_dir.mkdir(parents=True, exist_ok=True)
            
            # ç”ŸæˆTXTå†…å®¹
            txt_content = self._generate_txt_log_content(task, result, log_data)
            
            # è·å–test_idï¼ˆç”¨äºæ–‡ä»¶åï¼‰
            # test_idç°åœ¨å·²ç»åœ¨ç”Ÿæˆæ—¶åŒ…å«äº†æ¨¡å‹ååœ¨æœ€å‰é¢
            if log_data and 'test_id' in log_data:
                test_id = log_data['test_id']
            else:
                # å¤‡ç”¨ï¼šå¦‚æœæ²¡æœ‰test_idï¼Œç”Ÿæˆä¸€ä¸ªæ–°çš„ï¼ˆæ¨¡å‹ååœ¨å‰ï¼‰
                model_safe = task.model.replace('-', '_').replace('.', '_')
                if task.is_flawed and task.flaw_type:
                    test_id = f"{model_safe}_{task.task_type}_inst0_test{random.randint(0, 999)}_flawed_{task.flaw_type}_{task.prompt_type}"
                else:
                    test_id = f"{model_safe}_{task.task_type}_inst0_test{random.randint(0, 999)}_{task.prompt_type}"
            
            # ä¿å­˜TXTæ–‡ä»¶
            txt_file = log_dir / f"{test_id}_log.txt"
            with open(txt_file, 'w', encoding='utf-8') as f:
                f.write(txt_content)
            
            # ä¹Ÿä¿å­˜JSONæ ¼å¼ï¼ˆå¦‚æœæœ‰log_dataï¼‰
            if log_data:
                json_file = log_dir / f"{test_id}_log.json"
                with open(json_file, 'w', encoding='utf-8') as f:
                    json.dump(log_data, f, indent=2, ensure_ascii=False)
                self.logger.debug(f"Saved detailed logs: {json_file} and {txt_file}")
            else:
                self.logger.debug(f"Saved log: {txt_file}")
            
            return txt_file  # è¿”å›TXTæ–‡ä»¶è·¯å¾„ä¾›AIåˆ†ç±»ä½¿ç”¨
            
        except Exception as e:
            self.logger.error(f"Failed to save interaction log: {e}")
            return None
    
    def _lazy_init(self):
        """å»¶è¿Ÿåˆå§‹åŒ–ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        if self._initialized:
            return
        
        with self._init_lock:
            # Double-check locking pattern
            if self._initialized:
                return
            
            self.logger.info("Initializing test components...")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é¢„ç”Ÿæˆçš„workflowæ–‡ä»¶
            # é€šè¿‡æ£€æŸ¥ä»»åŠ¡åº“æ˜¯å¦åŒ…å«workflowå­—æ®µæ¥åˆ¤æ–­
            self.use_pregenerated_workflows = False
            try:
                # å°è¯•åŠ è½½ä¸€ä¸ªä»»åŠ¡åº“æ–‡ä»¶æ£€æŸ¥
                sample_path = Path("mcp_generated_library/difficulty_versions/task_library_enhanced_v3_easy_with_workflows.json")
                if sample_path.exists():
                    with open(sample_path, 'r') as f:
                        sample_data = json.load(f)
                        tasks = sample_data.get('tasks', sample_data if isinstance(sample_data, list) else [])
                        if tasks and len(tasks) > 0 and 'workflow' in tasks[0]:
                            self.use_pregenerated_workflows = True
                            self.logger.info("Found pre-generated workflows, will use them to save memory")
            except Exception as e:
                self.logger.debug(f"Could not check for pre-generated workflows: {e}")
            
            # åªåœ¨æ²¡æœ‰é¢„ç”Ÿæˆworkflowæ—¶åˆå§‹åŒ–ç”Ÿæˆå™¨
            if not self.use_pregenerated_workflows:
                self.logger.info("No pre-generated workflows found, initializing MDPWorkflowGenerator")
                self.generator = MDPWorkflowGenerator(
                    model_path="checkpoints/best_model.pt",
                    use_embeddings=True
                )
            else:
                # âš¡ ä¼˜åŒ–æ–¹æ¡ˆï¼šä½¿ç”¨çœŸæ­£çš„MDPWorkflowGeneratorï¼Œä½†è·³è¿‡æ¨¡å‹åŠ è½½
                self.logger.info("âš¡ [OPTIMIZATION] Using MDPWorkflowGenerator with SKIP_MODEL_LOADING=true")
                self.logger.info("âš¡ This saves ~350MB memory while keeping all functionality intact")
                
                # è®¾ç½®ç¯å¢ƒå˜é‡ä»¥è·³è¿‡ç¥ç»ç½‘ç»œæ¨¡å‹åŠ è½½
                import os
                os.environ['SKIP_MODEL_LOADING'] = 'true'
                
                # ä½¿ç”¨çœŸæ­£çš„MDPWorkflowGenerator
                # å®ƒä¼šæ£€æµ‹SKIP_MODEL_LOADINGç¯å¢ƒå˜é‡å¹¶è·³è¿‡æ¨¡å‹åŠ è½½
                from mdp_workflow_generator import MDPWorkflowGenerator
                self.generator = MDPWorkflowGenerator(
                    model_path="checkpoints/best_model.pt",  # è·¯å¾„ä¼šè¢«å¿½ç•¥å› ä¸ºSKIP_MODEL_LOADING=true
                    use_embeddings=True  # ä¿ç•™æ‰€æœ‰å…¶ä»–åŠŸèƒ½
                )
                
                self.logger.info("âœ… MDPWorkflowGenerator initialized successfully:")
                self.logger.info(f"  - task_manager: {'âœ“' if hasattr(self.generator, 'task_manager') else 'âœ—'}")
                self.logger.info(f"  - output_verifier: {'âœ“' if hasattr(self.generator, 'output_verifier') else 'âœ—'}")
                self.logger.info(f"  - embedding_manager: {'âœ“' if hasattr(self.generator, 'embedding_manager') else 'âœ—'}")
                self.logger.info(f"  - tool_capabilities: {len(self.generator.tool_capabilities)} tools")
                self.logger.info(f"  - neural network: {'loaded' if self.generator.q_network or self.generator.network else 'skipped (saving 350MB)'}")
                
                # ä¸å†éœ€è¦MockGeneratorï¼æ‰€æœ‰åŠŸèƒ½éƒ½å®Œæ•´ä¿ç•™
            
            # åˆå§‹åŒ–ç¼ºé™·ç”Ÿæˆå™¨ï¼ˆå§‹ç»ˆéœ€è¦ï¼‰
            self.flawed_generator = FlawedWorkflowGenerator(
                self.generator.full_tool_registry
            )
            
            # åˆå§‹åŒ–å¢å¼ºç‰ˆç´¯ç§¯æµ‹è¯•ç®¡ç†å™¨ï¼ˆå®æ—¶é”™è¯¯åˆ†ç±»ï¼‰
            self.manager = EnhancedCumulativeManager(use_ai_classification=self.use_ai_classification)
            
            # åˆ›å»ºå­˜å‚¨é€‚é…å™¨
            self.storage_adapter = create_storage_adapter(self.manager)
            
            # åˆå§‹åŒ– WorkflowQualityTesterï¼ˆç”¨äºç»§æ‰¿promptåˆ›å»ºæ–¹æ³•å’Œè¯„åˆ†ï¼‰
            # æ³¨æ„ï¼šä¼ é€’generatorä½œä¸ºç¬¬ä¸€ä¸ªå‚æ•°
            self.quality_tester = WorkflowQualityTester(
                generator=self.generator,  # å¿…éœ€çš„ç¬¬ä¸€ä¸ªå‚æ•°
                model='gpt-4o-mini',
                use_phase2_scoring=True,  # å¯ç”¨Phase2è¯„åˆ†ä»¥åˆå§‹åŒ–stable_scorer
                save_logs=self.save_logs  # ä½¿ç”¨ä¼ å…¥çš„save_logså‚æ•°
            )
            
            # åŠ è½½ä»»åŠ¡åº“ï¼ˆé»˜è®¤easyï¼Œåç»­å¯ä»¥æ ¹æ®éœ€è¦é‡æ–°åŠ è½½ï¼‰
            self._load_task_library(difficulty="easy")
            self._current_difficulty = "easy"
            
            self._initialized = True
            self.logger.info("Initialization complete")
    
    def _smart_checkpoint_save(self, results, task_model=None, force=False):
        """æ™ºèƒ½checkpointä¿å­˜ - æ”¯æŒå¤šé‡è§¦å‘æ¡ä»¶"""
        if not self.checkpoint_interval or self.enable_database_updates:
            return
        
        # å°†ç»“æœæ·»åŠ åˆ°pendingç¼“å­˜
        if results:
            if isinstance(results, list):
                self.pending_results.extend(results)
            else:
                self.pending_results.append(results)
        
        # å¤šé‡è§¦å‘æ¡ä»¶æ£€æŸ¥
        current_time = time.time()
        time_since_last_save = current_time - getattr(self, '_last_checkpoint_time', current_time)
        result_count = len(self.pending_results)
        
        # è‡ªé€‚åº”é˜ˆå€¼
        effective_threshold = self.checkpoint_interval
        if hasattr(self, '_adaptive_checkpoint') and self._adaptive_checkpoint:
            if result_count > 0 and time_since_last_save > 300:  # 5åˆ†é’Ÿå¼ºåˆ¶ä¿å­˜
                effective_threshold = 1
            elif time_since_last_save > 180:  # 3åˆ†é’Ÿé™ä½é˜ˆå€¼
                effective_threshold = max(1, self.checkpoint_interval // 2)
        
        # è§¦å‘æ¡ä»¶
        should_save = (force or 
                      result_count >= effective_threshold or
                      (result_count > 0 and time_since_last_save > 600) or  # 10åˆ†é’Ÿå¼ºåˆ¶ä¿å­˜
                      (result_count >= 3 and time_since_last_save > 120))   # 2åˆ†é’Ÿéƒ¨åˆ†ä¿å­˜
        
        if should_save and self.pending_results:
            print(f"ğŸ’¾ æ™ºèƒ½Checkpoint: ä¿å­˜{len(self.pending_results)}ä¸ªç»“æœ...")
            print(f"   è§¦å‘åŸå› : æ•°é‡={result_count}, æ—¶é—´={time_since_last_save:.1f}s, å¼ºåˆ¶={force}")
            
            # ç¡®ä¿å·²åˆå§‹åŒ–manager
            self._lazy_init()
            
            # ä¿å­˜é€»è¾‘ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
            try:
                from cumulative_test_manager import TestRecord
                saved_count = 0
                
                for result in self.pending_results:
                    if result and not result.get('_saved', False):
                        record = TestRecord(
                            model=result.get('model', task_model or 'unknown'),
                            task_type=result.get('task_type', 'unknown'),
                            prompt_type=result.get('prompt_type', 'baseline'),
                            difficulty=result.get('difficulty', 'easy')
                        )
                        
                        # è®¾ç½®å…¶ä»–å­—æ®µï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
                        for field in ['timestamp', 'success', 'success_level', 'execution_time', 'turns',
                                    'tool_calls', 'workflow_score', 'phase2_score', 'quality_score',
                                    'final_score', 'error_type', 'tool_success_rate', 'is_flawed',
                                    'flaw_type', 'format_error_count', 'api_issues', 'executed_tools',
                                    'required_tools', 'tool_coverage_rate', 'task_instance', 'execution_history',
                                    'ai_error_category', '_ai_error_category']:
                            if field in result:
                                if field == '_ai_error_category':
                                    setattr(record, 'ai_error_category', result[field])
                                else:
                                    setattr(record, field, result[field])
                        
                        # ä¿å­˜è®°å½•ï¼ˆä½¿ç”¨ç»Ÿä¸€çš„å­˜å‚¨é€‚é…å™¨ï¼‰
                        try:
                            if self.storage_adapter and self.storage_adapter.write_result(record):
                                result['_saved'] = True
                                saved_count += 1
                        except Exception as e:
                            print(f"ä¿å­˜è®°å½•å¤±è´¥: {e}")
                
                print(f"âœ… Checkpointå®Œæˆ: æˆåŠŸä¿å­˜ {saved_count}/{len(self.pending_results)} ä¸ªç»“æœ")
                
                # æ¸…ç©ºå·²ä¿å­˜çš„ç»“æœ
                self.pending_results = [r for r in self.pending_results if not r.get('_saved', False)]
                self._last_checkpoint_time = current_time
                
            except Exception as e:
                print(f"âŒ Checkpointå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()

    def _load_task_library(self, difficulty="easy", num_instances=20):
        """åŠ è½½ä»»åŠ¡åº“
        
        Args:
            difficulty: éš¾åº¦çº§åˆ« (very_easy, easy, medium, hard, very_hard)
            num_instances: æ¯ä¸ªä»»åŠ¡ç±»å‹åŠ è½½çš„å®ä¾‹æ•°é‡ï¼ˆç”¨äºéƒ¨åˆ†åŠ è½½ï¼‰
        """
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨éƒ¨åˆ†åŠ è½½ï¼ˆé€šè¿‡ç¯å¢ƒå˜é‡æˆ–å±æ€§ï¼‰
        use_partial_loading = os.environ.get('USE_PARTIAL_LOADING', 'false').lower() == 'true'
        if hasattr(self, 'use_partial_loading'):
            use_partial_loading = self.use_partial_loading
        
        # ä»ç¯å¢ƒå˜é‡è·å–åŠ è½½æ•°é‡
        if os.environ.get('TASK_LOAD_COUNT'):
            try:
                num_instances = int(os.environ.get('TASK_LOAD_COUNT'))
            except ValueError:
                pass
        
        # é¦–å…ˆå°è¯•åŠ è½½å¸¦workflowçš„ç‰ˆæœ¬
        workflow_enhanced_path = Path(f"mcp_generated_library/difficulty_versions/task_library_enhanced_v3_{difficulty}_with_workflows.json")
        
        if workflow_enhanced_path.exists():
            # ä½¿ç”¨é¢„ç”Ÿæˆworkflowçš„ç‰ˆæœ¬
            task_lib_path = workflow_enhanced_path
            self.logger.info(f"Loading task library with pre-generated workflows: {task_lib_path.name}")
        else:
            # æ²¡æœ‰workflowç‰ˆæœ¬ï¼Œä½¿ç”¨æ™®é€šç‰ˆæœ¬
            # æ ¹æ®éš¾åº¦é€‰æ‹©å¯¹åº”çš„ä»»åŠ¡åº“æ–‡ä»¶
            task_lib_path = Path(f"mcp_generated_library/difficulty_versions/task_library_enhanced_v3_{difficulty}.json")
            if not task_lib_path.exists():
                # å¦‚æœæ²¡æœ‰å¯¹åº”éš¾åº¦çš„æ–‡ä»¶ï¼Œå°è¯•é»˜è®¤çš„easyæ–‡ä»¶
                self.logger.warning(f"Task library for difficulty '{difficulty}' not found, trying 'easy'")
                task_lib_path = Path("mcp_generated_library/difficulty_versions/task_library_enhanced_v3_easy.json")
                if not task_lib_path.exists():
                    # æœ€åå°è¯•é»˜è®¤æ–‡ä»¶
                    task_lib_path = Path("mcp_generated_library/task_library_enhanced_v3.json")
            self.logger.info(f"Loading standard task library (workflows will be generated): {task_lib_path.name}")
        
        if task_lib_path.exists():
            if use_partial_loading:
                # ä½¿ç”¨éƒ¨åˆ†åŠ è½½ç­–ç•¥
                self._load_task_library_partial(task_lib_path, difficulty, num_instances)
            else:
                # ä½¿ç”¨ä¼ ç»Ÿå…¨é‡åŠ è½½
                with open(task_lib_path, 'r') as f:
                    data = json.load(f)
                
                # å¤„ç†ä»»åŠ¡æ•°æ®
                if isinstance(data, dict):
                    tasks = data.get('tasks', [])
                else:
                    tasks = data
                
                # æŒ‰ç±»å‹ç»„ç»‡ä»»åŠ¡
                self.tasks_by_type = {}
                for task in tasks:
                    if isinstance(task, dict) and 'task_type' in task:
                        task_type = task['task_type']
                        if task_type not in self.tasks_by_type:
                            self.tasks_by_type[task_type] = []
                        self.tasks_by_type[task_type].append(task)
                
                self.logger.info(f"Loaded {len(tasks)} tasks from {task_lib_path.name}")
        else:
            self.logger.warning("Task library not found")
            self.tasks_by_type = {}
    
    def _load_task_library_partial(self, task_lib_path: Path, difficulty: str, num_instances: int):
        """éƒ¨åˆ†åŠ è½½ä»»åŠ¡åº“ - åªåŠ è½½éœ€è¦çš„ä»»åŠ¡æ•°é‡
        
        Args:
            task_lib_path: ä»»åŠ¡åº“æ–‡ä»¶è·¯å¾„
            difficulty: éš¾åº¦çº§åˆ«
            num_instances: æ¯ä¸ªä»»åŠ¡ç±»å‹åŠ è½½çš„å®ä¾‹æ•°é‡
        """
        self.logger.info(f"Using partial loading: {num_instances} tasks per type")
        
        # ç¬¬ä¸€é˜¶æ®µï¼šæ„å»ºè½»é‡çº§ç´¢å¼•
        self.logger.debug("Phase 1: Building task index...")
        task_index = {}
        
        with open(task_lib_path, 'r') as f:
            data = json.load(f)
        
        # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
        if isinstance(data, dict):
            all_tasks = data.get('tasks', [])
        else:
            all_tasks = data
        
        # æ„å»ºç´¢å¼•
        for i, task in enumerate(all_tasks):
            if isinstance(task, dict) and 'task_type' in task:
                task_type = task['task_type']
                if task_type not in task_index:
                    task_index[task_type] = []
                task_index[task_type].append(i)
        
        self.logger.debug(f"Found {sum(len(indices) for indices in task_index.values())} total tasks")
        
        # ç¬¬äºŒé˜¶æ®µï¼šéšæœºé€‰æ‹©å¹¶åŠ è½½å…·ä½“ä»»åŠ¡
        self.logger.debug(f"Phase 2: Loading {num_instances} tasks per type...")
        self.tasks_by_type = {}
        
        for task_type, indices in task_index.items():
            # éšæœºé€‰æ‹©num_instancesä¸ªç´¢å¼•
            num_to_select = min(num_instances, len(indices))
            selected_indices = random.sample(indices, num_to_select)
            
            # åªåŠ è½½é€‰ä¸­çš„ä»»åŠ¡
            self.tasks_by_type[task_type] = [all_tasks[i] for i in selected_indices]
            self.logger.debug(f"Selected {len(self.tasks_by_type[task_type])} tasks for {task_type}")
        
        # ç»Ÿè®¡ä¿¡æ¯
        total_loaded = sum(len(tasks) for tasks in self.tasks_by_type.values())
        self.logger.info(f"Partially loaded {total_loaded} tasks (vs {len(all_tasks)} total)")
        
        # ä¼°ç®—å†…å­˜èŠ‚çœ
        memory_saved_percent = (1 - total_loaded / len(all_tasks)) * 100
        self.logger.info(f"Estimated memory saving: {memory_saved_percent:.1f}%")
    
    def run_single_test(self, model: str, task_type: str, prompt_type: str,
                       deployment: Optional[str] = None, is_flawed: bool = False, 
                       flaw_type: Optional[str] = None, timeout: int = 30, 
                       tool_success_rate: float = 0.8, difficulty: str = "easy") -> Dict:
        """è¿è¡Œå•ä¸ªæµ‹è¯•å¹¶è¿”å›ç»“æœ
        
        Args:
            difficulty: ä»»åŠ¡éš¾åº¦çº§åˆ« (very_easy, easy, medium, hard, very_hard)
        """
        self._lazy_init()
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åŠ è½½ä¸åŒéš¾åº¦çš„ä»»åŠ¡åº“
        if not hasattr(self, '_current_difficulty') or self._current_difficulty != difficulty:
            self.logger.info(f"Loading task library for difficulty: {difficulty}")
            self._load_task_library(difficulty=difficulty)
            self._current_difficulty = difficulty
        
        self.logger.debug(f"Starting single test: model={model}, task_type={task_type}, "
                         f"prompt_type={prompt_type}, is_flawed={is_flawed}, flaw_type={flaw_type}")
        
        try:
            # è·å–ä»»åŠ¡
            tasks = self.tasks_by_type.get(task_type, [])
            self.logger.debug(f"Found {len(tasks)} tasks for type {task_type}")
            if not tasks:
                error_msg = f'No tasks found for {task_type}'
                self.logger.error(error_msg)
                # åˆ›å»ºåŸºæœ¬çš„log_dataç”¨äºAIåˆ†ç±»
                basic_log_data = {
                    'test_id': f"{model.replace('-', '_').replace('.', '_')}_{task_type}_notask_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                    'task_type': task_type,
                    'prompt_type': prompt_type,
                    'timestamp': datetime.now().isoformat(),
                    'task_instance': {'required_tools': []},
                    'prompt': '',
                    'llm_response': 'No response - no tasks found',
                    'extracted_tool_calls': [],
                    'conversation_history': [],
                    'execution_history': [],
                    'is_flawed': is_flawed,
                    'flaw_type': flaw_type,
                    'result': {'success': False, 'error': error_msg}
                }
                return {
                    'success': False,
                    'error': error_msg,
                    'success_level': 'failure',
                    'tool_calls': [],
                    'turns': 0,
                    'workflow_score': 0.0,
                    'phase2_score': 0.0,
                    'quality_score': 0.0,
                    'final_score': 0.0,
                    '_log_data': basic_log_data
                }
            
            task = random.choice(tasks)
            # å…¼å®¹æ€§å¤„ç†ï¼šæ”¯æŒinstance_idå’Œidä¸¤ç§å­—æ®µ
            task_id = task.get('instance_id') or task.get('id', 'unknown')
            self.logger.debug(f"Selected task: {task_id}")
            
            # è·å–æˆ–ç”Ÿæˆå·¥ä½œæµ
            # é¦–å…ˆæ£€æŸ¥ä»»åŠ¡æ˜¯å¦å·²æœ‰é¢„ç”Ÿæˆçš„workflow
            if 'workflow' in task and task['workflow'] is not None:
                # ä½¿ç”¨é¢„ç”Ÿæˆçš„workflow
                self.logger.debug(f"Using pre-generated workflow for task")
                workflow = task['workflow']
                
                # å¦‚æœæ˜¯flawedæµ‹è¯•ï¼Œåœ¨é¢„ç”Ÿæˆçš„workflowä¸Šæ³¨å…¥ç¼ºé™·
                if is_flawed and flaw_type:
                    self.logger.debug(f"Injecting flaw type: {flaw_type} on pre-generated workflow")
                    # å¤åˆ¶workflowä»¥é¿å…ä¿®æ”¹åŸå§‹æ•°æ®
                    import copy
                    normal_workflow = copy.deepcopy(workflow)
                    workflow = self.flawed_generator.inject_specific_flaw(normal_workflow, flaw_type)
            else:
                # æ²¡æœ‰é¢„ç”Ÿæˆçš„workflowï¼Œéœ€è¦åŠ¨æ€ç”Ÿæˆ
                self.logger.debug(f"No pre-generated workflow found, generating dynamically")
                
                if is_flawed and flaw_type:
                    # å…ˆç”Ÿæˆæ­£å¸¸å·¥ä½œæµ
                    # å…¼å®¹æ€§å¤„ç†ï¼šæ”¯æŒinstance_idå’Œidä¸¤ç§å­—æ®µ
                    task_id = task.get('instance_id') or task.get('id', 'unknown')
                    task_dict = {
                        'id': task_id,
                        'description': task.get('description', ''),
                        'task_type': task_type,
                        'required_tools': task.get('required_tools', []),
                        'expected_outputs': task.get('expected_outputs', {})
                    }
                    self.logger.debug(f"Generating normal workflow for flawed test")
                    normal_workflow = self.generator.generate_workflow(
                        task_type,
                        task_instance=task_dict
                    )
                    if not normal_workflow:
                        # åˆ›å»ºåŸºæœ¬çš„log_dataç”¨äºAIåˆ†ç±»
                        basic_log_data = {
                            'test_id': f"{model.replace('-', '_').replace('.', '_')}_{task_type}_noworkflow_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                            'task_type': task_type,
                            'prompt_type': f'flawed_{flaw_type}',
                            'timestamp': datetime.now().isoformat(),
                            'task_instance': task_dict,
                            'prompt': '',
                            'llm_response': 'No response - workflow generation failed',
                            'extracted_tool_calls': [],
                            'conversation_history': [],
                            'execution_history': [],
                            'is_flawed': True,
                            'flaw_type': flaw_type,
                            'result': {'success': False, 'error': 'Failed to generate base workflow'}
                        }
                        return {
                            'success': False,
                            'error': 'Failed to generate base workflow',
                            'success_level': 'failure',
                            'tool_calls': [],
                            'turns': 0,
                            'workflow_score': 0.0,
                            'phase2_score': 0.0,
                            'quality_score': 0.0,
                            'final_score': 0.0,
                            '_log_data': basic_log_data
                        }
                    
                    # æ³¨å…¥ç¼ºé™·
                    self.logger.debug(f"Injecting flaw type: {flaw_type}")
                    workflow = self.flawed_generator.inject_specific_flaw(normal_workflow, flaw_type)
                else:
                    # å…¼å®¹æ€§å¤„ç†ï¼šæ”¯æŒinstance_idå’Œidä¸¤ç§å­—æ®µ
                    task_id = task.get('instance_id') or task.get('id', 'unknown')
                    task_dict = {
                        'id': task_id,
                        'description': task.get('description', ''),
                        'task_type': task_type,
                        'required_tools': task.get('required_tools', []),
                        'expected_outputs': task.get('expected_outputs', {})
                    }
                    self.logger.debug(f"Generating workflow")
                    workflow = self.generator.generate_workflow(
                        task_type,
                        task_instance=task_dict
                    )
            
            if not workflow:
                # åˆ›å»ºåŸºæœ¬çš„log_dataç”¨äºAIåˆ†ç±»
                task_id = task.get('instance_id') or task.get('id', 'unknown')
                basic_log_data = {
                    'test_id': f"{model.replace('-', '_').replace('.', '_')}_{task_type}_noworkflow_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                    'task_type': task_type,
                    'prompt_type': prompt_type,
                    'timestamp': datetime.now().isoformat(),
                    'task_instance': {
                        'id': task_id,
                        'description': task.get('description', ''),
                        'task_type': task_type,
                        'required_tools': task.get('required_tools', []),
                        'expected_outputs': task.get('expected_outputs', {})
                    },
                    'prompt': '',
                    'llm_response': 'No response - workflow generation failed',
                    'extracted_tool_calls': [],
                    'conversation_history': [],
                    'execution_history': [],
                    'is_flawed': is_flawed,
                    'flaw_type': flaw_type,
                    'result': {'success': False, 'error': 'Failed to generate workflow'}
                }
                return {
                    'success': False,
                    'error': 'Failed to generate workflow',
                    'success_level': 'failure',
                    'tool_calls': [],
                    'turns': 0,
                    'workflow_score': 0.0,
                    'phase2_score': 0.0,
                    'quality_score': 0.0,
                    'final_score': 0.0,
                    '_log_data': basic_log_data
                }
            
            # åˆ›å»ºæ‰§è¡Œå™¨ï¼ˆä¸workflow_quality_test_flawedä¿æŒä¸€è‡´ï¼‰
            # ä¼ é€’prompt_typeä»¥ä¾¿IdealLab APIé€‰æ‹©æ­£ç¡®çš„key
            actual_prompt_type = prompt_type
            if is_flawed and flaw_type:
                actual_prompt_type = f"flawed_{flaw_type}"
            
            # ä½¿ç”¨deploymentè¿›è¡ŒAPIè°ƒç”¨ï¼Œå¦‚æœæ²¡æœ‰æŒ‡å®šåˆ™ä½¿ç”¨model
            # ç‰¹æ®Šå¤„ç†qwen-keyè™šæ‹Ÿå®ä¾‹
            if deployment and deployment.startswith("qwen-key"):
                # qwen-key0/1 æ˜¯è™šæ‹Ÿå®ä¾‹ï¼Œéœ€è¦ä½¿ç”¨å®é™…çš„æ¨¡å‹å
                # åŒæ—¶æå–keyç´¢å¼•ç”¨äºAPI keyé€‰æ‹©
                api_model = model  # ä½¿ç”¨å®é™…çš„æ¨¡å‹å
                # keyç´¢å¼•å·²ç»é€šè¿‡self.idealab_key_indexä¼ é€’
                self.logger.debug(f"Using qwen virtual instance {deployment}, actual model: {api_model}")
            else:
                api_model = deployment if deployment else model
            
            executor = InteractiveExecutor(
                tool_registry=self.generator.full_tool_registry,
                llm_client=None,  # è®©InteractiveExecutorè‡ªåŠ¨è·å–
                max_turns=10,     # ä¸workflow_quality_test_flawedä¸€è‡´
                success_rate=tool_success_rate, # å¯é…ç½®çš„å·¥å…·æˆåŠŸç‡
                model=api_model,  # ä½¿ç”¨deploymentåç§°è¿›è¡ŒAPIè°ƒç”¨
                idealab_key_index=self.idealab_key_index,  # ä¼ é€’API keyç´¢å¼•
                prompt_type=actual_prompt_type,  # ä¼ é€’prompt_typeç”¨äºAPI keyé€‰æ‹©
                silent=self.silent  # ä¼ é€’é™é»˜æ¨¡å¼æ ‡å¿—
            )
            
            # æ‰§è¡Œæµ‹è¯•
            self.logger.debug(f"Executing workflow with {len(workflow.get('optimal_sequence', []))} steps")
            start_time = time.time()
            
            # å‡†å¤‡task_instanceï¼ˆå…¼å®¹æ€§å¤„ç†ï¼‰
            task_id = task.get('instance_id') or task.get('id', 'unknown')
            task_instance = {
                'id': task_id,
                'description': task.get('description', ''),
                'task_type': task_type,
                'required_tools': task.get('required_tools', []),
                'expected_outputs': task.get('expected_outputs', {})
            }
            
            # æ ¹æ®ä¸åŒçš„promptç±»å‹åˆ›å»ºç›¸åº”çš„æç¤º
            if is_flawed and flaw_type:
                # ç¼ºé™·æµ‹è¯•ï¼šä½¿ç”¨ç»§æ‰¿çš„_create_flawed_promptæ–¹æ³•
                initial_prompt = self.quality_tester._create_flawed_prompt(
                    task_type=task_type,
                    workflow=workflow,
                    flaw_type=flaw_type,
                    fixed_task_instance=task_instance
                )
                execution_prompt_type = 'flawed'
            elif prompt_type == 'baseline':
                # åŸºç¡€æç¤ºï¼šä½¿ç”¨ç»§æ‰¿çš„_create_baseline_promptæ–¹æ³•
                initial_prompt = self.quality_tester._create_baseline_prompt(
                    task_type=task_type,
                    fixed_task_instance=task_instance
                )
                execution_prompt_type = 'baseline'
            elif prompt_type == 'optimal':
                # æœ€ä¼˜æç¤ºï¼šä½¿ç”¨ç»§æ‰¿çš„_create_optimal_promptæ–¹æ³•
                initial_prompt = self.quality_tester._create_optimal_prompt(
                    task_type=task_type,
                    workflow=workflow,
                    fixed_task_instance=task_instance
                )
                execution_prompt_type = 'optimal'
            elif prompt_type == 'cot':
                # æ€ç»´é“¾æç¤ºï¼šä½¿ç”¨ç»§æ‰¿çš„_create_cot_promptæ–¹æ³•
                initial_prompt = self.quality_tester._create_cot_prompt(
                    task_type=task_type,
                    workflow=workflow,
                    fixed_task_instance=task_instance
                )
                execution_prompt_type = 'cot'
            else:
                # é»˜è®¤ä½¿ç”¨ä»»åŠ¡æè¿°
                initial_prompt = task.get('description', '')
                execution_prompt_type = prompt_type
            
            # å‡†å¤‡æ—¥å¿—æ•°æ®ï¼ˆä¸workflow_quality_test_flawedä¸€è‡´ï¼‰
            # åœ¨test_idä¸­åŒ…å«æ¨¡å‹åï¼Œæ”¾åœ¨æœ€å‰é¢ä»¥ä¾¿è¯†åˆ«
            model_safe = model.replace('-', '_').replace('.', '_')
            log_data = {
                'test_id': f"{model_safe}_{task_type}_inst{task_id}_test{random.randint(0, 999)}_{prompt_type}",
                'task_type': task_type,
                'prompt_type': prompt_type,
                'timestamp': datetime.now().isoformat(),
                'task_instance': task_instance,
                'prompt': initial_prompt,
                'is_flawed': is_flawed,
                'flaw_type': flaw_type
            }
            
            # QPSæ§åˆ¶å·²ç»ç§»åˆ°interactive_executor._get_llm_responseä¸­
            # åœ¨æ¯æ¬¡å®é™…APIè°ƒç”¨å‰è¿›è¡Œé™æµï¼Œè€Œä¸æ˜¯åœ¨ä»»åŠ¡å¼€å§‹æ—¶
            
            result = executor.execute_interactive(
                initial_prompt=initial_prompt,
                task_instance=task_instance,
                workflow=workflow,
                prompt_type=execution_prompt_type
            )
            execution_time = time.time() - start_time
            
            # ä¿å­˜å®Œæ•´çš„äº¤äº’å†å²åˆ°æ—¥å¿—
            log_data['conversation_history'] = result.get('conversation_history', [])
            log_data['execution_history'] = [
                {
                    'tool': getattr(h, 'tool_name', None) if hasattr(h, 'tool_name') else h.get('tool', ''),
                    'success': getattr(h, 'success', False) if hasattr(h, 'success') else h.get('success', False),
                    'output': str(getattr(h, 'output', '')) if hasattr(h, 'output') else h.get('output'),
                    'error': getattr(h, 'error', None) if hasattr(h, 'error') else h.get('error'),
                    'execution_time': getattr(h, 'execution_time', 0) if hasattr(h, 'execution_time') else h.get('execution_time', 0)
                }
                for h in result.get('execution_history', [])
            ]
            log_data['llm_response'] = json.dumps({
                'conversation': result.get('conversation_history', []),
                'final_state': {
                    'task_completed': result.get('success', False),
                    'tools_executed': result.get('tool_calls', []),
                    'total_turns': result.get('turns', 0)
                }
            }, indent=2)
            log_data['extracted_tool_calls'] = result.get('tool_calls', [])
            # æ·»åŠ APIé—®é¢˜ä¿¡æ¯åˆ°æ—¥å¿—
            log_data['api_issues'] = result.get('api_issues', [])
            
            # æ·»åŠ required_toolså’Œäº¤äº’å†å²åˆ°ç»“æœä¸­
            result['required_tools'] = task.get('required_tools', [])
            result['conversation_history'] = result.get('conversation_history', [])
            # ä½¿ç”¨æ¸…ç†è¿‡çš„execution_historyï¼Œç¡®ä¿å¯åºåˆ—åŒ–
            result['execution_history'] = log_data['execution_history']
            self.logger.debug(f"Execution completed in {execution_time:.2f}s")
            
            # è®¡ç®—workflow adherenceï¼ˆç»§æ‰¿workflow_quality_test_flawedçš„é€»è¾‘ï¼‰
            tool_calls = result.get('tool_calls', [])
            expected_sequence = workflow.get('optimal_sequence', [])
            expected_tools = expected_sequence  # optimal_sequenceç›´æ¥å°±æ˜¯å·¥å…·åˆ—è¡¨
            execution_history = result.get('execution_history', [])
            
            # è°ƒç”¨ç»§æ‰¿çš„æ–¹æ³•è®¡ç®—adherence
            if hasattr(self.quality_tester, '_calculate_workflow_adherence'):
                adherence_scores = self.quality_tester._calculate_workflow_adherence(
                    expected_tools, tool_calls, execution_history
                )
            else:
                # å¤‡ç”¨ç®€å•è®¡ç®—
                adherence_scores = {'overall_adherence': 0.0}
                if expected_tools and tool_calls:
                    expected_set = set(expected_tools)
                    actual_set = set(tool_calls)
                    coverage = len(expected_set & actual_set) / len(expected_set) if expected_set else 0
                    adherence_scores['overall_adherence'] = coverage
            
            # å°†adherence_scoresæ·»åŠ åˆ°resultä¸­
            result['adherence_scores'] = adherence_scores
            
            # åˆ¤æ–­æˆåŠŸ
            execution_status = result.get('execution_status', 'failure')
            success = execution_status in ['full_success', 'partial_success']
            
            # è·å–é”™è¯¯ä¿¡æ¯ï¼ˆä¼˜å…ˆä½¿ç”¨InteractiveExecutorç”Ÿæˆçš„æ™ºèƒ½é”™è¯¯æ¶ˆæ¯ï¼‰
            error_message = None
            if not success:
                # ä½¿ç”¨æ™ºèƒ½é”™è¯¯æ¶ˆæ¯ï¼ˆå¦‚æœæœ‰ï¼‰
                error_message = result.get('error_message', None)
                if not error_message:
                    # é€€å›åˆ°æ—§é€»è¾‘
                    if result.get('turns', 0) >= 10:
                        error_message = f"Max turns reached ({result.get('turns', 0)})"
                    else:
                        error_message = result.get('error', 'Unknown error')
            
            # è®¡ç®—score (ä½¿ç”¨ç»§æ‰¿çš„è¯„åˆ†å™¨)
            workflow_score = 0.0
            phase2_score = 0.0
            quality_score = 0.0
            final_score = 0.0
            
            # ä½¿ç”¨å·²è®¡ç®—çš„adherence_scores
            workflow_score = adherence_scores.get('overall_adherence', 0.0) if adherence_scores else 0.0
            
            # ä½¿ç”¨stable_scorerè®¡ç®—phase2å’Œqualityåˆ†æ•°
            if hasattr(self.quality_tester, 'stable_scorer') and self.quality_tester.stable_scorer:
                try:
                    # å‡†å¤‡è¯„åˆ†æ‰€éœ€çš„æ•°æ®
                    execution_data = {
                        'tool_calls': result.get('tool_calls', []),
                        'execution_time': result.get('execution_time', 0.0),
                        'success_level': execution_status,
                        'output_generated': len(result.get('tool_calls', [])) > 0,
                        'turns': result.get('turns', 0)
                    }
                    
                    evaluation_context = {
                        'task': task,
                        'workflow': workflow,
                        'required_tools': workflow.get('required_tools', []),
                        'expected_time': 10.0,
                        'adherence_scores': adherence_scores
                    }
                    
                    # è°ƒç”¨calculate_stable_scoreæ–¹æ³•
                    phase2_score, score_breakdown = self.quality_tester.stable_scorer.calculate_stable_score(
                        execution_data, evaluation_context
                    )
                    quality_score = score_breakdown.get('execution_quality', 0.0)
                    
                    # ç¡®ä¿è¿”å›çš„scoreä¸æ˜¯None
                    phase2_score = phase2_score if phase2_score is not None else 0.0
                    quality_score = quality_score if quality_score is not None else 0.0
                    
                except Exception as e:
                    self.logger.warning(f"Error in calculate_stable_score: {e}")
                    phase2_score = 0.0
                    quality_score = 0.0
                    # ç¡®ä¿workflow_scoreä¹Ÿä¸ä¸ºNone
                    workflow_score = workflow_score if workflow_score is not None else 0.0
            else:
                # stable_scorerä¸å¯ç”¨ï¼Œç›´æ¥æŠ¥é”™
                raise RuntimeError(
                    "stable_scorer is not available but required for quality scoring. "
                    "Check WorkflowQualityTester initialization and ensure use_phase2_scoring=True"
                )
            
            # æ ¹æ®use_phase2_scoringå†³å®šfinal_score
            use_phase2_scoring = getattr(self.quality_tester, 'use_phase2_scoring', True)
            final_score = phase2_score if use_phase2_scoring else workflow_score
            
            # è®¡ç®—tool_coverage_rate
            required_tools = result.get('required_tools', [])
            executed_tools = result.get('executed_tools', result.get('tool_calls', []))
            tool_coverage_rate = 0.0
            if required_tools:
                required_set = set(required_tools)
                executed_set = set(executed_tools) if executed_tools else set()
                covered_tools = required_set.intersection(executed_set)
                tool_coverage_rate = len(covered_tools) / len(required_tools)
            
            # æ­£ç¡®åˆ¤å®šsuccess_levelåŸºäºåˆ†æ•°ï¼ˆå¤„ç†Noneå€¼ï¼‰
            workflow_score = workflow_score or 0.0
            phase2_score = phase2_score or 0.0
            if workflow_score >= 0.8 and phase2_score >= 0.8:
                success_level = 'full_success'
                success = True
            elif workflow_score >= 0.5 or phase2_score >= 0.5:
                success_level = 'partial_success'
                success = True
            else:
                success_level = 'failure'
                success = False
            
            # è°ƒè¯•è¾“å‡º
            if self.debug:
                print(f"[DEBUG] Scores - workflow: {workflow_score:.3f}, phase2: {phase2_score:.3f}, quality: {quality_score:.3f}, final: {final_score:.3f}")
                print(f"[DEBUG] Tool coverage: {tool_coverage_rate:.2%} (required: {required_tools}, executed: {executed_tools})")
                print(f"[DEBUG] Success level: {success_level} (workflow>=0.8: {workflow_score>=0.8}, phase2>=0.8: {phase2_score>=0.8})")
            
            # AIé”™è¯¯åˆ†ç±»ï¼ˆå¦‚æœå¯ç”¨ä¸”æµ‹è¯•å¤±è´¥ï¼‰
            ai_error_category = None
            ai_error_reason = None
            ai_confidence = 0.0
            
            # æ³¨æ„ï¼šAIé”™è¯¯åˆ†ç±»ç°åœ¨åœ¨æ‰¹å¤„ç†è¿‡ç¨‹ä¸­åŸºäºå®Œæ•´çš„äº¤äº’æ—¥å¿—æ•°æ®è¿›è¡Œ
            # è¿™æ ·å¯ä»¥è·å¾—æ›´å¤šçš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒåŒ…æ‹¬å®Œæ•´çš„å¯¹è¯å†å²å’Œæ‰§è¡Œè¯¦æƒ…
            
            # è¿”å›ç»“æœ - åŒ…å«æ‰€æœ‰å¿…è¦çš„ç»Ÿè®¡ä¿¡æ¯å’Œåˆ†æ•°
            return_result = {
                'success': success,
                'success_level': success_level,  # ä½¿ç”¨åŸºäºåˆ†æ•°çš„åˆ¤å®š
                'execution_time': execution_time,
                'error': error_message,
                'tool_calls': result.get('tool_calls', []),
                'turns': result.get('turns', 0),
                # åˆ†æ•°æŒ‡æ ‡
                'workflow_score': workflow_score,
                'phase2_score': phase2_score,
                'quality_score': quality_score,
                'final_score': final_score,
                # æ·»åŠ æ›´å¤šä¿¡æ¯ä»¥ä¾¿ç»Ÿè®¡
                'task_type': task_type,
                'prompt_type': prompt_type,
                'difficulty': difficulty,  # æ·»åŠ difficultyå­—æ®µ
                'is_flawed': is_flawed,
                'flaw_type': flaw_type,
                'tool_success_rate': tool_success_rate,
                # æ·»åŠ assistedç»Ÿè®¡ç›¸å…³å­—æ®µ
                'format_error_count': result.get('format_error_count', 0),
                'format_issues': result.get('format_issues', []),
                'api_issues': result.get('api_issues', []),
                # æ·»åŠ å®Œæ•´çš„äº¤äº’å†å²
                'conversation_history': result.get('conversation_history', []),
                'execution_history': result.get('execution_history', []),
                # æ·»åŠ required_toolså’Œexecuted_toolsç”¨äºtool_coverageè®¡ç®—
                'required_tools': required_tools,
                'executed_tools': executed_tools,
                'tool_coverage_rate': tool_coverage_rate,  # æ·»åŠ è®¡ç®—å¥½çš„tool_coverage_rate
                # æ·»åŠ task_instance
                'task_instance': task_instance,
                # æ·»åŠ log_dataä»¥ä¾¿ä¿å­˜
                '_log_data': log_data,
                # AIåˆ†ç±»ç»“æœ
                'ai_error_category': ai_error_category,
                'ai_error_reason': ai_error_reason,
                'ai_confidence': ai_confidence
            }
            
            # ä¿å­˜æ—¥å¿—æ–‡ä»¶ï¼ˆå¦‚æœå¯ç”¨ï¼‰- æ·»åŠ æ¨¡å‹ååˆ°æ–‡ä»¶å
            if self.save_logs:
                # åˆ›å»ºä¸€ä¸ªåŒ…å«æ¨¡å‹ä¿¡æ¯çš„TestTaskå¯¹è±¡ï¼Œç”¨äº_save_interaction_log
                task_obj = TestTask(
                    model=model,
                    task_type=task_type,
                    prompt_type=prompt_type,
                    difficulty=difficulty,
                    tool_success_rate=tool_success_rate,
                    is_flawed=is_flawed,
                    flaw_type=flaw_type
                )
                txt_file_path = self._save_interaction_log(task_obj, return_result, log_data)
                if self.debug and txt_file_path:
                    print(f"[DEBUG] Saved interaction log to {txt_file_path.name}")
            
            return return_result
            
        except Exception as e:
            self.logger.error(f"Test failed with exception: {str(e)}")
            if self.debug:
                self.logger.error(traceback.format_exc())
            
            # åˆ›å»ºé”™è¯¯ç»“æœ
            error_result = {
                'success': False,
                'error': str(e),
                'execution_time': 0,
                'format_error_count': 0,
                'format_issues': [],
                'api_issues': [],
                'execution_history': [],
                'conversation_history': []
            }
            
            # å³ä½¿å‡ºç°å¼‚å¸¸ä¹Ÿä¿å­˜æ—¥å¿—æ–‡ä»¶ï¼Œä¾¿äºè°ƒè¯•
            if self.save_logs:
                try:
                    # åˆ›å»ºåŒ…å«é”™è¯¯ä¿¡æ¯çš„log_data
                    error_log_data = {
                        'test_id': f"{model}_{task_type}_{prompt_type}_error_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                        'task_type': task_type,
                        'prompt_type': prompt_type,
                        'model': model,
                        'error': str(e),
                        'error_type': 'exception',
                        'test_metadata': {
                            'model': model,
                            'task_type': task_type,
                            'prompt_type': prompt_type,
                            'timestamp': datetime.now().isoformat(),
                            'success': False,
                            'failure_reason': f"Exception: {str(e)}"
                        },
                        'conversation_history': [],
                        'test_result': error_result
                    }
                    
                    # åˆ›å»ºTestTaskå¯¹è±¡ç”¨äºä¿å­˜æ—¥å¿—
                    task_obj = TestTask(
                        model=model,
                        task_type=task_type,
                        prompt_type=prompt_type,
                        difficulty=difficulty,
                        tool_success_rate=tool_success_rate,
                        is_flawed=is_flawed,
                        flaw_type=flaw_type
                    )
                    
                    # ä¿å­˜é”™è¯¯æ—¥å¿—
                    txt_file_path = self._save_interaction_log(task_obj, error_result, error_log_data)
                    if self.debug and txt_file_path:
                        self.logger.debug(f"Saved exception log to {txt_file_path.name}")
                except Exception as log_error:
                    self.logger.error(f"Failed to save exception log: {log_error}")
            
            return error_result
    
    def run_concurrent_batch(self, tasks: List[TestTask], workers: int = 20, 
                           qps: float = 20.0) -> List[Dict]:
        """å¹¶å‘è¿è¡Œæ‰¹é‡æµ‹è¯•"""
        self.logger.info(f"Running {len(tasks)} tests with {workers} workers, QPS limit: {qps}")
        
        # ä¿å­˜QPSå€¼ä¾›åç»­ä½¿ç”¨
        self.qps = qps
        
        # é¢„åˆå§‹åŒ–ï¼ˆåœ¨ä¸»çº¿ç¨‹ä¸­å®Œæˆåˆå§‹åŒ–ï¼‰
        self._lazy_init()
        
        # æ£€æµ‹æ˜¯å¦æ˜¯Azure API
        api_provider = None
        if tasks and any(x in tasks[0].model.lower() for x in ['gpt-4o-mini', 'gpt-5', 'deepseek', 'llama-3.3']):
            api_provider = 'azure'
            self.logger.info("Detected Azure API, disabling QPS sleep for better performance")
        
        results = []
        self._test_counter = 0
        self._success_counter = 0
        
        # æœ¬åœ°è®°å½•ç¼“å­˜ - é¿å…å¹¶å‘å†™å…¥æ•°æ®åº“
        local_records = []
        
        # QPSæ§åˆ¶ï¼ˆAzure APIä¸éœ€è¦ä¸¥æ ¼é™åˆ¶ï¼‰
        # ç¡®ä¿qpsä¸ä¸ºNone
        qps = qps if qps is not None else 0
        min_interval = 0 if api_provider == 'azure' else (1.0 / qps if qps > 0 else 0)
        
        with ThreadPoolExecutor(max_workers=workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            self.logger.info(f"Starting batch test with {len(tasks)} tasks, {workers} workers")
            self.logger.info(f"Each task timeout: 10 minutes, Total batch timeout: 20 minutes")
            future_to_task = {}
            for task in tasks:
                # QPSé™åˆ¶ï¼ˆåªåœ¨éœ€è¦æ—¶åº”ç”¨ï¼‰
                if min_interval > 0:
                    with self._request_lock:
                        now = time.time()
                        time_since_last = now - self._last_request_time
                        if time_since_last < min_interval:
                            time.sleep(min_interval - time_since_last)
                        self._last_request_time = time.time()
                
                future = executor.submit(
                    self._run_single_test_safe,
                    task
                )
                future_to_task[future] = task
            
            # æ”¶é›†ç»“æœ
            try:
                # åˆç†çš„è¶…æ—¶ç­–ç•¥ï¼šæ¯ä¸ªä»»åŠ¡60ç§’ï¼Œä½†è‡³å°‘1å°æ—¶ï¼Œæœ€å¤š4å°æ—¶
                total_timeout = max(3600, min(14400, len(tasks) * 60))  # è‡³å°‘1å°æ—¶ï¼Œæœ€å¤š4å°æ—¶
                self.logger.info(f"Batch timeout set to {total_timeout}s ({total_timeout/60:.1f} minutes) for {len(tasks)} tasks")
                
                for future in as_completed(future_to_task, timeout=total_timeout):
                    task = future_to_task[future]
                    try:
                        result = future.result(timeout=1)  # å¿«é€Ÿè·å–å·²å®Œæˆçš„ç»“æœ
                        results.append(result)
                        print(f"[DEBUG] Got result for task: has_result={result is not None}, save_logs={self.save_logs}")
                        
                        # å§‹ç»ˆè·å–log_dataç”¨äºAIåˆ†ç±»ï¼ˆæ— è®ºæ˜¯å¦å¯ç”¨save_logsï¼‰
                        log_data = None
                        if result:
                            # ä»resultä¸­è·å–log_dataï¼ˆæ€»æ˜¯å­˜åœ¨ï¼‰
                            log_data = result.pop('_log_data', None)
                            if self.debug and log_data:
                                print(f"[DEBUG] Got log_data for {task.task_type}_{task.prompt_type}")
                                print(f"[DEBUG] Log data has conversation_history: {'conversation_history' in log_data}")
                        
                        # ç”ŸæˆTXTå†…å®¹ç”¨äºAIåˆ†ç±»ï¼ˆä¸å†™æ–‡ä»¶ï¼‰
                        ai_error_category = None
                        ai_error_reason = None
                        ai_confidence = 0.0
                        
                        if result and log_data:
                            # æ£€æŸ¥æ˜¯å¦éœ€è¦AIåˆ†ç±»ï¼ˆéfull_successéƒ½éœ€è¦åˆ†ç±»ï¼‰
                            success_level = result.get('success_level', '')
                            if not success_level:
                                # å¦‚æœæ²¡æœ‰success_levelï¼Œéœ€è¦æ›´ç»†è‡´çš„åˆ¤æ–­
                                if result.get('success', False):
                                    # æ£€æŸ¥æ˜¯å¦æ˜¯partial_success
                                    # é€šè¿‡åˆ†æ•°åˆ¤æ–­ï¼šworkflow_score < 1.0 æˆ– phase2_score < 1.0
                                    workflow_score = result.get('workflow_score', 1.0)
                                    phase2_score = result.get('phase2_score', 1.0)
                                    # ç¡®ä¿ä¸æ˜¯Noneå€¼å†è¿›è¡Œæ¯”è¾ƒ
                                    workflow_score = workflow_score if workflow_score is not None else 1.0
                                    phase2_score = phase2_score if phase2_score is not None else 1.0
                                    if workflow_score < 0.8 or phase2_score < 0.8:
                                        success_level = 'partial_success'
                                        print(f"[AI_DEBUG] æ£€æµ‹åˆ°partial_success (workflow={workflow_score:.2f}, phase2={phase2_score:.2f})")
                                    else:
                                        success_level = 'full_success'
                                else:
                                    success_level = 'failure'
                            
                            # partial_successå’Œfailureéƒ½éœ€è¦AIåˆ†ç±»
                            if success_level != 'full_success':
                                print(f"[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ({success_level})ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»")
                                # ç”ŸæˆTXTæ ¼å¼å†…å®¹ï¼ˆåœ¨å†…å­˜ä¸­ï¼‰
                                txt_content = self._generate_txt_log_content(task, result, log_data)
                                print(f"[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: {len(txt_content) if txt_content else 0}")
                                
                                # ä½¿ç”¨TXTå†…å®¹è¿›è¡ŒAIåˆ†ç±»
                                ai_error_category, ai_error_reason, ai_confidence = self._ai_classify_with_txt_content(task, result, txt_content)
                                print(f"[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category={ai_error_category}, confidence={ai_confidence}")
                                if self.debug and ai_error_category:
                                    print(f"[DEBUG] AI classification: {ai_error_category} (confidence: {ai_confidence:.2f})")
                            
                            # ä¿å­˜æ—¥å¿—æ–‡ä»¶ï¼ˆæ‰€æœ‰æµ‹è¯•éƒ½ä¿å­˜ï¼Œä¸ä»…ä»…æ˜¯å¤±è´¥çš„ï¼‰
                            if self.save_logs:
                                txt_file_path = self._save_interaction_log(task, result, log_data)
                                if self.debug and txt_file_path:
                                    print(f"[DEBUG] Saved interaction log to {txt_file_path.name}")
                        
                        # ä¿å­˜åˆ°ç´¯ç§¯æ•°æ®åº“
                        if result:
                            record = TestRecord(
                                model=task.model,
                                task_type=task.task_type,
                                prompt_type=task.prompt_type,
                                difficulty=task.difficulty,
                                success=result.get('success', False),
                                execution_time=result.get('execution_time', 0),
                                error_message=result.get('error'),
                                is_flawed=task.is_flawed,
                                flaw_type=task.flaw_type,
                                timestamp=datetime.now().isoformat()
                            )
                            # æ·»åŠ é¢å¤–çš„åº¦é‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                            record.turns = result.get('turns', 0)
                            record.tool_calls = result.get('tool_calls', [])
                            record.tool_reliability = task.tool_success_rate  # è®°å½•å·¥å…·æˆåŠŸç‡
                            record.required_tools = result.get('required_tools', [])
                            record.executed_tools = result.get('executed_tools', [])
                            # è®¾ç½®éƒ¨åˆ†æˆåŠŸæ ‡å¿—
                            if result.get('success_level') == 'partial_success':
                                record.partial_success = True
                            # è®¾ç½®æˆåŠŸçº§åˆ« - ä½¿ç”¨execution_statusä»¥ä¸cumulative_test_managerä¿æŒä¸€è‡´
                            record.execution_status = result.get('success_level', 'failure')
                            record.success_level = result.get('success_level', 'failure')  # ä¿ç•™ä»¥ä¾¿å‘åå…¼å®¹
                            # æ·»åŠ åˆ†æ•°æŒ‡æ ‡
                            record.workflow_score = result.get('workflow_score')
                            record.phase2_score = result.get('phase2_score')
                            record.quality_score = result.get('quality_score')
                            record.final_score = result.get('final_score')
                            
                            # æ·»åŠ é‡è¦çš„ç¼ºå¤±å­—æ®µ
                            record.format_error_count = result.get('format_error_count', 0)
                            record.api_issues = result.get('api_issues', [])
                            record.executed_tools = result.get('executed_tools', [])
                            record.required_tools = result.get('required_tools', task.required_tools if hasattr(task, 'required_tools') else [])
                            
                            # æ·»åŠ AIåˆ†ç±»ç»“æœï¼ˆåŸºäºå®Œæ•´äº¤äº’æ—¥å¿—ï¼‰
                            if ai_error_category:
                                record.ai_error_category = ai_error_category
                                record.ai_error_reason = ai_error_reason
                                record.ai_confidence = ai_confidence
                                if self.debug:
                                    print(f"[DEBUG] Added enhanced AI classification: {ai_error_category} (confidence: {ai_confidence:.2f})")
                            
                            # æ·»åŠ å…³é”®çš„ç¼ºå¤±å­—æ®µ - tool_coverage_rateå’Œtask_instance
                            record.tool_coverage_rate = result.get('tool_coverage_rate', 0.0)
                            record.tool_success_rate = task.tool_success_rate
                            record.task_instance = task.task_instance if hasattr(task, 'task_instance') else {}
                            
                            # è°ƒè¯•è¾“å‡º
                            if self.debug and record.workflow_score is not None:
                                phase2_str = f"{record.phase2_score:.3f}" if record.phase2_score is not None else "0.000"
                                final_str = f"{record.final_score:.3f}" if record.final_score is not None else "0.000"
                                print(f"[DEBUG] Caching record - workflow: {record.workflow_score:.3f}, phase2: {phase2_str}, final: {final_str}")
                            
                            # æ·»åŠ åˆ°æœ¬åœ°ç¼“å­˜è€Œä¸æ˜¯ç›´æ¥å†™æ•°æ®åº“
                            with self._lock:
                                local_records.append(record)
                            
                            # æ£€æŸ¥æ˜¯å¦éœ€è¦checkpoint
                            if not self.enable_database_updates and self.checkpoint_interval > 0:
                                # æ·»åŠ åˆ°ç»“æœä¸­ä»¥ä¾¿checkpoint
                                result['model'] = task.model
                                result['difficulty'] = task.difficulty
                                self._smart_checkpoint_save([result], task.model)
                        
                        # æ›´æ–°è¿›åº¦
                        if not self.silent:
                            with self._lock:
                                self._test_counter += 1
                                if result.get('success', False):
                                    self._success_counter += 1
                                if self._test_counter % 10 == 0:
                                    print(f"Progress: {self._test_counter}/{len(tasks)} "
                                         f"(Success: {self._success_counter})")
                    
                    except Exception as e:
                        self.logger.error(f"Task failed: {e}")
                        if self.debug:
                            import traceback
                            traceback.print_exc()
                        results.append({'success': False, 'error': str(e)})
                        # è®°å½•å¤±è´¥çš„æµ‹è¯•
                        try:
                                record = TestRecord(
                                    model=task.model,
                                    task_type=task.task_type,
                                    prompt_type=task.prompt_type,
                                    difficulty=task.difficulty,
                                    success=False,
                                    execution_time=0,
                                    error_message=str(e),
                                    timestamp=datetime.now().isoformat()
                                )
                                # æ·»åŠ ç¼ºå¤±çš„å­—æ®µä»¥é¿å…AttributeError
                                record.format_error_count = 0
                                record.api_issues = []
                                record.executed_tools = []
                                record.required_tools = task.required_tools if hasattr(task, 'required_tools') else []
                                # æ·»åŠ åˆ°æœ¬åœ°ç¼“å­˜
                                with self._lock:
                                    local_records.append(record)
                        except Exception as record_error:
                            self.logger.error(f"Failed to create error record: {record_error}")
            
            except TimeoutError:
                # æ•´ä½“è¶…æ—¶ï¼Œå–æ¶ˆå‰©ä½™çš„ä»»åŠ¡
                self.logger.warning(f"Batch execution timeout, cancelling remaining tasks")
                for future in future_to_task:
                    if not future.done():
                        future.cancel()
                # ç­‰å¾…æ­£åœ¨æ‰§è¡Œçš„ä»»åŠ¡å®Œæˆåå…³é—­executor
                # æ³¨æ„ï¼šè¿™é‡Œåº”è¯¥ç­‰å¾…ï¼Œå¦åˆ™å¯èƒ½ä¸¢å¤±æœ€åçš„ä»»åŠ¡ç»“æœ
                executor.shutdown(wait=True, cancel_futures=True)
        
        # å¦‚æœä½¿ç”¨checkpointï¼Œæœ€åä¿å­˜å‰©ä½™çš„
        if not self.enable_database_updates and self.checkpoint_interval > 0:
            self._smart_checkpoint_save([], force=True)
            
        # ç¡®ä¿æ‰€æœ‰å¾…ä¿å­˜çš„æ•°æ®éƒ½å·²å†™å…¥
        # è¿™å¾ˆé‡è¦ï¼Œç‰¹åˆ«æ˜¯å¯¹äºæœ€åä¸€æ‰¹ä¸è¶³checkpoint_intervalçš„æ•°æ®
        if self.pending_results:
            self.logger.info(f"Final flush: Saving remaining {len(self.pending_results)} results")
            self._smart_checkpoint_save([], force=True)
        
        # æ‰¹é‡å†™å…¥æ‰€æœ‰è®°å½•åˆ°æ•°æ®åº“
        # ç»Ÿè®¡æ¨¡å‹åˆ†å¸ƒ
        model_counts = {}
        for record in local_records:
            model = getattr(record, 'model', 'unknown')
            model_counts[model] = model_counts.get(model, 0) + 1
        
        model_distribution = ", ".join([f"{model}:{count}" for model, count in model_counts.items()])
        write_start_msg = f"Batch writing {len(local_records)} records to database ({model_distribution})"
        print(f"\n[INFO] {write_start_msg}")
        self.logger.info(write_start_msg)
        
        # ä½¿ç”¨å­˜å‚¨é€‚é…å™¨æ‰¹é‡å†™å…¥
        successful_writes = 0
        if self.storage_adapter:
            successful_writes = self.storage_adapter.write_batch(local_records)
        else:
            self.logger.error("No storage adapter available for writing records")
        
        write_complete_msg = f"Successfully wrote {successful_writes}/{len(local_records)} records ({model_distribution})"
        print(f"[INFO] {write_complete_msg}")
        self.logger.info(write_complete_msg)
        
        # æœ€ç»ˆä¿å­˜å’Œé”™è¯¯ç»Ÿè®¡
        if isinstance(self.manager, EnhancedCumulativeManager):
            # å®Œæˆæ‰€æœ‰ç»Ÿè®¡å¹¶ç”ŸæˆæŠ¥å‘Š
            self.manager.finalize()
            
            # æ˜¾ç¤ºè¿è¡Œæ—¶é”™è¯¯ç»Ÿè®¡æ‘˜è¦
            summary = self.manager.get_runtime_summary()
            for model_name, model_summary in summary.items():
                self.logger.info(f"Error summary for {model_name}: {model_summary['total_errors']} total errors")
        else:
            self.manager.save_database()
        self.logger.info("Database saved successfully")
        
        # æ˜¾ç¤ºä¿å­˜ä½ç½®ï¼ˆå¹¶å‘æ¨¡å¼ï¼‰
        if not self.silent:
            db_path = Path("pilot_bench_cumulative_results/master_database.json")
            if db_path.exists():
                save_msg = f"Statistics saved to: {db_path.absolute()}"
                print(f"\nğŸ“Š {save_msg}")
                self.logger.info(save_msg)
        
        # è®°å½•æœ€ç»ˆç»Ÿè®¡
        self.logger.info("="*60)
        self.logger.info(f"Batch test completed at {datetime.now().isoformat()}")
        self.logger.info(f"Summary:")
        self.logger.info(f"  - Total tests: {len(results)}")
        self.logger.info(f"  - Successful: {self._success_counter}")
        self.logger.info(f"  - Failed: {len(results) - self._success_counter}")
        if results:
            self.logger.info(f"  - Success rate: {self._success_counter/len(results)*100:.1f}%")
        self.logger.info(f"  - Log file: {self.log_filename}")
        self.logger.info("="*60)
        
        return results
    
    def _checkpoint_save(self, results, task_model=None, force=False):
        """ä¸­é—´ä¿å­˜æ£€æŸ¥ç‚¹"""
        if not self.checkpoint_interval or self.enable_database_updates:
            return  # å¦‚æœæ²¡æœ‰è®¾ç½®checkpointæˆ–å·²ç»å®æ—¶å†™å…¥ï¼Œåˆ™ä¸éœ€è¦
        
        # å°†ç»“æœæ·»åŠ åˆ°pendingç¼“å­˜
        if results:
            if isinstance(results, list):
                self.pending_results.extend(results)
            else:
                self.pending_results.append(results)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿å­˜
        # ä¿®æ”¹ä¿å­˜æ¡ä»¶ï¼šforceæ—¶å¼ºåˆ¶ä¿å­˜ï¼Œæˆ–è€…è¾¾åˆ°checkpoint_intervalï¼Œæˆ–è€…intervalä¸º0æ—¶ç«‹å³ä¿å­˜
        should_save = force or (self.checkpoint_interval == 0 and len(self.pending_results) > 0) or                      (self.checkpoint_interval > 0 and len(self.pending_results) >= self.checkpoint_interval)
        
        if should_save and self.pending_results:
            print(f"\nğŸ’¾ Checkpoint: ä¿å­˜{len(self.pending_results)}ä¸ªä¸­é—´ç»“æœ...")
            
            # ç¡®ä¿å·²åˆå§‹åŒ–manager
            self._lazy_init()
            
            # ä½¿ç”¨ç°æœ‰çš„managerå®ä¾‹ï¼ˆä¸åˆ›å»ºæ–°çš„ï¼ï¼‰
            try:
                from cumulative_test_manager import TestRecord
                
                saved_count = 0
                
                for result in self.pending_results:
                    if result and not result.get('_saved', False):
                        # ä»ç»“æœä¸­æå–ä¿¡æ¯åˆ›å»ºTestRecord
                        record = TestRecord(
                            model=result.get('model', task_model or 'unknown'),
                            task_type=result.get('task_type', 'unknown'),
                            prompt_type=result.get('prompt_type', 'baseline'),
                            difficulty=result.get('difficulty', 'easy')
                        )
                        
                        # è®¾ç½®å…¶ä»–å­—æ®µ
                        for field in ['timestamp', 'success', 'success_level', 'execution_time', 'turns',
                                    'tool_calls', 'workflow_score', 'phase2_score', 'quality_score',
                                    'final_score', 'error_type', 'tool_success_rate', 'is_flawed',
                                    'flaw_type', 'format_error_count', 'api_issues', 'executed_tools',
                                    'required_tools', 'tool_coverage_rate', 'task_instance', 'execution_history',
                                    'ai_error_category', '_ai_error_category']:  # Added AI classification fields
                            if field in result:
                                # Handle _ai_error_category -> ai_error_category conversion
                                if field == '_ai_error_category':
                                    setattr(record, 'ai_error_category', result[field])
                                else:
                                    setattr(record, field, result[field])
                        
                        # ä½¿ç”¨å­˜å‚¨é€‚é…å™¨ä¿å­˜
                        if self.storage_adapter and self.storage_adapter.write_result(record):
                            result['_saved'] = True
                            saved_count += 1
                
                # åˆ·æ–°ç¼“å†²åŒºï¼ˆç‰¹åˆ«é‡è¦å¯¹äºParquetæ ¼å¼ï¼‰
                if hasattr(self.manager, '_flush_buffer'):
                    self.logger.debug(f"[Checkpoint] Flushing buffer after {saved_count} records")
                    self.manager._flush_buffer()
                
                print(f"âœ… Checkpointå®Œæˆ: å·²ä¿å­˜{saved_count}ä¸ªç»“æœ")
                
                # æ¸…ç©ºå·²ä¿å­˜çš„ç»“æœ
                self.pending_results = [r for r in self.pending_results if not r.get('_saved', False)]
                
            except Exception as e:
                self.logger.error(f"Checkpoint save failed: {e}")
                print(f"âš ï¸  Checkpointä¿å­˜å¤±è´¥: {e}")
    
    def run_adaptive_concurrent_batch(self, tasks: List[TestTask], 
                                    initial_workers: int = 20,  # æ¿€è¿›çš„åˆå§‹å¹¶å‘æ•°
                                    initial_qps: float = 50.0) -> List[Dict]:  # é«˜åˆå§‹QPS
        """ä½¿ç”¨è‡ªé€‚åº”é™æµçš„å¹¶å‘æ‰¹é‡æµ‹è¯•"""
        self.logger.info(f"Running {len(tasks)} tests with adaptive rate limiting")
        self.logger.info(f"Initial settings: workers={initial_workers}, QPS={initial_qps}")
        
        # é¢„åˆå§‹åŒ–
        self._lazy_init()
        
        # æ£€æµ‹APIæä¾›å•†å¹¶è‡ªåŠ¨è°ƒæ•´å‚æ•°
        api_provider = None
        if tasks:
            model = tasks[0].model
            if any(x in model.lower() for x in ['qwen', 'llama-4-scout', 'o1']):
                api_provider = 'idealab'
                self.logger.info(f"Detected idealab API for model {model}, using conservative settings")
            elif any(x in model.lower() for x in ['deepseek', 'llama-3.3', 'gpt-4o-mini', 'gpt-5']):
                # Azure APIæ¨¡å‹ï¼Œä½¿ç”¨æé«˜å¹¶å‘
                api_provider = 'azure'
                initial_workers = max(80, initial_workers)  # è‡³å°‘80å¹¶å‘
                initial_qps = max(150, initial_qps)  # è‡³å°‘150 QPS
                self.logger.info(f"Detected Azure API for model {model}, using very high concurrency settings (100+ workers max)")
        
        # åˆ›å»ºè‡ªé€‚åº”é™æµå™¨ï¼ˆAzure APIä½¿ç”¨æ›´é«˜çš„ä¸Šé™ï¼‰
        max_workers_limit = 100 if api_provider == 'azure' else 50  # Azureå…è®¸100å¹¶å‘
        max_qps_limit = 200 if api_provider == 'azure' else 100  # Azureå…è®¸200 QPS
        
        rate_limiter = AdaptiveRateLimiter(
            initial_workers=initial_workers,
            initial_qps=initial_qps,
            min_workers=2,  # æœ€å°ä¹Ÿä¿æŒ2ä¸ªå¹¶å‘
            max_workers=max_workers_limit,  # Azure: 100, å…¶ä»–: 50
            min_qps=5,  # æœ€å°ä¹Ÿä¿æŒ5 QPS
            max_qps=max_qps_limit,  # Azure: 200, å…¶ä»–: 100
            backoff_factor=0.7,  # é™é€Ÿä¸é‚£ä¹ˆæ¿€è¿›ï¼ˆåŸæ¥0.5ï¼‰
            recovery_factor=1.8,  # æé€Ÿæ›´æ¿€è¿›ï¼ˆåŸæ¥1.5ï¼‰
            stable_threshold=3,  # æ›´å¿«è§¦å‘æé€Ÿï¼ˆåŸæ¥5ï¼‰
            logger=self.logger,
            api_provider=api_provider  # ä¼ é€’APIæä¾›å•†ä¿¡æ¯
        )
        
        results = []
        self._test_counter = 0
        self._success_counter = 0
        completed_tasks = 0
        failed_tasks = 0
        retry_queue = []
        
        # ä¸»æ‰§è¡Œå¾ªç¯
        task_index = 0
        while task_index < len(tasks) or retry_queue:
            # è·å–å½“å‰é™åˆ¶
            workers, qps = rate_limiter.get_current_limits()
            
            # å†³å®šæœ¬æ‰¹æ¬¡æ‰§è¡Œçš„ä»»åŠ¡
            if retry_queue and len(retry_queue) >= workers:
                # ä¼˜å…ˆå¤„ç†é‡è¯•é˜Ÿåˆ—
                batch_tasks = retry_queue[:workers]
                retry_queue = retry_queue[workers:]
                is_retry = True
            else:
                # æ··åˆæ–°ä»»åŠ¡å’Œé‡è¯•ä»»åŠ¡
                batch_size = min(workers, len(tasks) - task_index + len(retry_queue))
                batch_tasks = []
                
                # å…ˆåŠ å…¥é‡è¯•ä»»åŠ¡
                while retry_queue and len(batch_tasks) < batch_size:
                    batch_tasks.append((retry_queue.pop(0), True))
                
                # å†åŠ å…¥æ–°ä»»åŠ¡
                while task_index < len(tasks) and len(batch_tasks) < batch_size:
                    batch_tasks.append((tasks[task_index], False))
                    task_index += 1
                
                if not batch_tasks:
                    break
            
            self.logger.info(
                f"Executing batch: {len(batch_tasks)} tasks "
                f"(workers={workers}, QPS={qps}, completed={completed_tasks}/{len(tasks)})"
            )
            
            # QPSæ§åˆ¶
            # ç¡®ä¿qpsä¸ä¸ºNone
            qps = qps if qps is not None else 0
            min_interval = 1.0 / qps if qps > 0 else 0
            
            with ThreadPoolExecutor(max_workers=workers) as executor:
                future_to_task = {}
                
                for item in batch_tasks:
                    if isinstance(item, tuple):
                        task, is_retry = item
                    else:
                        task = item
                        is_retry = False
                    
                    # QPSé™åˆ¶
                    if min_interval > 0:
                        rate_limiter.wait_if_needed()
                    
                    future = executor.submit(self._run_single_test_safe, task)
                    future_to_task[future] = (task, is_retry)
                
                # æ”¶é›†ç»“æœ
                for future in as_completed(future_to_task):
                    task, is_retry = future_to_task[future]
                    try:
                        result = future.result()
                        
                        # æ£€æŸ¥æ˜¯å¦æ˜¯é™æµé”™è¯¯
                        if result and not result.get('success', False):
                            error_msg = result.get('error', '')
                            if 'TPM/RPMé™æµ' in error_msg or 'rate limit' in error_msg.lower():
                                # è®°å½•é™æµ
                                rate_limiter.record_rate_limit(error_msg)
                                # åŠ å…¥é‡è¯•é˜Ÿåˆ—
                                if not is_retry or task not in [t for t, _ in retry_queue if isinstance(t, tuple)]:
                                    retry_queue.append(task)
                                    self.logger.info(f"Task added to retry queue due to rate limit")
                                continue
                            else:
                                # å…¶ä»–é”™è¯¯
                                rate_limiter.record_error(error_msg)
                        elif result and result.get('success', False):
                            # æˆåŠŸ
                            rate_limiter.record_success()
                            completed_tasks += 1
                            self._success_counter += 1
                        
                        # æ·»åŠ åˆ°ç»“æœ
                        results.append(result)
                        
                        # æ£€æŸ¥æ˜¯å¦éœ€è¦checkpointä¿å­˜
                        if not self.enable_database_updates and self.checkpoint_interval > 0:
                            # æ·»åŠ å¿…è¦çš„å­—æ®µ
                            if result:
                                result['model'] = task.model
                                result['difficulty'] = task.difficulty
                            self._smart_checkpoint_save([result], task.model)
                        
                        # ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆä¿®å¤ï¼šrun_adaptive_concurrent_batchç¼ºå°‘çš„æ•°æ®ä¿å­˜ï¼‰
                        if result:
                            record = TestRecord(
                                model=task.model,
                                task_type=task.task_type,
                                prompt_type=task.prompt_type,
                                difficulty=task.difficulty,
                                success=result.get('success', False),
                                execution_time=result.get('execution_time', 0),
                                error_message=result.get('error'),
                                is_flawed=task.is_flawed,
                                flaw_type=task.flaw_type,
                                timestamp=datetime.now().isoformat()
                            )
                            # æ·»åŠ é¢å¤–ä¿¡æ¯
                            record.turns = result.get('turns', 0)
                            record.tool_calls = result.get('tool_calls', [])
                            record.tool_reliability = task.tool_success_rate
                            record.required_tools = result.get('required_tools', [])
                            record.executed_tools = result.get('executed_tools', [])
                            if result.get('success_level') == 'partial_success':
                                record.partial_success = True
                            record.execution_status = result.get('success_level', 'failure')
                            record.success_level = result.get('success_level', 'failure')
                            record.workflow_score = result.get('workflow_score')
                            record.phase2_score = result.get('phase2_score')
                            record.quality_score = result.get('quality_score')
                            record.final_score = result.get('final_score')
                            record.format_error_count = result.get('format_error_count', 0)
                            record.api_issues = result.get('api_issues', [])
                            record.executed_tools = result.get('executed_tools', [])  # Fixed: should be executed_tools not tool_calls
                            record.required_tools = result.get('required_tools', task.required_tools if hasattr(task, 'required_tools') else [])
                            
                            # æ·»åŠ å…³é”®çš„ç¼ºå¤±å­—æ®µ
                            record.tool_coverage_rate = result.get('tool_coverage_rate', 0.0)
                            record.tool_success_rate = task.tool_success_rate
                            record.task_instance = task.task_instance if hasattr(task, 'task_instance') else {}
                            
                            # ä½¿ç”¨å­˜å‚¨é€‚é…å™¨ä¿å­˜
                            if self.storage_adapter:
                                try:
                                    self.storage_adapter.write_result(record)
                                except Exception as e:
                                    self.logger.error(f"Failed to save record: {e}")
                        
                        # è·å–æˆ–åˆ›å»ºlog_dataç”¨äºAIåˆ†ç±»
                        log_data = None
                        if result:
                            # å¯¹äºè¶…æ—¶çš„æƒ…å†µï¼Œåˆ›å»ºåŸºæœ¬çš„log_data
                            if result.get('error_type') == 'timeout':
                                log_data = {
                                    'test_id': f"{task.task_type}_timeout_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                                    'task_type': task.task_type,
                                    'prompt_type': task.prompt_type,
                                    'timestamp': datetime.now().isoformat(),
                                    'task_instance': {'required_tools': task.required_tools or []},
                                    'prompt': 'Timeout before prompt execution',
                                    'llm_response': 'Timeout - no response received',
                                    'extracted_tool_calls': [],
                                    'conversation_history': [],
                                    'execution_history': [],
                                    'is_flawed': task.is_flawed,
                                    'flaw_type': task.flaw_type,
                                    'result': {
                                        'success': False,
                                        'error': result.get('error', 'Timeout'),
                                        'error_type': 'timeout',
                                        'execution_time': result.get('execution_time', 60)
                                    }
                                }
                            else:
                                # ä»resultä¸­è·å–log_dataï¼ˆæ€»æ˜¯å­˜åœ¨ï¼‰
                                log_data = result.pop('_log_data', None)
                        
                        # ç”ŸæˆTXTå†…å®¹ç”¨äºAIåˆ†ç±»ï¼ˆä¸å†™æ–‡ä»¶ï¼‰
                        if result and log_data:
                            # å¦‚æœæœ‰é”™è¯¯ï¼Œç”ŸæˆTXTå†…å®¹å¹¶è¿›è¡ŒAIåˆ†ç±»
                            if not result.get('success', False):
                                try:
                                    # ç”ŸæˆTXTæ ¼å¼å†…å®¹ï¼ˆåœ¨å†…å­˜ä¸­ï¼‰
                                    txt_content = self._generate_txt_log_content(task, result, log_data)
                                    
                                    # ä½¿ç”¨TXTå†…å®¹è¿›è¡ŒAIåˆ†ç±»
                                    ai_cat, ai_reason, ai_conf = self._ai_classify_with_txt_content(task, result, txt_content)
                                    if ai_cat:
                                        # å°†AIåˆ†ç±»ç»“æœæ·»åŠ åˆ°resultä¸­ï¼Œåç»­åˆ›å»ºrecordæ—¶ä¼šä½¿ç”¨
                                        result['_ai_error_category'] = ai_cat
                                        result['_ai_error_reason'] = ai_reason
                                        result['_ai_confidence'] = ai_conf
                                        if self.debug:
                                            print(f"[DEBUG] AI classification for timeout: {ai_cat} (confidence: {ai_conf:.2f})")
                                except Exception as e:
                                    if self.debug:
                                        print(f"[DEBUG] AI classification failed: {e}")
                            
                            # å¦‚æœéœ€è¦ä¿å­˜æ—¥å¿—æ–‡ä»¶
                            if self.save_logs:
                                txt_file_path = self._save_interaction_log(task, result, log_data)
                                if self.debug and txt_file_path:
                                    print(f"[DEBUG] Saved interaction log to {txt_file_path.name}")
                        
                        # ä¿å­˜åˆ°æ•°æ®åº“
                        if result:
                            record = TestRecord(
                                model=task.model,
                                task_type=task.task_type,
                                prompt_type=task.prompt_type,
                                difficulty=task.difficulty,
                                success=result.get('success', False),
                                execution_time=result.get('execution_time', 0),
                                error_message=result.get('error'),
                                is_flawed=task.is_flawed,
                                flaw_type=task.flaw_type,
                                timestamp=datetime.now().isoformat()
                            )
                            # æ·»åŠ é¢å¤–ä¿¡æ¯
                            record.turns = result.get('turns', 0)
                            record.tool_calls = result.get('tool_calls', [])
                            record.tool_reliability = task.tool_success_rate
                            record.required_tools = result.get('required_tools', [])
                            record.executed_tools = result.get('executed_tools', [])
                            if result.get('success_level') == 'partial_success':
                                record.partial_success = True
                            record.execution_status = result.get('success_level', 'failure')
                            record.success_level = result.get('success_level', 'failure')
                            record.workflow_score = result.get('workflow_score')
                            record.phase2_score = result.get('phase2_score')
                            record.quality_score = result.get('quality_score')
                            record.final_score = result.get('final_score')
                            record.tool_coverage_rate = result.get('tool_coverage_rate', 0.0)
                            record.required_tools = result.get('required_tools', task.required_tools if hasattr(task, 'required_tools') else [])
                            record.format_error_count = result.get('format_error_count', 0)
                            
                            # æ·»åŠ å…¶ä»–å…³é”®å­—æ®µ
                            record.tool_success_rate = task.tool_success_rate
                            record.task_instance = task.task_instance if hasattr(task, 'task_instance') else {}
                            
                            # æ·»åŠ AIåˆ†ç±»ç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                            if result.get('_ai_error_category'):
                                record.ai_error_category = result['_ai_error_category']
                                record.ai_error_reason = result.get('_ai_error_reason', '')
                                record.ai_confidence = result.get('_ai_confidence', 0.0)
                                if self.debug:
                                    print(f"[DEBUG] Added AI classification to record: {record.ai_error_category}")
                            
                            # ä½¿ç”¨å­˜å‚¨é€‚é…å™¨ä¿å­˜
                            if self.storage_adapter:
                                self.storage_adapter.write_result(record)
                        
                        # æ›´æ–°è®¡æ•°å™¨
                        with self._lock:
                            self._test_counter += 1
                            if self._test_counter % 10 == 0:
                                self.logger.info(f"Progress: {self._test_counter} tests completed")
                        
                    except Exception as e:
                        self.logger.error(f"Failed to process result: {e}")
                        failed_tasks += 1
            
            # æ‰“å°å½“å‰ç»Ÿè®¡
            stats = rate_limiter.get_stats_summary()
            self.logger.info(f"Adaptive stats: {stats}")
            
            # å¦‚æœè¿ç»­é™æµå¤ªå¤šæ¬¡ï¼Œå¢åŠ ç­‰å¾…ï¼ˆä½†æ›´æ™ºèƒ½ï¼‰
            if rate_limiter.consecutive_rate_limits > 10:  # æé«˜é˜ˆå€¼
                # Azure APIå‡ ä¹ä¸éœ€è¦ç­‰å¾…
                if api_provider == 'azure':
                    wait_time = 0.5  # Azureæœ€å¤šç­‰0.5ç§’
                else:
                    wait_time = min(2.0, rate_limiter.get_retry_delay())  # å…¶ä»–APIæœ€å¤šç­‰2ç§’
                if wait_time > 0:
                    self.logger.warning(f"Too many consecutive rate limits ({rate_limiter.consecutive_rate_limits}), waiting {wait_time:.1f}s...")
                    time.sleep(wait_time)
        
        # æœ€åä¸€æ¬¡checkpointä¿å­˜ï¼ˆå¼ºåˆ¶ï¼‰
        if not self.enable_database_updates and self.checkpoint_interval > 0:
            self._smart_checkpoint_save([], force=True)
        
        # ç¡®ä¿æ‰€æœ‰pending_resultséƒ½è¢«ä¿å­˜ï¼ˆå³ä½¿ä¸æ»¡checkpoint_intervalï¼‰
        if self.pending_results and not self.enable_database_updates:
            print(f"\nğŸ’¾ Final save: ä¿å­˜å‰©ä½™çš„{len(self.pending_results)}ä¸ªç»“æœ...")
            self._smart_checkpoint_save([], force=True)
        
        # æœ€ç»ˆç»Ÿè®¡
        self.logger.info("="*60)
        self.logger.info(f"Adaptive batch test completed")
        self.logger.info(f"Final statistics:")
        self.logger.info(f"  - Total tests: {len(results)}")
        self.logger.info(f"  - Successful: {self._success_counter}")
        self.logger.info(f"  - Failed: {len(results) - self._success_counter}")
        self.logger.info(f"  - Retries: {len(retry_queue)} remaining")
        if results:
            self.logger.info(f"  - Success rate: {self._success_counter/len(results)*100:.1f}%")
        
        final_stats = rate_limiter.get_stats_summary()
        self.logger.info(f"  - Rate limit hits: {final_stats['rate_limit_count']}")
        self.logger.info(f"  - Final workers: {final_stats['current_workers']}")
        self.logger.info(f"  - Final QPS: {final_stats['current_qps']}")
        self.logger.info("="*60)
        
        # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½ä¿å­˜åˆ°æ•°æ®åº“ï¼ˆä¿®å¤ï¼šåˆ·æ–°ç¼“å†²åŒºï¼‰
        if isinstance(self.manager, EnhancedCumulativeManager):
            self.manager.finalize()
            self.logger.info("Database updates finalized")
        
        return results
    
    def _run_single_test_safe(self, task: TestTask) -> Dict:
        """çº¿ç¨‹å®‰å…¨çš„å•ä¸ªæµ‹è¯•æ‰§è¡Œï¼ˆä½¿ç”¨åµŒå¥—çº¿ç¨‹æ± å®ç°è¶…æ—¶ï¼‰"""
        import sys
        import threading
        from datetime import datetime
        from concurrent.futures import ThreadPoolExecutor, TimeoutError
        import signal
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # æ— è®ºæ˜¯å¦åœ¨ä¸»çº¿ç¨‹ï¼Œéƒ½ä½¿ç”¨ThreadPoolExecutorå®ç°è¶…æ—¶
        # è¿™æ ·å¯ä»¥ç¡®ä¿åœ¨workerçº¿ç¨‹ä¸­ä¹Ÿèƒ½æ­£ç¡®è¶…æ—¶
        with ThreadPoolExecutor(max_workers=1) as executor:
            future = executor.submit(
                self.run_single_test,
                model=task.model,
                deployment=getattr(task, 'deployment', None),  # ä¼ é€’éƒ¨ç½²å®ä¾‹åï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                task_type=task.task_type,
                prompt_type=task.prompt_type,
                is_flawed=task.is_flawed,
                flaw_type=task.flaw_type,
                timeout=task.timeout,
                tool_success_rate=task.tool_success_rate,
                difficulty=task.difficulty
            )
            
            # ä½¿ç”¨åˆç†çš„è¶…æ—¶ç­–ç•¥
            timeout_seconds = 900  # 15åˆ†é’Ÿè¶…æ—¶ï¼ˆå¹³è¡¡ç¨³å®šæ€§å’Œæ•ˆç‡ï¼‰
            
            try:
                # ç­‰å¾…ç»“æœï¼Œä½†ä½¿ç”¨æ›´çŸ­çš„è¶…æ—¶
                result = future.result(timeout=timeout_seconds)
                return result
            except TimeoutError:
                # è®°å½•å®é™…è¿è¡Œæ—¶é—´
                actual_runtime = time.time() - start_time
                self.logger.error(f"Test timeout after {actual_runtime:.1f} seconds (limit: {timeout_seconds}s)")
                
                # å°è¯•å–æ¶ˆï¼ˆè™½ç„¶å¯èƒ½æ— æ•ˆï¼‰
                cancelled = future.cancel()
                if not cancelled:
                    self.logger.warning("Failed to cancel the running task (already executing)")
                    # å¼ºåˆ¶å…³é—­executorï¼Œä¸ç­‰å¾…ä»»åŠ¡å®Œæˆ
                    executor.shutdown(wait=False, cancel_futures=True)
                
                self.logger.error(f"Test forcibly terminated after {timeout_seconds} seconds")
                # ä¸ºè¶…æ—¶åˆ›å»ºåŸºæœ¬çš„log_dataï¼Œç”¨äºAIåˆ†ç±»
                timeout_log_data = {
                    'test_id': f"{task.task_type}_timeout_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                    'task_type': task.task_type,
                    'prompt_type': task.prompt_type,
                    'timestamp': datetime.now().isoformat(),
                    'task_instance': {'required_tools': task.required_tools or []},
                    'prompt': 'Task timeout after 10 minutes',
                    'llm_response': 'No response - timeout after 10 minutes',
                    'extracted_tool_calls': [],
                    'conversation_history': [],
                    'execution_history': [],
                    'is_flawed': task.is_flawed,
                    'flaw_type': task.flaw_type,
                    'result': {
                        'success': False,
                        'error': 'Test timeout after 10 minutes',
                        'error_type': 'timeout',
                        'execution_time': 600
                    }
                }
                
                return {
                    'success': False,
                    'error': 'Test timeout after 10 minutes',
                    'error_type': 'timeout',
                    'execution_time': 600,
                    'success_level': 'failure',
                    'tool_calls': [],
                    'turns': 0,
                    'workflow_score': 0.0,
                    'phase2_score': 0.0,
                    'quality_score': 0.0,
                    'final_score': 0.0,
                    'task_type': task.task_type,
                    'prompt_type': task.prompt_type,
                    'difficulty': task.difficulty,
                    'is_flawed': task.is_flawed,
                    'flaw_type': task.flaw_type,
                    'tool_success_rate': task.tool_success_rate,
                    'required_tools': task.required_tools or [],
                    'executed_tools': [],
                    'tool_coverage_rate': 0.0,
                    '_log_data': timeout_log_data  # æ·»åŠ log_dataä»¥æ”¯æŒAIåˆ†ç±»
                }
            except Exception as e:
                self.logger.error(f"Test execution failed: {e}")
                # ä¸ºå¼‚å¸¸åˆ›å»ºåŸºæœ¬çš„log_dataï¼Œç”¨äºAIåˆ†ç±»
                exception_log_data = {
                    'test_id': f"{task.task_type}_exception_{datetime.now().strftime('%Y%m%d_%H%M%S')}",
                    'task_type': task.task_type,
                    'prompt_type': task.prompt_type,
                    'timestamp': datetime.now().isoformat(),
                    'task_instance': {'required_tools': task.required_tools or []},
                    'prompt': f'Task failed with exception: {str(e)}',
                    'llm_response': 'No response - execution exception',
                    'extracted_tool_calls': [],
                    'conversation_history': [],
                    'execution_history': [],
                    'is_flawed': task.is_flawed,
                    'flaw_type': task.flaw_type,
                    'result': {
                        'success': False,
                        'error': str(e),
                        'error_type': 'execution_error',
                        'execution_time': 0
                    }
                }
                
                return {
                    'success': False,
                    'error': str(e),
                    'error_type': 'execution_error',
                    'execution_time': 0,
                    'success_level': 'failure',
                    'tool_calls': [],
                    'turns': 0,
                    'workflow_score': 0.0,
                    'phase2_score': 0.0,
                    'quality_score': 0.0,
                    'final_score': 0.0,
                    'task_type': task.task_type,
                    'prompt_type': task.prompt_type,
                    'difficulty': task.difficulty,
                    'is_flawed': task.is_flawed,
                    'flaw_type': task.flaw_type,
                    'tool_success_rate': task.tool_success_rate,
                    'required_tools': task.required_tools or [],
                    'executed_tools': [],
                    'tool_coverage_rate': 0.0,
                    '_log_data': exception_log_data  # æ·»åŠ log_dataä»¥æ”¯æŒAIåˆ†ç±»
                }
    
    def get_smart_tasks(self, model: str, count: int, difficulty: str = "easy", 
                        tool_success_rate: float = 0.8,
                        selected_prompt_types: List[str] = None,
                        selected_task_types: List[str] = None) -> List[TestTask]:
        """æ™ºèƒ½é€‰æ‹©éœ€è¦æµ‹è¯•çš„ä»»åŠ¡
        
        Args:
            count: æ¯ç§ç»„åˆçš„æµ‹è¯•æ•°é‡ (prompt Ã— task Ã— difficulty)
            selected_prompt_types: è¦æµ‹è¯•çš„promptç±»å‹ï¼ŒNoneæˆ–["all"]è¡¨ç¤ºå…¨éƒ¨
            selected_task_types: è¦æµ‹è¯•çš„ä»»åŠ¡ç±»å‹ï¼ŒNoneæˆ–["all"]è¡¨ç¤ºå…¨éƒ¨
        
        å®éªŒè®¡åˆ’ï¼š10ç§promptç­–ç•¥ Ã— 5ç§task_type = 50ç§ç»„åˆ
        - 3ç§åŸºæœ¬prompt: baseline, optimal, cot
        - 7ç§ç¼ºé™·prompt: æ¯ç§flaw_typeå¯¹åº”ä¸€ä¸ª
        - 5ç§ä»»åŠ¡ç±»å‹: simple_task, basic_task, data_pipeline, api_integration, multi_stage_pipeline
        """
        self._lazy_init()
        
        # é»˜è®¤çš„ä»»åŠ¡ç±»å‹
        all_task_types = ["simple_task", "basic_task", "data_pipeline", "api_integration", "multi_stage_pipeline"]
        
        # å†³å®šè¦æµ‹è¯•çš„ä»»åŠ¡ç±»å‹
        if selected_task_types is None or (len(selected_task_types) == 1 and selected_task_types[0] == "all"):
            task_types = all_task_types
        else:
            # éªŒè¯é€‰æ‹©çš„ä»»åŠ¡ç±»å‹
            task_types = [t for t in selected_task_types if t in all_task_types]
            if not task_types:
                self.logger.warning(f"No valid task types found in {selected_task_types}, using all")
                task_types = all_task_types
        
        # å®šä¹‰æ‰€æœ‰å¯èƒ½çš„promptç­–ç•¥
        all_prompt_strategies = [
            # 3ç§åŸºæœ¬prompt
            {"prompt_type": "baseline", "is_flawed": False, "flaw_type": None},
            {"prompt_type": "optimal", "is_flawed": False, "flaw_type": None},
            {"prompt_type": "cot", "is_flawed": False, "flaw_type": None},
            # 7ç§ç¼ºé™·promptï¼ˆä½¿ç”¨å…·ä½“çš„prompt_typeåç§°ï¼‰
            {"prompt_type": "flawed_sequence_disorder", "is_flawed": True, "flaw_type": "sequence_disorder"},
            {"prompt_type": "flawed_tool_misuse", "is_flawed": True, "flaw_type": "tool_misuse"},
            {"prompt_type": "flawed_parameter_error", "is_flawed": True, "flaw_type": "parameter_error"},
            {"prompt_type": "flawed_missing_step", "is_flawed": True, "flaw_type": "missing_step"},
            {"prompt_type": "flawed_redundant_operations", "is_flawed": True, "flaw_type": "redundant_operations"},
            {"prompt_type": "flawed_logical_inconsistency", "is_flawed": True, "flaw_type": "logical_inconsistency"},
            {"prompt_type": "flawed_semantic_drift", "is_flawed": True, "flaw_type": "semantic_drift"},
        ]
        
        # å†³å®šè¦æµ‹è¯•çš„promptç­–ç•¥
        if selected_prompt_types is None or (len(selected_prompt_types) == 1 and selected_prompt_types[0] == "all"):
            prompt_strategies = all_prompt_strategies
        else:
            prompt_strategies = []
            for pt in selected_prompt_types:
                if pt == "baseline":
                    prompt_strategies.append({"prompt_type": "baseline", "is_flawed": False, "flaw_type": None})
                elif pt == "optimal":
                    prompt_strategies.append({"prompt_type": "optimal", "is_flawed": False, "flaw_type": None})
                elif pt == "cot":
                    prompt_strategies.append({"prompt_type": "cot", "is_flawed": False, "flaw_type": None})
                elif pt == "flawed":
                    # æ·»åŠ æ‰€æœ‰ç¼ºé™·ç±»å‹
                    for s in all_prompt_strategies:
                        if s["is_flawed"]:
                            prompt_strategies.append(s)
                elif pt.startswith("flawed_"):
                    # ç‰¹å®šçš„ç¼ºé™·ç±»å‹
                    flaw_type = pt.replace("flawed_", "")
                    for s in all_prompt_strategies:
                        if s["is_flawed"] and s["flaw_type"] == flaw_type:
                            prompt_strategies.append(s)
                            break
            
            if not prompt_strategies:
                self.logger.warning(f"No valid prompt types found in {selected_prompt_types}, using all")
                prompt_strategies = all_prompt_strategies
        
        # ç”Ÿæˆæ‰€æœ‰ç»„åˆ
        all_combinations = []
        for strategy in prompt_strategies:
            for task_type in task_types:
                combination = {
                    'task_type': task_type,
                    'strategy': strategy
                }
                all_combinations.append(combination)
        
        # countç°åœ¨è¡¨ç¤ºæ¯ç§ç»„åˆçš„æµ‹è¯•æ•°é‡
        self.logger.info(f"Creating {count} tests for each of {len(all_combinations)} combinations")
        self.logger.info(f"Total tests: {count * len(all_combinations)}")
        
        tasks = []
        
        # ä¸ºæ¯ç§ç»„åˆåˆ›å»ºæŒ‡å®šæ•°é‡çš„æµ‹è¯•
        for combo in all_combinations:
            for _ in range(count):
                strategy = combo['strategy']
                task = TestTask(
                    model=model,
                    task_type=combo['task_type'],
                    prompt_type=strategy["prompt_type"],
                    difficulty=difficulty,
                    is_flawed=strategy["is_flawed"],
                    flaw_type=strategy["flaw_type"],
                    tool_success_rate=tool_success_rate
                )
                tasks.append(task)
        
        # æ‰“ä¹±é¡ºåºä»¥é¿å…æ‰¹æ¬¡æ•ˆåº”
        random.shuffle(tasks)
        
        # è®°å½•åˆ†é…ç»Ÿè®¡
        if not self.silent:
            self._log_task_distribution(tasks)
            self._log_detailed_distribution(tasks)
        
        return tasks
    
    def _log_task_distribution(self, tasks: List[TestTask]):
        """è®°å½•ä»»åŠ¡åˆ†é…ç»Ÿè®¡"""
        distribution = {}
        for task in tasks:
            if task.is_flawed:
                key = f"flawed_{task.flaw_type}"
            else:
                key = task.prompt_type
            distribution[key] = distribution.get(key, 0) + 1
        
        self.logger.info("Task distribution by prompt strategy:")
        for strategy, count in sorted(distribution.items()):
            self.logger.info(f"  {strategy}: {count}")
    
    def _log_detailed_distribution(self, tasks: List[TestTask]):
        """è®°å½•è¯¦ç»†çš„ä»»åŠ¡åˆ†é…ç»Ÿè®¡"""
        # æŒ‰task_typeç»Ÿè®¡
        task_type_dist = {}
        for task in tasks:
            task_type_dist[task.task_type] = task_type_dist.get(task.task_type, 0) + 1
        
        self.logger.info("Task distribution by task type:")
        for task_type, count in sorted(task_type_dist.items()):
            self.logger.info(f"  {task_type}: {count}")
        
        # æ£€æŸ¥åˆ†é…å‡è¡¡æ€§
        combo_stats = {}
        for task in tasks:
            if task.is_flawed:
                combo_key = f"{task.task_type}_{task.flaw_type}"
            else:
                combo_key = f"{task.task_type}_{task.prompt_type}"
            combo_stats[combo_key] = combo_stats.get(combo_key, 0) + 1
        
        counts = list(combo_stats.values())
        min_count = min(counts) if counts else 0
        max_count = max(counts) if counts else 0
        
        self.logger.info(f"Combination balance: min={min_count}, max={max_count}, diff={max_count-min_count}")
    
    # æ³¨æ„ï¼špromptåˆ›å»ºæ–¹æ³•ç°åœ¨é€šè¿‡ self.quality_tester ç»§æ‰¿
    # ä¸å†éœ€è¦æœ¬åœ°å®ç° _create_flawed_workflow_prompt å’Œ _format_workflow_sequence
    
    def show_progress(self, model: str, detailed: bool = False):
        """æ˜¾ç¤ºæµ‹è¯•è¿›åº¦"""
        self._lazy_init()
        
        progress = self.manager.get_progress_report(model)
        
        print(f"\n{'='*60}")
        print(f"Progress Report for {model}")
        print(f"{'='*60}")
        
        if model in progress.get('models', {}):
            model_data = progress['models'][model]
            print(f"Total tests: {model_data['total_tests']}")
            print(f"Total success: {model_data['total_success']}")
            
            if detailed:
                print("\nBy task type:")
                for task_type, stats in model_data.get('by_task_type', {}).items():
                    print(f"  {task_type}: {stats['total']} tests, {stats['success']} success")
                
                print("\nBy prompt type:")
                for prompt_type, stats in model_data.get('by_prompt_type', {}).items():
                    print(f"  {prompt_type}: {stats['total']} tests, {stats['success']} success")
                
                print("\nBy flaw type:")
                for flaw_type, stats in model_data.get('by_flaw_type', {}).items():
                    print(f"  {flaw_type}: {stats['total']} tests, {stats['success']} success")
        else:
            print("No test data found for this model")
        
        print(f"{'='*60}\n")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='PILOT-Bench Batch Test Runner')
    
    # åŸºæœ¬å‚æ•°
    parser.add_argument('--model', type=str, default='gpt-4o-mini',
                       help='Model to test')
    parser.add_argument('--count', type=int, default=2,
                       help='Number of tests per combination (prompt Ã— task Ã— difficulty)')
    parser.add_argument('--difficulty', type=str, default='easy',
                       choices=['very_easy', 'easy', 'medium', 'hard', 'very_hard'],
                       help='Task difficulty level')
    
    # æµ‹è¯•æ¨¡å¼
    parser.add_argument('--smart', action='store_true',
                       help='Smart mode - prioritize needed tests')
    parser.add_argument('--progress', action='store_true',
                       help='Show progress only')
    parser.add_argument('--detailed', action='store_true',
                       help='Show detailed progress')
    
    # å¹¶å‘å‚æ•°
    parser.add_argument('--concurrent', action='store_true',
                       help='Enable concurrent execution')
    parser.add_argument('--adaptive', action='store_true',
                       help='Enable adaptive rate limiting (recommended)')
    parser.add_argument('--workers', type=int, default=20,
                       help='Number of concurrent workers (or initial workers for adaptive mode)')
    parser.add_argument('--qps', type=float, default=20.0,
                       help='Queries per second limit (or initial QPS for adaptive mode)')
    
    # è¾“å‡ºæ§åˆ¶
    parser.add_argument('--silent', action='store_true',
                       help='Silent mode - minimal output')
    parser.add_argument('--debug', action='store_true',
                       help='Debug mode - verbose output')
    
    # å…¶ä»–å‚æ•°
    parser.add_argument('--timeout', type=int, default=30,
                       help='Timeout per test in seconds')
    parser.add_argument('--tool-success-rate', type=float, default=0.8,
                       help='Tool success rate (0.0-1.0), default 0.8')
    parser.add_argument('--task-types', nargs='+', default=['all'],
                       help='Task types to test (default: all). Use "all" for all types')
    parser.add_argument('--prompt-types', nargs='+', default=['all'],
                       help='Prompt types to test (default: all). Options: baseline, optimal, cot, flawed, flawed_<type>, all')
    parser.add_argument('--clear', action='store_true',
                       help='Clear previous cumulative records before running')
    parser.add_argument('--save-logs', action='store_true',
                       help='Save detailed interaction logs for each test')
    parser.add_argument('--ai-classification', action='store_true',
                       help='Enable AI-powered error classification using gpt-5-nano')
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¿è¡Œå™¨
    runner = BatchTestRunner(
        debug=args.debug, 
        silent=args.silent, 
        save_logs=args.save_logs,
        use_ai_classification=args.ai_classification
    )
    
    # å¦‚æœéœ€è¦æ¸…é™¤ä¹‹å‰çš„è®°å½•
    if args.clear:
        # ç¡®ä¿managerè¢«åˆå§‹åŒ–
        runner._lazy_init()
        if runner.manager:
            runner.manager.clear_database()
    
    # å¦‚æœåªæ˜¯æŸ¥çœ‹è¿›åº¦
    if args.progress:
        runner.show_progress(args.model, detailed=args.detailed)
        return
    
    # å‡†å¤‡æµ‹è¯•ä»»åŠ¡ - ç»Ÿä¸€ä½¿ç”¨get_smart_tasks
    tasks = runner.get_smart_tasks(
        model=args.model, 
        count=args.count, 
        difficulty=args.difficulty, 
        tool_success_rate=args.tool_success_rate,
        selected_prompt_types=args.prompt_types,
        selected_task_types=args.task_types
    )
    
    # è¿è¡Œæµ‹è¯•
    print(f"Starting batch test: {len(tasks)} tests")
    print(f"Model: {args.model}")
    print(f"Difficulty: {args.difficulty}")
    
    if args.adaptive:
        # ä½¿ç”¨è‡ªé€‚åº”é™æµ
        results = runner.run_adaptive_concurrent_batch(
            tasks,
            initial_workers=min(3, args.workers),  # ä¿å®ˆçš„åˆå§‹å€¼
            initial_qps=min(5.0, args.qps)
        )
    elif args.concurrent:
        # ä½¿ç”¨å›ºå®šå¹¶å‘
        results = runner.run_concurrent_batch(tasks, workers=args.workers, qps=args.qps)
    else:
        # ä¸²è¡Œæ‰§è¡Œ
        results = []
        for i, task in enumerate(tasks):
            if not args.silent:
                print(f"Running test {i+1}/{len(tasks)}...")
            result = runner.run_single_test(
                model=task.model,
                task_type=task.task_type,
                difficulty=task.difficulty,
                prompt_type=task.prompt_type,
                is_flawed=task.is_flawed,
                flaw_type=task.flaw_type,
                timeout=task.timeout,
                tool_success_rate=task.tool_success_rate
            )
            results.append(result)
            
            # å§‹ç»ˆè·å–log_dataç”¨äºAIåˆ†ç±»ï¼ˆæ— è®ºæ˜¯å¦å¯ç”¨save_logsï¼‰
            log_data = None
            if result:
                # ä»resultä¸­è·å–log_dataï¼ˆæ€»æ˜¯å­˜åœ¨ï¼‰
                log_data = result.pop('_log_data', None)
                if args.debug and log_data:
                    print(f"[DEBUG] Got log_data for {task.task_type}_{task.prompt_type}")
            
            # ç”ŸæˆTXTå†…å®¹ç”¨äºAIåˆ†ç±»ï¼ˆä¸å†™æ–‡ä»¶ï¼‰
            ai_error_category = None
            ai_error_reason = None
            ai_confidence = 0.0
            
            if result and log_data:
                # å¦‚æœæœ‰é”™è¯¯ï¼Œç”ŸæˆTXTå†…å®¹å¹¶è¿›è¡ŒAIåˆ†ç±»
                if not result.get('success', False):
                    # ç”ŸæˆTXTæ ¼å¼å†…å®¹ï¼ˆåœ¨å†…å­˜ä¸­ï¼‰
                    txt_content = runner._generate_txt_log_content(task, result, log_data)
                    
                    # ä½¿ç”¨TXTå†…å®¹è¿›è¡ŒAIåˆ†ç±»
                    ai_error_category, ai_error_reason, ai_confidence = runner._ai_classify_with_txt_content(task, result, txt_content)
                    if ai_error_category and args.debug:
                        print(f"[DEBUG] AI classification: {ai_error_category} (confidence: {ai_confidence:.2f})")
                
                # å¦‚æœéœ€è¦ä¿å­˜æ—¥å¿—æ–‡ä»¶
                if runner.save_logs:
                    txt_file_path = runner._save_interaction_log(task, result, log_data)
                    if args.debug and txt_file_path:
                        print(f"[DEBUG] Saved interaction log to {txt_file_path.name}")
            
            # ä¿å­˜åˆ°ç´¯ç§¯æ•°æ®åº“
            record = TestRecord(
                model=task.model,
                task_type=task.task_type,
                prompt_type=task.prompt_type,
                difficulty=task.difficulty,
                success=result.get('success', False),
                execution_time=result.get('execution_time', 0),
                error_message=result.get('error'),
                is_flawed=task.is_flawed,
                flaw_type=task.flaw_type,
                timestamp=datetime.now().isoformat()
            )
            # æ·»åŠ é¢å¤–çš„åº¦é‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            record.turns = result.get('turns', 0)
            record.tool_calls = result.get('tool_calls', [])
            record.tool_reliability = task.tool_success_rate  # è®°å½•å·¥å…·æˆåŠŸç‡
            record.required_tools = result.get('required_tools', [])
            record.executed_tools = result.get('executed_tools', [])
            # è®¾ç½®éƒ¨åˆ†æˆåŠŸæ ‡å¿—
            if result.get('success_level') == 'partial_success':
                record.partial_success = True
            # è®¾ç½®æˆåŠŸçº§åˆ« - ä½¿ç”¨execution_statusä»¥ä¸cumulative_test_managerä¿æŒä¸€è‡´
            record.execution_status = result.get('success_level', 'failure')
            record.success_level = result.get('success_level', 'failure')  # ä¿ç•™ä»¥ä¾¿å‘åå…¼å®¹
            # æ·»åŠ åˆ†æ•°æŒ‡æ ‡
            record.workflow_score = result.get('workflow_score')
            record.phase2_score = result.get('phase2_score')
            record.quality_score = result.get('quality_score')
            record.final_score = result.get('final_score')
            
            # æ·»åŠ é‡è¦çš„ç¼ºå¤±å­—æ®µ
            record.format_error_count = result.get('format_error_count', 0)
            record.api_issues = result.get('api_issues', [])
            record.executed_tools = result.get('executed_tools', [])
            record.required_tools = task.required_tools if hasattr(task, 'required_tools') else []
            
            # åœ¨å•ç‹¬æµ‹è¯•ä¸­ï¼ŒAIåˆ†ç±»ä¼šåœ¨è¿™é‡Œå¤„ç†ï¼ˆå¦‚æœå¯ç”¨äº†save_logså’ŒAIåˆ†ç±»ï¼‰
            # æ³¨æ„ï¼šè¿™æ˜¯åŒæ­¥æµ‹è¯•è·¯å¾„ï¼Œéœ€è¦ä»log_dataä¸­è·å–AIåˆ†ç±»ç»“æœ
            
            runner.manager.add_test_result_with_classification(record)
    
    # å®Œæˆé”™è¯¯ç»Ÿè®¡
    if isinstance(runner.manager, EnhancedCumulativeManager):
        runner.manager.finalize()
    
    # æ˜¾ç¤ºç»“æœ
    success_count = sum(1 for r in results if r.get('success', False))
    print(f"\n{'='*60}")
    print(f"Test Complete!")
    print(f"Total: {len(results)}")
    print(f"Success: {success_count} ({success_count/len(results)*100:.1f}%)")
    print(f"Failed: {len(results) - success_count}")
    print(f"{'='*60}")
    
    # æ˜¾ç¤ºç»Ÿè®¡ç»“æœä¿å­˜ä½ç½®
    db_path = Path("pilot_bench_cumulative_results/master_database.json")
    if db_path.exists():
        print(f"\nğŸ“Š ç»Ÿè®¡ç»“æœå·²ä¿å­˜åˆ°:")
        print(f"   {db_path.absolute()}")
        print(f"   æ–‡ä»¶å¤§å°: {db_path.stat().st_size / 1024:.1f} KB")
        print(f"\nğŸ’¡ æŸ¥çœ‹ç»Ÿè®¡æŠ¥å‘Š:")
        print(f"   python view_test_statistics.py --model {args.model}")
    
    # æ˜¾ç¤ºæœ€ç»ˆè¿›åº¦
    if not args.silent:
        runner.show_progress(args.model, detailed=False)


if __name__ == "__main__":
    main()