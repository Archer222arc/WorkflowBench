#!/usr/bin/env python3
"""
å°†master_database.jsonè½¬æ¢ä¸ºParquetæ ¼å¼
ä¿ç•™æ‰€æœ‰å±‚æ¬¡ç»“æ„å’Œæ•°æ®å®Œæ•´æ€§
"""

import json
import pandas as pd
from pathlib import Path
from datetime import datetime
import pyarrow as pa
import pyarrow.parquet as pq

def flatten_hierarchical_data(db_data):
    """
    å°†å±‚æ¬¡åŒ–çš„JSONæ•°æ®å±•å¹³ä¸ºDataFrameæ ¼å¼
    ä¿ç•™æ‰€æœ‰ç»´åº¦ä¿¡æ¯
    """
    records = []
    
    # éå†modelsæ•°æ®
    models_data = db_data.get('models', {})
    
    for model_name, model_data in models_data.items():
        # æå–overall_stats
        overall_stats = model_data.get('overall_stats', {})
        
        # éå†prompt types
        by_prompt = model_data.get('by_prompt_type', {})
        
        for prompt_type, prompt_data in by_prompt.items():
            # éå†tool_success_rate
            by_rate = prompt_data.get('by_tool_success_rate', {})
            
            for rate, rate_data in by_rate.items():
                # éå†difficulty
                by_diff = rate_data.get('by_difficulty', {})
                
                for difficulty, diff_data in by_diff.items():
                    # éå†task_type
                    by_task = diff_data.get('by_task_type', {})
                    
                    for task_type, task_data in by_task.items():
                        # æ„å»ºè®°å½•
                        record = {
                            # åŸºæœ¬ç»´åº¦
                            'model': model_name,
                            'prompt_type': prompt_type,
                            'tool_success_rate': float(rate),
                            'difficulty': difficulty,
                            'task_type': task_type,
                            
                            # æµ‹è¯•ç»Ÿè®¡
                            'total': task_data.get('total', 0),
                            'successful': task_data.get('successful', 0),
                            'partial': task_data.get('partial', 0),
                            'failed': task_data.get('failed', 0),
                            
                            # æˆåŠŸç‡æŒ‡æ ‡
                            'success_rate': task_data.get('success_rate', 0),
                            'partial_rate': task_data.get('partial_rate', 0),
                            'failure_rate': task_data.get('failure_rate', 0),
                            'weighted_success_score': task_data.get('weighted_success_score', 0),
                            
                            # æ‰§è¡ŒæŒ‡æ ‡
                            'avg_execution_time': task_data.get('avg_execution_time', 0),
                            'avg_turns': task_data.get('avg_turns', 0),
                            'tool_coverage_rate': task_data.get('tool_coverage_rate', 0),
                            'avg_tool_calls': task_data.get('avg_tool_calls', 0),
                            
                            # å…ƒæ•°æ®
                            'timestamp': datetime.now().isoformat(),
                            'source': 'master_database.json'
                        }
                        
                        # ä½¿ç”¨æ­£ç¡®çš„å­—æ®µå
                        full_success = task_data.get('full_success', 0) or task_data.get('successful', 0)
                        partial_success = task_data.get('partial_success', 0) or task_data.get('partial', 0)
                        failures = task_data.get('total', 0) - full_success - partial_success
                        
                        # ä¸ºæ¯ä¸ªæˆåŠŸçš„æµ‹è¯•åˆ›å»ºè®°å½•ï¼ˆä¿æŒåŸå­æ€§ï¼‰
                        for i in range(full_success):
                            test_record = record.copy()
                            test_record['test_id'] = f"{model_name}_{prompt_type}_{rate}_{difficulty}_{task_type}_success_{i}"
                            test_record['success'] = True
                            test_record['partial_success'] = False
                            records.append(test_record)
                        
                        # ä¸ºæ¯ä¸ªéƒ¨åˆ†æˆåŠŸçš„æµ‹è¯•åˆ›å»ºè®°å½•
                        for i in range(partial_success):
                            test_record = record.copy()
                            test_record['test_id'] = f"{model_name}_{prompt_type}_{rate}_{difficulty}_{task_type}_partial_{i}"
                            test_record['success'] = False
                            test_record['partial_success'] = True
                            records.append(test_record)
                        
                        # ä¸ºæ¯ä¸ªå¤±è´¥çš„æµ‹è¯•åˆ›å»ºè®°å½•
                        for i in range(failures):
                            test_record = record.copy()
                            test_record['test_id'] = f"{model_name}_{prompt_type}_{rate}_{difficulty}_{task_type}_failed_{i}"
                            test_record['success'] = False
                            test_record['partial_success'] = False
                            records.append(test_record)
    
    return records

def convert_json_to_parquet():
    """ä¸»è½¬æ¢å‡½æ•°"""
    # è·¯å¾„é…ç½®
    json_path = Path("pilot_bench_cumulative_results/master_database.json")
    parquet_dir = Path("pilot_bench_parquet_data")
    parquet_dir.mkdir(exist_ok=True)
    
    # è¯»å–JSONæ•°æ®
    print(f"ğŸ“– è¯»å– {json_path}...")
    with open(json_path, 'r') as f:
        db_data = json.load(f)
    
    # ç»Ÿè®¡åŸå§‹æ•°æ®
    models_count = len(db_data.get('models', {}))
    total_tests = db_data.get('summary', {}).get('total_tests', 0)
    
    # è®¡ç®—å®é™…æµ‹è¯•æ•°
    actual_tests = 0
    for model_data in db_data.get('models', {}).values():
        actual_tests += model_data.get('total_tests', 0)
    
    print(f"  - æ¨¡å‹æ•°é‡: {models_count}")
    print(f"  - æ€»æµ‹è¯•æ•°: {total_tests if total_tests > 0 else actual_tests}")
    
    # å±•å¹³æ•°æ®
    print("\nğŸ”„ è½¬æ¢æ•°æ®ç»“æ„...")
    records = flatten_hierarchical_data(db_data)
    print(f"  - ç”Ÿæˆè®°å½•æ•°: {len(records)}")
    
    if not records:
        print("âš ï¸  æ²¡æœ‰æ•°æ®éœ€è¦è½¬æ¢")
        return False
    
    # åˆ›å»ºDataFrame
    df = pd.DataFrame(records)
    
    # ä¼˜åŒ–æ•°æ®ç±»å‹
    print("\nğŸ“Š ä¼˜åŒ–æ•°æ®ç±»å‹...")
    
    # ç±»åˆ«å‹æ•°æ®
    categorical_cols = ['model', 'prompt_type', 'difficulty', 'task_type', 'source']
    for col in categorical_cols:
        if col in df.columns:
            df[col] = df[col].astype('category')
    
    # å¸ƒå°”å‹æ•°æ®
    bool_cols = ['success', 'partial_success']
    for col in bool_cols:
        if col in df.columns:
            df[col] = df[col].astype(bool)
    
    # æµ®ç‚¹å‹æ•°æ®
    float_cols = ['tool_success_rate', 'success_rate', 'partial_rate', 'failure_rate', 
                  'weighted_success_score', 'avg_execution_time', 'avg_turns', 
                  'tool_coverage_rate', 'avg_tool_calls']
    for col in float_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype('float32')
    
    # æ•´å‹æ•°æ®
    int_cols = ['total', 'successful', 'partial', 'failed']
    for col in int_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0).astype('int32')
    
    # ä¿å­˜ä¸»æ•°æ®æ–‡ä»¶
    main_file = parquet_dir / "test_results.parquet"
    print(f"\nğŸ’¾ ä¿å­˜åˆ° {main_file}...")
    df.to_parquet(main_file, index=False, compression='snappy')
    
    # åˆ›å»ºæ±‡æ€»ç»Ÿè®¡æ–‡ä»¶
    print("\nğŸ“ˆ ç”Ÿæˆæ±‡æ€»ç»Ÿè®¡...")
    
    # æ¨¡å‹çº§åˆ«ç»Ÿè®¡
    model_stats = df.groupby('model').agg({
        'success': ['count', 'sum', 'mean'],
        'partial_success': 'sum',
        'avg_execution_time': 'mean',
        'tool_coverage_rate': 'mean'
    }).round(4)
    
    stats_file = parquet_dir / "model_stats.parquet"
    model_stats.to_parquet(stats_file)
    
    # åˆ›å»ºå…ƒæ•°æ®æ–‡ä»¶
    metadata = {
        'conversion_time': datetime.now().isoformat(),
        'source_file': str(json_path),
        'total_records': len(df),
        'models': df['model'].unique().tolist(),
        'prompt_types': df['prompt_type'].unique().tolist(),
        'difficulties': df['difficulty'].unique().tolist(),
        'task_types': df['task_type'].unique().tolist(),
        'data_schema': {
            'columns': list(df.columns),
            'dtypes': {col: str(dtype) for col, dtype in df.dtypes.items()}
        }
    }
    
    metadata_file = parquet_dir / "metadata.json"
    with open(metadata_file, 'w') as f:
        json.dump(metadata, f, indent=2)
    
    # éªŒè¯æ•°æ®
    print("\nâœ… è½¬æ¢å®Œæˆï¼éªŒè¯ç»“æœï¼š")
    print(f"  - ä¸»æ•°æ®æ–‡ä»¶: {main_file} ({main_file.stat().st_size / 1024:.1f} KB)")
    print(f"  - ç»Ÿè®¡æ–‡ä»¶: {stats_file} ({stats_file.stat().st_size / 1024:.1f} KB)")
    print(f"  - å…ƒæ•°æ®æ–‡ä»¶: {metadata_file}")
    
    # æ˜¾ç¤ºæ•°æ®æ‘˜è¦
    print("\nğŸ“‹ æ•°æ®æ‘˜è¦ï¼š")
    print(f"  - æ€»è®°å½•æ•°: {len(df)}")
    print(f"  - æ¨¡å‹æ•°: {df['model'].nunique()}")
    print(f"  - æˆåŠŸæµ‹è¯•: {df['success'].sum()}")
    print(f"  - éƒ¨åˆ†æˆåŠŸ: {df['partial_success'].sum()}")
    print(f"  - å¤±è´¥æµ‹è¯•: {(~df['success'] & ~df['partial_success']).sum()}")
    
    # æ˜¾ç¤ºå‰å‡ ä¸ªæ¨¡å‹çš„ç»Ÿè®¡
    print("\nğŸ† æ¨¡å‹æ€§èƒ½ï¼ˆå‰5ä¸ªï¼‰ï¼š")
    top_models = df.groupby('model')['success'].agg(['count', 'mean']).sort_values('mean', ascending=False).head()
    for model, stats in top_models.iterrows():
        print(f"  - {model}: {stats['count']:.0f} æµ‹è¯•, {stats['mean']*100:.1f}% æˆåŠŸç‡")
    
    return True

if __name__ == "__main__":
    success = convert_json_to_parquet()
    
    if success:
        print("\n" + "="*60)
        print("ğŸ‰ è½¬æ¢æˆåŠŸï¼ç°åœ¨å¯ä»¥ä½¿ç”¨Parquetæ ¼å¼è¿›è¡Œå¢é‡æµ‹è¯•")
        print("="*60)
        print("\nä¸‹ä¸€æ­¥ï¼š")
        print("1. è®¾ç½®ç¯å¢ƒå˜é‡: export STORAGE_FORMAT=parquet")
        print("2. è¿è¡Œå¢é‡æµ‹è¯•: python smart_batch_runner.py ...")
        print("3. æŸ¥çœ‹æ•°æ®: python view_parquet_data.py")
    else:
        print("\nâŒ è½¬æ¢å¤±è´¥ï¼Œè¯·æ£€æŸ¥æºæ–‡ä»¶")