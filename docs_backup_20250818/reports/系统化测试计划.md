# 系统化测试计划

## 实验目标
对表格中所有模型进行系统化的工作流质量测试，评估其在不同任务类型和难度下的表现。

## 测试阶段

### 第一阶段：Optimal Prompt 基准测试
**目标**: 建立模型的基准性能
**测试配置**:
- Prompt类型: `optimal`
- 任务类型: 全部5种 (`simple_task`, `basic_task`, `data_pipeline`, `api_integration`, `multi_stage_pipeline`)
- 每种任务类型: 20个实例
- 总测试数: 100个测试/模型
- 难度: 默认（不指定）

**测试脚本**:
```bash
# 基准测试脚本
python batch_test_runner.py \
    --model <MODEL_NAME> \
    --test-type optimal \
    --task-types all \
    --num-instances 20 \
    --output-dir baseline_results
```

### 第二阶段：Easy难度测试
**目标**: 评估模型在简单任务上的表现
**测试配置**:
- Prompt类型: `optimal`
- 任务类型: 全部5种
- 每种任务类型: 20个实例
- 总测试数: 100个测试/模型
- 难度: `easy`

**测试脚本**:
```bash
# Easy难度测试脚本
python batch_test_runner.py \
    --model <MODEL_NAME> \
    --test-type optimal \
    --task-types all \
    --num-instances 20 \
    --difficulty easy \
    --output-dir easy_results
```

### 第三阶段：缺陷工作流测试（后续）
**目标**: 评估模型对缺陷工作流的处理能力
**测试配置**:
- Prompt类型: 7种缺陷类型
- 任务类型: 全部5种
- 每种组合: 10个实例
- 总测试数: 350个测试/模型

## 模型列表

### 闭源模型（优先级高）
1. GPT-4o
2. GPT-o1
3. GPT-o3
4. Claude-Opus-4
5. Claude-Sonnet-4
6. Gemini-2.5-Pro
7. GPT-4o-mini
8. Claude-Sonnet-3.7
9. Claude-Haiku-3.5
10. Gemini-2.5-Flash

### 开源模型
1. DeepSeek-V3-671B ✓ (已部分测试)
2. DeepSeek-R1-671B ✗ (性能问题)
3. Qwen2.5-72B-Instruct
4. Llama-3.3-70B-Instruct
5. Qwen2.5-32B-Instruct
6. Llama-4-Scout-17B
7. Qwen2.5-14B-Instruct
8. Qwen2.5-7B-Instruct
9. Qwen2.5-3B-Instruct ✗ (性能太差)

## 批量测试脚本

### 1. 单模型完整测试
```bash
#!/bin/bash
# test_single_model.sh

MODEL=$1
PHASE=$2  # baseline or easy

if [ "$PHASE" == "baseline" ]; then
    python batch_test_runner.py \
        --model $MODEL \
        --test-config configs/optimal_baseline.json \
        --output-dir results/${MODEL}_baseline
elif [ "$PHASE" == "easy" ]; then
    python batch_test_runner.py \
        --model $MODEL \
        --test-config configs/optimal_easy.json \
        --output-dir results/${MODEL}_easy
fi
```

### 2. 批量模型测试
```bash
#!/bin/bash
# test_all_models.sh

# 闭源模型列表
CLOSED_MODELS=(
    "gpt-4o"
    "gpt-o1"
    "claude-opus-4"
    "claude-sonnet-4"
    "gemini-2.5-pro"
)

# 开源模型列表
OPEN_MODELS=(
    "deepseek-v3-671b"
    "qwen2.5-72b-instruct"
    "llama-3.3-70b-instruct"
)

# 测试所有模型的baseline
for model in "${CLOSED_MODELS[@]}" "${OPEN_MODELS[@]}"; do
    echo "Testing $model - Baseline"
    ./test_single_model.sh $model baseline
done

# 测试所有模型的easy难度
for model in "${CLOSED_MODELS[@]}" "${OPEN_MODELS[@]}"; do
    echo "Testing $model - Easy"
    ./test_single_model.sh $model easy
done
```

## 测试配置文件

### configs/optimal_baseline.json
```json
{
    "prompt_type": "optimal",
    "task_types": ["simple_task", "basic_task", "data_pipeline", "api_integration", "multi_stage_pipeline"],
    "instances_per_task": 20,
    "timeout": 30,
    "max_workers": 5,
    "save_logs": true,
    "difficulty": null
}
```

### configs/optimal_easy.json
```json
{
    "prompt_type": "optimal",
    "task_types": ["simple_task", "basic_task", "data_pipeline", "api_integration", "multi_stage_pipeline"],
    "instances_per_task": 20,
    "timeout": 30,
    "max_workers": 5,
    "save_logs": true,
    "difficulty": "easy"
}
```

## 执行计划

### 第一周：闭源模型测试
- Day 1-2: GPT系列 (GPT-4o, GPT-o1, GPT-o3, GPT-4o-mini)
- Day 3-4: Claude系列 (Opus-4, Sonnet-4, Sonnet-3.7, Haiku-3.5)
- Day 5: Gemini系列 (2.5-Pro, 2.5-Flash)

### 第二周：开源模型测试
- Day 1: DeepSeek-V3-671B (完成剩余测试)
- Day 2: Qwen2.5-72B-Instruct
- Day 3: Llama-3.3-70B-Instruct
- Day 4: 中等规模模型 (32B, 17B, 14B)
- Day 5: 小规模模型 (7B)

## 数据收集指标
1. **成功率指标**
   - 总体成功率
   - 完全成功率
   - 部分成功率
   - 失败率

2. **质量指标**
   - 加权成功分数
   - 工作流质量分数
   - Phase2分数

3. **执行指标**
   - 平均执行步数
   - 工具覆盖率
   - 平均响应时间

4. **错误分析**
   - 错误类型分布
   - 超时率
   - 格式错误率

## 结果输出
- 每个模型的详细测试报告
- 汇总到 `综合实验评估计划.md` 的表格
- 错误分析报告
- 性能对比图表

## 注意事项
1. 确保API配额充足
2. 监控测试进度，及时处理异常
3. 定期备份测试结果
4. 对于性能极差的模型（如成功率<5%），可提前终止测试