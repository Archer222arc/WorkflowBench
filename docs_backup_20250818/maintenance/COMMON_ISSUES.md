# å¸¸è§é—®é¢˜è§£å†³æ–¹æ¡ˆ (Common Issues & Solutions)

## ğŸ“‹ ç›®å½•
1. [å¯åŠ¨å’Œé…ç½®é—®é¢˜](#å¯åŠ¨å’Œé…ç½®é—®é¢˜)
2. [è¿è¡Œæ—¶é”™è¯¯](#è¿è¡Œæ—¶é”™è¯¯)
3. [ç»Ÿè®¡å’Œæ•°æ®é—®é¢˜](#ç»Ÿè®¡å’Œæ•°æ®é—®é¢˜)
4. [æ€§èƒ½é—®é¢˜](#æ€§èƒ½é—®é¢˜)
5. [å¿«é€Ÿä¿®å¤è„šæœ¬](#å¿«é€Ÿä¿®å¤è„šæœ¬)

---

## å¯åŠ¨å’Œé…ç½®é—®é¢˜

### é—®é¢˜1: ModuleNotFoundError
```
ModuleNotFoundError: No module named 'openai'
```
**è§£å†³æ–¹æ¡ˆ**:
```bash
pip install openai
pip install -r requirements.txt  # å¦‚æœæœ‰requirementsæ–‡ä»¶
```

### é—®é¢˜2: APIå¯†é’¥é…ç½®é”™è¯¯
```
ValueError: OpenAI API key not configured
```
**è§£å†³æ–¹æ¡ˆ**:
1. æ£€æŸ¥ `config/config.json`:
```json
{
    "openai_api_key": "your-key-here",
    "idealab_api_key": "your-key-here"
}
```
2. æˆ–è®¾ç½®ç¯å¢ƒå˜é‡:
```bash
export OPENAI_API_KEY="your-key-here"
export IDEALAB_API_KEY="your-key-here"
```

### é—®é¢˜3: ä»»åŠ¡åº“æ–‡ä»¶ç¼ºå¤±
```
FileNotFoundError: mcp_generated_library/task_library.json
```
**è§£å†³æ–¹æ¡ˆ**:
```python
# æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
import os
task_library_path = "mcp_generated_library/task_library.json"
if not os.path.exists(task_library_path):
    print("Task library missing! Please generate it first.")
```

### é—®é¢˜4: æ¨¡å‹ä¸æ”¯æŒ
```
ValueError: Unsupported model: xxx
```
**è§£å†³æ–¹æ¡ˆ**:
æ£€æŸ¥ `api_client_manager.py` ä¸­çš„ `SUPPORTED_MODELS` åˆ—è¡¨

---

## è¿è¡Œæ—¶é”™è¯¯

### é—®é¢˜1: AttributeError - ç¼ºå°‘ format_error_count
```python
AttributeError: 'TestRecord' object has no attribute 'format_error_count'
```
**åŸå› **: TestRecord åˆ›å»ºæ—¶æœªè®¾ç½®è¯¥å­—æ®µ

**è§£å†³æ–¹æ¡ˆ**:
```python
# batch_test_runner.py
record.format_error_count = result.get('format_error_count', 0)
record.api_issues = result.get('api_issues', [])
record.executed_tools = result.get('executed_tools', [])
```

### é—®é¢˜2: KeyError - assisted_attempt
```python
KeyError: 'assisted_attempt'
```
**åŸå› **: æ—§ç‰ˆæœ¬ä½¿ç”¨äº† assisted_attemptï¼Œæ–°ç‰ˆæœ¬æ”¹ä¸º assisted_failure/assisted_success

**è§£å†³æ–¹æ¡ˆ**:
```python
# ä¸è¦ä½¿ç”¨
stats.assisted_attempt

# ä½¿ç”¨
stats.assisted_failure  # å¾—åˆ°å¸®åŠ©ä½†å¤±è´¥
stats.assisted_success  # å¾—åˆ°å¸®åŠ©åæˆåŠŸ
```

### é—®é¢˜3: TypeError - NoneType è¿ç®—
```python
TypeError: unsupported operand type(s) for +: 'NoneType' and 'float'
```
**åŸå› **: åˆ†æ•°å­—æ®µå¯èƒ½ä¸º None

**è§£å†³æ–¹æ¡ˆ**:
```python
# æ·»åŠ  None æ£€æŸ¥
if score is not None:
    total_score += score

# æˆ–ä½¿ç”¨é»˜è®¤å€¼
score = record.get('workflow_score') or 0.0
```

### é—®é¢˜4: å¹¶å‘å†™å…¥å†²çª
```
PermissionError: [Errno 13] Permission denied: 'master_database.json'
```
**åŸå› **: å¤šä¸ªè¿›ç¨‹åŒæ—¶å†™å…¥åŒä¸€æ–‡ä»¶

**è§£å†³æ–¹æ¡ˆ**:
```python
# cumulative_test_manager.py
import uuid
temp_file = self.db_file.parent / f"{self.db_file.stem}_{uuid.uuid4().hex}.tmp"
```

---

## ç»Ÿè®¡å’Œæ•°æ®é—®é¢˜

### é—®é¢˜1: ç»Ÿè®¡æ•°æ®ä¸ä¸€è‡´
**ç—‡çŠ¶**: total_tests != full_success + partial_success + failure

**æ£€æŸ¥æ­¥éª¤**:
```python
def validate_statistics(stats):
    total = stats.full_success + stats.partial_success + stats.failure
    assert stats.total_tests == total, f"Inconsistent: {stats.total_tests} != {total}"
```

**è§£å†³æ–¹æ¡ˆ**:
ç¡®ä¿ update_from_test() ä¸­æ²¡æœ‰æ¡ä»¶åˆ†æ”¯å½±å“ total_tests è®¡æ•°:
```python
# æ­£ç¡® âœ…
self.overall_success.total_tests += 1
if success_level == 'full_success':
    self.overall_success.full_success += 1

# é”™è¯¯ âŒ
if success_level == 'assisted_attempt':
    self.overall_success.assisted_attempt += 1
else:
    self.overall_success.total_tests += 1  # æ¡ä»¶åˆ†æ”¯ï¼
```

### é—®é¢˜2: tool_coverage_rate ä¸º 0
**åŸå› **: æ²¡æœ‰æ­£ç¡®è®°å½•å·¥å…·ä½¿ç”¨

**è§£å†³æ–¹æ¡ˆ**:
```python
# cumulative_data_structure.py
# ä¼˜å…ˆä½¿ç”¨ executed_tools
tools_to_track = test_record.get('executed_tools', test_record.get('tool_calls', []))
```

### é—®é¢˜3: é”™è¯¯åˆ†ç±»å…¨ä¸º 0
**åŸå› **: é”™è¯¯æ¶ˆæ¯è¢«è¦†ç›–æˆ– API é”™è¯¯è¢«é”™è¯¯åˆ†ç±»

**è§£å†³æ–¹æ¡ˆ**:
1. ä¿ç•™åŸå§‹é”™è¯¯æ¶ˆæ¯
2. API é”™è¯¯è®°å½•åˆ° api_issuesï¼Œä¸è®¡å…¥å·¥ä½œæµé”™è¯¯

---

## æ€§èƒ½é—®é¢˜

### é—®é¢˜1: API è°ƒç”¨é¢‘ç¹å¤±è´¥
**ç—‡çŠ¶**: å¤§é‡ 400 æˆ–é™æµé”™è¯¯

**è§£å†³æ–¹æ¡ˆ**:
```python
# interactive_executor.py
# å¢åŠ é‡è¯•å’Œä¼˜åŒ–é—´éš”
max_retries = 5  # å¢åŠ é‡è¯•æ¬¡æ•°
base_wait = random.uniform(0.5, 1.5)  # å‡å°‘åŸºç¡€ç­‰å¾…
wait_time = min(base_wait * (1.5 ** attempt), 10)  # æ¸©å’Œå¢é•¿
```

### é—®é¢˜2: æµ‹è¯•æ‰§è¡Œç¼“æ…¢
**åŸå› **: ä¸²è¡Œæ‰§è¡Œæˆ– QPS é™åˆ¶è¿‡ä¸¥

**è§£å†³æ–¹æ¡ˆ**:
```bash
# ä½¿ç”¨å¹¶å‘æ‰§è¡Œ
python batch_test_runner.py --concurrent --workers 20 --qps 10
```

### é—®é¢˜3: å†…å­˜å ç”¨è¿‡å¤§
**åŸå› **: ä¿å­˜äº†æ‰€æœ‰æµ‹è¯•å®ä¾‹

**è§£å†³æ–¹æ¡ˆ**:
ä½¿ç”¨ç´¯ç§¯ç»Ÿè®¡è€Œä¸æ˜¯ä¿å­˜å®ä¾‹:
```python
# ä¸è¦ä¿å­˜å®ä¾‹
"instances": []  # ä¿æŒä¸ºç©º

# åªæ›´æ–°ç»Ÿè®¡
stats["total"] += 1
stats["success"] += 1 if success else 0
```

---

## å¿«é€Ÿä¿®å¤è„šæœ¬

### 1. æ¸…ç†æŸåçš„æ•°æ®åº“
```python
#!/usr/bin/env python3
"""fix_database.py - ä¿®å¤æŸåçš„æ•°æ®åº“"""

import json
from pathlib import Path
from datetime import datetime

def fix_database():
    db_path = Path("pilot_bench_cumulative_results/master_database.json")
    
    # å¤‡ä»½
    backup_path = db_path.parent / f"backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    if db_path.exists():
        import shutil
        shutil.copy(db_path, backup_path)
        print(f"Backed up to {backup_path}")
    
    # å°è¯•ä¿®å¤
    try:
        with open(db_path) as f:
            data = json.load(f)
        
        # ä¿®å¤ç¼ºå¤±å­—æ®µ
        for model_name, model_stats in data.get('models', {}).items():
            if 'overall_success' in model_stats:
                success = model_stats['overall_success']
                # æ·»åŠ æ–°å­—æ®µ
                if 'assisted_failure' not in success:
                    success['assisted_failure'] = 0
                if 'assisted_success' not in success:
                    success['assisted_success'] = 0
                if 'total_assisted_turns' not in success:
                    success['total_assisted_turns'] = 0
                if 'tests_with_assistance' not in success:
                    success['tests_with_assistance'] = 0
        
        # ä¿å­˜ä¿®å¤åçš„æ•°æ®
        with open(db_path, 'w') as f:
            json.dump(data, f, indent=2)
        print("Database fixed successfully")
        
    except Exception as e:
        print(f"Failed to fix database: {e}")
        print(f"Restore from backup: {backup_path}")

if __name__ == "__main__":
    fix_database()
```

### 2. éªŒè¯æ•°æ®ä¸€è‡´æ€§
```python
#!/usr/bin/env python3
"""validate_stats.py - éªŒè¯ç»Ÿè®¡ä¸€è‡´æ€§"""

import json
from pathlib import Path

def validate_database():
    db_path = Path("pilot_bench_cumulative_results/master_database.json")
    
    with open(db_path) as f:
        data = json.load(f)
    
    issues = []
    
    for model_name, model_stats in data.get('models', {}).items():
        if 'overall_success' in model_stats:
            s = model_stats['overall_success']
            
            # æ£€æŸ¥åŸºæœ¬ç»Ÿè®¡
            total = s.get('full_success', 0) + s.get('partial_success', 0) + s.get('failure', 0)
            if s.get('total_tests', 0) != total:
                issues.append(f"{model_name}: total_tests mismatch")
            
            # æ£€æŸ¥ assisted ç»Ÿè®¡
            assisted_total = s.get('assisted_success', 0) + s.get('assisted_failure', 0)
            if assisted_total > s.get('total_tests', 0):
                issues.append(f"{model_name}: assisted > total")
    
    if issues:
        print("Issues found:")
        for issue in issues:
            print(f"  - {issue}")
    else:
        print("All statistics are consistent âœ“")

if __name__ == "__main__":
    validate_database()
```

### 3. è¿ç§»æ—§æ•°æ®æ ¼å¼
```python
#!/usr/bin/env python3
"""migrate_data.py - è¿ç§»æ—§æ ¼å¼æ•°æ®"""

def migrate_assisted_attempt_to_new_format(data):
    """å°† assisted_attempt è¿ç§»åˆ° assisted_failure"""
    
    for model_stats in data.get('models', {}).values():
        if 'overall_success' in model_stats:
            success = model_stats['overall_success']
            
            # è¿ç§» assisted_attempt -> assisted_failure
            if 'assisted_attempt' in success:
                success['assisted_failure'] = success.pop('assisted_attempt')
                success['assisted_success'] = 0
                success['total_assisted_turns'] = 0
                success['tests_with_assistance'] = success['assisted_failure']
    
    return data
```

---

## å¸¸ç”¨è°ƒè¯•æŠ€å·§

### 1. æ·»åŠ è°ƒè¯•è¾“å‡º
```python
# åœ¨å…³é”®ä½ç½®æ·»åŠ 
print(f"[DEBUG] Variable: {variable_name}")
print(f"[DEBUG] Type: {type(variable_name)}")
print(f"[DEBUG] Keys: {variable_name.keys() if isinstance(variable_name, dict) else 'N/A'}")
```

### 2. ä½¿ç”¨ pdb è°ƒè¯•å™¨
```python
import pdb
pdb.set_trace()  # è®¾ç½®æ–­ç‚¹
```

### 3. è®°å½•è¯¦ç»†æ—¥å¿—
```python
import logging
logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)
logger.debug(f"Processing: {item}")
```

### 4. å•å…ƒæµ‹è¯•ç‰¹å®šåŠŸèƒ½
```python
# åˆ›å»º test_specific.py
def test_error_classification():
    from cumulative_data_structure import ErrorMetrics
    
    metrics = ErrorMetrics()
    metrics.categorize_error("TIMEOUT: Operation timed out")
    assert metrics.timeout_errors == 1
    print("Test passed âœ“")

if __name__ == "__main__":
    test_error_classification()
```

---

## é¢„é˜²æªæ–½

### 1. æäº¤å‰æ£€æŸ¥æ¸…å•
- [ ] è¿è¡Œå•ä¸ªæµ‹è¯•ç¡®è®¤åŸºæœ¬åŠŸèƒ½
- [ ] æ£€æŸ¥æ‰€æœ‰æ–°å­—æ®µéƒ½æœ‰é»˜è®¤å€¼
- [ ] ç¡®è®¤æ²¡æœ‰é—ç•™çš„ print è°ƒè¯•è¯­å¥
- [ ] éªŒè¯ç»Ÿè®¡ä¸€è‡´æ€§

### 2. å®šæœŸç»´æŠ¤
- æ¯å‘¨å¤‡ä»½ master_database.json
- å®šæœŸè¿è¡ŒéªŒè¯è„šæœ¬
- è®°å½•æ‰€æœ‰å·²çŸ¥é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### 3. ç›‘æ§æŒ‡æ ‡
- API è°ƒç”¨æˆåŠŸç‡
- å¹³å‡æ‰§è¡Œæ—¶é—´
- é”™è¯¯åˆ†ç±»åˆ†å¸ƒ
- assisted ç»Ÿè®¡è¶‹åŠ¿

---

**æ–‡æ¡£åˆ›å»ºæ—¶é—´**: 2025-01-08
**æœ€åæ›´æ–°**: 2025-01-08
**ç‰ˆæœ¬**: 1.0