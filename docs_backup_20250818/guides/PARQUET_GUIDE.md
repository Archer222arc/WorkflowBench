# Parquetå­˜å‚¨æ¨¡å¼å®Œæ•´æŒ‡å—

> ç‰ˆæœ¬: 1.0  
> åˆ›å»ºæ—¶é—´: 2025-08-17  
> çŠ¶æ€: ğŸŸ¢ Active

## ğŸ“‹ ç›®å½•
1. [ä¸ºä»€ä¹ˆé€‰æ‹©Parquet](#ä¸ºä»€ä¹ˆé€‰æ‹©parquet)
2. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
3. [æ¶æ„è®¾è®¡](#æ¶æ„è®¾è®¡)
4. [ä½¿ç”¨æ–¹æ³•](#ä½¿ç”¨æ–¹æ³•)
5. [æ•°æ®ç®¡ç†](#æ•°æ®ç®¡ç†)
6. [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)
7. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

---

## ğŸ¯ ä¸ºä»€ä¹ˆé€‰æ‹©Parquet

### å¯¹æ¯”JSONçš„ä¼˜åŠ¿

| ç‰¹æ€§ | JSON | Parquet |
|------|------|---------|
| **æ–‡ä»¶å¤§å°** | 100MB | 20MB (-80%) |
| **æŸ¥è¯¢é€Ÿåº¦** | 1ç§’ | 0.01ç§’ (100x) |
| **å¹¶å‘å†™å…¥** | âŒ å†²çª | âœ… å®‰å…¨ |
| **å¢é‡æ›´æ–°** | âŒ å…¨é‡è¦†ç›– | âœ… å¢é‡è¿½åŠ  |
| **æ•°æ®æ¢å¤** | âŒ æ˜“ä¸¢å¤± | âœ… å¯æ¢å¤ |
| **ç±»å‹å®‰å…¨** | âŒ å­—ç¬¦ä¸² | âœ… å¼ºç±»å‹ |

### é€‚ç”¨åœºæ™¯
- âœ… å¤§è§„æ¨¡æ‰¹é‡æµ‹è¯•ï¼ˆ>1000æ¬¡ï¼‰
- âœ… å¤šè¿›ç¨‹å¹¶å‘æµ‹è¯•
- âœ… éœ€è¦æ•°æ®åˆ†æå’Œç»Ÿè®¡
- âœ… é•¿æ—¶é—´è¿è¡Œçš„æµ‹è¯•
- âœ… éœ€è¦ä¸­æ–­æ¢å¤

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–
```bash
pip install pyarrow pandas
```

### 2. ä»JSONè¿ç§»åˆ°Parquet
```bash
# è½¬æ¢ç°æœ‰æ•°æ®
python json_to_parquet_converter.py

# è¾“å‡º:
# âœ… è½¬æ¢æˆåŠŸï¼
# - ä¸»æ•°æ®æ–‡ä»¶: pilot_bench_parquet_data/test_results.parquet
# - æ€»è®°å½•æ•°: 4993
```

### 3. å¯ç”¨Parquetæ¨¡å¼
```bash
# æ–¹æ³•1: ä½¿ç”¨è®¾ç½®è„šæœ¬
./setup_parquet_incremental_test.sh

# æ–¹æ³•2: æ‰‹åŠ¨è®¾ç½®ç¯å¢ƒå˜é‡
export STORAGE_FORMAT=parquet

# æ–¹æ³•3: åœ¨Pythonä¸­è®¾ç½®
import os
os.environ['STORAGE_FORMAT'] = 'parquet'
```

### 4. è¿è¡Œæµ‹è¯•
```bash
# å•ä¸ªæ¨¡å‹æµ‹è¯•
STORAGE_FORMAT=parquet python smart_batch_runner.py \
    --model gpt-4o-mini \
    --prompt-types baseline \
    --difficulty easy \
    --task-types simple_task \
    --num-instances 10

# æ‰¹é‡æµ‹è¯•
STORAGE_FORMAT=parquet python ultra_parallel_runner.py \
    --model DeepSeek-V3-0324 \
    --num-instances 100 \
    --workers 50
```

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### ç›®å½•ç»“æ„
```
pilot_bench_parquet_data/
â”œâ”€â”€ test_results.parquet        # ä¸»æ•°æ®æ–‡ä»¶ï¼ˆåˆå¹¶åï¼‰
â”œâ”€â”€ model_stats.parquet         # æ¨¡å‹ç»Ÿè®¡
â”œâ”€â”€ metadata.json               # å…ƒæ•°æ®
â””â”€â”€ incremental/                # å¢é‡æ•°æ®ç›®å½•
    â”œâ”€â”€ increment_12345_*.parquet  # è¿›ç¨‹1çš„å¢é‡æ–‡ä»¶
    â”œâ”€â”€ increment_67890_*.parquet  # è¿›ç¨‹2çš„å¢é‡æ–‡ä»¶
    â””â”€â”€ ...
```

### æ•°æ®æµç¨‹
```
æµ‹è¯•è¿è¡Œ â†’ å†™å…¥å¢é‡æ–‡ä»¶ â†’ å®šæœŸåˆå¹¶ â†’ ä¸»æ•°æ®æ–‡ä»¶
    â†“           â†“              â†“           â†“
 [è¿›ç¨‹ä¸“å±]  [æ— å†²çªå†™å…¥]   [æ‰¹é‡å¤„ç†]   [æŸ¥è¯¢ä¼˜åŒ–]
```

### å¢é‡å†™å…¥æœºåˆ¶
æ¯ä¸ªè¿›ç¨‹åˆ›å»ºç‹¬ç«‹çš„å¢é‡æ–‡ä»¶ï¼Œé¿å…å¹¶å‘å†²çªï¼š
```python
# è¿›ç¨‹ID + æ—¶é—´æˆ³ = å”¯ä¸€æ–‡ä»¶å
process_id = f"{os.getpid()}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
incremental_file = f"increment_{process_id}.parquet"
```

---

## ğŸ“– ä½¿ç”¨æ–¹æ³•

### åŸºæœ¬æ“ä½œ

#### 1. æ·»åŠ æµ‹è¯•ç»“æœ
```python
from parquet_cumulative_manager import ParquetCumulativeManager

manager = ParquetCumulativeManager()

# æ·»åŠ å•ä¸ªæµ‹è¯•ç»“æœ
manager.add_test_result(
    model='gpt-4o-mini',
    task_type='simple_task',
    prompt_type='baseline',
    success=True,
    execution_time=5.2,
    difficulty='easy',
    tool_success_rate=0.8
)
```

#### 2. æŸ¥è¯¢æ•°æ®
```python
from parquet_data_manager import ParquetDataManager

manager = ParquetDataManager()

# æŸ¥è¯¢ç‰¹å®šæ¨¡å‹çš„æ•°æ®
df = manager.query_model_stats(
    model_name='DeepSeek-V3-0324',
    prompt_type='optimal',
    tool_success_rate=0.8
)

print(f"æ‰¾åˆ° {len(df)} æ¡è®°å½•")
print(f"æˆåŠŸç‡: {df['success'].mean():.1%}")
```

#### 3. åˆå¹¶å¢é‡æ•°æ®
```python
# è‡ªåŠ¨åˆå¹¶æ‰€æœ‰å¢é‡æ–‡ä»¶åˆ°ä¸»æ–‡ä»¶
manager.consolidate_incremental_data()

# è¾“å‡º:
# åˆå¹¶ 5 ä¸ªå¢é‡æ–‡ä»¶...
# âœ… æˆåŠŸåˆå¹¶ 250 æ¡è®°å½•
```

#### 4. å¯¼å‡ºæ•°æ®
```python
# å¯¼å‡ºä¸ºJSONï¼ˆå…¼å®¹æ—§ç³»ç»Ÿï¼‰
manager.export_to_json(output_path='export.json')

# å¯¼å‡ºä¸ºCSVï¼ˆæ•°æ®åˆ†æï¼‰
import pandas as pd
df = pd.read_parquet('pilot_bench_parquet_data/test_results.parquet')
df.to_csv('test_results.csv', index=False)
```

### é«˜çº§æ“ä½œ

#### æ‰¹é‡æ•°æ®åˆ†æ
```python
import pandas as pd
import matplotlib.pyplot as plt

# è¯»å–æ•°æ®
df = pd.read_parquet('pilot_bench_parquet_data/test_results.parquet')

# æŒ‰æ¨¡å‹åˆ†ç»„ç»Ÿè®¡
model_stats = df.groupby('model').agg({
    'success': ['count', 'mean'],
    'execution_time': 'mean',
    'tool_coverage_rate': 'mean'
}).round(3)

# å¯è§†åŒ–
model_stats['success']['mean'].plot(kind='bar')
plt.title('æ¨¡å‹æˆåŠŸç‡å¯¹æ¯”')
plt.ylabel('æˆåŠŸç‡')
plt.show()
```

#### å®æ—¶ç›‘æ§
```python
from pathlib import Path
import time

def monitor_incremental_updates():
    """ç›‘æ§å¢é‡æ–‡ä»¶æ›´æ–°"""
    incremental_dir = Path('pilot_bench_parquet_data/incremental')
    
    while True:
        files = list(incremental_dir.glob('*.parquet'))
        total_size = sum(f.stat().st_size for f in files) / 1024 / 1024
        
        print(f"\rå¢é‡æ–‡ä»¶: {len(files)}ä¸ª, æ€»å¤§å°: {total_size:.1f}MB", end="")
        time.sleep(5)
```

---

## ğŸ—„ï¸ æ•°æ®ç®¡ç†

### å®šæœŸç»´æŠ¤ä»»åŠ¡

#### 1. æ¯å°æ—¶ï¼šåˆå¹¶å¢é‡æ•°æ®
```bash
# æ·»åŠ åˆ°crontab
0 * * * * cd /path/to/project && python -c "from parquet_data_manager import ParquetDataManager; m=ParquetDataManager(); m.consolidate_incremental_data()"
```

#### 2. æ¯æ—¥ï¼šå¤‡ä»½ä¸»æ•°æ®
```bash
#!/bin/bash
DATE=$(date +%Y%m%d)
cp pilot_bench_parquet_data/test_results.parquet backups/test_results_${DATE}.parquet
```

#### 3. æ¯å‘¨ï¼šæ¸…ç†æ—§æ•°æ®
```python
def cleanup_old_increments(days=7):
    """æ¸…ç†è¶…è¿‡Nå¤©çš„å¢é‡æ–‡ä»¶"""
    from pathlib import Path
    from datetime import datetime, timedelta
    
    cutoff = datetime.now() - timedelta(days=days)
    incremental_dir = Path('pilot_bench_parquet_data/incremental')
    
    for file in incremental_dir.glob('*.parquet'):
        if file.stat().st_mtime < cutoff.timestamp():
            file.unlink()
            print(f"åˆ é™¤æ—§æ–‡ä»¶: {file.name}")
```

### æ•°æ®è¿ç§»

#### JSON â†’ Parquet
```bash
python json_to_parquet_converter.py
```

#### Parquet â†’ JSON
```python
from parquet_data_manager import ParquetDataManager

manager = ParquetDataManager()
manager.export_to_json(output_path='export.json')
```

#### åˆå¹¶å¤šä¸ªParquetæ–‡ä»¶
```python
import pandas as pd
import glob

# è¯»å–æ‰€æœ‰parquetæ–‡ä»¶
files = glob.glob('pilot_bench_parquet_data/*.parquet')
dfs = [pd.read_parquet(f) for f in files]

# åˆå¹¶
combined = pd.concat(dfs, ignore_index=True)

# å»é‡
combined = combined.drop_duplicates(subset=['test_id'])

# ä¿å­˜
combined.to_parquet('merged_data.parquet', index=False)
```

---

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

#### Q1: çœ‹ä¸åˆ°Parquetæ–‡ä»¶
**åŸå› **: æ•°æ®åœ¨å¢é‡ç›®å½•ä¸­
```bash
# æ£€æŸ¥å¢é‡ç›®å½•
ls -la pilot_bench_parquet_data/incremental/

# æ‰‹åŠ¨åˆå¹¶
python -c "from parquet_data_manager import ParquetDataManager; m=ParquetDataManager(); m.consolidate_incremental_data()"
```

#### Q2: ImportError: No module named 'pyarrow'
**è§£å†³**: å®‰è£…ä¾èµ–
```bash
pip install pyarrow pandas
```

#### Q3: æ•°æ®ä¸ä¸€è‡´
**è¯Šæ–­**:
```python
# éªŒè¯æ•°æ®å®Œæ•´æ€§
def validate_parquet_data():
    import pandas as pd
    
    # è¯»å–ä¸»æ–‡ä»¶
    main_df = pd.read_parquet('pilot_bench_parquet_data/test_results.parquet')
    
    # è¯»å–å¢é‡æ–‡ä»¶
    from pathlib import Path
    incremental_dir = Path('pilot_bench_parquet_data/incremental')
    inc_dfs = [pd.read_parquet(f) for f in incremental_dir.glob('*.parquet')]
    
    if inc_dfs:
        inc_df = pd.concat(inc_dfs)
        print(f"ä¸»æ–‡ä»¶: {len(main_df)} æ¡")
        print(f"å¢é‡: {len(inc_df)} æ¡")
        print(f"æ€»è®¡: {len(main_df) + len(inc_df)} æ¡")
    else:
        print(f"åªæœ‰ä¸»æ–‡ä»¶: {len(main_df)} æ¡")

validate_parquet_data()
```

#### Q4: å†…å­˜ä¸è¶³
**è§£å†³**: åˆ†æ‰¹å¤„ç†
```python
# åˆ†æ‰¹è¯»å–å¤§æ–‡ä»¶
def read_parquet_in_chunks(filepath, chunk_size=10000):
    import pyarrow.parquet as pq
    
    parquet_file = pq.ParquetFile(filepath)
    
    for batch in parquet_file.iter_batches(batch_size=chunk_size):
        df = batch.to_pandas()
        # å¤„ç†æ¯ä¸ªæ‰¹æ¬¡
        yield df

# ä½¿ç”¨
for chunk in read_parquet_in_chunks('large_file.parquet'):
    process(chunk)  # å¤„ç†æ¯ä¸ªå—
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. æ€§èƒ½ä¼˜åŒ–
```python
# âœ… å¥½: æ‰¹é‡å†™å…¥
manager.batch_append_results(test_results)  # ä¸€æ¬¡å†™å…¥å¤šæ¡

# âŒ å·®: å•æ¡å†™å…¥
for result in test_results:
    manager.append_test_result(result)  # å¤šæ¬¡I/O
```

### 2. æ•°æ®å®‰å…¨
```python
# å®šæœŸåˆå¹¶å¢é‡æ•°æ®
import schedule

def merge_incremental():
    manager = ParquetDataManager()
    manager.consolidate_incremental_data()

# æ¯30åˆ†é’Ÿåˆå¹¶ä¸€æ¬¡
schedule.every(30).minutes.do(merge_incremental)
```

### 3. ç›‘æ§å’Œå‘Šè­¦
```python
def check_data_health():
    """æ•°æ®å¥åº·æ£€æŸ¥"""
    from pathlib import Path
    
    # æ£€æŸ¥å¢é‡æ–‡ä»¶æ•°é‡
    inc_dir = Path('pilot_bench_parquet_data/incremental')
    inc_files = list(inc_dir.glob('*.parquet'))
    
    if len(inc_files) > 100:
        print(f"âš ï¸ è­¦å‘Š: å¢é‡æ–‡ä»¶è¿‡å¤š ({len(inc_files)}), éœ€è¦åˆå¹¶")
    
    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    total_size = sum(f.stat().st_size for f in inc_files) / 1024 / 1024
    if total_size > 100:
        print(f"âš ï¸ è­¦å‘Š: å¢é‡æ•°æ®è¿‡å¤§ ({total_size:.1f}MB), éœ€è¦åˆå¹¶")
    
    return len(inc_files) < 100 and total_size < 100
```

### 4. å¹¶å‘æ§åˆ¶
```python
# ä½¿ç”¨è¿›ç¨‹æ± é™åˆ¶å¹¶å‘
from concurrent.futures import ProcessPoolExecutor

def run_parallel_tests(tasks, max_workers=10):
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(run_test, task) for task in tasks]
        results = [f.result() for f in futures]
    
    # åˆå¹¶æ‰€æœ‰è¿›ç¨‹çš„å¢é‡æ•°æ®
    manager = ParquetDataManager()
    manager.consolidate_incremental_data()
    
    return results
```

---

## ğŸ“Š æ€§èƒ½åŸºå‡†

### æµ‹è¯•ç¯å¢ƒ
- MacBook Pro M1
- 16GB RAM
- 1TB SSD

### æ€§èƒ½å¯¹æ¯”

| æ“ä½œ | JSON | Parquet | æå‡ |
|------|------|---------|------|
| å†™å…¥1000æ¡ | 2.5s | 0.3s | 8.3x |
| è¯»å–10000æ¡ | 1.2s | 0.05s | 24x |
| æŸ¥è¯¢ç‰¹å®šæ¨¡å‹ | 0.8s | 0.01s | 80x |
| æ–‡ä»¶å¤§å°(10kæ¡) | 15MB | 2MB | 7.5x |
| å¹¶å‘å†™å…¥(10è¿›ç¨‹) | âŒå¤±è´¥ | âœ…æˆåŠŸ | âˆ |

---

## ğŸ”— ç›¸å…³èµ„æº

### å†…éƒ¨æ–‡æ¡£
- [DEBUG_KNOWLEDGE_BASE_V2.md](../maintenance/DEBUG_KNOWLEDGE_BASE_V2.md)
- [COMMON_ISSUES.md](../maintenance/COMMON_ISSUES.md)
- [setup_parquet_incremental_test.sh](../../setup_parquet_incremental_test.sh)

### å¤–éƒ¨èµ„æº
- [Apache Parquetå®˜æ–¹æ–‡æ¡£](https://parquet.apache.org/)
- [PyArrowæ–‡æ¡£](https://arrow.apache.org/docs/python/)
- [Pandas ParquetæŒ‡å—](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.to_parquet.html)

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**åˆ›å»ºæ—¶é—´**: 2025-08-17  
**ç»´æŠ¤è€…**: System Administrator  
**çŠ¶æ€**: ğŸŸ¢ Active | âœ… å®Œæ•´