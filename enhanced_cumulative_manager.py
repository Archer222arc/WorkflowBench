#!/usr/bin/env python3
"""Enhanced cumulative manager with real-time error classification"""

from dataclasses import dataclass, field, asdict
from typing import Dict, List, Optional, Any
from threading import Lock
from datetime import datetime
from pathlib import Path
import json

# V3ç‰ˆæœ¬ï¼šä¸å†å¯¼å…¥æ•°æ®ç»“æ„ç±»ï¼Œç›´æ¥ä½¿ç”¨å­—å…¸æ“ä½œ
# æ”¯æŒå­˜å‚¨æ ¼å¼é€‰æ‹©
import os
storage_format = os.environ.get('STORAGE_FORMAT', 'json').lower()
if storage_format == 'parquet':
    try:
        from parquet_cumulative_manager import (
            ParquetCumulativeManager as CumulativeTestManager,
            TestRecord,
            add_test_result,
            check_progress,
            is_test_completed,
            finalize
        )
        print(f"[INFO] ä½¿ç”¨Parquetå­˜å‚¨æ ¼å¼")
    except ImportError:
        from cumulative_test_manager import CumulativeTestManager, TestRecord, normalize_model_name
        print(f"[INFO] Parquetä¸å¯ç”¨ï¼Œä½¿ç”¨JSONå­˜å‚¨æ ¼å¼")
else:
    from cumulative_test_manager import CumulativeTestManager, TestRecord, normalize_model_name
    print(f"[INFO] ä½¿ç”¨JSONå­˜å‚¨æ ¼å¼")


@dataclass
class RuntimeErrorStats:
    """Runtime error statistics - lightweight in-memory tracking"""
    # Error categorization counts
    format_errors: int = 0
    timeout_errors: int = 0
    max_turns_errors: int = 0
    tool_selection_errors: int = 0
    parameter_errors: int = 0
    sequence_errors: int = 0
    dependency_errors: int = 0
    other_errors: int = 0
    total_errors: int = 0
    
    # Assisted statistics
    tests_with_assistance: int = 0
    total_assisted_turns: int = 0
    assisted_success: int = 0
    assisted_failure: int = 0
    
    def categorize_and_count(self, error_msg: str) -> str:
        """Categorize error and increment counter"""
        if not error_msg:
            return 'unknown'
        
        self.total_errors += 1
        error_lower = error_msg.lower()
        
        # Format errors
        if any(x in error_lower for x in ['format errors detected', 'format recognition issue', 
                                          'tool call format', 'understand tool call format']):
            self.format_errors += 1
            return 'format'
        
        # Max turns without tool calls (also format)
        if ('no tool calls' in error_lower and 'turns' in error_lower) or \
           ('max turns reached' in error_lower and 'no tool calls' in error_lower):
            self.format_errors += 1
            return 'format'
        
        # Pure max turns
        if 'max turns reached' in error_lower:
            self.max_turns_errors += 1
            return 'max_turns'
        
        # Agent-level timeout (not tool-level timeout)
        if ('test timeout after' in error_lower) or \
           ('timeout after' in error_lower and ('seconds' in error_lower or 'minutes' in error_lower)) or \
           ('agent timeout' in error_lower) or \
           ('execution timeout' in error_lower):
            self.timeout_errors += 1
            return 'timeout'
        
        # Tool selection
        if ('tool' in error_lower and ('select' in error_lower or 'choice' in error_lower)) or \
           'tool calls failed' in error_lower:
            self.tool_selection_errors += 1
            return 'tool_selection'
        
        # Parameter errors
        if any(x in error_lower for x in ['parameter', 'argument', 'invalid_input', 
                                          'permission_denied', 'validation failed']):
            self.parameter_errors += 1
            return 'parameter'
        
        # Sequence errors
        if any(x in error_lower for x in ['sequence', 'order', 'required tools not completed']):
            self.sequence_errors += 1
            return 'sequence'
        
        # Dependency errors
        if 'dependency' in error_lower or 'prerequisite' in error_lower:
            self.dependency_errors += 1
            return 'dependency'
        
        # Tool-level errors (not Agent errors) - for statistical completeness
        if any(tool_error in error_lower for tool_error in ['timeout', 'network_error', 'permission_denied', 
                                                           'file_not_found', 'operation_failed', 'resource_error']):
            self.other_errors += 1
            return 'tool_level_error'
        
        # Other
        self.other_errors += 1
        return 'other'
    
    def add_assisted_test(self, success: bool, format_turns: int):
        """Add assisted test statistics"""
        self.tests_with_assistance += 1
        self.total_assisted_turns += format_turns
        if success:
            self.assisted_success += 1
        else:
            self.assisted_failure += 1
    
    def to_error_metrics(self) -> Dict:
        """Convert to dictionary for database storage"""
        return {
            "total_errors": self.total_errors,
            "tool_call_format_errors": self.format_errors,
            "timeout_errors": self.timeout_errors,
            "max_turns_errors": self.max_turns_errors,
            "tool_selection_errors": self.tool_selection_errors,
            "parameter_config_errors": self.parameter_errors,
            "sequence_order_errors": self.sequence_errors,
            "dependency_errors": self.dependency_errors,
            "other_errors": self.other_errors
        }


class EnhancedCumulativeManager(CumulativeTestManager):
    """Enhanced manager with runtime error classification and V2 support"""
    
    def __init__(self, use_v2_model=True, use_ai_classification=True, db_suffix=''):
        super().__init__(db_suffix=db_suffix)
        # Runtime statistics (not persisted)
        self.runtime_stats = {}  # model -> prompt_type -> RuntimeErrorStats
        self.runtime_lock = Lock()
        
        # AIåˆ†ç±»æ”¯æŒ
        self.use_ai_classification = use_ai_classification
        self.ai_classifier = None
        if use_ai_classification:
            try:
                from focused_ai_classifier import FocusedAIClassifier
                self.ai_classifier = FocusedAIClassifier(model_name="gpt-5-nano")
                print(f"[INFO] Enhanced manager: AIé”™è¯¯åˆ†ç±»ç³»ç»Ÿå·²å¯ç”¨")
            except Exception as e:
                print(f"[ERROR] Failed to initialize AI classifier in manager: {e}")
                self.use_ai_classification = False
        
        # Buffer for batch updates
        self.update_buffer = []
        self.buffer_size = 3  # å‡å°ç¼“å†²åŒºï¼Œæ›´é¢‘ç¹åœ°ä¿å­˜ï¼ˆåŸæ¥æ˜¯10ï¼‰
        self.last_flush_time = datetime.now()
        self.flush_interval = 30  # æ¯30ç§’å¼ºåˆ¶flushä¸€æ¬¡ï¼Œå³ä½¿ç¼“å†²åŒºæœªæ»¡
        
        # Enable V2 model by default
        self.use_v2_model = use_v2_model
        
    def _get_record_attr(self, record, attr_name, default=None):
        """Safely get attribute from record whether it's dict or object"""
        if isinstance(record, dict):
            return record.get(attr_name, default)
        else:
            return getattr(record, attr_name, default)
    
    def add_test_result_with_classification(self, record: TestRecord) -> bool:
        """Add test result with real-time error classification"""
        with self.runtime_lock:
            # Get or create runtime stats
            model_name = self._get_record_attr(record, 'model', 'unknown')
            normalized_model = normalize_model_name(model_name)
            model_stats = self.runtime_stats.setdefault(normalized_model, {})
            
            # Determine effective prompt type
            is_flawed = self._get_record_attr(record, 'is_flawed', False)
            flaw_type = self._get_record_attr(record, 'flaw_type', None)
            prompt_type = self._get_record_attr(record, 'prompt_type', 'optimal')
            success = self._get_record_attr(record, 'success', False)
                
            if is_flawed and flaw_type:
                effective_prompt = f"flawed_{flaw_type}"
            else:
                effective_prompt = prompt_type
            
            prompt_stats = model_stats.setdefault(effective_prompt, RuntimeErrorStats())
            
            # Classify error if test failed or partial success (which also has errors)
            success_level = self._get_record_attr(record, 'success_level', '')
            has_errors = (not success or success_level == 'partial_success')
            
            if has_errors:
                error_type = 'unknown'
                
                # é¦–å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»æœ‰AIåˆ†ç±»ç»“æœï¼ˆç”±batch_test_runneræä¾›ï¼‰
                ai_error_category = self._get_record_attr(record, 'ai_error_category', None)
                    
                if ai_error_category:
                    # ä½¿ç”¨batch_test_runnerå·²ç»åšå¥½çš„AIåˆ†ç±»
                    ai_category = str(ai_error_category).lower()
                    prompt_stats.total_errors += 1
                    
                    print(f"[AI-CLASSIFY-EXISTING] Using existing AI classification: {ai_category}")
                    
                    # æ ¹æ®AIåˆ†ç±»ç»“æœæ›´æ–°ç»Ÿè®¡
                    if 'tool_selection' in ai_category:
                        prompt_stats.tool_selection_errors += 1
                        error_type = 'tool_selection_errors'
                    elif 'parameter' in ai_category or 'parameter_config' in ai_category:
                        prompt_stats.parameter_errors += 1
                        error_type = 'parameter_config_errors'
                    elif 'sequence' in ai_category or 'sequence_order' in ai_category:
                        prompt_stats.sequence_errors += 1
                        error_type = 'sequence_order_errors'
                    elif 'dependency' in ai_category:
                        prompt_stats.dependency_errors += 1
                        error_type = 'dependency_errors'
                    elif 'timeout' in ai_category:
                        prompt_stats.timeout_errors += 1
                        error_type = 'timeout_errors'
                    elif 'format' in ai_category or 'tool_call_format' in ai_category:
                        prompt_stats.format_errors += 1
                        error_type = 'tool_call_format_errors'
                    elif 'max_turns' in ai_category:
                        prompt_stats.max_turns_errors += 1
                        error_type = 'max_turns_errors'
                    else:
                        prompt_stats.other_errors += 1
                        error_type = 'other_errors'
                    
                    if hasattr(record, 'ai_confidence'):
                        task_type = self._get_record_attr(record, 'task_type', 'unknown')
                        ai_confidence = self._get_record_attr(record, 'ai_confidence', 0.0)
                        print(f"[AI-CLASSIFY-EXISTING] {normalized_model} {task_type}: {error_type} (confidence: {ai_confidence:.2f})")
                    
                # å¦‚æœæ²¡æœ‰ç°æœ‰åˆ†ç±»ï¼Œå†ä½¿ç”¨è‡ªå·±çš„AIåˆ†ç±»å™¨
                elif self.use_ai_classification and self.ai_classifier:
                    try:
                        from focused_ai_classifier import ErrorContext
                        
                        # æ„å»ºé”™è¯¯ä¸Šä¸‹æ–‡
                        # å¯¹äºpartial successï¼Œå¦‚æœæ²¡æœ‰æ˜ç¡®é”™è¯¯æ¶ˆæ¯ï¼Œæ„å»ºæè¿°æ€§æ¶ˆæ¯
                        error_msg = self._get_record_attr(record, 'error_message', '')
                        if not error_msg and getattr(record, 'success_level', '') == 'partial_success':
                            format_count = getattr(record, 'format_error_count', 0)
                            if format_count > 0:
                                error_msg = f"Format issues detected: {format_count} format helps needed"
                            else:
                                error_msg = "Partial success - quality issues detected"
                        
                        context = ErrorContext(
                            task_description=getattr(record, 'task_description', 'Unknown task'),
                            task_type=self._get_record_attr(record, 'task_type', 'unknown'),
                            required_tools=getattr(record, 'required_tools', []),
                            executed_tools=getattr(record, 'executed_tools', []),
                            is_partial_success=(getattr(record, 'success_level', '') == 'partial_success'),
                            tool_execution_results=getattr(record, 'tool_calls', []),
                            execution_time=self._get_record_attr(record, 'execution_time', 0.0),
                            total_turns=getattr(record, 'turns', 0),
                            error_message=error_msg or "Unknown error"
                        )
                        
                        # è¿›è¡ŒAIåˆ†ç±»
                        category, reason, confidence = self.ai_classifier.classify_error(context)
                        
                        # æ ¹æ®AIåˆ†ç±»ç»“æœæ›´æ–°ç»Ÿè®¡
                        error_type = category.value
                        prompt_stats.total_errors += 1
                        
                        if category.value == 'tool_selection_errors':
                            prompt_stats.tool_selection_errors += 1
                        elif category.value == 'parameter_config_errors':
                            prompt_stats.parameter_errors += 1
                        elif category.value == 'sequence_order_errors':
                            prompt_stats.sequence_errors += 1
                        elif category.value == 'dependency_errors':
                            prompt_stats.dependency_errors += 1
                        elif category.value == 'timeout_errors':
                            prompt_stats.timeout_errors += 1
                        elif category.value == 'tool_call_format_errors':
                            prompt_stats.format_errors += 1
                        elif category.value == 'max_turns_errors':
                            prompt_stats.max_turns_errors += 1
                        else:
                            prompt_stats.other_errors += 1
                        
                        task_type = self._get_record_attr(record, 'task_type', 'unknown')
                        print(f"[AI-CLASSIFY-NEW] {normalized_model} {task_type}: {error_type} (confidence: {confidence:.2f}) - {reason[:50]}")
                        
                    except Exception as e:
                        print(f"[ERROR] AI classification failed: {e}, falling back to rule-based")
                        import traceback
                        print(f"[DEBUG] Full traceback: {traceback.format_exc()}")
                        # å›é€€åˆ°åŸºäºè§„åˆ™çš„åˆ†ç±»
                        error_msg = self._get_record_attr(record, 'error_message', None)
                        if error_msg:
                            error_type = prompt_stats.categorize_and_count(error_msg)
                        else:
                            prompt_stats.total_errors += 1
                            # æ²¡æœ‰é”™è¯¯æ¶ˆæ¯æ—¶ï¼Œå½’ç±»ä¸ºother_errors
                            prompt_stats.other_errors += 1
                            error_type = 'other_errors'
                else:
                    # ä½¿ç”¨åŸºäºè§„åˆ™çš„åˆ†ç±»ï¼ˆåŸæœ‰é€»è¾‘ï¼‰
                    error_msg = self._get_record_attr(record, 'error_message', None)
                    if error_msg:
                        error_type = prompt_stats.categorize_and_count(error_msg)
                    else:
                        prompt_stats.total_errors += 1
                        # æ²¡æœ‰é”™è¯¯æ¶ˆæ¯æ—¶ï¼Œå½’ç±»ä¸ºother_errors
                        prompt_stats.other_errors += 1
                        error_type = 'other_errors'
                
                # å­˜å‚¨åˆ†ç±»ç»“æœ
                if not isinstance(record, dict):
                    record.error_classification = error_type
            
            # Track assisted statistics
            format_error_count = self._get_record_attr(record, 'format_error_count', 0)
            if format_error_count > 0:
                success = self._get_record_attr(record, 'success', False) or self._get_record_attr(record, 'partial_success', False)
                prompt_stats.add_assisted_test(success, format_error_count)
            
            # Add to buffer
            self.update_buffer.append(record)
            
            # Check if we should update database
            should_flush = False
            
            # æ¡ä»¶1ï¼šç¼“å†²åŒºæ»¡äº†
            if len(self.update_buffer) >= self.buffer_size:
                should_flush = True
                print(f"[INFO] Buffer full ({len(self.update_buffer)} records), triggering flush...")
            
            # æ¡ä»¶2ï¼šè·ç¦»ä¸Šæ¬¡flushè¶…è¿‡æŒ‡å®šæ—¶é—´
            time_since_flush = (datetime.now() - self.last_flush_time).total_seconds()
            if time_since_flush >= self.flush_interval and self.update_buffer:
                should_flush = True
                print(f"[INFO] Time-based flush ({time_since_flush:.1f}s since last flush)...")
            
            if should_flush:
                self._flush_buffer()
                self.last_flush_time = datetime.now()
        
        return True
    
    def append_test_result(self, record: TestRecord) -> bool:
        """
        åˆ«åæ–¹æ³•ï¼Œä¸ºäº†å…¼å®¹æ€§
        è°ƒç”¨ add_test_result_with_classification
        """
        return self.add_test_result_with_classification(record)
    
    def _flush_buffer(self):
        """Flush buffer to database with aggregated statistics"""
        if not self.update_buffer:
            return
        
        print(f"[INFO] Flushing {len(self.update_buffer)} records to database...")
        
        try:
            # Process each buffered record
            for i, record in enumerate(self.update_buffer):
                if self.use_v2_model:
                    # Use V2 model processing
                    self._add_test_result_v2(record)
                else:
                    # Call parent method to update basic statistics
                    super().add_test_result(record)
                
                # æ¯å¤„ç†5æ¡è®°å½•æ‰“å°ä¸€æ¬¡è¿›åº¦
                if (i + 1) % 5 == 0:
                    print(f"[INFO] Processed {i + 1}/{len(self.update_buffer)} records...")
            
            print(f"[INFO] All records processed, updating error metrics...")
            # Update error classifications in database
            self._update_error_metrics()
            
            # é‡æ–°è®¡ç®—total_testsåŸºäºå®é™…çš„æµ‹è¯•ç»“æœ
            self._recalculate_total_tests()
            
            # Clear buffer BEFORE saving to avoid re-processing if save fails
            self.update_buffer.clear()
            
            # Save database with timeout protection
            print(f"[INFO] Saving database...")
            import time
            start_time = time.time()
            self.save_database()
            elapsed = time.time() - start_time
            print(f"[INFO] Database saved successfully in {elapsed:.2f}s")
            
        except Exception as e:
            print(f"[ERROR] Failed to flush buffer: {e}")
            import traceback
            traceback.print_exc()
            # æ¸…ç©ºç¼“å†²åŒºä»¥é¿å…é‡å¤å¤„ç†
            self.update_buffer.clear()
            raise  # é‡æ–°æŠ›å‡ºå¼‚å¸¸è®©è°ƒç”¨è€…çŸ¥é“å‡ºé”™äº†
    
    def _recalculate_total_tests(self):
        """é‡æ–°è®¡ç®—æ¯ä¸ªæ¨¡å‹çš„total_testsåŸºäºå®é™…çš„æµ‹è¯•ç»“æœ"""
        for model_name in self.database["models"]:
            model_data = self.database["models"][model_name]
            
            # æ£€æŸ¥model_dataæ˜¯å¦æ˜¯å­—å…¸ï¼ˆV3ç»“æ„ï¼‰è¿˜æ˜¯ModelStatisticså¯¹è±¡ï¼ˆV2ç»“æ„ï¼‰
            if not isinstance(model_data, dict):
                # å¦‚æœæ˜¯ModelStatisticså¯¹è±¡ï¼Œè·³è¿‡é‡æ–°è®¡ç®—
                continue
            
            # è®¡ç®—å®é™…çš„æµ‹è¯•æ•°é‡
            actual_total = 0
            by_prompt_type = model_data.get('by_prompt_type', {})
            
            for prompt_type, prompt_data in by_prompt_type.items():
                by_tool_success = prompt_data.get('by_tool_success_rate', {})
                for rate, rate_data in by_tool_success.items():
                    by_diff = rate_data.get('by_difficulty', {})
                    for diff, diff_data in by_diff.items():
                        by_task = diff_data.get('by_task_type', {})
                        for task, task_data in by_task.items():
                            actual_total += task_data.get('total', 0)
            
            # æ›´æ–°total_tests
            model_data['total_tests'] = actual_total
    
    def _update_error_metrics(self):
        """Update error metrics in database from runtime stats"""
        for model_name, model_prompts in self.runtime_stats.items():
            if model_name not in self.database["models"]:
                continue
            
            model_data = self.database["models"][model_name]
            
            # Update overall error metrics
            overall_errors = RuntimeErrorStats()
            for prompt_stats in model_prompts.values():
                overall_errors.format_errors += prompt_stats.format_errors
                overall_errors.timeout_errors += prompt_stats.timeout_errors
                overall_errors.max_turns_errors += prompt_stats.max_turns_errors
                overall_errors.tool_selection_errors += prompt_stats.tool_selection_errors
                overall_errors.parameter_errors += prompt_stats.parameter_errors
                overall_errors.sequence_errors += prompt_stats.sequence_errors
                overall_errors.dependency_errors += prompt_stats.dependency_errors
                overall_errors.other_errors += prompt_stats.other_errors
                overall_errors.total_errors += prompt_stats.total_errors
            
            # Only process dict-based model_data (V3 structure)
            # Skip any ModelStatistics objects from older versions
            if not isinstance(model_data, dict):
                continue
            
            # V3 structure - error metrics are handled in _add_test_result_v2
            # No need to update here as they're already in the dict structure
    
    def finalize(self):
        """Finalize and flush all pending updates"""
        with self.runtime_lock:
            if self.update_buffer:
                self._flush_buffer()
            
            # Save final statistics report
            self._save_runtime_report()
        
        # ä¿å­˜æ•°æ®åº“åˆ°æ–‡ä»¶
        self.save_database_enhanced()
    
    def _save_runtime_report(self):
        """Save runtime statistics report"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        # ä¿å­˜åˆ°runtime_reportsç›®å½•ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º
        report_dir = Path("runtime_reports")
        report_dir.mkdir(exist_ok=True)
        report_file = report_dir / f"runtime_error_report_{timestamp}.json"
        
        report = {
            "timestamp": timestamp,
            "models": {}
        }
        
        for model_name, model_prompts in self.runtime_stats.items():
            model_report = {}
            for prompt_type, stats in model_prompts.items():
                model_report[prompt_type] = {
                    "total_errors": stats.total_errors,
                    "error_breakdown": {
                        "format": stats.format_errors,
                        "timeout": stats.timeout_errors,
                        "max_turns": stats.max_turns_errors,
                        "tool_selection": stats.tool_selection_errors,
                        "parameter": stats.parameter_errors,
                        "sequence": stats.sequence_errors,
                        "dependency": stats.dependency_errors,
                        "other": stats.other_errors
                    },
                    "assisted_stats": {
                        "tests_with_assistance": stats.tests_with_assistance,
                        "total_assisted_turns": stats.total_assisted_turns,
                        "assisted_success": stats.assisted_success,
                        "assisted_failure": stats.assisted_failure
                    }
                }
            report["models"][model_name] = model_report
        
        with open(report_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        # ä½¿ç”¨ç®€å•çš„å­—ç¬¦ä¸²è·¯å¾„ï¼Œé¿å…relative_toçš„é—®é¢˜
        print(f"[INFO] Runtime error report saved to: {report_file}")
    
    def get_runtime_summary(self) -> Dict:
        """Get current runtime statistics summary"""
        with self.runtime_lock:
            summary = {}
            for model_name, model_prompts in self.runtime_stats.items():
                model_summary = {
                    "total_errors": 0,
                    "by_prompt_type": {}
                }
                
                for prompt_type, stats in model_prompts.items():
                    model_summary["total_errors"] += stats.total_errors
                    model_summary["by_prompt_type"][prompt_type] = {
                        "total_errors": stats.total_errors,
                        "format": stats.format_errors,
                        "timeout": stats.timeout_errors,
                        "max_turns": stats.max_turns_errors,
                        "assisted": stats.tests_with_assistance
                    }
                
                summary[model_name] = model_summary
            
            return summary
    
    def _add_test_result_v2(self, record: TestRecord):
        """Add test result using V2 model with hierarchical structure"""
        # è§„èŒƒåŒ–æ¨¡å‹åç§°
        model_name = self._get_record_attr(record, 'model', 'unknown')
        normalized_model = normalize_model_name(model_name)
        
        # ç¡®ä¿æ¨¡å‹å­˜åœ¨äºæ•°æ®åº“ä¸­ - å†…å±‚æ•°æ®ä¿æŠ¤ä¿®å¤
        if normalized_model not in self.database["models"]:
            # ğŸ”§ æ•°æ®ä¿æŠ¤ä¿®å¤ï¼šå…ˆæ£€æŸ¥ç£ç›˜ä¸Šæ˜¯å¦æœ‰æœ€æ–°æ•°æ®
            try:
                if self.db_file.exists():
                    # é‡æ–°åŠ è½½ç£ç›˜æ•°æ®ï¼Œæ£€æŸ¥å…¶ä»–è¿›ç¨‹æ˜¯å¦å·²åˆ›å»ºæ­¤æ¨¡å‹
                    with open(self.db_file, 'r', encoding='utf-8') as f:
                        latest_disk_data = json.load(f)
                    
                    if normalized_model in latest_disk_data.get("models", {}):
                        # å…¶ä»–è¿›ç¨‹å·²åˆ›å»ºï¼Œåˆå¹¶ç£ç›˜æ•°æ®é¿å…è¦†ç›–
                        self.database["models"][normalized_model] = latest_disk_data["models"][normalized_model]
                        print(f"[ENHANCED_DATA_PROTECTION] åˆå¹¶æ¥è‡ªç£ç›˜çš„æ¨¡å‹æ•°æ®: {normalized_model}")
                    else:
                        # çœŸæ­£çš„æ–°æ¨¡å‹ï¼Œåˆ›å»ºç©ºç»“æ„
                        self.database["models"][normalized_model] = {
                            "model_name": normalized_model,
                            "first_test_time": datetime.now().isoformat(),
                            "last_test_time": datetime.now().isoformat(),
                            "total_tests": 0,
                            "overall_stats": {},
                            "by_prompt_type": {}
                        }
                        print(f"[ENHANCED_DATA_PROTECTION] åˆ›å»ºæ–°æ¨¡å‹ç»“æ„: {normalized_model}")
                else:
                    # æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°æ¨¡å‹
                    self.database["models"][normalized_model] = {
                        "model_name": normalized_model,
                        "first_test_time": datetime.now().isoformat(),
                        "last_test_time": datetime.now().isoformat(),
                        "total_tests": 0,
                        "overall_stats": {},
                        "by_prompt_type": {}
                    }
                    print(f"[ENHANCED_DATA_PROTECTION] åˆ›å»ºé¦–ä¸ªæ¨¡å‹ç»“æ„: {normalized_model}")
            except Exception as e:
                print(f"[ENHANCED_DATA_PROTECTION] ç£ç›˜æ•°æ®åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨å†…å­˜æ•°æ®: {e}")
                # å›é€€åˆ°åŸå§‹é€»è¾‘
                self.database["models"][normalized_model] = {
                    "model_name": normalized_model,
                    "first_test_time": datetime.now().isoformat(),
                    "last_test_time": datetime.now().isoformat(),
                    "total_tests": 0,
                    "overall_stats": {},
                    "by_prompt_type": {}
                }
        
        model_data = self.database["models"][normalized_model]
        model_data["last_test_time"] = datetime.now().isoformat()
        # æ³¨æ„ï¼šä¸åœ¨è¿™é‡Œå¢åŠ total_testsï¼Œè€Œæ˜¯åŸºäºå®é™…çš„æµ‹è¯•ç»“æœè®¡ç®—
        
        # ç›´æ¥ä½¿ç”¨å­—å…¸æ“ä½œï¼Œä¸ä½¿ç”¨V2å¯¹è±¡
        
        # å‡†å¤‡æµ‹è¯•è®°å½•å­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰å…³é”®å­—æ®µ
        # å¤„ç†recordå¯èƒ½æ˜¯dictæˆ–TestRecordå¯¹è±¡çš„æƒ…å†µ
        if isinstance(record, dict):
            test_dict = {
                'success': record.get('success', False),
                'partial_success': record.get('partial_success', False),
                'execution_time': record.get('execution_time', 0.0),
                'turns': record.get('turns', 0),
                'tool_calls': record.get('tool_calls', []),
                'error_message': record.get('error_message', None),
                # æ·»åŠ å…³é”®çš„ç¼ºå¤±å­—æ®µ
                'workflow_score': record.get('workflow_score', 0.0),
                'phase2_score': record.get('phase2_score', 0.0),
                'quality_score': record.get('quality_score', 0.0),
                'final_score': record.get('final_score', 0.0),
                'required_tools': record.get('required_tools', []),
                'executed_tools': record.get('executed_tools', []),
                'tool_coverage_rate': record.get('tool_coverage_rate', 0.0),
                'success_level': record.get('success_level', 'failure'),
                'task_instance': record.get('task_instance', {}),
                'format_error_count': record.get('format_error_count', 0),
                'tool_reliability': record.get('tool_reliability', 100.0)
            }
        else:
            test_dict = {
                'success': record.success,
                'partial_success': getattr(record, 'partial_success', False),
                'execution_time': record.execution_time,
                'turns': record.turns,
                'tool_calls': record.tool_calls if record.tool_calls else [],
                'error_message': record.error_message,
                # æ·»åŠ å…³é”®çš„ç¼ºå¤±å­—æ®µ
                'workflow_score': getattr(record, 'workflow_score', 0.0),
                'phase2_score': getattr(record, 'phase2_score', 0.0),
                'quality_score': getattr(record, 'quality_score', 0.0),
                'final_score': getattr(record, 'final_score', 0.0),
                'required_tools': getattr(record, 'required_tools', []),
                'executed_tools': getattr(record, 'executed_tools', []),
                'tool_coverage_rate': getattr(record, 'tool_coverage_rate', 0.0),
                'success_level': getattr(record, 'success_level', 'failure'),
                'task_instance': getattr(record, 'task_instance', {}),
                'format_error_count': getattr(record, 'format_error_count', 0),
                'tool_reliability': getattr(record, 'tool_reliability', 100.0)
            }
        
        # è·å–tool_success_rateï¼ˆå¯èƒ½éœ€è¦ä»recordå±æ€§ä¸­æå–ï¼‰
        if isinstance(record, dict):
            tool_success_rate = record.get('tool_success_rate', 1.0)
        else:
            tool_success_rate = getattr(record, 'tool_success_rate', 1.0)
        
        # æ›´æ–°overall_stats
        if "overall_stats" not in model_data:
            model_data["overall_stats"] = {
                "total_success": 0,
                "total_partial": 0,
                "total_full": 0,
                "total_failure": 0,
                "success_rate": 0.0,
                "weighted_success_score": 0.0,
                "avg_execution_time": 0.0,
                "avg_turns": 0.0,
                "tool_coverage_rate": 0.0
            }
        
        # æ›´æ–°ç»Ÿè®¡ï¼ˆè¿™äº›ä¼šåœ¨_recalculate_total_testsä¸­é‡æ–°è®¡ç®—ï¼‰
        model_data["total_tests"] = model_data.get("total_tests", 0) + 1
        
        # ç¡®å®šæœ‰æ•ˆçš„prompt_typeï¼ˆä¸runtime_statsä¿æŒä¸€è‡´ï¼‰
        if isinstance(record, dict):
            is_flawed = record.get('is_flawed', False)
            flaw_type = record.get('flaw_type', None)
            prompt_type = record.get('prompt_type', 'optimal')
        else:
            is_flawed = record.is_flawed
            flaw_type = record.flaw_type
            prompt_type = record.prompt_type
            
        if is_flawed and flaw_type:
            effective_prompt = f"flawed_{flaw_type}"
        else:
            effective_prompt = prompt_type
        
        # æ›´æ–°å±‚æ¬¡ç»“æ„
        if effective_prompt not in model_data["by_prompt_type"]:
            model_data["by_prompt_type"][effective_prompt] = {
                "by_tool_success_rate": {},
                "summary": {}
            }
        
        prompt_data = model_data["by_prompt_type"][effective_prompt]
        
        # å°†tool_success_rateåˆ†æ¡¶ï¼ˆä¿ç•™4ä½å°æ•°ç²¾åº¦ï¼‰
        rate_bucket = str(round(tool_success_rate, 4))
        
        if rate_bucket not in prompt_data["by_tool_success_rate"]:
            prompt_data["by_tool_success_rate"][rate_bucket] = {
                "by_difficulty": {}
            }
        
        rate_data = prompt_data["by_tool_success_rate"][rate_bucket]
        
        # è·å–difficultyå’Œtask_type
        if isinstance(record, dict):
            difficulty = record.get('difficulty', 'easy')
            task_type = record.get('task_type', 'unknown')
        else:
            difficulty = record.difficulty
            task_type = record.task_type
        
        if difficulty not in rate_data["by_difficulty"]:
            rate_data["by_difficulty"][difficulty] = {
                "by_task_type": {}
            }
        
        diff_data = rate_data["by_difficulty"][difficulty]
        
        if task_type not in diff_data["by_task_type"]:
            diff_data["by_task_type"][task_type] = {}
        
        # ç›´æ¥æ›´æ–°ä»»åŠ¡ç»Ÿè®¡å­—å…¸
        task_data = diff_data["by_task_type"][task_type]
        
        # åˆå§‹åŒ–ç»Ÿè®¡å­—æ®µï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        if "total" not in task_data:
            task_data.update({
                "total": 0,
                "success": 0,
                "partial": 0,
                "failed": 0,
                "success_rate": 0.0,
                "partial_rate": 0.0,
                "failure_rate": 0.0,
                "weighted_success_score": 0.0,
                "full_success_rate": 0.0,
                "partial_success_rate": 0.0,
                "avg_execution_time": 0.0,
                "avg_turns": 0.0,
                "avg_tool_calls": 0.0,
                "tool_coverage_rate": 0.0,
                "avg_workflow_score": 0.0,
                "avg_phase2_score": 0.0,
                "avg_quality_score": 0.0,
                "avg_final_score": 0.0,
                "total_errors": 0,
                "tool_call_format_errors": 0,
                "timeout_errors": 0,
                "dependency_errors": 0,
                "parameter_config_errors": 0,
                "tool_selection_errors": 0,
                "sequence_order_errors": 0,
                "max_turns_errors": 0,
                "other_errors": 0,
                "tool_selection_error_rate": 0.0,
                "parameter_error_rate": 0.0,
                "sequence_error_rate": 0.0,
                "dependency_error_rate": 0.0,
                "timeout_error_rate": 0.0,
                "format_error_rate": 0.0,
                "max_turns_error_rate": 0.0,
                "other_error_rate": 0.0,
                "assisted_failure": 0,
                "assisted_success": 0,
                "total_assisted_turns": 0,
                "tests_with_assistance": 0,
                "avg_assisted_turns": 0.0,
                "assisted_success_rate": 0.0,
                "assistance_rate": 0.0
            })
        
        # æ›´æ–°ç»Ÿè®¡
        task_data["total"] += 1
        # å®‰å…¨è·å–successå€¼ï¼Œæ”¯æŒdictå’Œå¯¹è±¡
        success_value = record.get('success', False) if isinstance(record, dict) else getattr(record, 'success', False)
        if success_value:
            task_data["success"] += 1
            if test_dict.get("success_level") == "full_success":
                task_data.setdefault("full_success", 0)
                task_data["full_success"] += 1
            elif test_dict.get("success_level") == "partial_success":
                task_data.setdefault("partial_success", 0)
                task_data["partial_success"] += 1
                # æ³¨æ„ï¼špartial_successè®¡å…¥successä½†ä¹Ÿéœ€è¦å•ç‹¬è®°å½•
                task_data.setdefault("partial", 0)
                task_data["partial"] += 1
        else:
            # å¤±è´¥çš„æƒ…å†µ
            task_data.setdefault("failed", 0)
            task_data["failed"] += 1
        
        # æ›´æ–°è¾…åŠ©ç»Ÿè®¡
        if test_dict.get("format_error_count", 0) > 0:
            task_data["tests_with_assistance"] += 1
            task_data["total_assisted_turns"] += test_dict["format_error_count"]
            # ä½¿ç”¨ä¹‹å‰è®¡ç®—çš„success_valueï¼Œé¿å…é‡å¤è®¿é—®
            if success_value:
                task_data["assisted_success"] += 1
            else:
                task_data["assisted_failure"] += 1
        
        # æ›´æ–°é”™è¯¯ç»Ÿè®¡
        # æ³¨æ„ï¼špartial_successä¹Ÿåº”è¯¥æœ‰é”™è¯¯ç±»å‹ï¼Œå› ä¸ºæ²¡æœ‰å®Œå…¨æˆåŠŸè¯´æ˜å­˜åœ¨é—®é¢˜
        success_level = test_dict.get("success_level", "failure")
        
        if success_level != "full_success":  # full_successä¹‹å¤–éƒ½åº”è¯¥æœ‰é”™è¯¯åˆ†ç±»
            task_data["total_errors"] += 1
            
            # é¦–å…ˆæ£€æŸ¥æ˜¯å¦å·²ç»æœ‰AIåˆ†ç±»ç»“æœï¼ˆç”±batch_test_runneræä¾›ï¼‰
            ai_error_category = self._get_record_attr(record, 'ai_error_category', None)
            if ai_error_category:
                # ä½¿ç”¨batch_test_runnerå·²ç»åšå¥½çš„AIåˆ†ç±»
                ai_category = str(ai_error_category).lower()
                
                # å°è¯•ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…ç¡®ä¿æ­£ç¡®åˆ†ç±»
                try:
                    from fuzzy_error_matcher import FuzzyErrorMatcher
                    matched_error = FuzzyErrorMatcher.match_error_category(ai_category, threshold=0.6)
                    if matched_error:
                        print(f"[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: {ai_category} -> {matched_error}")
                        # æ›´æ–°å¯¹åº”çš„é”™è¯¯è®¡æ•°
                        if matched_error == 'tool_selection_errors':
                            task_data["tool_selection_errors"] += 1
                        elif matched_error == 'parameter_config_errors':
                            task_data["parameter_config_errors"] += 1
                        elif matched_error == 'sequence_order_errors':
                            task_data["sequence_order_errors"] += 1
                        elif matched_error == 'dependency_errors':
                            task_data["dependency_errors"] += 1
                        elif matched_error == 'timeout_errors':
                            task_data["timeout_errors"] += 1
                        elif matched_error == 'tool_call_format_errors':
                            task_data["tool_call_format_errors"] += 1
                        elif matched_error == 'max_turns_errors':
                            task_data["max_turns_errors"] += 1
                        else:
                            task_data["other_errors"] += 1
                    else:
                        # æ— æ³•åŒ¹é…ï¼Œå½’ä¸ºother_errors
                        print(f"[TASK-AI-CLASSIFY] Cannot match AI classification: {ai_category}, using other_errors")
                        task_data["other_errors"] += 1
                except ImportError:
                    # å¦‚æœæ²¡æœ‰fuzzy matcherï¼Œä½¿ç”¨ç®€å•å­—ç¬¦ä¸²åŒ¹é…
                    print(f"[TASK-AI-CLASSIFY] Using existing AI classification (no fuzzy): {ai_category}")
                    
                    if 'tool_selection' in ai_category:
                        task_data["tool_selection_errors"] += 1
                    elif 'parameter' in ai_category or 'param' in ai_category:
                        task_data["parameter_config_errors"] += 1
                    elif 'sequence' in ai_category:
                        task_data["sequence_order_errors"] += 1
                    elif 'dependency' in ai_category:
                        task_data["dependency_errors"] += 1
                    elif 'timeout' in ai_category:
                        task_data["timeout_errors"] += 1
                    elif 'format' in ai_category or 'tool_call_format' in ai_category:
                        task_data["tool_call_format_errors"] += 1
                    elif 'max_turns' in ai_category:
                        task_data["max_turns_errors"] += 1
                    else:
                        task_data["other_errors"] += 1
                
                ai_confidence = self._get_record_attr(record, 'ai_confidence', None)
                if ai_confidence is not None:
                    task_type_str = self._get_record_attr(record, 'task_type', 'unknown')
                    print(f"[TASK-AI-CLASSIFY] {normalized_model} {task_type_str}: confidence={ai_confidence:.2f}")
            
            # å¦‚æœæ²¡æœ‰ç°æœ‰AIåˆ†ç±»ï¼Œæ£€æŸ¥æ˜¯å¦ä¸ºæ˜æ˜¾çš„format error
            else:
                # æ™ºèƒ½æ£€æŸ¥æ˜¯å¦ä¸ºformat errorï¼ˆä¸å†è¿‡äºæ¿€è¿›ï¼‰
                tool_calls = getattr(record, 'tool_calls', [])
                executed_tools = getattr(record, 'executed_tools', [])
                error_msg = getattr(record, 'error_message', '')
                
                # æ›´ç²¾ç¡®çš„æ ¼å¼é”™è¯¯æ£€æµ‹æ¡ä»¶
                is_format_error = False
                # ä¿®å¤ï¼šå¤„ç†tool_callså¯èƒ½æ˜¯intçš„æƒ…å†µ
                if isinstance(tool_calls, int):
                    tool_calls_len = tool_calls
                elif tool_calls:
                    tool_calls_len = len(tool_calls)
                else:
                    tool_calls_len = 0
                
                if isinstance(executed_tools, int):
                    executed_tools_len = executed_tools
                elif executed_tools:
                    executed_tools_len = len(executed_tools)
                else:
                    executed_tools_len = 0
                
                if tool_calls_len == 0 and executed_tools_len == 0:
                    # åˆ†æä¸ºä»€ä¹ˆæ²¡æœ‰å·¥å…·è°ƒç”¨
                    if error_msg:
                        error_lower = error_msg.lower()
                        # åªæœ‰æ˜ç¡®çš„æ ¼å¼ç›¸å…³é”™è¯¯æ‰å½’ç±»ä¸ºformat error
                        format_indicators = [
                            'format errors detected', 'format recognition issue',
                            'tool call format', 'understand tool call format',
                            'invalid json', 'malformed', 'parse error',
                            'unable to parse', 'json syntax error'
                        ]
                        # æ’é™¤æ˜æ˜¾ä¸æ˜¯æ ¼å¼é—®é¢˜çš„æƒ…å†µ
                        non_format_indicators = [
                            'timeout', 'connection', 'network', 'api error',
                            'service unavailable', 'rate limit', 'unauthorized',
                            'internal server error', 'bad gateway'
                        ]
                        
                        if any(indicator in error_lower for indicator in format_indicators):
                            is_format_error = True
                        elif any(indicator in error_lower for indicator in non_format_indicators):
                            # æ˜æ˜¾æ˜¯ç¯å¢ƒ/APIé—®é¢˜ï¼Œä¸æ˜¯æ ¼å¼é—®é¢˜
                            is_format_error = False
                        else:
                            # æ²¡æœ‰æ˜ç¡®é”™è¯¯æ¶ˆæ¯æˆ–æ— æ³•åˆ¤æ–­ï¼Œä¿å®ˆåœ°ä¸å½’ç±»ä¸ºæ ¼å¼é”™è¯¯
                            # è®©AIåˆ†ç±»å™¨æˆ–fallbacké€»è¾‘æ¥å¤„ç†
                            is_format_error = False
                    else:
                        # æ²¡æœ‰é”™è¯¯æ¶ˆæ¯çš„æƒ…å†µï¼Œå¯èƒ½æ˜¯ç¯å¢ƒé—®é¢˜
                        is_format_error = False
                
                if is_format_error:
                    task_data["tool_call_format_errors"] += 1
                    task_type_str = self._get_record_attr(record, 'task_type', 'unknown')
                    print(f"[FORMAT-ERROR-DETECTED] {normalized_model} {task_type_str}: Confirmed format error based on error message")
                elif self.use_ai_classification and self.ai_classifier:
                    # ä½¿ç”¨AIåˆ†ç±»å™¨è¿›è¡Œé”™è¯¯åˆ†ç±»
                    try:
                        from focused_ai_classifier import ErrorContext
                        
                        # æ„å»ºé”™è¯¯ä¸Šä¸‹æ–‡
                        error_msg = self._get_record_attr(record, 'error_message', '')
                        if not error_msg and success_level == 'partial_success':
                            format_count = getattr(record, 'format_error_count', 0)
                            if format_count > 0:
                                error_msg = f"Format issues detected: {format_count} format helps needed"
                            else:
                                error_msg = "Partial success - quality issues detected"
                        
                        task_type_str = self._get_record_attr(record, 'task_type', 'unknown')
                        context = ErrorContext(
                            task_description=self._get_record_attr(record, 'task_description', f'{task_type_str} task'),
                            task_type=self._get_record_attr(record, 'task_type', 'unknown'),
                            required_tools=getattr(record, 'required_tools', []),
                            executed_tools=getattr(record, 'executed_tools', []),
                            is_partial_success=(success_level == 'partial_success'),
                            tool_execution_results=getattr(record, 'execution_history', []),
                            execution_time=self._get_record_attr(record, 'execution_time', 0.0),
                            total_turns=self._get_record_attr(record, 'turns', 0),
                            error_message=error_msg
                        )
                        
                        category, reason, confidence = self.ai_classifier.classify_error(context)
                        error_type = category.value
                        
                        # æ ¹æ®AIåˆ†ç±»ç»“æœæ›´æ–°ä»»åŠ¡ç»Ÿè®¡
                        if category.value == 'tool_selection_errors':
                            task_data["tool_selection_errors"] += 1
                        elif category.value == 'parameter_config_errors':
                            task_data["parameter_config_errors"] += 1
                        elif category.value == 'sequence_order_errors':
                            task_data["sequence_order_errors"] += 1
                        elif category.value == 'dependency_errors':
                            task_data["dependency_errors"] += 1
                        elif category.value == 'timeout_errors':
                            task_data["timeout_errors"] += 1
                        elif category.value == 'tool_call_format_errors':
                            task_data["tool_call_format_errors"] += 1
                        elif category.value == 'max_turns_errors':
                            task_data["max_turns_errors"] += 1
                        else:
                            task_data["other_errors"] += 1
                        
                        task_type_str = self._get_record_attr(record, 'task_type', 'unknown')
                        print(f"[AI-CLASSIFY-TASK] {normalized_model} {task_type_str}: {error_type} (confidence: {confidence:.2f})")
                        
                    except Exception as e:
                        print(f"[AI-CLASSIFY-TASK] Failed: {e}, using fallback classification")
                        # AIåˆ†ç±»å¤±è´¥ï¼Œä½¿ç”¨fallbacké€»è¾‘
                        self._apply_fallback_error_classification(task_data, record)
                else:
                    # æ²¡æœ‰AIåˆ†ç±»å™¨ï¼Œä½¿ç”¨fallbacké€»è¾‘
                    self._apply_fallback_error_classification(task_data, record)
        
        # é‡æ–°è®¡ç®—ç»Ÿè®¡ç‡ï¼ˆæ¯æ¬¡æ›´æ–°åéƒ½è¦è®¡ç®—ï¼‰
        if task_data["total"] > 0:
            task_data["success_rate"] = task_data["success"] / task_data["total"]
            task_data["partial_rate"] = task_data.get("partial", 0) / task_data["total"]
            task_data["failure_rate"] = task_data.get("failed", 0) / task_data["total"]
            
            # è®¡ç®—full_successå’Œpartial_successç‡
            full_success_count = task_data.get("full_success", 0)
            partial_success_count = task_data.get("partial_success", 0)
            task_data["full_success_rate"] = full_success_count / task_data["total"]
            task_data["partial_success_rate"] = partial_success_count / task_data["total"]
            
            # è®¡ç®—weighted_success_score
            task_data["weighted_success_score"] = (full_success_count * 1.0 + partial_success_count * 0.5) / task_data["total"]
            
            # è®¡ç®—é”™è¯¯ç‡ï¼ˆåŸºäºæ€»é”™è¯¯æ•°ï¼Œè€Œä¸æ˜¯æ€»æµ‹è¯•æ•°ï¼‰
            total = task_data["total"]
            total_errors = task_data.get("total_errors", 0)
            if total_errors > 0:
                task_data["tool_selection_error_rate"] = task_data.get("tool_selection_errors", 0) / total_errors
                task_data["parameter_error_rate"] = task_data.get("parameter_config_errors", 0) / total_errors
                task_data["sequence_error_rate"] = task_data.get("sequence_order_errors", 0) / total_errors
                task_data["dependency_error_rate"] = task_data.get("dependency_errors", 0) / total_errors
                task_data["timeout_error_rate"] = task_data.get("timeout_errors", 0) / total_errors
                task_data["format_error_rate"] = task_data.get("tool_call_format_errors", 0) / total_errors
                task_data["max_turns_error_rate"] = task_data.get("max_turns_errors", 0) / total_errors
                task_data["other_error_rate"] = task_data.get("other_errors", 0) / total_errors
            else:
                # æ²¡æœ‰é”™è¯¯æ—¶ï¼Œæ‰€æœ‰é”™è¯¯ç‡éƒ½ä¸º0
                task_data["tool_selection_error_rate"] = 0.0
                task_data["parameter_error_rate"] = 0.0
                task_data["sequence_error_rate"] = 0.0
                task_data["dependency_error_rate"] = 0.0
                task_data["timeout_error_rate"] = 0.0
                task_data["format_error_rate"] = 0.0
                task_data["max_turns_error_rate"] = 0.0
                task_data["other_error_rate"] = 0.0
            
            # è®¡ç®—è¾…åŠ©ç»Ÿè®¡ç‡
            if task_data.get("tests_with_assistance", 0) > 0:
                task_data["assisted_success_rate"] = task_data.get("assisted_success", 0) / task_data["tests_with_assistance"]
                task_data["avg_assisted_turns"] = task_data.get("total_assisted_turns", 0) / task_data["tests_with_assistance"]
            task_data["assistance_rate"] = task_data.get("tests_with_assistance", 0) / total
            
            # ä½¿ç”¨å¢é‡å¹³å‡æ›´æ–°æ‰§è¡Œç»Ÿè®¡
            n = task_data["total"]
            # å®‰å…¨è·å–execution_timeå’Œturns
            exec_time = record.get('execution_time', 0) if isinstance(record, dict) else getattr(record, 'execution_time', 0)
            turns = record.get('turns', 0) if isinstance(record, dict) else getattr(record, 'turns', 0)
            task_data["avg_execution_time"] += (exec_time - task_data["avg_execution_time"]) / n
            task_data["avg_turns"] += (turns - task_data["avg_turns"]) / n
            # å¤„ç†tool_callså¯èƒ½æ˜¯intæˆ–listçš„æƒ…å†µ
            tool_calls_count = 0
            # å®‰å…¨è·å–tool_calls
            tool_calls = record.get('tool_calls', []) if isinstance(record, dict) else getattr(record, 'tool_calls', [])
            if tool_calls:
                if isinstance(tool_calls, (list, tuple)):
                    tool_calls_count = len(tool_calls)
                elif isinstance(tool_calls, int):
                    tool_calls_count = tool_calls
            task_data["avg_tool_calls"] += (tool_calls_count - task_data["avg_tool_calls"]) / n
            task_data["tool_coverage_rate"] += (test_dict.get("tool_coverage_rate", 0) - task_data["tool_coverage_rate"]) / n
            
            # æ›´æ–°è´¨é‡åˆ†æ•°
            for score_key in ["workflow_score", "phase2_score", "quality_score", "final_score"]:
                avg_key = f"avg_{score_key}"
                score = test_dict.get(score_key, 0) or 0  # ç¡®ä¿ä¸æ˜¯None
                current_avg = task_data.get(avg_key, 0) or 0  # ç¡®ä¿ä¸æ˜¯None
                task_data[avg_key] = current_avg + (score - current_avg) / n
        
        # æ›´æ–°prompt_typeçš„summary
        self._update_prompt_summary(prompt_data)
    
    def _apply_fallback_error_classification(self, task_data: dict, record):
        """åº”ç”¨fallbackåˆ†ç±»é€»è¾‘"""
        
        # ä¼˜å…ˆæ£€æŸ¥format error: å¦‚æœå·¥å…·è°ƒç”¨æ¬¡æ•°ä¸º0ï¼Œå¾ˆå¯èƒ½æ˜¯æ ¼å¼é”™è¯¯
        tool_calls = getattr(record, 'tool_calls', [])
        executed_tools = getattr(record, 'executed_tools', [])
        
        # ä¿®å¤ï¼šå¤„ç†å¯èƒ½æ˜¯intçš„æƒ…å†µ
        if isinstance(tool_calls, int):
            tool_calls_len = tool_calls
        elif tool_calls:
            tool_calls_len = len(tool_calls)
        else:
            tool_calls_len = 0
        
        if isinstance(executed_tools, int):
            executed_tools_len = executed_tools
        elif executed_tools:
            executed_tools_len = len(executed_tools)
        else:
            executed_tools_len = 0
        
        if tool_calls_len == 0 and executed_tools_len == 0:
            # æ²¡æœ‰æˆåŠŸæ‰§è¡Œä»»ä½•å·¥å…·è°ƒç”¨ï¼Œè¿™æ˜¯å…¸å‹çš„format error
            task_data["tool_call_format_errors"] += 1
            return
        
        # ä½¿ç”¨ä¼ ç»Ÿçš„é”™è¯¯åˆ†ç±»é€»è¾‘
        # å®‰å…¨è·å–error_message
        error_msg = record.get('error_message', None) if isinstance(record, dict) else getattr(record, 'error_message', None)
        if error_msg:
            error_type = self._classify_error(error_msg)
            if error_type == "timeout":
                task_data["timeout_errors"] += 1
            elif error_type == "format":
                task_data["tool_call_format_errors"] += 1
            elif error_type == "max_turns":
                task_data["max_turns_errors"] += 1
            elif error_type == "tool_selection":
                task_data["tool_selection_errors"] += 1
            elif error_type == "parameter":
                task_data["parameter_config_errors"] += 1
            elif error_type == "sequence":
                task_data["sequence_order_errors"] += 1
            elif error_type == "dependency":
                task_data["dependency_errors"] += 1
            elif error_type in ["other", "tool_level_error", "unknown"]:
                task_data["other_errors"] += 1
        else:
            # æ— é”™è¯¯æ¶ˆæ¯ä½†éfull_successï¼Œæ£€æŸ¥execution_historyä¸­çš„å·¥å…·çº§é”™è¯¯
            execution_history = getattr(record, 'execution_history', [])
            
            if execution_history:
                # åˆ†æexecution_historyä¸­çš„å¤±è´¥å·¥å…·
                failed_tools = []
                for h in execution_history:
                    # å¤„ç†ä¸åŒç±»å‹çš„execution_historyæ¡ç›®
                    if hasattr(h, 'success'):
                        # ToolExecutionResultå¯¹è±¡
                        if not getattr(h, 'success', True):
                            failed_tools.append(h)
                    elif isinstance(h, dict):
                        # å­—å…¸æ ¼å¼
                        if not h.get('success', True):
                            failed_tools.append(h)
                
                if failed_tools:
                    # æœ‰å·¥å…·æ‰§è¡Œå¤±è´¥ï¼ŒæŒ‰æœ€ä¸»è¦çš„é”™è¯¯ç±»å‹åˆ†ç±»
                    error_types = []
                    for failed in failed_tools:
                        if hasattr(failed, 'error'):
                            error = failed.error or ''
                        else:
                            error = failed.get('error', '')
                        if error:
                            error_types.append(self._classify_error(error))
                    
                    if error_types:
                        # é€‰æ‹©æœ€å¸¸è§çš„é”™è¯¯ç±»å‹
                        from collections import Counter
                        most_common_error = Counter(error_types).most_common(1)[0][0]
                        
                        if most_common_error == "timeout":
                            task_data["timeout_errors"] += 1
                        elif most_common_error == "format":
                            # å†æ¬¡æ£€æŸ¥æ˜¯å¦ç¡®å®æ˜¯æ ¼å¼é”™è¯¯
                            if self._is_confirmed_format_error([most_common_error]):
                                task_data["tool_call_format_errors"] += 1
                            else:
                                # ä¸ç¡®å®šçš„æ ¼å¼é”™è¯¯ï¼Œå½’ç±»ä¸ºå…¶ä»–é”™è¯¯
                                task_data["other_errors"] += 1
                        elif most_common_error == "max_turns":
                            task_data["max_turns_errors"] += 1
                        elif most_common_error == "tool_selection":
                            task_data["tool_selection_errors"] += 1
                        elif most_common_error == "parameter":
                            task_data["parameter_config_errors"] += 1
                        elif most_common_error == "sequence":
                            task_data["sequence_order_errors"] += 1
                        elif most_common_error == "dependency":
                            task_data["dependency_errors"] += 1
                        else:
                            task_data["other_errors"] += 1
                    else:
                        # æ— æ³•åˆ†ç±»çš„é”™è¯¯ï¼Œä¸å½’ä¸ºother_errors
                        # task_data["other_errors"] += 1
                        pass  # è·³è¿‡æœªåˆ†ç±»çš„é”™è¯¯
                else:
                    # æ²¡æœ‰å·¥å…·å¤±è´¥ï¼Œä½†éfull_successï¼Œä¸å½’ä¸ºother_errors
                    # task_data["other_errors"] += 1
                    pass  # è·³è¿‡æœªåˆ†ç±»çš„é”™è¯¯
            else:
                # æ²¡æœ‰execution_historyï¼Œæ— æ³•è¿›ä¸€æ­¥åˆ†æï¼Œä¸å½’ä¸ºother_errors
                # task_data["other_errors"] += 1
                pass  # è·³è¿‡æœªåˆ†ç±»çš„é”™è¯¯
    
    def _update_prompt_summary(self, prompt_data):
        """æ›´æ–°prompt_typeçº§åˆ«çš„æ±‡æ€»ç»Ÿè®¡"""
        summary = prompt_data.get("summary", {})
        
        # åˆå§‹åŒ–summaryå¦‚æœä¸å­˜åœ¨
        if not summary:
            summary = {
                "total": 0,
                "success": 0,
                "success_rate": 0.0,
                "weighted_success_score": 0.0,
                "full_success_rate": 0.0,
                "partial_success_rate": 0.0,
                "failure_rate": 0.0,
                "assisted_failure": 0,
                "assisted_success": 0,
                "total_assisted_turns": 0,
                "tests_with_assistance": 0,
                "avg_assisted_turns": 0.0,
                "assisted_success_rate": 0.0,
                "assistance_rate": 0.0,
                "avg_execution_time": 0.0,
                "avg_turns": 0.0,
                "avg_tool_calls": 0.0,
                "tool_coverage_rate": 0.0,
                "avg_workflow_score": 0.0,
                "avg_phase2_score": 0.0,
                "avg_quality_score": 0.0,
                "avg_final_score": 0.0,
                "total_errors": 0,
                "tool_call_format_errors": 0,
                "timeout_errors": 0,
                "dependency_errors": 0,
                "parameter_config_errors": 0,
                "tool_selection_errors": 0,
                "sequence_order_errors": 0,
                "max_turns_errors": 0,
                "other_errors": 0,
                "tool_selection_error_rate": 0.0,
                "parameter_error_rate": 0.0,
                "sequence_error_rate": 0.0,
                "dependency_error_rate": 0.0,
                "timeout_error_rate": 0.0,
                "format_error_rate": 0.0,
                "max_turns_error_rate": 0.0
            }
        
        # ä»æ‰€æœ‰task_typeèšåˆæ•°æ®
        total = 0
        success = 0
        full_success = 0
        partial_success = 0
        
        for rate_data in prompt_data.get("by_tool_success_rate", {}).values():
            for diff_data in rate_data.get("by_difficulty", {}).values():
                for task_data in diff_data.get("by_task_type", {}).values():
                    total += task_data.get("total", 0)
                    success += task_data.get("success", 0)
                    full_success += task_data.get("full_success", 0)
                    partial_success += task_data.get("partial_success", 0)
        
        # æ›´æ–°æ±‡æ€»
        summary["total"] = total
        summary["success"] = success
        if total > 0:
            summary["success_rate"] = success / total
            summary["weighted_success_score"] = (full_success * 1.0 + partial_success * 0.5) / total
            summary["full_success_rate"] = full_success / total
            summary["partial_success_rate"] = partial_success / total
            summary["failure_rate"] = 1.0 - summary["success_rate"]
        
        prompt_data["summary"] = summary
    
    def _classify_error(self, error_msg: str) -> str:
        """Classify error message into specific error type"""
        if not error_msg:
            return 'unknown'
        
        error_lower = error_msg.lower()
        
        # Format errors
        if any(x in error_lower for x in ['format errors detected', 'format recognition issue', 
                                          'tool call format', 'understand tool call format']):
            return 'format'
        
        # Max turns without tool calls (also format)
        if ('no tool calls' in error_lower and 'turns' in error_lower) or \
           ('max turns reached' in error_lower and 'no tool calls' in error_lower):
            return 'format'
        
        # Pure max turns
        if 'max turns reached' in error_lower:
            return 'max_turns'
        
        # Agent-level timeout (not tool-level timeout)
        if ('test timeout after' in error_lower) or \
           ('timeout after' in error_lower and ('seconds' in error_lower or 'minutes' in error_lower)) or \
           ('agent timeout' in error_lower) or \
           ('execution timeout' in error_lower):
            return 'timeout'
        
        # Tool selection
        if ('tool' in error_lower and ('select' in error_lower or 'choice' in error_lower)) or \
           'tool calls failed' in error_lower:
            return 'tool_selection'
        
        # Parameter errors
        if any(x in error_lower for x in ['parameter', 'argument', 'invalid_input', 
                                          'permission_denied', 'validation failed']):
            return 'parameter'
        
        # Sequence errors
        if any(x in error_lower for x in ['sequence', 'order', 'required tools not completed']):
            return 'sequence'
        
        # Dependency errors
        if 'dependency' in error_lower or 'prerequisite' in error_lower:
            return 'dependency'
        
        # Tool-level errors (not Agent errors) - for statistical completeness
        if any(tool_error in error_lower for tool_error in ['timeout', 'network_error', 'permission_denied', 
                                                           'file_not_found', 'operation_failed', 'resource_error']):
            return 'tool_level_error'
        
        # Other
        return 'other'# æ·»åŠ è¾…åŠ©æ–¹æ³•åˆ°æ–‡ä»¶æœ«å°¾

    def _is_confirmed_format_error(self, error_types: list) -> bool:
        """ç¡®è®¤æ˜¯å¦æ˜¯çœŸæ­£çš„æ ¼å¼é”™è¯¯"""
        # è¿™ä¸ªæ–¹æ³•å¯ä»¥æ ¹æ®å…·ä½“æƒ…å†µè¿›ä¸€æ­¥ä¼˜åŒ–
        # ç›®å‰å…ˆè¿”å›Trueä¿æŒç°æœ‰è¡Œä¸ºï¼Œä½†å¯ä»¥åç»­æ”¹è¿›
        return True
    
    def save_database_enhanced(self):
        """å¢å¼ºçš„æ•°æ®åº“ä¿å­˜æ–¹æ³• - æ›´å¼ºçš„æ•°æ®ä¿æŠ¤ï¼Œé¿å…å¡æ­»"""
        import time
        import os
        
        print(f"[SAVE_ENHANCED] å¼€å§‹å¢å¼ºä¿å­˜ï¼Œæ—¶é—´: {datetime.now().strftime('%H:%M:%S')}")
        
        # è®¾ç½®ä¿å­˜è¶…æ—¶æ—¶é—´ï¼Œé¿å…æ— é™ç­‰å¾…
        MAX_SAVE_TIME = 30  # 30ç§’è¶…æ—¶
        start_time = time.time()
        
        try:
            # å…ˆå°è¯•ä½¿ç”¨çˆ¶ç±»çš„å®‰å…¨ä¿å­˜æ–¹æ³•
            if hasattr(self, 'file_lock') and self.file_lock:
                # æœ‰æ–‡ä»¶é”çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨éé˜»å¡æœºåˆ¶
                print(f"[SAVE_ENHANCED] ä½¿ç”¨æ–‡ä»¶é”æœºåˆ¶ä¿å­˜")
                
                def timeout_update_func(current_data):
                    # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
                    if time.time() - start_time > MAX_SAVE_TIME:
                        raise TimeoutError(f"ä¿å­˜æ“ä½œè¶…æ—¶({MAX_SAVE_TIME}ç§’)")
                    
                    # æ™ºèƒ½åˆå¹¶é€»è¾‘
                    if current_data and isinstance(current_data, dict):
                        merged_models = self.database.get("models", {}).copy()
                        
                        # ä¿ç•™ç£ç›˜ä¸Šå…¶ä»–è¿›ç¨‹å†™å…¥çš„æ•°æ®
                        for model_name, disk_model_data in current_data.get("models", {}).items():
                            if model_name not in merged_models:
                                merged_models[model_name] = disk_model_data
                                print(f"[SAVE_ENHANCED] ä¿ç•™å…¶ä»–è¿›ç¨‹çš„æ¨¡å‹æ•°æ®: {model_name}")
                            else:
                                # åˆå¹¶prompt_typeæ•°æ®
                                memory_model = merged_models[model_name]
                                if isinstance(memory_model, dict) and isinstance(disk_model_data, dict):
                                    memory_prompts = memory_model.get("by_prompt_type", {})
                                    disk_prompts = disk_model_data.get("by_prompt_type", {})
                                    
                                    # ä¿ç•™ç£ç›˜ä¸Šçš„æ–°prompt_type
                                    for prompt_type, disk_prompt_data in disk_prompts.items():
                                        if prompt_type not in memory_prompts:
                                            memory_prompts[prompt_type] = disk_prompt_data
                                            print(f"[SAVE_ENHANCED] ä¿ç•™{model_name}çš„æ–°prompt_type: {prompt_type}")
                                    
                                    memory_model["by_prompt_type"] = memory_prompts
                        
                        self.database["models"] = merged_models
                    
                    return self._serialize_database()
                
                # å°è¯•å®‰å…¨æ›´æ–°
                success = self.file_lock.update_json_safe(timeout_update_func)
                if success:
                    print(f"[SAVE_ENHANCED] æ–‡ä»¶é”ä¿å­˜æˆåŠŸ")
                    return
                else:
                    print(f"[SAVE_ENHANCED] æ–‡ä»¶é”ä¿å­˜å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•")
            
            # å¤‡ç”¨ä¿å­˜æ–¹æ³•ï¼šéé˜»å¡åŸå­å†™å…¥
            self._save_database_atomic_safe()
            
        except Exception as e:
            print(f"[SAVE_ENHANCED] ä¿å­˜è¿‡ç¨‹å‡ºç°å¼‚å¸¸: {e}")
            # æœ€åçš„å¤‡ç”¨æ–¹æ³•ï¼šç›´æ¥è°ƒç”¨çˆ¶ç±»æ–¹æ³•
            try:
                super().save_database()
                print(f"[SAVE_ENHANCED] ä½¿ç”¨çˆ¶ç±»å¤‡ç”¨æ–¹æ³•ä¿å­˜æˆåŠŸ")
            except Exception as e2:
                print(f"[SAVE_ENHANCED] æ‰€æœ‰ä¿å­˜æ–¹æ³•éƒ½å¤±è´¥: {e2}")
                raise
    
    def _save_database_atomic_safe(self):
        """åŸå­å®‰å…¨ä¿å­˜ - éé˜»å¡ç‰ˆæœ¬"""
        import tempfile
        import shutil
        
        print(f"[ATOMIC_SAFE] å¼€å§‹åŸå­å®‰å…¨ä¿å­˜")
        
        with self.lock:
            # æ›´æ–°æ—¶é—´æˆ³
            self.database["last_updated"] = datetime.now().isoformat()
            
            # å¦‚æœå­˜åœ¨æ•°æ®åº“æ–‡ä»¶ï¼Œå…ˆè¯»å–å¹¶åˆå¹¶
            if self.db_file.exists():
                try:
                    with open(self.db_file, 'r', encoding='utf-8') as f:
                        disk_data = json.load(f)
                    
                    # ç®€å•çš„æ•°æ®åˆå¹¶ï¼šä¿ç•™ç£ç›˜ä¸Šå…¶ä»–è¿›ç¨‹æ·»åŠ çš„æ¨¡å‹
                    if "models" in disk_data:
                        for model_name, disk_model_data in disk_data["models"].items():
                            if model_name not in self.database.get("models", {}):
                                self.database["models"][model_name] = disk_model_data
                                print(f"[ATOMIC_SAFE] åˆå¹¶ç£ç›˜æ¨¡å‹: {model_name}")
                                
                except Exception as e:
                    print(f"[ATOMIC_SAFE] åˆå¹¶ç£ç›˜æ•°æ®æ—¶å‡ºé”™(ç»§ç»­ä¿å­˜): {e}")
            
            # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶åŸå­å†™å…¥
            try:
                # åˆ›å»ºä¸´æ—¶æ–‡ä»¶åœ¨åŒä¸€ç›®å½•ä¸‹
                temp_fd, temp_path = tempfile.mkstemp(
                    dir=self.db_file.parent, 
                    suffix='.tmp', 
                    prefix=f'{self.db_file.stem}_'
                )
                
                with os.fdopen(temp_fd, 'w', encoding='utf-8') as f:
                    json.dump(self._serialize_database(), f, indent=2, ensure_ascii=False)
                
                # åŸå­æ›¿æ¢
                shutil.move(temp_path, self.db_file)
                print(f"[ATOMIC_SAFE] åŸå­ä¿å­˜æˆåŠŸ")
                
            except Exception as e:
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                try:
                    if os.path.exists(temp_path):
                        os.unlink(temp_path)
                except:
                    pass
                raise e
