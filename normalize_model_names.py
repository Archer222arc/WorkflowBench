#!/usr/bin/env python3
"""
æ¨¡å‹åç§°å½’ä¸€åŒ–å·¥å…·
å°†å¹¶è¡Œå®ä¾‹ï¼ˆå¦‚deepseek-v3-0324-2ï¼‰å½’å¹¶åˆ°ä¸»æ¨¡å‹åï¼ˆå¦‚DeepSeek-V3-0324ï¼‰
"""

import json
import pandas as pd
from pathlib import Path
from datetime import datetime
import shutil
from typing import Dict, List, Tuple

# æ¨¡å‹åç§°æ˜ å°„è§„åˆ™
MODEL_NORMALIZATION_RULES = {
    # DeepSeekç³»åˆ—
    'deepseek-v3-0324-2': 'DeepSeek-V3-0324',
    'deepseek-v3-0324-3': 'DeepSeek-V3-0324',
    'deepseek-r1-0528-2': 'DeepSeek-R1-0528',
    'deepseek-r1-0528-3': 'DeepSeek-R1-0528',
    
    # Llamaç³»åˆ—
    'llama-3.3-70b-instruct-2': 'Llama-3.3-70B-Instruct',
    'llama-3.3-70b-instruct-3': 'Llama-3.3-70B-Instruct',
    
    # Qwenç³»åˆ— - è¿™äº›æ˜¯ä¸åŒçš„æ¨¡å‹ï¼Œä¸åº”åˆå¹¶
    # 'qwen2.5-32b-instruct': 'qwen2.5-32b-instruct',  # ä¿æŒåŸæ ·
    # 'qwen2.5-3b-instruct': 'qwen2.5-3b-instruct',    # ä¿æŒåŸæ ·
}

def normalize_model_name(model_name: str) -> str:
    """
    æ ‡å‡†åŒ–æ¨¡å‹åç§°
    å°†å¹¶è¡Œå®ä¾‹åç§°è½¬æ¢ä¸ºä¸»æ¨¡å‹åç§°
    """
    # æ£€æŸ¥æ˜¯å¦åœ¨æ˜ å°„è§„åˆ™ä¸­
    if model_name in MODEL_NORMALIZATION_RULES:
        return MODEL_NORMALIZATION_RULES[model_name]
    
    # é€šç”¨è§„åˆ™ï¼šå»é™¤ -2, -3 ç­‰åç¼€ï¼ˆä»…å¯¹å·²çŸ¥çš„å¹¶è¡Œæ¨¡å‹ï¼‰
    if any(base in model_name.lower() for base in ['deepseek', 'llama', 'grok']):
        # ç§»é™¤ -æ•°å­— åç¼€
        import re
        cleaned = re.sub(r'-\d+$', '', model_name)
        
        # æ ‡å‡†åŒ–å¤§å°å†™
        if 'deepseek-v3' in cleaned.lower():
            return 'DeepSeek-V3-0324'
        elif 'deepseek-r1' in cleaned.lower():
            return 'DeepSeek-R1-0528'
        elif 'llama-3.3' in cleaned.lower():
            return 'Llama-3.3-70B-Instruct'
    
    return model_name

def fix_parquet_data():
    """ä¿®å¤Parquetæ•°æ®ä¸­çš„æ¨¡å‹åç§°"""
    parquet_file = Path('pilot_bench_parquet_data/test_results.parquet')
    
    if not parquet_file.exists():
        print("âŒ Parquetæ–‡ä»¶ä¸å­˜åœ¨")
        return False
    
    # å¤‡ä»½åŸæ–‡ä»¶
    backup_file = parquet_file.with_suffix('.parquet.backup')
    shutil.copy(parquet_file, backup_file)
    print(f"âœ… å·²å¤‡ä»½åˆ° {backup_file}")
    
    # è¯»å–æ•°æ®
    df = pd.read_parquet(parquet_file)
    original_count = len(df)
    
    # ç»Ÿè®¡éœ€è¦ä¿®æ”¹çš„è®°å½•
    models_to_fix = df['model'].apply(lambda x: x in MODEL_NORMALIZATION_RULES)
    fix_count = models_to_fix.sum()
    
    print(f"\nğŸ“Š Parquetæ•°æ®ç»Ÿè®¡:")
    print(f"  æ€»è®°å½•æ•°: {original_count}")
    print(f"  éœ€è¦ä¿®å¤: {fix_count}")
    
    if fix_count > 0:
        # æ˜¾ç¤ºä¿®æ”¹è¯¦æƒ…
        print("\nğŸ“ ä¿®æ”¹è¯¦æƒ…:")
        for old_name in MODEL_NORMALIZATION_RULES.keys():
            count = (df['model'] == old_name).sum()
            if count > 0:
                new_name = MODEL_NORMALIZATION_RULES[old_name]
                print(f"  {old_name} -> {new_name}: {count} æ¡")
        
        # åº”ç”¨å½’ä¸€åŒ–
        df['model'] = df['model'].apply(normalize_model_name)
        
        # é‡æ–°ç”Ÿæˆtest_idï¼ˆå¦‚æœéœ€è¦ï¼‰
        # æ³¨æ„ï¼šä¿ç•™åŸå§‹test_idå¯èƒ½æ›´å¥½ï¼Œç”¨äºè¿½è¸ª
        
        # ä¿å­˜ä¿®å¤åçš„æ•°æ®
        df.to_parquet(parquet_file, index=False)
        print(f"\nâœ… Parquetæ•°æ®å·²ä¿®å¤")
        
        # æ˜¾ç¤ºåˆå¹¶åçš„ç»Ÿè®¡
        print("\nğŸ“ˆ åˆå¹¶åç»Ÿè®¡:")
        for model in df['model'].unique():
            if model in ['DeepSeek-V3-0324', 'DeepSeek-R1-0528', 'Llama-3.3-70B-Instruct']:
                count = (df['model'] == model).sum()
                print(f"  {model}: {count} æ¡")
    else:
        print("âœ… Parquetæ•°æ®æ— éœ€ä¿®å¤")
    
    return True

def fix_json_data():
    """ä¿®å¤JSONæ•°æ®ä¸­çš„æ¨¡å‹åç§°"""
    json_file = Path('pilot_bench_cumulative_results/master_database.json')
    
    if not json_file.exists():
        print("âŒ JSONæ–‡ä»¶ä¸å­˜åœ¨")
        return False
    
    # å¤‡ä»½åŸæ–‡ä»¶
    backup_file = json_file.with_suffix('.json.backup')
    shutil.copy(json_file, backup_file)
    print(f"âœ… å·²å¤‡ä»½åˆ° {backup_file}")
    
    # è¯»å–æ•°æ®
    with open(json_file, 'r') as f:
        data = json.load(f)
    
    models_data = data.get('models', {})
    
    # ç»Ÿè®¡éœ€è¦åˆå¹¶çš„æ¨¡å‹
    models_to_merge = [m for m in models_data.keys() if m in MODEL_NORMALIZATION_RULES]
    
    print(f"\nğŸ“Š JSONæ•°æ®ç»Ÿè®¡:")
    print(f"  æ€»æ¨¡å‹æ•°: {len(models_data)}")
    print(f"  éœ€è¦åˆå¹¶: {len(models_to_merge)}")
    
    if models_to_merge:
        print("\nğŸ“ åˆå¹¶è¯¦æƒ…:")
        
        for old_name in models_to_merge:
            new_name = MODEL_NORMALIZATION_RULES[old_name]
            old_data = models_data[old_name]
            
            print(f"\n  {old_name} -> {new_name}")
            print(f"    æµ‹è¯•æ•°: {old_data.get('total_tests', 0)}")
            
            # å¦‚æœç›®æ ‡æ¨¡å‹å·²å­˜åœ¨ï¼Œéœ€è¦åˆå¹¶æ•°æ®
            if new_name in models_data:
                print(f"    âš ï¸ ç›®æ ‡æ¨¡å‹å·²å­˜åœ¨ï¼Œåˆå¹¶æ•°æ®...")
                merge_model_data(models_data[new_name], old_data)
            else:
                # ç›´æ¥æ”¹å
                models_data[new_name] = old_data
                models_data[new_name]['model_name'] = new_name
            
            # åˆ é™¤æ—§æ¨¡å‹
            del models_data[old_name]
        
        # æ›´æ–°æ±‡æ€»ç»Ÿè®¡
        update_summary_stats(data)
        
        # ä¿å­˜ä¿®å¤åçš„æ•°æ®
        data['last_updated'] = datetime.now().isoformat()
        with open(json_file, 'w') as f:
            json.dump(data, f, indent=2)
        
        print(f"\nâœ… JSONæ•°æ®å·²ä¿®å¤")
        
        # æ˜¾ç¤ºåˆå¹¶åçš„ç»Ÿè®¡
        print("\nğŸ“ˆ åˆå¹¶åç»Ÿè®¡:")
        for model in ['DeepSeek-V3-0324', 'DeepSeek-R1-0528', 'Llama-3.3-70B-Instruct']:
            if model in models_data:
                tests = models_data[model].get('total_tests', 0)
                print(f"  {model}: {tests} ä¸ªæµ‹è¯•")
    else:
        print("âœ… JSONæ•°æ®æ— éœ€ä¿®å¤")
    
    return True

def merge_model_data(target_data: Dict, source_data: Dict):
    """
    åˆå¹¶ä¸¤ä¸ªæ¨¡å‹çš„æ•°æ®
    """
    # æ›´æ–°æ€»æµ‹è¯•æ•°
    target_data['total_tests'] = target_data.get('total_tests', 0) + source_data.get('total_tests', 0)
    
    # åˆå¹¶overall_stats
    if 'overall_stats' in source_data:
        if 'overall_stats' not in target_data:
            target_data['overall_stats'] = source_data['overall_stats']
        else:
            # éœ€è¦é‡æ–°è®¡ç®—ç»Ÿè®¡æ•°æ®
            merge_stats(target_data['overall_stats'], source_data['overall_stats'])
    
    # åˆå¹¶by_prompt_typeå±‚æ¬¡ç»“æ„
    if 'by_prompt_type' in source_data:
        if 'by_prompt_type' not in target_data:
            target_data['by_prompt_type'] = source_data['by_prompt_type']
        else:
            merge_hierarchical_data(target_data['by_prompt_type'], source_data['by_prompt_type'])
    
    # æ›´æ–°æ—¶é—´æˆ³
    target_data['last_test_time'] = datetime.now().isoformat()

def merge_stats(target_stats: Dict, source_stats: Dict):
    """åˆå¹¶ç»Ÿè®¡æ•°æ®"""
    # ç´¯åŠ è®¡æ•°ç±»å­—æ®µ
    for field in ['total_tests', 'successful', 'partial', 'failed']:
        if field in source_stats:
            target_stats[field] = target_stats.get(field, 0) + source_stats[field]
    
    # é‡æ–°è®¡ç®—æ¯”ç‡
    total = target_stats.get('total_tests', 0)
    if total > 0:
        target_stats['success_rate'] = target_stats.get('successful', 0) / total
        target_stats['partial_rate'] = target_stats.get('partial', 0) / total
        target_stats['failure_rate'] = target_stats.get('failed', 0) / total

def merge_hierarchical_data(target: Dict, source: Dict):
    """é€’å½’åˆå¹¶å±‚æ¬¡åŒ–æ•°æ®"""
    for key, value in source.items():
        if key not in target:
            target[key] = value
        elif isinstance(value, dict) and isinstance(target[key], dict):
            # é€’å½’åˆå¹¶å­—å…¸
            merge_hierarchical_data(target[key], value)
        elif isinstance(value, (int, float)) and key in ['total', 'successful', 'partial', 'failed']:
            # ç´¯åŠ æ•°å€¼
            target[key] = target.get(key, 0) + value
        # å…¶ä»–æƒ…å†µä¿æŒtargetçš„å€¼

def update_summary_stats(data: Dict):
    """æ›´æ–°æ±‡æ€»ç»Ÿè®¡"""
    models_data = data.get('models', {})
    
    total_tests = 0
    total_success = 0
    total_partial = 0
    total_failure = 0
    
    for model_data in models_data.values():
        total_tests += model_data.get('total_tests', 0)
        stats = model_data.get('overall_stats', {})
        total_success += stats.get('successful', 0)
        total_partial += stats.get('partial', 0)
        total_failure += stats.get('failed', 0)
    
    data['summary'] = {
        'total_tests': total_tests,
        'total_success': total_success,
        'total_partial': total_partial,
        'total_failure': total_failure,
        'models_tested': list(models_data.keys()),
        'last_test_time': datetime.now().isoformat()
    }

def verify_normalization():
    """éªŒè¯å½’ä¸€åŒ–ç»“æœ"""
    print("\n" + "="*60)
    print("ğŸ” éªŒè¯å½’ä¸€åŒ–ç»“æœ")
    print("="*60)
    
    # éªŒè¯Parquet
    if Path('pilot_bench_parquet_data/test_results.parquet').exists():
        df = pd.read_parquet('pilot_bench_parquet_data/test_results.parquet')
        problematic = [m for m in df['model'].unique() if '-2' in m or '-3' in m]
        if problematic:
            print(f"âš ï¸ Parquetä¸­ä»æœ‰å¹¶è¡Œå®ä¾‹: {problematic}")
        else:
            print("âœ… Parquetæ•°æ®å·²å®Œå…¨å½’ä¸€åŒ–")
    
    # éªŒè¯JSON
    if Path('pilot_bench_cumulative_results/master_database.json').exists():
        with open('pilot_bench_cumulative_results/master_database.json') as f:
            data = json.load(f)
        
        problematic = [m for m in data.get('models', {}).keys() if '-2' in m or '-3' in m]
        if problematic:
            print(f"âš ï¸ JSONä¸­ä»æœ‰å¹¶è¡Œå®ä¾‹: {problematic}")
        else:
            print("âœ… JSONæ•°æ®å·²å®Œå…¨å½’ä¸€åŒ–")

def main():
    """ä¸»å‡½æ•°"""
    print("="*60)
    print("ğŸ”§ æ¨¡å‹åç§°å½’ä¸€åŒ–å·¥å…·")
    print("="*60)
    print("\næ­¤å·¥å…·å°†ï¼š")
    print("1. å°†å¹¶è¡Œå®ä¾‹ï¼ˆå¦‚deepseek-v3-0324-2ï¼‰åˆå¹¶åˆ°ä¸»æ¨¡å‹")
    print("2. å¤‡ä»½åŸå§‹æ•°æ®")
    print("3. æ›´æ–°Parquetå’ŒJSONæ•°æ®")
    print("")
    
    # ç¡®è®¤æ‰§è¡Œ
    response = input("ç¡®è®¤æ‰§è¡Œå½’ä¸€åŒ–ï¼Ÿ(y/n): ")
    if response.lower() != 'y':
        print("å·²å–æ¶ˆ")
        return
    
    print("\nå¼€å§‹å¤„ç†...")
    
    # ä¿®å¤Parquetæ•°æ®
    print("\n" + "="*40)
    print("1ï¸âƒ£ ä¿®å¤Parquetæ•°æ®")
    print("="*40)
    fix_parquet_data()
    
    # ä¿®å¤JSONæ•°æ®
    print("\n" + "="*40)
    print("2ï¸âƒ£ ä¿®å¤JSONæ•°æ®")
    print("="*40)
    fix_json_data()
    
    # éªŒè¯ç»“æœ
    verify_normalization()
    
    print("\n" + "="*60)
    print("âœ… å½’ä¸€åŒ–å®Œæˆï¼")
    print("="*60)
    print("\nå¤‡ä»½æ–‡ä»¶ï¼š")
    print("  - pilot_bench_parquet_data/test_results.parquet.backup")
    print("  - pilot_bench_cumulative_results/master_database.json.backup")

if __name__ == "__main__":
    main()