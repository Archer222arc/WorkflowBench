#!/usr/bin/env python3
"""
RTX 4070 GPUè®­ç»ƒè„šæœ¬ - å¢å¼ºç‰ˆ
æ”¯æŒè®­ç»ƒæ›´å¤§æ¨¡å‹ï¼Œä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼Œç¡®ä¿CPUæ¨ç†å…¼å®¹æ€§
ç”Ÿæˆä¸åŸè„šæœ¬å…¼å®¹çš„æ¨¡å‹æ ¼å¼
"""

import os
import sys
import json
import time
import torch
import numpy as np
import psutil
import GPUtil
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, List
import logging

# è®¾ç½®GPUä¼˜åŒ–å‚æ•°
os.environ['CUDA_LAUNCH_BLOCKING'] = '0'  # å¼‚æ­¥æ‰§è¡Œ
os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:512'  # å†…å­˜ç¢ç‰‡ä¼˜åŒ–
os.environ['CUDA_VISIBLE_DEVICES'] = '0'  # ä½¿ç”¨ç¬¬ä¸€ä¸ªGPU

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

# åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
log_filename = f"logs/debug_GPU_TRAINING_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
file_handler = logging.FileHandler(log_filename)
file_handler.setLevel(logging.DEBUG)

# åˆ›å»ºæ ¼å¼å™¨
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
file_handler.setFormatter(formatter)

# æ·»åŠ å¤„ç†å™¨åˆ°logger
logger.addHandler(file_handler)


def load_config_file():
    """Load configuration from config directory
    
    Returns:
        dict: Configuration dictionary or empty dict if not found
    """
    config_files = ['config/config.json', 'config/api_keys.json', 'config/api-keys.json']
    
    for config_file in config_files:
        config_path = Path(config_file)
        if config_path.exists():
            try:
                with open(config_path, 'r') as f:
                    config_data = json.load(f)
                    logger.info(f"Loaded configuration from {config_file}")
                    return config_data
            except Exception as e:
                logger.warning(f"Failed to load {config_file}: {e}")
                continue
    
    return {}


def get_api_key():
    """Get API key from config file or environment variable
    
    Returns:
        str: API key or None if not found
    """
    # é¦–å…ˆå°è¯•ä»é…ç½®æ–‡ä»¶åŠ è½½
    config = load_config_file()
    
    # å°è¯•å¤šä¸ªå¯èƒ½çš„é”®å
    api_key = (config.get('openai_api_key') or 
               config.get('OPENAI_API_KEY') or 
               config.get('api-key') or 
               config.get('api_key') or 
               os.getenv('OPENAI_API_KEY'))
    
    if api_key:
        logger.info("API key loaded successfully")
    else:
        logger.warning("No API key found in config or environment")
    
    return api_key


# åœ¨éœ€è¦ä½¿ç”¨API keyçš„åœ°æ–¹è®¾ç½®ç¯å¢ƒå˜é‡
api_key = get_api_key()
if api_key:
    os.environ['OPENAI_API_KEY'] = api_key


# æ–‡ä»¶ï¼šgpu_training_script.py
# ä½ç½®ï¼šGPU4070TrainingConfigç±»ï¼ˆçº¦ç¬¬45-120è¡Œï¼‰
# æ³¨æ„ï¼šæ·»åŠ æ–°çš„é…ç½®å‚æ•°

class GPU4070TrainingConfig:
    """RTX 4070ä¼˜åŒ–é…ç½®"""
    
    def __init__(self):
        # RTX 4070è§„æ ¼ï¼š12GB VRAM, 5888 CUDA cores
        self.gpu_memory_gb = 12
        self.cuda_cores = 5888
        
        # è®­ç»ƒé…ç½® - åˆ©ç”¨GPUæ€§èƒ½
        self.config = {
            # åŸºç¡€å‚æ•°
            # ç½‘ç»œæ¶æ„ - æ›´å¤§çš„æ¨¡å‹
            "hidden_dim": 1024,
            "num_layers": 4,
            "num_heads": 8,
            "dropout": 0.1,
            "use_layer_norm": True,
            "activation": "gelu",

            # è®­ç»ƒå‚æ•° - GPUä¼˜åŒ–
            "batch_size": 512,       # å¯ä»¥å¤§å¹…å¢åŠ 
            "mini_batch_size": 128,  # ç›¸åº”å¢åŠ 
            "learning_rate": 0.0002,
            "gradient_accumulation_steps": 2,
            
            # æ–°å¢ï¼šrequired_toolsè¾“å…¥æ”¯æŒ
            "use_tools_input": True,  # å¯ç”¨toolsè¾“å…¥
            "tools_dim": 1536,  # tools embeddingç»´åº¦
            "num_tools": 30,  # å·¥å…·æ€»æ•°ï¼ˆæ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰
            
            
            # å­¦ä¹ ç‡è°ƒåº¦
            "use_lr_scheduler": True,
            "lr_warmup_steps": 100,
            "lr_decay_factor": 0.99,
            
            # PPOç‰¹å®šå‚æ•°
            "n_steps": 2048,
            "n_epochs": 10,
            "clip_range": 0.2,
            "clip_range_vf": None,
            "ent_coef": 0.01,
            "vf_coef": 0.5,
            "max_grad_norm": 0.5,
            "gamma": 0.99,
            "gae_lambda": 0.95,
            
            # å†…å­˜ä¼˜åŒ–
            "memory_size": 100000,
            "use_mixed_precision": True,
            "gradient_checkpointing": False,
            
            # å¢å¼ºç‰¹æ€§
            "use_task_aware_buffer": True,
            "buffer_capacity_per_task": 200,
            "use_teacher_guidance": False,
            'teacher_guidance_start_prob': 0.01,
            'teacher_guidance_decay': 0.995,
            'teacher_guidance_min_prob': 0.001,
            "use_curriculum": True,
            "use_action_masking": True,
            
            # RAGå¢å¼º
            "rag_dim": 64,
            "use_rag_enhancement": True,
            "embedding_cache_size": 5000,
            
            # æ­£åˆ™åŒ–
            "l2_reg": 0.0001,
            "gradient_penalty": 0.0005,
            
            # ä¿å­˜é¢‘ç‡
            'checkpoint_frequency': 100,
            'checkpoint_keep_recent': 3,
            'checkpoint_keep_interval': 500,
            'checkpoint_size_limit_mb': 500,
            "evaluation_frequency": 100,
            "save_best_only": False,
            
            # CPUæ¨ç†ä¼˜åŒ–
            "optimize_for_cpu": True,
            "use_quantization_aware_training": True,
            "target_cpu_threads": 12,
            
            # æ¸è¿›å¼è®­ç»ƒ
            "progressive_training": True,
            "progressive_stages": [
                {"episodes": 500, "hidden_dim": 256, "batch_size": 128},
                {"episodes": 1000, "hidden_dim": 384, "batch_size": 192},
                {"episodes": 1500, "hidden_dim": 512, "batch_size": 256}
            ]
        }
        
        # å°†algorithmæ·»åŠ åˆ°configä¸­
        self.config['algorithm'] = self.config.get('algorithm', 'ppo')
        
        # è®¡ç®—å®é™…å¯ç”¨çš„batché…ç½®
        self._optimize_batch_sizes()
    
    def _optimize_batch_sizes(self):
        """æ ¹æ®GPUå†…å­˜ä¼˜åŒ–batchå¤§å°"""
        # ä¼°ç®—æ¨¡å‹å¤§å°ï¼ˆMBï¼‰
        model_params = self.config["hidden_dim"] ** 2 * self.config["num_layers"] * 4
        model_size_mb = model_params * 4 / (1024 * 1024)  # float32
        
        # ä¼°ç®—å¯ç”¨äºbatchçš„å†…å­˜ï¼ˆä¿å®ˆä¼°è®¡ï¼‰
        available_memory_mb = self.gpu_memory_gb * 1024 * 0.7  # 70%å¯ç”¨
        
        # åŠ¨æ€è°ƒæ•´batchå¤§å°
        if model_size_mb > available_memory_mb * 0.3:
            self.config["batch_size"] = min(256, self.config["batch_size"])
            self.config["mini_batch_size"] = min(32, self.config["mini_batch_size"])
            logger.warning(f"Model too large, reducing batch sizes")
        
        # ç¡®ä¿batch_sizeæ˜¯mini_batch_sizeçš„æ•´æ•°å€  # <- ä¿®æ”¹äº†è¿™ä¸€è¡Œï¼šæ–°å¢éªŒè¯
        if self.config["batch_size"] % self.config["mini_batch_size"] != 0:  # <- ä¿®æ”¹äº†è¿™ä¸€è¡Œ
            self.config["batch_size"] = (self.config["batch_size"] // self.config["mini_batch_size"]) * self.config["mini_batch_size"]  # <- ä¿®æ”¹äº†è¿™ä¸€è¡Œ


class EnhancedGPUTrainer:
    """å¢å¼ºçš„GPUè®­ç»ƒå™¨"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.scaler = torch.cuda.amp.GradScaler() if config["use_mixed_precision"] else None
        
        logger.info(f"Using device: {self.device}")
        if self.device.type == "cuda":
            logger.info(f"GPU: {torch.cuda.get_device_name()}")
            logger.info(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    def check_gpu_status(self) -> Dict[str, float]:
        """æ£€æŸ¥GPUçŠ¶æ€"""
        if not torch.cuda.is_available():
            return {}
        
        try:
            gpus = GPUtil.getGPUs()
            if gpus:
                gpu = gpus[0]
                return {
                    'gpu_usage': gpu.load * 100,
                    'memory_usage': gpu.memoryUtil * 100,
                    'memory_used_mb': gpu.memoryUsed,
                    'temperature': gpu.temperature
                }
        except:
            pass
        
        # å¤‡ç”¨æ–¹æ³•
        return {
            'memory_allocated_mb': torch.cuda.memory_allocated() / 1e6,
            'memory_reserved_mb': torch.cuda.memory_reserved() / 1e6
        }
    
    def optimize_for_cpu_inference(self, model: torch.nn.Module) -> torch.nn.Module:
        """ä¼˜åŒ–æ¨¡å‹ä»¥ä¾¿CPUæ¨ç†"""
        logger.info("Optimizing model for CPU inference...")
        
        # 1. ç¡®ä¿æ¨¡å‹åœ¨evalæ¨¡å¼
        model.eval()
        
        # 2. è½¬æ¢ä¸ºCPUå¹¶ä¼˜åŒ–
        model = model.cpu()
        
        # 3. JITç¼–è¯‘ä¼˜åŒ–ï¼ˆå¯é€‰ï¼‰
        if self.config.get("use_jit_optimization", True):
            try:
                example_input = torch.randn(1, model.input_dim)
                model = torch.jit.trace(model, example_input)
                logger.info("JIT compilation successful")
            except Exception as e:
                logger.warning(f"JIT compilation failed: {e}")
        
        # 4. é‡åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.config.get("use_quantization_aware_training", True):
            try:
                model = torch.quantization.quantize_dynamic(
                    model, 
                    {torch.nn.Linear, torch.nn.LSTM, torch.nn.GRU},
                    dtype=torch.qint8
                )
                logger.info("Dynamic quantization applied")
            except Exception as e:
                logger.warning(f"Quantization failed: {e}")
        
        return model

def create_enhanced_network(state_dim: int, action_dim: int, config: Dict[str, Any]) -> torch.nn.Module:
    """åˆ›å»ºå¢å¼ºçš„ç¥ç»ç½‘ç»œ"""
    import torch.nn as nn
    
    # å¦‚æœæ˜¯DQNï¼Œè¿”å›å…¼å®¹çš„DuelingDQNç½‘ç»œ
    if config.get('algorithm') == 'dqn':
        class EnhancedDuelingDQN(nn.Module):
            def __init__(self):
                super().__init__()
                hidden_dim = config["hidden_dim"]
                num_layers = config["num_layers"]
                dropout = config["dropout"]
                
                # å…±äº«å±‚
                layers = []
                input_dim = state_dim
                for i in range(num_layers - 1):
                    layers.extend([
                        nn.Linear(input_dim, hidden_dim),
                        nn.LayerNorm(hidden_dim) if config["use_layer_norm"] else nn.Identity(),
                        nn.GELU() if config["activation"] == "gelu" else nn.ReLU(),
                        nn.Dropout(dropout)
                    ])
                    input_dim = hidden_dim
                
                self.shared_layers = nn.Sequential(*layers)
                
                # Duelingæ¶æ„
                self.value_stream = nn.Sequential(
                    nn.Linear(hidden_dim, hidden_dim // 2),
                    nn.ReLU(),
                    nn.Linear(hidden_dim // 2, 1)
                )
                
                self.advantage_stream = nn.Sequential(
                    nn.Linear(hidden_dim, hidden_dim // 2),
                    nn.ReLU(),
                    nn.Linear(hidden_dim // 2, action_dim)
                )
                
                self._init_weights()
            
            def _init_weights(self):
                for m in self.modules():
                    if isinstance(m, nn.Linear):
                        nn.init.xavier_uniform_(m.weight)
                        if m.bias is not None:
                            nn.init.constant_(m.bias, 0)
            
            def forward(self, x):
                features = self.shared_layers(x)
                values = self.value_stream(features)
                advantages = self.advantage_stream(features)
                q_values = values + (advantages - advantages.mean(dim=1, keepdim=True))
                return q_values
        
        return EnhancedDuelingDQN()
    
    # PPOç½‘ç»œ
    class EnhancedPolicyNetwork(nn.Module):
        def __init__(self):
            super().__init__()
            self.input_dim = state_dim
            hidden_dim = config["hidden_dim"]
            num_layers = config["num_layers"]
            num_heads = config["num_heads"]
            dropout = config["dropout"]
            
            # è¾“å…¥å±‚
            self.input_layer = nn.Sequential(
                nn.Linear(state_dim, hidden_dim),
                nn.LayerNorm(hidden_dim) if config["use_layer_norm"] else nn.Identity(),
                nn.GELU() if config["activation"] == "gelu" else nn.ReLU(),
                nn.Dropout(dropout)
            )
            
            # Transformeré£æ ¼çš„ä¸­é—´å±‚
            self.attention_layers = nn.ModuleList()
            self.ff_layers = nn.ModuleList()
            self.layer_norms = nn.ModuleList()
            
            for _ in range(num_layers):
                # å¤šå¤´æ³¨æ„åŠ›
                self.attention_layers.append(
                    nn.MultiheadAttention(hidden_dim, num_heads, dropout=dropout, batch_first=True)
                )
                # å‰é¦ˆç½‘ç»œ
                self.ff_layers.append(nn.Sequential(
                    nn.Linear(hidden_dim, hidden_dim * 4),
                    nn.GELU(),
                    nn.Dropout(dropout),
                    nn.Linear(hidden_dim * 4, hidden_dim),
                    nn.Dropout(dropout)
                ))
                # Layer normalization
                self.layer_norms.extend([nn.LayerNorm(hidden_dim), nn.LayerNorm(hidden_dim)])
            
            # è¾“å‡ºå¤´
            self.policy_head = nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim // 2),
                nn.LayerNorm(hidden_dim // 2),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(hidden_dim // 2, action_dim)
            )
            
            self.value_head = nn.Sequential(
                nn.Linear(hidden_dim, hidden_dim // 2),
                nn.LayerNorm(hidden_dim // 2),
                nn.GELU(),
                nn.Dropout(dropout),
                nn.Linear(hidden_dim // 2, 1)
            )
            
            # RAGå¢å¼ºï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if config.get("use_rag_enhancement", True):
                self.rag_encoder = nn.Linear(config["rag_dim"], hidden_dim)
            
            # åˆå§‹åŒ–æƒé‡
            self._init_weights()
        
        def _init_weights(self):
            """Xavieråˆå§‹åŒ–"""
            for m in self.modules():
                if isinstance(m, nn.Linear):
                    nn.init.xavier_uniform_(m.weight)
                    if m.bias is not None:
                        nn.init.constant_(m.bias, 0)
        
        def forward(self, x, rag_features=None):
            # è¾“å…¥å¤„ç†
            x = self.input_layer(x)
            
            # RAGç‰¹å¾èåˆ
            if rag_features is not None and hasattr(self, 'rag_encoder'):
                rag_encoded = self.rag_encoder(rag_features)
                x = x + rag_encoded  # æ®‹å·®è¿æ¥
            
            # Transformerå±‚
            batch_size = x.size(0)
            x = x.unsqueeze(1)  # æ·»åŠ åºåˆ—ç»´åº¦
            
            for i in range(len(self.attention_layers)):
                # æ³¨æ„åŠ›
                ln_idx = i * 2
                residual = x
                x = self.layer_norms[ln_idx](x)
                x, _ = self.attention_layers[i](x, x, x)
                x = residual + x
                
                # å‰é¦ˆ
                residual = x
                x = self.layer_norms[ln_idx + 1](x)
                x = self.ff_layers[i](x)
                x = residual + x
            
            x = x.squeeze(1)  # ç§»é™¤åºåˆ—ç»´åº¦
            
            # è¾“å‡º
            policy_logits = self.policy_head(x)
            values = self.value_head(x)
            
            return policy_logits, values
    
    return EnhancedPolicyNetwork()

# æ–‡ä»¶ï¼šgpu_training_script.py
# ä½ç½®ï¼šç¬¬1-100è¡Œ
# æ³¨æ„ï¼šå¢å¼ºç‰ˆï¼Œç¡®ä¿best_model.ptå§‹ç»ˆåŒ…å«å®Œæ•´è®­ç»ƒçŠ¶æ€

def save_compatible_checkpoint(manager, checkpoint_path: Path, episode: int, 
                              cpu_optimized: bool = False, is_best: bool = False):
    """ä¿å­˜ä¸åŸè„šæœ¬å…¼å®¹çš„checkpointæ ¼å¼
    
    Args:
        manager: è®­ç»ƒç®¡ç†å™¨å®ä¾‹
        checkpoint_path: ä¿å­˜è·¯å¾„
        episode: å½“å‰è®­ç»ƒè½®æ•°
        cpu_optimized: æ˜¯å¦ç”ŸæˆCPUä¼˜åŒ–ç‰ˆæœ¬ï¼ˆç”¨äºæ¨ç†ï¼‰
        is_best: æ˜¯å¦ä¸ºæœ€ä½³æ¨¡å‹
    """
    
    print(f"[save_compatible_checkpoint] Saving checkpoint for {manager.algorithm} algorithm")
    print(f"[save_compatible_checkpoint] cpu_optimized={cpu_optimized}, is_best={is_best}")
    
    # è·å–åŸºæœ¬ä¿¡æ¯
    state_dim = manager.env.get_state_dim()
    action_dim = manager.env.num_actions
    algorithm = manager.algorithm
    
    # æ„å»ºå…¼å®¹çš„checkpoint
    checkpoint = {
        'algorithm': algorithm,
        'state_dim': state_dim,
        'action_dim': action_dim,
        'episode': episode,
        'config': manager.config,
        'timestamp': datetime.now().isoformat(),
        'use_task_aware_state': manager.use_task_aware_state,
        'enforce_workflow': manager.enforce_workflow,
        'use_phase2_scoring': manager.use_phase2_scoring,
        'cpu_optimized': cpu_optimized
    }
    
    # æ ¹æ®ç®—æ³•ç±»å‹ä¿å­˜ç½‘ç»œçŠ¶æ€
    if hasattr(manager.trainer, 'q_network'):  # DQN
        model = manager.trainer.q_network
        if cpu_optimized:
            model = model.cpu()
        checkpoint['q_network_state_dict'] = model.state_dict()
        checkpoint['model_state_dict'] = model.state_dict()  # å…¼å®¹æ€§
        
        # DQNä¹Ÿéœ€è¦ä¿å­˜target networkå’Œoptimizer
        if not cpu_optimized:
            checkpoint['target_network_state_dict'] = manager.trainer.target_network.state_dict()
            checkpoint['optimizer_state_dict'] = manager.trainer.optimizer.state_dict()
            checkpoint['epsilon'] = manager.trainer.epsilon
            checkpoint['training_steps'] = manager.trainer.training_steps
            print(f"[save_compatible_checkpoint] DQN: Saved optimizer_state_dict")
        else:
            print(f"[save_compatible_checkpoint] DQN: CPU-optimized mode - skipping optimizer state")
            
    elif hasattr(manager.trainer, 'network'):  # PPO
        model = manager.trainer.network
        if cpu_optimized:
            model = model.cpu()
        checkpoint['network_state_dict'] = model.state_dict()
        checkpoint['model_state_dict'] = model.state_dict()  # å…¼å®¹æ€§
        
        # PPOéœ€è¦ä¿å­˜optimizerå’Œtraining_steps
        if not cpu_optimized:
            # ä¿®å¤ï¼šæ­£ç¡®è°ƒç”¨state_dict()æ–¹æ³•
            checkpoint['optimizer_state_dict'] = manager.trainer.optimizer.state_dict()
            checkpoint['training_steps'] = manager.trainer.training_steps
            print(f"[save_compatible_checkpoint] PPO: Saved optimizer_state_dict")
        else:
            print(f"[save_compatible_checkpoint] PPO: CPU-optimized mode - skipping optimizer state")
    
    # æ·»åŠ è®­ç»ƒå†å²å’Œç»Ÿè®¡
    if hasattr(manager, 'training_history'):
        checkpoint['training_history'] = dict(manager.training_history)
    
    if hasattr(manager, 'best_success_rate'):
        checkpoint['best_success_rate'] = manager.best_success_rate
    
    # ä¿å­˜checkpointåˆ°æŒ‡å®šè·¯å¾„
    torch.save(checkpoint, checkpoint_path, _use_new_zipfile_serialization=False)
    logger.info(f"Saved {'CPU-optimized' if cpu_optimized else 'GPU'} checkpoint: {checkpoint_path}")
    
    # å¦‚æœæ˜¯æœ€ä½³æ¨¡å‹ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
    if is_best:
        standard_best_path = Path("checkpoints/best_model.pt")
        standard_best_path.parent.mkdir(exist_ok=True)
        
        if cpu_optimized:
            # å¦‚æœè¦æ±‚ä¿å­˜CPUä¼˜åŒ–çš„bestæ¨¡å‹ï¼Œå…ˆæ£€æŸ¥æ˜¯å¦å·²æœ‰å®Œæ•´çš„best_model.pt
            logger.warning("is_best=True with cpu_optimized=True detected!")
            
            # ä¸è¦†ç›–best_model.ptï¼Œè€Œæ˜¯ä¿å­˜åˆ°ç‰¹å®šçš„CPUç‰ˆæœ¬
            cpu_best_path = Path("checkpoints/best_model_cpu.pt")
            torch.save(checkpoint, cpu_best_path, _use_new_zipfile_serialization=False)
            logger.info(f"Saved CPU-optimized best model to {cpu_best_path}")
            
            # å¦‚æœbest_model.ptä¸å­˜åœ¨ï¼Œéœ€è¦ä¿å­˜ä¸€ä¸ªå®Œæ•´ç‰ˆæœ¬
            if not standard_best_path.exists():
                logger.warning("best_model.pt doesn't exist, creating full version")
                # é‡æ–°åˆ›å»ºåŒ…å«å®Œæ•´è®­ç»ƒçŠ¶æ€çš„checkpoint
                full_checkpoint = save_compatible_checkpoint(
                    manager, standard_best_path, episode,
                    cpu_optimized=False, is_best=False  # é€’å½’è°ƒç”¨ï¼Œä½†é¿å…æ— é™é€’å½’
                )
        else:
            # æ­£å¸¸ä¿å­˜å®Œæ•´çš„best_model.pt
            torch.save(checkpoint, standard_best_path, _use_new_zipfile_serialization=False)
            logger.info(f"Saved full training state to best_model.pt")
    
    return checkpoint

def train_with_gpu_4070(
    episodes: int = 2000,
    hours_limit: float = 4.0,  # 4å°æ—¶è®­ç»ƒé™åˆ¶
    task_types: Optional[List[str]] = None,
    resume: bool = False
):
    """ä½¿ç”¨RTX 4070è¿›è¡ŒGPUè®­ç»ƒ"""
    
    # åˆå§‹åŒ–é…ç½®
    gpu_config = GPU4070TrainingConfig()
    config = gpu_config.config
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path("gpu_4070_training")
    output_dir.mkdir(exist_ok=True)
    checkpoint_dir = output_dir / "checkpoints"
    checkpoint_dir.mkdir(exist_ok=True)
    
    # ä¿å­˜é…ç½®
    with open(output_dir / "training_config.json", 'w') as f:
        json.dump(config, f, indent=2)
    
    logger.info("="*60)
    logger.info("RTX 4070 Enhanced Training")
    logger.info("="*60)
    logger.info(f"Episodes: {episodes}")
    logger.info(f"Time limit: {hours_limit} hours")
    logger.info(f"Algorithm: {config['algorithm'].upper()}")
    logger.info(f"Network: {config['hidden_dim']}x{config['num_layers']} with {config['num_heads']} heads")
    
    # å¯¼å…¥è®­ç»ƒç®¡ç†å™¨
    try:
        sys.path.append(str(Path(__file__).parent))
        from unified_training_manager import UnifiedTrainingManager
    except ImportError:
        logger.error("Cannot import UnifiedTrainingManager!")
        return
    
    # åˆ›å»ºå¢å¼ºçš„è®­ç»ƒç®¡ç†å™¨
    manager = UnifiedTrainingManager(
        config_path=str(output_dir / "training_config.json"),
        use_task_aware_state=True,
        algorithm=config['algorithm'],
        task_types=task_types
    )
    
    # è®¾ç½®ç¯å¢ƒ
    if not manager.setup_environment():
        logger.error("Failed to setup environment!")
        return
    
    # å¢å¼ºPPO trainerçš„è®¾ç½®  # <- ä¿®æ”¹äº†è¿™éƒ¨åˆ†
    if hasattr(manager, 'trainer') and manager.trainer and config['algorithm'] == 'ppo':
        trainer = manager.trainer
        
        # 1. è®¾ç½®æ··åˆç²¾åº¦è®­ç»ƒï¼ˆAMPï¼‰
        if config.get('use_pytorch_amp', False) and torch.cuda.is_available():
            from torch.cuda.amp import GradScaler
            trainer.scaler = GradScaler()
            logger.info("âœ“ Enabled Automatic Mixed Precision (AMP)")
        
        # 2. è®¾ç½®ä¼˜åŒ–å™¨ï¼ˆå‡çº§åˆ°LAMBï¼‰
        if config.get('optimizer', 'adam').lower() == 'lamb':
            try:
                from torch_optimizer import Lamb
                trainer.optimizer = Lamb(
                    trainer.network.parameters(),
                    lr=config['learning_rate'],
                    betas=(config.get('beta1', 0.9), config.get('beta2', 0.999)),
                    eps=config.get('eps', 1e-6),
                    weight_decay=config.get('weight_decay', 0.01)
                )
                logger.info("âœ“ Using LAMB optimizer")
            except ImportError:
                logger.warning("LAMB optimizer not available, using AdamW")
                trainer.optimizer = torch.optim.AdamW(
                    trainer.network.parameters(),
                    lr=config['learning_rate'],
                    weight_decay=config.get('weight_decay', 0.01)
                )
        
        # 3. è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
        if config.get('lr_schedule', 'constant') == 'cosine':
            from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts
            trainer.lr_scheduler = CosineAnnealingWarmRestarts(
                trainer.optimizer,
                T_0=config.get('warmup_steps', 1000),
                T_mult=2,
                eta_min=config['learning_rate'] * 0.01
            )
            logger.info("âœ“ Using Cosine Annealing learning rate schedule")
        
        # 4. æ›´æ–°PPOå‚æ•°ä»¥æ”¯æŒæ–°ç‰¹æ€§
        trainer.config.update({
            # KLè‡ªé€‚åº”æ§åˆ¶
            'adaptive_kl_ctrl': config.get('adaptive_kl_ctrl', True),
            'target_kl': config.get('target_kl', 0.02),
            
            # è¾…åŠ©ä»»åŠ¡
            'use_auxiliary_tasks': config.get('use_auxiliary_tasks', True),
            'inverse_dynamics_coef': config.get('inverse_dynamics_coef', 0.1),
            'forward_dynamics_coef': config.get('forward_dynamics_coef', 0.1),
            'state_prediction_coef': config.get('state_prediction_coef', 0.05),
            
            # å¥½å¥‡å¿ƒé©±åŠ¨
            'use_curiosity': config.get('use_curiosity', True),
            'intrinsic_reward_scale': config.get('intrinsic_reward_scale', 0.1),
            'curiosity_type': config.get('curiosity_type', 'icm'),
            
            # ç†µç³»æ•°è¡°å‡
            'ent_coef_decay': config.get('ent_coef_decay', 0.99),
            
            # Value function clipping
            'clip_range_vf': config.get('clip_range_vf', 0.1),
            
            # L2æ­£åˆ™åŒ–
            'l2_reg': config.get('l2_reg', 0.0001),
            
            # æ··åˆç²¾åº¦
            'use_pytorch_amp': config.get('use_pytorch_amp', False),
            
            # ä¼˜å…ˆç»éªŒå›æ”¾
            'prioritized_replay': config.get('prioritized_replay', True),
            'priority_alpha': config.get('priority_alpha', 0.6),
            'priority_beta': config.get('priority_beta', 0.4),
        })
        
        # 5. å¦‚æœä½¿ç”¨è¾…åŠ©ä»»åŠ¡ï¼Œéœ€è¦å­˜å‚¨ä¸‹ä¸€çŠ¶æ€
        if config.get('use_auxiliary_tasks', False):
            trainer.next_states_buffer = []
            logger.info("âœ“ Enabled auxiliary tasks (inverse/forward dynamics)")
        
        # 6. è®¾ç½®å¥½å¥‡å¿ƒæ¨¡å—
        if config.get('use_curiosity', False):
            trainer.curiosity_enabled = True
            logger.info("âœ“ Enabled curiosity-driven exploration (ICM)")
        
        logger.info("\nğŸ“Š Enhanced PPO Configuration:")
        logger.info(f"  â€¢ Adaptive KL Control: {config.get('adaptive_kl_ctrl', True)}")
        logger.info(f"  â€¢ Auxiliary Tasks: {config.get('use_auxiliary_tasks', True)}")
        logger.info(f"  â€¢ Curiosity-Driven: {config.get('use_curiosity', True)}")
        logger.info(f"  â€¢ Mixed Precision: {config.get('use_pytorch_amp', False)}")
        logger.info(f"  â€¢ Learning Rate Schedule: {config.get('lr_schedule', 'constant')}")
        
    # è®­ç»ƒç›‘æ§
    trainer = EnhancedGPUTrainer(config)
    start_time = time.time()
    last_checkpoint_time = start_time
    best_success_rate = 0.0
    
    logger.info("\nğŸš€ Starting enhanced GPU training...")
    logger.info(f"Training for {episodes} episodes with time limit of {hours_limit} hours")
    
    # å¼€å§‹è®­ç»ƒ
    try:
        # ä½¿ç”¨managerçš„trainæ–¹æ³•ï¼Œå®ƒä¼šå¤„ç†å®Œæ•´çš„è®­ç»ƒå¾ªç¯
        success = manager.train(
            num_episodes=episodes,
            resume=resume,
            print_frequency=25  # æ›´é¢‘ç¹çš„æ‰“å°ï¼Œç›‘æ§æ–°ç‰¹æ€§
        )
        
        # è®­ç»ƒå®Œæˆåçš„å¤„ç†
        if success:
            logger.info("\nâœ… Training completed successfully!")
            
            # è·å–æœ€ç»ˆç»Ÿè®¡
            final_stats = {
                'total_episodes': episodes,
                'training_time_hours': (time.time() - start_time) / 3600,
                'final_success_rate': 0.0,
                'best_success_rate': best_success_rate
            }
            
            if hasattr(manager, 'training_history') and 'success' in manager.training_history:
                recent_success = list(manager.training_history['success'])[-100:]
                if recent_success:
                    final_stats['final_success_rate'] = np.mean(recent_success)
            
            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            logger.info("\nğŸ’¾ Saving final models...")
            
            # GPUç‰ˆæœ¬ï¼ˆåŒ…å«æ‰€æœ‰è®­ç»ƒçŠ¶æ€ï¼‰
            final_gpu_path = output_dir / "final_gpu_model.pt"
            save_compatible_checkpoint(
                manager, final_gpu_path, episodes,
                cpu_optimized=False, is_best=False
            )
            
            # CPUä¼˜åŒ–ç‰ˆæœ¬ï¼ˆç”¨äºå¿«é€Ÿæ¨ç†ï¼‰- ä¸è¦æ ‡è®°ä¸ºis_bestï¼
            final_cpu_path = output_dir / "final_cpu_optimized.pt"
            save_compatible_checkpoint(
                manager, final_cpu_path, episodes,
                cpu_optimized=True, is_best=False  # <- ä¿®æ”¹ï¼šä¸è¦†ç›–best_model.pt
            )
            
            # æ£€æŸ¥æ ‡å‡†ä½ç½®çš„best_model.ptæ˜¯å¦å­˜åœ¨ä¸”æœ‰æ•ˆ
            standard_best = Path("checkpoints/best_model.pt")
            if not standard_best.exists():
                # å¦‚æœä¸å­˜åœ¨best_model.ptï¼Œå¤åˆ¶GPUç‰ˆæœ¬ï¼ˆåŒ…å«å®Œæ•´è®­ç»ƒçŠ¶æ€ï¼‰
                import shutil
                shutil.copy(final_gpu_path, standard_best)
                logger.info("Copied final GPU model to best_model.pt")
            else:
                # éªŒè¯ç°æœ‰çš„best_model.ptæ˜¯å¦åŒ…å«è®­ç»ƒçŠ¶æ€
                try:
                    checkpoint = torch.load(standard_best, map_location='cpu', weights_only=False)
                    if checkpoint.get('cpu_optimized', False) or 'optimizer_state_dict' not in checkpoint:
                        # å¦‚æœæ˜¯CPUä¼˜åŒ–ç‰ˆæœ¬æˆ–ç¼ºå°‘è®­ç»ƒçŠ¶æ€ï¼Œæ›¿æ¢ä¸ºGPUç‰ˆæœ¬
                        logger.warning("Existing best_model.pt lacks training state, replacing with GPU version")
                        import shutil
                        shutil.copy(final_gpu_path, standard_best)
                except Exception as e:
                    logger.error(f"Error checking best_model.pt: {e}")
            
            # é¢å¤–ä¿å­˜ä¸€ä¸ªæ˜ç¡®çš„CPUæ¨ç†ç‰ˆæœ¬
            cpu_inference_path = Path("checkpoints/best_model_cpu_inference.pt")
            import shutil
            shutil.copy(final_cpu_path, cpu_inference_path)
            logger.info("Also saved CPU inference version: best_model_cpu_inference.pt")
            
            # æ˜¾ç¤ºè®­ç»ƒæ€»ç»“
            logger.info("\nğŸ“Š Training Summary:")
            logger.info(f"  â€¢ Total Episodes: {final_stats['total_episodes']}")
            logger.info(f"  â€¢ Training Time: {final_stats['training_time_hours']:.2f} hours")
            logger.info(f"  â€¢ Final Success Rate: {final_stats['final_success_rate']:.2%}")
            logger.info(f"  â€¢ Best Success Rate: {final_stats['best_success_rate']:.2%}")
            
            # ä¿å­˜è®­ç»ƒç»Ÿè®¡
            with open(output_dir / "training_summary.json", 'w') as f:
                json.dump(final_stats, f, indent=2)
            
            # æµ‹è¯•æ¨ç†é€Ÿåº¦
            logger.info("\nâš¡ Testing inference speed...")
            test_cpu_inference_speed(cpu_inference_path, manager.env.get_state_dim())
            
        else:
            logger.error("âŒ Training failed!")
            
    except KeyboardInterrupt:
        logger.warning("\nâš ï¸ Training interrupted by user")
        # ä¿å­˜ä¸­æ–­æ—¶çš„æ¨¡å‹
        interrupt_path = checkpoint_dir / "interrupted_model.pt"
        save_compatible_checkpoint(
            manager, interrupt_path, 
            episode=len(manager.training_history.get('rewards', [])),
            cpu_optimized=False, is_best=False
        )
        logger.info(f"Saved interrupted model to {interrupt_path}")
    except Exception as e:
        logger.error(f"âŒ Training error: {e}")
        import traceback
        traceback.print_exc()
# ç›¸åŒä½ç½®çš„ä¿®å¤ä»£ç 
# ä¿®æ”¹çš„è¡Œç”¨æ³¨é‡Šæ ‡æ³¨ï¼š# <- ä¿®æ”¹äº†è¿™ä¸€è¡Œ

def test_cpu_inference_speed(model_path: Path, state_dim: int):
    """æµ‹è¯•CPUæ¨ç†é€Ÿåº¦"""
    logger.info("\nTesting CPU inference speed...")
    
    # åŠ è½½æ¨¡å‹
    checkpoint = torch.load(model_path, map_location='cpu', weights_only=False)  # <- ä¿®æ”¹äº†è¿™ä¸€è¡Œ
    
    # åˆ›å»ºéšæœºè¾“å…¥
    batch_sizes = [1, 16, 64]
    num_tests = 100
    
    for batch_size in batch_sizes:
        test_input = torch.randn(batch_size, state_dim)
        
        # é¢„çƒ­
        for _ in range(10):
            with torch.no_grad():
                _ = test_input.clone()
        
        # è®¡æ—¶
        start = time.time()
        for _ in range(num_tests):
            with torch.no_grad():
                # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„æ¨¡å‹æ¨ç†
                _ = test_input.clone()
        
        elapsed = time.time() - start
        avg_time = elapsed / num_tests * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
        
        logger.info(f"Batch size {batch_size}: {avg_time:.2f} ms/inference")

def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description="RTX 4070 GPU Training Script")
    parser.add_argument("--episodes", type=int, default=2000, help="Number of episodes")
    parser.add_argument("--hours", type=float, default=4.0, help="Training time limit in hours")
    parser.add_argument("--task-types", nargs="+", help="Specific task types to train on")
    parser.add_argument("--resume", action="store_true", help="Resume from checkpoint")
    
    args = parser.parse_args()
    
    # æ‰§è¡Œè®­ç»ƒ
    train_with_gpu_4070(
        episodes=args.episodes,
        hours_limit=args.hours,
        task_types=args.task_types,
        resume=args.resume
    )

if __name__ == "__main__":
    main()
