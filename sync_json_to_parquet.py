#!/usr/bin/env python3
"""
å°†JSONæ•°æ®åº“åŒæ­¥åˆ°Parquetæ ¼å¼
"""

import json
import pandas as pd
from pathlib import Path
from datetime import datetime
from parquet_cumulative_manager import ParquetCumulativeManager

def sync_json_to_parquet():
    """å°†JSONæ•°æ®åŒæ­¥åˆ°Parquet"""
    
    # è¯»å–JSONæ•°æ®
    json_file = Path('pilot_bench_cumulative_results/master_database.json')
    if not json_file.exists():
        print("âŒ JSONæ–‡ä»¶ä¸å­˜åœ¨")
        return
    
    print("ğŸ“– è¯»å–JSONæ•°æ®...")
    with open(json_file, 'r', encoding='utf-8') as f:
        json_data = json.load(f)
    
    # åˆ›å»ºParquetç®¡ç†å™¨
    print("ğŸ”§ åˆå§‹åŒ–Parquetç®¡ç†å™¨...")
    manager = ParquetCumulativeManager()
    
    # ç»Ÿè®¡ä¿¡æ¯
    total_records = 0
    models_processed = []
    
    # éå†æ‰€æœ‰æ¨¡å‹æ•°æ®
    if 'models' in json_data:
        for model_name, model_data in json_data['models'].items():
            print(f"\nå¤„ç†æ¨¡å‹: {model_name}")
            model_records = 0
            
            # éå†prompt_type
            for prompt_type, prompt_data in model_data.get('by_prompt_type', {}).items():
                
                # éå†tool_success_rate
                for tool_rate, rate_data in prompt_data.get('by_tool_success_rate', {}).items():
                    
                    # éå†difficulty
                    for difficulty, diff_data in rate_data.get('by_difficulty', {}).items():
                        
                        # éå†task_type
                        for task_type, task_data in diff_data.get('by_task_type', {}).items():
                            
                            # åªå¤„ç†æœ‰æ•°æ®çš„è®°å½•
                            if task_data.get('total', 0) > 0:
                                # æ„å»ºæ±‡æ€»è®°å½• - åŒ¹é…ParquetCumulativeManagerçš„ç»“æ„
                                total = task_data.get('total', 0)
                                successful = task_data.get('successful', 0)
                                partial = task_data.get('partial', 0)
                                
                                summary = {
                                    'model': model_name,
                                    'prompt_type': prompt_type,
                                    'tool_success_rate': float(tool_rate),
                                    'difficulty': difficulty,
                                    'task_type': task_type,
                                    
                                    # è®¡æ•°å™¨ (ParquetCumulativeManageræœŸæœ›çš„å­—æ®µ)
                                    'total': total,
                                    'success': successful,  # JSONä¸­çš„successful -> success
                                    'full_success': successful - partial,  # å®Œå…¨æˆåŠŸ = æˆåŠŸ - éƒ¨åˆ†æˆåŠŸ
                                    'partial_success': partial,
                                    
                                    # ç´¯åŠ å™¨ï¼ˆç”¨äºè®¡ç®—å¹³å‡å€¼ï¼‰
                                    '_total_execution_time': task_data.get('avg_execution_time', 0.0) * total,
                                    '_total_turns': task_data.get('avg_turns', 0.0) * total,
                                    '_total_tool_calls': task_data.get('avg_tool_calls', 0.0) * total,
                                    '_total_tool_coverage': task_data.get('tool_coverage_rate', 0.0) * total,
                                    '_total_workflow_score': task_data.get('avg_workflow_score', 0.0) * total,
                                    '_total_phase2_score': task_data.get('avg_phase2_score', 0.0) * total,
                                    '_total_quality_score': task_data.get('avg_quality_score', 0.0) * total,
                                    '_total_final_score': task_data.get('avg_final_score', 0.0) * total,
                                    
                                    # å·²è®¡ç®—çš„ç‡ï¼ˆå¯é€‰ï¼Œ_flush_summary_to_diskä¼šé‡æ–°è®¡ç®—ï¼‰
                                    'success_rate': task_data.get('success_rate', 0.0),
                                    'partial_rate': task_data.get('partial_rate', 0.0),
                                    'failure_rate': task_data.get('failure_rate', 0.0),
                                    'weighted_success_score': task_data.get('weighted_success_score', 0.0),
                                    
                                    # é”™è¯¯è®¡æ•°ï¼ˆåˆå§‹åŒ–ä¸º0ï¼‰
                                    'total_errors': 0,
                                    'tool_call_format_errors': 0,
                                    'timeout_errors': 0,
                                    'dependency_errors': 0,
                                    'parameter_config_errors': 0,
                                    'tool_selection_errors': 0,
                                    'sequence_order_errors': 0,
                                    'max_turns_errors': 0,
                                    'other_errors': 0,
                                    
                                    # è¾…åŠ©ç»Ÿè®¡ï¼ˆåˆå§‹åŒ–ä¸º0ï¼‰
                                    'assisted_failure': 0,
                                    'assisted_success': 0,
                                    'total_assisted_turns': 0,
                                    'tests_with_assistance': 0,
                                }
                                
                                # æ·»åŠ åˆ°ç¼“å­˜
                                key = f"{model_name}|{prompt_type}|{tool_rate}|{difficulty}|{task_type}"
                                if not hasattr(manager, '_summary_cache'):
                                    manager._summary_cache = {}
                                manager._summary_cache[key] = summary
                                
                                model_records += 1
                                total_records += 1
            
            if model_records > 0:
                print(f"  âœ… æ·»åŠ äº† {model_records} æ¡è®°å½•")
                models_processed.append(model_name)
    
    # åˆ·æ–°åˆ°ç£ç›˜
    if total_records > 0:
        print(f"\nğŸ’¾ å°† {total_records} æ¡è®°å½•å†™å…¥Parquet...")
        manager._flush_buffer()
        
        # éªŒè¯å†™å…¥
        parquet_file = Path('pilot_bench_parquet_data/test_results.parquet')
        if parquet_file.exists():
            df = pd.read_parquet(parquet_file)
            print(f"\nâœ… æˆåŠŸï¼Parquetæ–‡ä»¶ç°æœ‰ {len(df)} æ¡è®°å½•")
            print(f"   å¤„ç†çš„æ¨¡å‹: {', '.join(models_processed)}")
            
            # æ˜¾ç¤ºä¸€äº›æ ·æœ¬æ•°æ®
            print("\nğŸ“Š æ ·æœ¬æ•°æ®:")
            for model in models_processed[:3]:
                model_df = df[df['model'] == model]
                if len(model_df) > 0:
                    print(f"   {model}: {len(model_df)} æ¡è®°å½•")
                    sample = model_df.iloc[0]
                    print(f"     - {sample['prompt_type']}/{sample['difficulty']}/{sample['task_type']}: "
                          f"æ€»æ•°={sample['total']:.0f}, æˆåŠŸç‡={sample['success_rate']:.1%}")
    else:
        print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°éœ€è¦åŒæ­¥çš„æ•°æ®")

if __name__ == "__main__":
    sync_json_to_parquet()