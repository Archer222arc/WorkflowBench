#!/usr/bin/env python3
"""
Ultra Parallel Runner - æœ€å¤§åŒ–å¹¶è¡Œåº¦çš„æµ‹è¯•æ‰§è¡Œå™¨
===============================================

æ ¸å¿ƒè®¾è®¡ï¼š
1. æ™ºèƒ½å®ä¾‹æ± ç®¡ç†ï¼šç»Ÿä¸€è°ƒåº¦9ä¸ªAzureå®ä¾‹
2. ä»»åŠ¡æ™ºèƒ½åˆ†ç‰‡ï¼šå°†å¤§ä»»åŠ¡æ‹†åˆ†åˆ°å¤šå®ä¾‹
3. åŠ¨æ€è´Ÿè½½å‡è¡¡ï¼šæ ¹æ®å®ä¾‹æ€§èƒ½è‡ªé€‚åº”åˆ†é…
4. èšåˆç»“æœç®¡ç†ï¼šç»Ÿä¸€æ”¶é›†å’Œå­˜å‚¨ç»“æœ

ç›®æ ‡ï¼šå°†èµ„æºåˆ©ç”¨ç‡ä»11%æå‡åˆ°90%+
"""

import asyncio
import concurrent.futures
import json
import os
import time
import logging
from dataclasses import dataclass
from typing import List, Dict, Set, Optional, Tuple
from pathlib import Path
import subprocess
import threading
from queue import Queue, Empty

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ============= Qwené˜Ÿåˆ—è°ƒåº¦å™¨ =============
class QwenQueueScheduler:
    """Qwenæ¨¡å‹é˜Ÿåˆ—è°ƒåº¦å™¨ - ç¡®ä¿åŒkeyä¸²è¡Œï¼Œä¸åŒkeyå¹¶è¡Œ
    
    é€‚ç”¨äºæ‰€æœ‰phasesï¼Œè‡ªåŠ¨ç®¡ç†API keyèµ„æº
    """
    
    _instance = None
    _lock = threading.Lock()
    
    def __new__(cls):
        """å•ä¾‹æ¨¡å¼"""
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self):
        if not hasattr(self, 'initialized'):
            self.initialized = True
            self.num_keys = 3
            self.key_queues = {i: Queue() for i in range(self.num_keys)}
            self.key_workers = {}
            self.key_busy = {i: False for i in range(self.num_keys)}
            self.results = []
            
            # å¯åŠ¨workerçº¿ç¨‹
            for key_idx in range(self.num_keys):
                worker = threading.Thread(
                    target=self._process_queue,
                    args=(key_idx,),
                    daemon=True,
                    name=f"QwenKey{key_idx}Worker"
                )
                worker.start()
                self.key_workers[key_idx] = worker
    
    def _process_queue(self, key_idx: int):
        """å¤„ç†å•ä¸ªkeyçš„ä»»åŠ¡é˜Ÿåˆ—"""
        while True:
            task = self.key_queues[key_idx].get()
            if task is None:  # é€€å‡ºä¿¡å·
                break
            
            self.key_busy[key_idx] = True
            try:
                logger.info(f"ğŸ”„ Key{key_idx}: æ‰§è¡Œ {task['model']}-{task.get('difficulty', 'unknown')}")
                result = task['func'](**task['kwargs'])
                self.results.append((key_idx, task['model'], result))
                logger.info(f"âœ… Key{key_idx}: å®Œæˆ {task['model']}-{task.get('difficulty', 'unknown')}")
            except Exception as e:
                logger.error(f"âŒ Key{key_idx}: å¤±è´¥ - {e}")
                self.results.append((key_idx, task['model'], False))
            finally:
                self.key_busy[key_idx] = False
                self.key_queues[key_idx].task_done()
    
    def submit_task(self, model: str, key_idx: int, func, **kwargs):
        """æäº¤ä»»åŠ¡åˆ°æŒ‡å®škeyçš„é˜Ÿåˆ—"""
        # å°†modelå‚æ•°åŒ…å«åœ¨kwargsä¸­ï¼Œç¡®ä¿funcè°ƒç”¨æ—¶èƒ½æ¥æ”¶åˆ°æ‰€æœ‰å‚æ•°
        kwargs['model'] = model
        task = {
            'model': model,
            'func': func,
            'kwargs': kwargs,
            'difficulty': kwargs.get('difficulty', 'unknown')
        }
        self.key_queues[key_idx].put(task)
    
    def wait_all(self):
        """ç­‰å¾…æ‰€æœ‰é˜Ÿåˆ—å®Œæˆ"""
        for key_idx in range(self.num_keys):
            self.key_queues[key_idx].join()
    
    def shutdown(self):
        """å…³é—­è°ƒåº¦å™¨"""
        for key_idx in range(self.num_keys):
            self.key_queues[key_idx].put(None)
# =========================================

# å¯é€‰å¯¼å…¥æ–°çš„ResultCollector
try:
    from result_collector import ResultCollector, ResultAggregator
    RESULT_COLLECTOR_AVAILABLE = True
except ImportError:
    RESULT_COLLECTOR_AVAILABLE = False
    logger.info("ResultCollectorä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼")

@dataclass
class InstanceConfig:
    """Azureå®ä¾‹é…ç½®"""
    name: str
    model_family: str  # "deepseek-v3", "deepseek-r1", "llama-3.3"
    max_workers: int = 100
    max_qps: float = 200.0
    is_busy: bool = False
    current_load: int = 0
    performance_score: float = 1.0  # æ€§èƒ½è¯„åˆ†ï¼ŒåŠ¨æ€è°ƒæ•´

@dataclass
class TaskShard:
    """ä»»åŠ¡åˆ†ç‰‡"""
    shard_id: str
    model: str
    prompt_types: str
    difficulty: str
    task_types: str
    num_instances: int
    instance_name: str
    tool_success_rate: float = 0.8

class UltraParallelRunner:
    """è¶…é«˜å¹¶è¡Œåº¦æµ‹è¯•æ‰§è¡Œå™¨"""
    
    def __init__(self, use_result_collector: bool = None):
        """
        åˆå§‹åŒ–UltraParallelRunner
        
        Args:
            use_result_collector: æ˜¯å¦ä½¿ç”¨æ–°çš„ResultCollectoræ¨¡å¼
                                 None=è‡ªåŠ¨æ£€æµ‹, True=å¼ºåˆ¶å¯ç”¨, False=ç¦ç”¨
        """
        self.instance_pool = self._initialize_instance_pool()
        self.active_tasks: Set[str] = set()
        self.task_queue = Queue()
        self.results_lock = threading.Lock()
        self.performance_stats = {}
        
        # åˆå§‹åŒ–Qwenè°ƒåº¦å™¨
        self.qwen_scheduler = QwenQueueScheduler()
        self._qwen_batch_mode = False  # æ‰¹é‡æ¨¡å¼æ ‡è®°
        
        # ç»“æœæ”¶é›†æ¨¡å¼é…ç½®
        if use_result_collector is None:
            # è‡ªåŠ¨æ£€æµ‹ï¼šä»ç¯å¢ƒå˜é‡æˆ–é…ç½®å†³å®š
            use_collector = os.environ.get('USE_RESULT_COLLECTOR', 'false').lower() == 'true'
        else:
            use_collector = use_result_collector
            
        if use_collector and RESULT_COLLECTOR_AVAILABLE:
            self.result_collector = ResultCollector()
            self.result_aggregator = ResultAggregator()
            self.use_collector_mode = True
            logger.info("ğŸ†• å¯ç”¨ResultCollectoræ¨¡å¼ï¼Œæ”¯æŒé›¶å†²çªå¹¶å‘")
        else:
            self.result_collector = None
            self.result_aggregator = None
            self.use_collector_mode = False
            if use_collector and not RESULT_COLLECTOR_AVAILABLE:
                logger.warning("âš ï¸ ResultCollectorä¸å¯ç”¨ï¼Œä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼")
            else:
                logger.info("ğŸ“œ ä½¿ç”¨ä¼ ç»Ÿæ•°æ®åº“å†™å…¥æ¨¡å¼")
        
    def _initialize_instance_pool(self) -> Dict[str, InstanceConfig]:
        """åˆå§‹åŒ–Azureå®ä¾‹æ± """
        instances = {}
        
        # 6ä¸ªDeepSeekå®ä¾‹ï¼ˆä¿æŒAzure APIè¦æ±‚çš„å¤§å°å†™ï¼‰
        deepseek_v3_instances = [
            "DeepSeek-V3-0324",
            "DeepSeek-V3-0324-2",  # ç»Ÿä¸€å¤§å°å†™
            "DeepSeek-V3-0324-3"   # ç»Ÿä¸€å¤§å°å†™
        ]
        
        deepseek_r1_instances = [
            "DeepSeek-R1-0528",
            "DeepSeek-R1-0528-2",  # ç»Ÿä¸€å¤§å°å†™
            "DeepSeek-R1-0528-3"   # ç»Ÿä¸€å¤§å°å†™
        ]
        
        # 3ä¸ªLlamaå®ä¾‹ï¼ˆä¿æŒAzure APIè¦æ±‚çš„å¤§å°å†™ï¼‰
        llama_instances = [
            "Llama-3.3-70B-Instruct",
            "Llama-3.3-70B-Instruct-2",  # ç»Ÿä¸€å¤§å°å†™
            "Llama-3.3-70B-Instruct-3"   # ç»Ÿä¸€å¤§å°å†™
        ]
        
        # 2ä¸ªIdealLab API Keyå¯¹åº”çš„è™šæ‹Ÿå®ä¾‹
        # ä¸ºå¼€æºæ¨¡å‹qwenåˆ›å»º2ä¸ªè™šæ‹Ÿå®ä¾‹ï¼Œå¯¹åº”2ä¸ªå¯ç”¨çš„API keys
        # è¿™æ ·å¯ä»¥å®ç°çœŸæ­£çš„å¹¶å‘ï¼ˆç¬¬3ä¸ªkeyæš‚æ—¶ä¸å¯ç”¨ï¼‰
        ideallab_qwen_instances = [
            "qwen-key0",      # å¯¹åº”API key 0 (baselineåå¥½)
            "qwen-key1"       # å¯¹åº”API key 1 (cot+optimalåå¥½)
        ]
        
        # æ³¨å†Œæ‰€æœ‰å®ä¾‹
        for name in deepseek_v3_instances:
            instances[name] = InstanceConfig(
                name=name,
                model_family="deepseek-v3",
                max_workers=100,
                max_qps=200.0
            )
            
        for name in deepseek_r1_instances:
            instances[name] = InstanceConfig(
                name=name,
                model_family="deepseek-r1", 
                max_workers=100,
                max_qps=200.0
            )
            
        for name in llama_instances:
            instances[name] = InstanceConfig(
                name=name,
                model_family="llama-3.3",
                max_workers=100,
                max_qps=200.0
            )
            
        # æ³¨å†ŒIdealLabå®ä¾‹ (API Keyçº§åˆ«å¹¶è¡Œ)
        for name in ideallab_qwen_instances:
            instances[name] = InstanceConfig(
                name=name,
                model_family="qwen",
                max_workers=1,   # é™åˆ¶ä¸º1é¿å…é™æµé—®é¢˜
                max_qps=5.0      # æ›´ä¿å®ˆçš„QPSé™åˆ¶
            )
        
        # æ·»åŠ é—­æºAzureæ¨¡å‹å®ä¾‹ (åªæœ‰ä¸€ä¸ªdeploymentï¼Œä½†å¯ä»¥model levelå¹¶å‘)
        azure_closed_models = [
            "gpt-4o-mini",
            "gpt-5-mini"
        ]
        
        for model in azure_closed_models:
            # é—­æºæ¨¡å‹åªæœ‰å•ä¸ªdeploymentï¼Œä½†å¯ä»¥ç”¨æ›´é«˜å¹¶å‘
            instances[model] = InstanceConfig(
                name=model,
                model_family=f"azure-{model}",
                max_workers=200,  # å•å®ä¾‹æ›´é«˜å¹¶å‘
                max_qps=400.0
            )
        
        # æ·»åŠ IdealLabé—­æºæ¨¡å‹å®ä¾‹ (åªæœ‰ä¸€ä¸ªAPI Keyå¯ç”¨ï¼Œä½†å¯ä»¥model levelå¹¶å‘)
        ideallab_closed_models = [
            "o3-0416-global",
            "gemini-2.5-flash-06-17", 
            "kimi-k2",
            "claude_sonnet4"
        ]
        
        for model in ideallab_closed_models:
            # é—­æºæ¨¡å‹åªèƒ½ç”¨ä¸€ä¸ªAPI Keyï¼Œä¸”æœ‰ä¸¥æ ¼é€Ÿç‡é™åˆ¶
            instances[model] = InstanceConfig(
                name=model,
                model_family=f"ideallab-{model}",
                max_workers=1,   # é™åˆ¶ä¸º1é¿å…é™æµé—®é¢˜
                max_qps=5.0      # æ›´ä¿å®ˆçš„QPSé™åˆ¶
            )
            
        logger.info(f"åˆå§‹åŒ–å®ä¾‹æ± : {len(instances)}ä¸ªå®ä¾‹ ({len([i for i in instances.values() if 'azure' in i.model_family])}ä¸ªAzure + {len([i for i in instances.values() if 'ideallab' in i.model_family or i.model_family == 'qwen'])}ä¸ªIdealLab)")
        return instances
        
    def get_available_instances(self, model_family: str) -> List[InstanceConfig]:
        """è·å–æŒ‡å®šæ¨¡å‹æ—çš„å¯ç”¨å®ä¾‹"""
        available = []
        for instance in self.instance_pool.values():
            if (instance.model_family == model_family and 
                not instance.is_busy and 
                instance.current_load < instance.max_workers * 0.8):
                available.append(instance)
        
        # æŒ‰æ€§èƒ½è¯„åˆ†æ’åºï¼Œä¼˜å…ˆä½¿ç”¨é«˜æ€§èƒ½å®ä¾‹
        available.sort(key=lambda x: x.performance_score, reverse=True)
        return available
        
    def _create_qwen_smart_shards(self, model: str, prompt_types: str, difficulty: str,
                                  task_types: str, num_instances: int, tool_success_rate: float) -> List[TaskShard]:
        """ä¸ºqwenæ¨¡å‹åˆ›å»ºæ™ºèƒ½åˆ†ç‰‡ï¼Œä½¿ç”¨API Keyè½®æ¢é¿å…å†²çª
        
        é‡è¦æ›´æ–°ï¼šå®æ–½API Keyè½®æ¢ç­–ç•¥ï¼Œæ¯ä¸ªæ¨¡å‹åªä½¿ç”¨ä¸€ä¸ªå›ºå®šçš„key
        é¿å…å¤šä¸ªæ¨¡å‹åŒæ—¶ä½¿ç”¨åŒä¸€ä¸ªkeyå¯¼è‡´çš„é™æµé—®é¢˜
        """
        shards = []
        
        # API Keyè½®æ¢æ˜ å°„è¡¨ - ä½¿ç”¨3ä¸ªå¯ç”¨çš„keys (key0, key1, key2)
        # ç­–ç•¥ï¼šæ ¹æ®æ¨¡å‹å¤§å°å›ºå®šåˆ†é…keyï¼Œç¡®ä¿è´Ÿè½½å‡è¡¡
        KEY_ROTATION_MAP = {
            "72b": 0,  # qwen2.5-72b â†’ key0
            "32b": 1,  # qwen2.5-32b â†’ key1
            "14b": 2,  # qwen2.5-14b â†’ key2
            "7b": 0,   # qwen2.5-7b â†’ key0ï¼ˆä¸72bé”™å¼€æ—¶é—´ï¼‰
            "3b": 1,   # qwen2.5-3b â†’ key1ï¼ˆä¸32bé”™å¼€æ—¶é—´ï¼‰
        }
        
        # ä»æ¨¡å‹åç§°æå–è§„æ¨¡æ ‡è¯†
        import re
        match = re.search(r'(\d+b)', model.lower())
        model_size = match.group(1) if match else None
        
        if model_size not in KEY_ROTATION_MAP:
            logger.warning(f"æœªçŸ¥çš„qwenæ¨¡å‹è§„æ¨¡: {model_size}ï¼Œé»˜è®¤ä½¿ç”¨key0")
            assigned_key = 0
        else:
            assigned_key = KEY_ROTATION_MAP[model_size]
        
        # ä¹Ÿå¯ä»¥ä»ç¯å¢ƒå˜é‡è¦†ç›–ï¼ˆç”¨äºæµ‹è¯•æˆ–ç‰¹æ®Šæƒ…å†µï¼‰
        env_key = os.environ.get(f'QWEN_{model_size.upper()}_KEY')
        if env_key and env_key.isdigit():
            assigned_key = int(env_key) % 3  # ç¡®ä¿åœ¨0-2èŒƒå›´å†…ï¼ˆ3ä¸ªkeysï¼‰
            logger.info(f"ä½¿ç”¨ç¯å¢ƒå˜é‡æŒ‡å®šçš„key: QWEN_{model_size.upper()}_KEY={assigned_key}")
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯5.3çš„å¤šprompt_typesæƒ…å†µï¼ˆä»…å¤„ç†flawedç±»å‹ï¼‰
        if "," in prompt_types and "flawed" in prompt_types:
            # 5.3åœºæ™¯ï¼šä¿æŒåŸæœ‰é€»è¾‘ï¼Œä½†ä½¿ç”¨assigned_keyè€Œéå›ºå®šåˆ†é…
            if "sequence_disorder" in prompt_types:
                group_name = "struct_defects"
            elif "missing_step" in prompt_types:
                group_name = "operation_defects"
            elif "logical_inconsistency" in prompt_types:
                group_name = "logic_defects"
            else:
                group_name = "unknown_defects"
            
            shard = TaskShard(
                shard_id=f"{model}_{difficulty}_{group_name}_key{assigned_key}",
                model=model,
                prompt_types=prompt_types,  # ä¿æŒåŸå§‹çš„å¤šä¸ªprompt_types
                difficulty=difficulty,
                task_types=task_types,
                num_instances=num_instances,
                instance_name=f"qwen-key{assigned_key}",  # ä½¿ç”¨åˆ†é…çš„key
                tool_success_rate=tool_success_rate
            )
            shards.append(shard)
            logger.info(f"ğŸ”„ API Keyè½®æ¢(5.3): {model}({model_size}) â†’ key{assigned_key} (ç¼ºé™·ç»„: {group_name})")
            return shards
        
        # 5.1/5.2/5.4/5.5åœºæ™¯ï¼šå¯ç”¨çœŸæ­£çš„å¤škeyå¹¶å‘ï¼
        # é‡è¦ä¿®å¤ï¼šåˆ›å»º3ä¸ªåˆ†ç‰‡ï¼Œåˆ†åˆ«ä½¿ç”¨key0ã€key1ã€key2å®ç°å¹¶å‘
        instances_per_key = max(1, num_instances // 3)  # æ¯ä¸ªkeyåˆ†é…çš„å®ä¾‹æ•°
        remaining_instances = num_instances % 3  # ä½™æ•°å®ä¾‹
        
        for key_idx in range(3):  # ä½¿ç”¨æ‰€æœ‰3ä¸ªkeys
            # åˆ†é…å®ä¾‹æ•°ï¼ˆä½™æ•°åˆ†é…ç»™å‰å‡ ä¸ªkeyï¼‰
            key_instances = instances_per_key + (1 if key_idx < remaining_instances else 0)
            
            if key_instances > 0:  # åªåˆ›å»ºæœ‰å®ä¾‹åˆ†é…çš„åˆ†ç‰‡
                shard = TaskShard(
                    shard_id=f"{model}_{difficulty}_{prompt_types}_key{key_idx}",
                    model=model,
                    prompt_types=prompt_types,
                    difficulty=difficulty,
                    task_types=task_types,
                    num_instances=key_instances,
                    instance_name=f"qwen-key{key_idx}",
                    tool_success_rate=tool_success_rate
                )
                shards.append(shard)
        
        logger.info(f"ğŸ”„ çœŸæ­£å¤šKeyå¹¶å‘ç­–ç•¥:")
        logger.info(f"   æ¨¡å‹: {model} (è§„æ¨¡: {model_size})")
        logger.info(f"   ä½¿ç”¨Keys: key0, key1, key2")
        logger.info(f"   æ€»å®ä¾‹æ•°: {num_instances}")
        logger.info(f"   åˆ†ç‰‡æ•°: {len(shards)} (æ¯ä¸ªkeyç‹¬ç«‹åˆ†ç‰‡)")
        logger.info(f"   å®ä¾‹åˆ†é…: {[shard.num_instances for shard in shards]}")
        logger.info(f"   ğŸš€ å¯ç”¨3å€APIå¹¶å‘ï¼")
        
        return shards
    
    def create_task_shards(self, model: str, prompt_types: str, difficulty: str, 
                          task_types: str, num_instances: int, tool_success_rate: float = 0.8) -> List[TaskShard]:
        """æ™ºèƒ½åˆ›å»ºä»»åŠ¡åˆ†ç‰‡"""
        
        # ç‰¹æ®Šå¤„ç†ï¼šqwenæ¨¡å‹ä½¿ç”¨æ™ºèƒ½åˆ†ç‰‡ç­–ç•¥
        if "qwen" in model.lower():
            logger.info(f"ğŸ¯ ä½¿ç”¨qwenæ™ºèƒ½åˆ†ç‰‡ç­–ç•¥: {model}")
            return self._create_qwen_smart_shards(model, prompt_types, difficulty,
                                                 task_types, num_instances, tool_success_rate)
        
        # ç¡®å®šæ¨¡å‹æ—
        if "deepseek-v3" in model.lower():
            model_family = "deepseek-v3"
            base_model = "DeepSeek-V3-0324"
        elif "deepseek-r1" in model.lower():
            model_family = "deepseek-r1"
            base_model = "DeepSeek-R1-0528"
        elif "llama" in model.lower():
            model_family = "llama-3.3"
            base_model = "Llama-3.3-70B-Instruct"
        # Azureé—­æºæ¨¡å‹
        elif model == "gpt-4o-mini":
            model_family = "azure-gpt-4o-mini"
            base_model = "gpt-4o-mini"
        elif model == "gpt-5-mini":
            model_family = "azure-gpt-5-mini"
            base_model = "gpt-5-mini"
        # IdealLabé—­æºæ¨¡å‹
        elif model == "o3-0416-global":
            model_family = "ideallab-o3-0416-global"
            base_model = "o3-0416-global"
        elif model == "gemini-2.5-flash-06-17":
            model_family = "ideallab-gemini-2.5-flash-06-17"
            base_model = "gemini-2.5-flash-06-17"
        elif model == "kimi-k2":
            model_family = "ideallab-kimi-k2"
            base_model = "kimi-k2"
        elif model == "claude_sonnet4":
            model_family = "ideallab-claude_sonnet4"
            base_model = "claude_sonnet4"
        else:
            logger.warning(f"æœªçŸ¥æ¨¡å‹æ—: {model}")
            return []
            
        # è·å–å¯ç”¨å®ä¾‹
        available_instances = self.get_available_instances(model_family)
        
        if not available_instances:
            logger.warning(f"æ²¡æœ‰å¯ç”¨çš„{model_family}å®ä¾‹")
            return []
            
        # è®¡ç®—åˆ†ç‰‡ç­–ç•¥
        shards = []
        
        # å¯¹äºé—­æºæ¨¡å‹ï¼Œä½¿ç”¨ç‰¹æ®Šçš„åˆ†ç‰‡ç­–ç•¥
        if model_family.startswith("ideallab-"):
            instances_to_use = 1  # IdealLabé—­æºæ¨¡å‹åªèƒ½ä½¿ç”¨å•åˆ†ç‰‡é¿å…API Keyå†²çª
            logger.info(f"IdealLabé—­æºæ¨¡å‹ {model} ä½¿ç”¨å•åˆ†ç‰‡ç­–ç•¥ï¼ˆé¿å…API Keyå†²çªï¼‰")
        elif model_family.startswith("azure-"):
            instances_to_use = 1  # Azureé—­æºæ¨¡å‹ä½¿ç”¨å•åˆ†ç‰‡é«˜å¹¶å‘ç­–ç•¥
            logger.info(f"Azureé—­æºæ¨¡å‹ {model} ä½¿ç”¨å•åˆ†ç‰‡é«˜å¹¶å‘ç­–ç•¥ï¼ˆå•deploymentä¼˜åŒ–ï¼‰")
        elif model_family.startswith("deepseek"):
            # DeepSeekæ¨¡å‹æš‚æ—¶åªä½¿ç”¨ç¬¬ä¸€ä¸ªéƒ¨ç½²å®ä¾‹ï¼Œé¿å…å¤šéƒ¨ç½²å¯èƒ½çš„å¹¶å‘é—®é¢˜
            instances_to_use = 1
            logger.info(f"DeepSeekæ¨¡å‹ {model} æš‚æ—¶ä½¿ç”¨å•éƒ¨ç½²ç­–ç•¥ï¼ˆé¿å…å¤šéƒ¨ç½²å¹¶å‘é—®é¢˜ï¼‰")
        elif model_family.startswith("llama"):
            # Llamaæ¨¡å‹ä¹Ÿæš‚æ—¶åªä½¿ç”¨ç¬¬ä¸€ä¸ªéƒ¨ç½²å®ä¾‹ï¼Œé¿å…å¤šéƒ¨ç½²å¯èƒ½çš„å¹¶å‘é—®é¢˜
            instances_to_use = 1
            logger.info(f"Llamaæ¨¡å‹ {model} æš‚æ—¶ä½¿ç”¨å•éƒ¨ç½²ç­–ç•¥ï¼ˆé¿å…å¤šéƒ¨ç½²å¹¶å‘é—®é¢˜ï¼‰")
        else:
            instances_to_use = min(len(available_instances), 3)  # å…¶ä»–å¼€æºæ¨¡å‹æœ€å¤šç”¨3ä¸ªå®ä¾‹
            
        instances_per_shard = max(1, num_instances // instances_to_use)
        
        logger.info(f"åˆ›å»ºä»»åŠ¡åˆ†ç‰‡: {instances_to_use}ä¸ªå®ä¾‹å¹¶è¡Œ")
        
        for i in range(instances_to_use):
            instance = available_instances[i]
            shard_instances = instances_per_shard
            
            # æœ€åä¸€ä¸ªåˆ†ç‰‡å¤„ç†ä½™æ•°
            if i == instances_to_use - 1:
                shard_instances += num_instances % instances_to_use
                
            shard = TaskShard(
                shard_id=f"{model}_{difficulty}_{i}",
                model=base_model,  # ä¿æŒåŸå§‹å¤§å°å†™ï¼Œnormalizeä¼šåœ¨å­˜å‚¨æ—¶å¤„ç†
                prompt_types=prompt_types,
                difficulty=difficulty,
                task_types=task_types,
                num_instances=shard_instances,
                instance_name=instance.name,  # ä¿ç•™åŸå§‹å¤§å°å†™ç”¨äºAPIè°ƒç”¨
                tool_success_rate=tool_success_rate
            )
            shards.append(shard)
            
        return shards
        
    def execute_shard_async(self, shard: TaskShard, rate_mode: str = "adaptive", result_suffix: str = "", silent: bool = False, max_workers: int = None, shard_index: int = 0) -> subprocess.Popen:
        """å¼‚æ­¥æ‰§è¡Œä»»åŠ¡åˆ†ç‰‡
        
        Args:
            shard: ä»»åŠ¡åˆ†ç‰‡
            rate_mode: é€Ÿç‡æ¨¡å¼ - "adaptive" æˆ– "fixed"
        """
        
        # æ ‡è®°å®ä¾‹ä¸ºå¿™ç¢Œ
        if shard.instance_name in self.instance_pool:
            self.instance_pool[shard.instance_name].is_busy = True
            self.instance_pool[shard.instance_name].current_load += shard.num_instances
            
        # è®¡ç®—promptæ•°é‡ï¼ˆç”¨äºåŠ¨æ€è°ƒæ•´workersï¼‰
        prompt_count = len(shard.prompt_types.split(",")) if "," in shard.prompt_types else 1
        use_prompt_parallel = "--prompt-parallel" if prompt_count > 1 else ""
        
        # ä¿æŒåŸå§‹çš„instance_nameä½œä¸ºdeployment
        deployment_name = shard.instance_name
        # æ³¨æ„ï¼šä¸è¦åœ¨è¿™é‡Œä¿®æ”¹deployment_nameï¼
        # qwen-key0/1 è™šæ‹Ÿå®ä¾‹åä¼šåœ¨ batch_test_runner.py ä¸­æ­£ç¡®å¤„ç†
        
        # æ ¹æ®æ¨¡å‹ç±»å‹å’Œrate_modeè°ƒæ•´å‚æ•°
        if shard.instance_name in self.instance_pool:
            instance = self.instance_pool[shard.instance_name]
            
            # IdealLabå¼€æºæ¨¡å‹ï¼ˆqwenç³»åˆ—ï¼‰
            if instance.model_family == "qwen" or shard.instance_name.startswith("qwen-key"):
                # IdealLab APIä¸¥æ ¼é™åˆ¶ï¼Œå¿…é¡»ä½¿ç”¨ä½å¹¶å‘
                # æ— è®ºç”¨æˆ·è®¾ç½®ä»€ä¹ˆï¼ˆåŒ…æ‹¬--max-workersï¼‰ï¼Œéƒ½å¼ºåˆ¶é™åˆ¶ä¸º1ä¸ªworker
                max_workers = 1  # å¼ºåˆ¶é™åˆ¶ï¼šæ¯ä¸ªkeyåªèƒ½1ä¸ªworkerï¼ˆä¸¥æ ¼é™æµï¼‰
                qps = 10  # QPSé™åˆ¶ä¸º10
                logger.info(f"  IdealLab qwenæ¨¡å‹é™åˆ¶: {shard.instance_name} å¼ºåˆ¶ä½¿ç”¨ max_workers={max_workers}, qps={qps}")
                logger.info(f"    æ³¨æ„: IdealLab APIå¹¶å‘é™åˆ¶ä¸¥æ ¼ï¼Œå¿½ç•¥--max-workersè®¾ç½®")
            # Azureå¼€æºæ¨¡å‹ï¼ˆDeepSeek, Llamaç­‰ï¼‰
            elif instance.model_family in ["deepseek-v3", "deepseek-r1", "llama-3.3"]:
                # å¦‚æœç”¨æˆ·æŒ‡å®šäº†max_workersï¼Œä¼˜å…ˆä½¿ç”¨
                if max_workers is not None:
                    base_workers = max_workers
                    max_workers = base_workers * prompt_count if prompt_count > 1 else base_workers
                    qps = None  # ç§»é™¤QPSé™åˆ¶
                    logger.info(f"  Azureå¼€æºæ¨¡å‹è‡ªå®šä¹‰: {prompt_count}ä¸ªprompt Ã— {base_workers} = {max_workers} workers")
                elif rate_mode == "fixed":
                    # å›ºå®šæ¨¡å¼ï¼šæ¯ä¸ªprompt 50 workers
                    base_workers = 50
                    max_workers = base_workers * prompt_count if prompt_count > 1 else base_workers
                    qps = None  # ç§»é™¤QPSé™åˆ¶
                    logger.info(f"  Azureå¼€æºæ¨¡å‹å›ºå®šæ¨¡å¼: {prompt_count}ä¸ªprompt Ã— {base_workers} = {max_workers} workers")
                else:
                    # è‡ªé€‚åº”æ¨¡å¼ï¼šæ¯ä¸ªprompt 100 workers
                    base_workers = 100
                    max_workers = base_workers * prompt_count if prompt_count > 1 else base_workers
                    qps = None  # adaptiveä¸éœ€è¦QPS
                    logger.info(f"  Azureå¼€æºæ¨¡å‹è‡ªé€‚åº”æ¨¡å¼: {prompt_count}ä¸ªprompt Ã— {base_workers} = {max_workers} workers")
            # IdealLabé—­æºæ¨¡å‹ï¼ˆå•API Keyé™åˆ¶ï¼‰
            elif instance.model_family.startswith("ideallab-"):
                # IdealLabé—­æºæ¨¡å‹ä¸¥æ ¼é™åˆ¶ä¸º1ä¸ªworkerï¼Œä¸ç®¡æœ‰å¤šå°‘prompts
                max_workers = 1  # å¼ºåˆ¶1ä¸ªworkerï¼Œå¿½ç•¥ç”¨æˆ·è®¾ç½®å’Œpromptæ•°é‡
                qps = 10  # QPSé™åˆ¶ä¸º10
                logger.info(f"  IdealLabé—­æºæ¨¡å‹: max_workers={max_workers}, qps={qps} (ä¸¥æ ¼é™æµï¼Œå¿½ç•¥promptæ•°é‡)")
            # Azureé—­æºæ¨¡å‹ï¼ˆå•deploymentä½†æ”¯æŒé«˜å¹¶å‘ï¼‰
            elif instance.model_family.startswith("azure-"):
                # å¦‚æœç”¨æˆ·æŒ‡å®šäº†max_workersï¼Œä¼˜å…ˆä½¿ç”¨
                if max_workers is not None:
                    base_workers = max_workers
                    max_workers = base_workers * prompt_count if prompt_count > 1 else base_workers
                    qps = None  # ç§»é™¤QPSé™åˆ¶
                    logger.info(f"  Azureé—­æºæ¨¡å‹è‡ªå®šä¹‰: {prompt_count}ä¸ªprompt Ã— {base_workers} = {max_workers} workers")
                elif rate_mode == "fixed":
                    # é—­æºæ¨¡å‹å›ºå®šæ¨¡å¼ï¼šå•deploymenté«˜å¹¶å‘
                    base_workers = 100  # æ›´é«˜çš„åŸºç¡€å¹¶å‘
                    max_workers = base_workers * prompt_count if prompt_count > 1 else base_workers
                    qps = None  # ç§»é™¤QPSé™åˆ¶
                    logger.info(f"  Azureé—­æºæ¨¡å‹å›ºå®šæ¨¡å¼: {prompt_count}ä¸ªprompt Ã— {base_workers} = {max_workers} workers")
                else:
                    # é—­æºæ¨¡å‹è‡ªé€‚åº”æ¨¡å¼ï¼šå•deploymentè¶…é«˜å¹¶å‘
                    base_workers = 200  # è¶…é«˜åŸºç¡€å¹¶å‘
                    max_workers = base_workers * prompt_count if prompt_count > 1 else base_workers
                    qps = None
                    logger.info(f"  Azureé—­æºæ¨¡å‹è‡ªé€‚åº”æ¨¡å¼: {prompt_count}ä¸ªprompt Ã— {base_workers} = {max_workers} workers")
            else:
                # å…¶ä»–æœªåˆ†ç±»æ¨¡å‹ - ä½¿ç”¨ä¿å®ˆé…ç½®
                logger.warning(f"  æœªè¯†åˆ«çš„æ¨¡å‹æ— {instance.model_family}ï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
                if max_workers is not None:
                    base_workers = max_workers
                    max_workers = base_workers * prompt_count if prompt_count > 1 else base_workers
                    qps = None  # ç§»é™¤QPSé™åˆ¶
                elif rate_mode == "fixed":
                    base_workers = 30  # ä¿å®ˆçš„å›ºå®šæ¨¡å¼é…ç½®
                    max_workers = base_workers * prompt_count if prompt_count > 1 else base_workers
                    qps = None  # ç§»é™¤QPSé™åˆ¶
                else:
                    base_workers = 50  # ä¿å®ˆçš„è‡ªé€‚åº”æ¨¡å¼é…ç½®
                    max_workers = base_workers * prompt_count if prompt_count > 1 else base_workers
                    qps = None
        else:
            # é»˜è®¤é…ç½®
            # å¦‚æœç”¨æˆ·æŒ‡å®šäº†max_workersï¼Œä¼˜å…ˆä½¿ç”¨
            if max_workers is not None:
                base_workers = max_workers
                max_workers = base_workers * prompt_count if prompt_count > 1 else base_workers
                qps = None  # ç§»é™¤QPSé™åˆ¶
            elif rate_mode == "fixed":
                base_workers = 30
                max_workers = base_workers * prompt_count if prompt_count > 1 else base_workers
                qps = None  # ç§»é™¤QPSé™åˆ¶
            else:
                base_workers = 50
                max_workers = base_workers * prompt_count if prompt_count > 1 else base_workers
                qps = None
        
        # æ„å»ºå‘½ä»¤
        cmd = [
            "python", "smart_batch_runner.py",
            "--model", shard.model,  # å°å†™çš„åŸºç¡€æ¨¡å‹åï¼ˆç”¨äºç»Ÿè®¡ï¼‰
            "--deployment", deployment_name,  # ä½¿ç”¨æ˜ å°„åçš„éƒ¨ç½²åï¼ˆå¯¹qwenæ˜¯æ¨¡å‹åï¼Œå¯¹Azureæ˜¯å®ä¾‹åï¼‰
            "--prompt-types", shard.prompt_types,
            "--difficulty", shard.difficulty,
            "--task-types", shard.task_types,
            "--num-instances", str(shard.num_instances),
            "--max-workers", str(max_workers),
            "--tool-success-rate", str(shard.tool_success_rate),
            "--batch-commit",
            "--checkpoint-interval", "20",
            "--ai-classification",
            "--no-save-logs"  # é¿å…æ—¥å¿—å†²çª
        ]
        
        # å¯¹äºqwenè™šæ‹Ÿå®ä¾‹ï¼Œæ·»åŠ API keyç´¢å¼•å‚æ•°
        if shard.instance_name.startswith("qwen-key"):
            key_index = int(shard.instance_name[-1])  # ä» "qwen-key0" æå– 0
            cmd.extend(["--idealab-key-index", str(key_index)])
            logger.info(f"  ä½¿ç”¨IdealLab API Key {key_index}")
        elif shard.instance_name == "qwen-serial":
            # ä¸²è¡Œæ¨¡å¼ï¼šä¸æŒ‡å®škeyï¼Œè®©smart_batch_runnerè‡ªå·±è½®è¯¢ä½¿ç”¨
            logger.info(f"  ä¸²è¡Œæ¨¡å¼: å°†åœ¨å†…éƒ¨è½®è¯¢ä½¿ç”¨å¤šä¸ªAPI keys")
        
        # æ·»åŠ é™é»˜æ¨¡å¼å‚æ•°ï¼ˆå¦‚æœæ˜¯è°ƒè¯•è¿›ç¨‹åˆ™ä¸é™é»˜ï¼‰
        debug_process_num = int(os.environ.get('DEBUG_PROCESS_NUM', '1'))
        if os.environ.get('DEBUG_LOG', 'false').lower() == 'true' and shard_index == debug_process_num:
            # è¿™æ˜¯è¦è°ƒè¯•çš„è¿›ç¨‹ï¼Œä¸ä½¿ç”¨é™é»˜æ¨¡å¼
            logger.info(f"ğŸ” è¿›ç¨‹ {shard_index} å¯ç”¨è°ƒè¯•æ¨¡å¼ï¼ˆè¯¦ç»†æ—¥å¿—ï¼‰")
        elif silent:
            cmd.append("--silent")
        
        # æ ¹æ®rate_modeæ·»åŠ å‚æ•°
        if rate_mode == "fixed":
            if qps is not None:
                cmd.extend(["--no-adaptive", "--qps", str(qps)])
            else:
                cmd.append("--no-adaptive")  # å›ºå®šæ¨¡å¼ä½†æ— QPSé™åˆ¶
        else:
            cmd.append("--adaptive")
        
        # æœ‰å¤šä¸ªpromptæ—¶æ‰æ·»åŠ prompt-parallel
        if use_prompt_parallel:
            cmd.append(use_prompt_parallel)
        
        # æ·»åŠ ç»“æœæ–‡ä»¶åç¼€
        if result_suffix:
            cmd.extend(["--result-suffix", result_suffix])
        
        logger.info(f"ğŸš€ å¯åŠ¨åˆ†ç‰‡ {shard.shard_id}: {shard.instance_name}")
        logger.info(f"   å®ä¾‹æ•°: {shard.num_instances}, æ¨¡å‹: {shard.model}")
        
        # å¯åŠ¨è¿›ç¨‹ï¼ˆä¿ç•™é”™è¯¯è¾“å‡ºä»¥ä¾¿è°ƒè¯•ï¼‰
        # ç¡®ä¿ä¼ é€’ç¯å¢ƒå˜é‡
        env = os.environ.copy()
        
        # ç¡®ä¿æ‰€æœ‰å…³é”®å†…å­˜ä¼˜åŒ–ç¯å¢ƒå˜é‡è¢«ä¼ é€’
        critical_env_vars = {
            'STORAGE_FORMAT': os.environ.get('STORAGE_FORMAT', 'json'),
            'USE_PARTIAL_LOADING': os.environ.get('USE_PARTIAL_LOADING', 'true'),
            'TASK_LOAD_COUNT': os.environ.get('TASK_LOAD_COUNT', '20'),
            'SKIP_MODEL_LOADING': os.environ.get('SKIP_MODEL_LOADING', 'true'),
            'USE_RESULT_COLLECTOR': os.environ.get('USE_RESULT_COLLECTOR', 'true'),
            'KMP_DUPLICATE_LIB_OK': 'TRUE',
            'PYTHONMALLOC': 'malloc'
        }
        
        # æ›´æ–°ç¯å¢ƒå˜é‡
        for key, value in critical_env_vars.items():
            if key not in env or not env.get(key):
                env[key] = value
                logger.info(f"   è®¾ç½®{key}={value}ç»™å­è¿›ç¨‹")
            else:
                logger.info(f"   ä¼ é€’{key}={env[key]}ç»™å­è¿›ç¨‹")
        
        # æ„å»ºç¯å¢ƒå˜é‡å‰ç¼€å‘½ä»¤ï¼ˆç¡®ä¿æ‰€æœ‰å…³é”®å˜é‡éƒ½ä¼ é€’ï¼‰
        env_prefix = []
        for key, value in critical_env_vars.items():
            env_prefix.append(f'{key}={env[key]}')
        
        cmd_with_env = ['env'] + env_prefix + cmd
        
        process = subprocess.Popen(
            cmd_with_env,
            stdout=None,  # å…è®¸è¾“å‡ºåˆ°ç»ˆç«¯ (ä¸é‡å®šå‘)
            stderr=subprocess.STDOUT,  # å°†é”™è¯¯è¾“å‡ºåˆå¹¶åˆ°æ ‡å‡†è¾“å‡º
            text=True,
            env=env  # æ˜¾å¼ä¼ é€’ç¯å¢ƒå˜é‡
        )
        
        self.active_tasks.add(shard.shard_id)
        return process
        
    def run_ultra_parallel_test(self, model: str, prompt_types: str, difficulty: str,
                               task_types: str = "all", num_instances: int = 20,
                               rate_mode: str = "adaptive", result_suffix: str = "",
                               silent: bool = False, tool_success_rate: float = 0.8,
                               max_workers: int = None) -> bool:
        """è¿è¡Œè¶…é«˜å¹¶è¡Œåº¦æµ‹è¯• - è‡ªåŠ¨æ£€æµ‹qwenæ¨¡å‹å¹¶ä½¿ç”¨é˜Ÿåˆ—è°ƒåº¦
        
        Args:
            model: æ¨¡å‹åç§°
            prompt_types: æç¤ºç±»å‹
            difficulty: éš¾åº¦
            task_types: ä»»åŠ¡ç±»å‹
            num_instances: å®ä¾‹æ•°
            rate_mode: é€Ÿç‡æ¨¡å¼ - "adaptive" æˆ– "fixed"
        """
        
        # æ£€æµ‹æ˜¯å¦æ˜¯qwenæ¨¡å‹ï¼Œå¦‚æœæ˜¯åˆ™ä½¿ç”¨é˜Ÿåˆ—è°ƒåº¦
        if "qwen" in model.lower() and not self._qwen_batch_mode:
            logger.info(f"\nğŸ¯ æ£€æµ‹åˆ°Qwenæ¨¡å‹ï¼Œä½¿ç”¨é˜Ÿåˆ—è°ƒåº¦å™¨")
            
            # è·å–åˆ†é…çš„key
            import re
            match = re.search(r'(\d+b)', model.lower())
            if match:
                model_size = match.group(1)
                KEY_MAP = {"72b": 0, "32b": 1, "14b": 2, "7b": 0, "3b": 1}
                key_idx = KEY_MAP.get(model_size, 0)
            else:
                key_idx = 0
            
            logger.info(f"   æ¨¡å‹: {model} â†’ Key{key_idx}")
            logger.info(f"   Promptç±»å‹: {prompt_types}")
            logger.info(f"   éš¾åº¦: {difficulty}")
            
            # æäº¤åˆ°é˜Ÿåˆ—
            self.qwen_scheduler.submit_task(
                model=model,
                key_idx=key_idx,
                func=self._run_qwen_test_internal,
                prompt_types=prompt_types,
                difficulty=difficulty,
                task_types=task_types,
                num_instances=num_instances,
                rate_mode=rate_mode,
                result_suffix=result_suffix,
                silent=silent,
                tool_success_rate=tool_success_rate,
                max_workers=max_workers
            )
            
            # å¦‚æœä¸æ˜¯æ‰¹é‡æ¨¡å¼ï¼Œç­‰å¾…å®Œæˆ
            if not self._qwen_batch_mode:
                self.qwen_scheduler.wait_all()
            
            return True
        
        # éqwenæ¨¡å‹æˆ–æ‰¹é‡æ¨¡å¼ä¸­çš„qwenï¼Œä½¿ç”¨åŸé€»è¾‘
        logger.info(f"\nğŸ”¥ å¯åŠ¨è¶…é«˜å¹¶è¡Œæµ‹è¯•")
        logger.info(f"   æ¨¡å‹: {model}")
        logger.info(f"   Promptç±»å‹: {prompt_types}") 
        logger.info(f"   éš¾åº¦: {difficulty}")
        logger.info(f"   å®ä¾‹æ•°: {num_instances}")
        logger.info(f"   é€Ÿç‡æ¨¡å¼: {rate_mode}")
        
        # åˆ›å»ºä»»åŠ¡åˆ†ç‰‡
        shards = self.create_task_shards(model, prompt_types, difficulty, 
                                        task_types, num_instances, tool_success_rate)
        
        if not shards:
            logger.error("æ— æ³•åˆ›å»ºä»»åŠ¡åˆ†ç‰‡")
            return False
            
        logger.info(f"ğŸ“Š åˆ›å»ºäº† {len(shards)} ä¸ªå¹¶è¡Œåˆ†ç‰‡")
        
        # å¹¶è¡Œå¯åŠ¨æ‰€æœ‰åˆ†ç‰‡ï¼ˆé”™å¼€å¯åŠ¨é¿å…workflowç”Ÿæˆå†²çªï¼‰
        processes = []
        start_time = time.time()
        
        # æ™ºèƒ½åˆ†ç»„å¯åŠ¨ï¼Œå¹³è¡¡å¹¶å‘æ€§å’Œç¨³å®šæ€§
        # ç­–ç•¥ï¼šç¬¬ä¸€ä¸ªåˆ†ç‰‡ç«‹å³å¯åŠ¨ï¼Œåç»­åˆ†ç‰‡å»¶è¿Ÿå¯åŠ¨
        # è¿™æ ·ç¬¬ä¸€ä¸ªåˆ†ç‰‡çš„workflowç”Ÿæˆå¯ä»¥ä¸åç»­åˆ†ç‰‡çš„å¯åŠ¨å¹¶è¡Œ
        for i, shard in enumerate(shards):
            if i == 0:
                # ç¬¬ä¸€ä¸ªåˆ†ç‰‡ç«‹å³å¯åŠ¨
                process = self.execute_shard_async(shard, rate_mode=rate_mode, result_suffix=result_suffix, silent=silent, max_workers=max_workers, shard_index=i+1)
                processes.append((shard, process))
                logger.info(f"ğŸš€ ç¬¬ä¸€ä¸ªåˆ†ç‰‡ {shard.shard_id} ç«‹å³å¯åŠ¨")
            elif i == 1:
                # ç¬¬äºŒä¸ªåˆ†ç‰‡å»¶è¿Ÿ5ç§’ï¼ˆç°åœ¨ä½¿ç”¨é¢„åŠ è½½workflowï¼Œæ— éœ€é•¿å»¶è¿Ÿï¼‰
                logger.info(f"â±ï¸  å»¶è¿Ÿ5ç§’åå¯åŠ¨ç¬¬äºŒä¸ªåˆ†ç‰‡...")
                time.sleep(5)
                process = self.execute_shard_async(shard, rate_mode=rate_mode, result_suffix=result_suffix, silent=silent, max_workers=max_workers, shard_index=i+1)
                processes.append((shard, process))
            else:
                # ç¬¬ä¸‰ä¸ªåŠåç»­åˆ†ç‰‡å»¶è¿Ÿ5ç§’ï¼ˆé¢„åŠ è½½workflowï¼Œå¿«é€Ÿå¯åŠ¨ï¼‰
                logger.info(f"â±ï¸  å»¶è¿Ÿ5ç§’åå¯åŠ¨åˆ†ç‰‡ {i+1}...")
                time.sleep(5)
                process = self.execute_shard_async(shard, rate_mode=rate_mode, result_suffix=result_suffix, silent=silent, max_workers=max_workers, shard_index=i+1)
                processes.append((shard, process))
            
        # å¹¶å‘ç­‰å¾…æ‰€æœ‰è¿›ç¨‹å®Œæˆï¼ˆçœŸæ­£çš„å¹¶å‘ï¼ï¼‰
        logger.info(f"â³ å¹¶å‘ç­‰å¾… {len(processes)} ä¸ªåˆ†ç‰‡å®Œæˆ...")
        
        success_count = 0
        failed_shards = []
        completed_shards = set()
        
        # ä½¿ç”¨è½®è¯¢æ–¹å¼æ£€æŸ¥æ‰€æœ‰è¿›ç¨‹çŠ¶æ€ï¼ˆéé˜»å¡ï¼‰
        while len(completed_shards) < len(processes):
            for shard, process in processes:
                if shard.shard_id in completed_shards:
                    continue
                    
                # éé˜»å¡æ£€æŸ¥è¿›ç¨‹çŠ¶æ€
                poll_result = process.poll()
                if poll_result is not None:  # è¿›ç¨‹å·²ç»“æŸ
                    completed_shards.add(shard.shard_id)
                    
                    if poll_result == 0:
                        logger.info(f"âœ… åˆ†ç‰‡ {shard.shard_id} å®Œæˆ")
                        success_count += 1
                        self._update_performance_score(shard.instance_name, True)
                    else:
                        # è¯»å–é”™è¯¯è¾“å‡º
                        stderr_output = process.stderr.read() if process.stderr else ""
                        logger.error(f"âŒ åˆ†ç‰‡ {shard.shard_id} å¤±è´¥ (é€€å‡ºç : {poll_result})")
                        if stderr_output:
                            # æ˜¾ç¤ºæ›´å¤šé”™è¯¯ä¿¡æ¯ï¼Œç‰¹åˆ«æ˜¯æœ€åçš„éƒ¨åˆ†
                            lines = stderr_output.strip().split('\n')
                            if len(lines) > 10:
                                logger.error(f"   é”™è¯¯ä¿¡æ¯ï¼ˆæœ€å10è¡Œï¼‰:")
                                for line in lines[-10:]:
                                    logger.error(f"     {line}")
                            else:
                                logger.error(f"   é”™è¯¯ä¿¡æ¯: {stderr_output}")
                        failed_shards.append(shard.shard_id)
                        self._update_performance_score(shard.instance_name, False)
                    
                    # é‡Šæ”¾å®ä¾‹
                    if shard.instance_name in self.instance_pool:
                        instance = self.instance_pool[shard.instance_name]
                        instance.is_busy = False
                        instance.current_load = max(0, instance.current_load - shard.num_instances)
                    
                    self.active_tasks.discard(shard.shard_id)
            
            # çŸ­æš‚ä¼‘çœ é¿å…CPUå ç”¨è¿‡é«˜
            if len(completed_shards) < len(processes):
                time.sleep(1)
                
        end_time = time.time()
        duration = end_time - start_time
        
        # æŠ¥å‘Šç»“æœ
        logger.info(f"\nğŸ“Š å¹¶è¡Œæµ‹è¯•å®Œæˆ")
        logger.info(f"   æˆåŠŸ: {success_count}/{len(shards)} ä¸ªåˆ†ç‰‡")
        logger.info(f"   æ€»è€—æ—¶: {duration:.1f}ç§’")
        logger.info(f"   ç†è®ºåŠ é€Ÿæ¯”: {len(shards)}x")
        
        if failed_shards:
            logger.warning(f"   å¤±è´¥åˆ†ç‰‡: {failed_shards}")
        
        # æ–°åŠŸèƒ½ï¼šæ”¶é›†å’Œèšåˆæ‰€æœ‰ç»“æœï¼ˆå¦‚æœå¯ç”¨äº†collectoræ¨¡å¼ï¼‰
        if self.use_collector_mode and success_count > 0:
            self._collect_and_aggregate_results(model)
            
        return len(failed_shards) == 0
    
    def _collect_and_aggregate_results(self, model: str):
        """æ”¶é›†å¹¶èšåˆæ‰€æœ‰åˆ†ç‰‡çš„ç»“æœï¼ˆæ–°åŠŸèƒ½ï¼‰"""
        logger.info("ğŸ”„ å¼€å§‹æ”¶é›†æ‰€æœ‰åˆ†ç‰‡çš„æµ‹è¯•ç»“æœ...")
        
        try:
            # æ”¶é›†æ‰€æœ‰å¾…å¤„ç†çš„ç»“æœ
            all_results = self.result_collector.collect_all_results(cleanup=True)
            
            if not all_results:
                logger.warning("âš ï¸ æœªå‘ç°ä»»ä½•å¾…å¤„ç†çš„ç»“æœ")
                return
            
            # èšåˆç»“æœ
            logger.info("ğŸ“Š å¼€å§‹èšåˆç»“æœ...")
            aggregated_db = self.result_aggregator.aggregate_results(all_results)
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            self._save_aggregated_results(aggregated_db)
            
            logger.info("âœ… ç»“æœæ”¶é›†å’Œèšåˆå®Œæˆï¼Œæ•°æ®å·²å®‰å…¨ä¿å­˜")
            
        except Exception as e:
            logger.error(f"âŒ ç»“æœæ”¶é›†å¤±è´¥: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œè®©æµ‹è¯•ç»§ç»­å®Œæˆ
            
    def _save_aggregated_results(self, aggregated_db: Dict):
        """ä¿å­˜èšåˆåçš„ç»“æœåˆ°æ•°æ®åº“"""
        # ä½¿ç”¨ä¼ ç»Ÿçš„æ•°æ®åº“ä¿å­˜æœºåˆ¶ï¼Œä½†ç°åœ¨æ˜¯å•çº¿ç¨‹å®‰å…¨çš„
        from pathlib import Path
        import json
        
        db_file = Path("pilot_bench_cumulative_results/master_database.json")
        db_file.parent.mkdir(exist_ok=True)
        
        # å¦‚æœå·²æœ‰æ•°æ®åº“ï¼Œéœ€è¦åˆå¹¶
        if db_file.exists():
            try:
                with open(db_file, 'r', encoding='utf-8') as f:
                    existing_db = json.load(f)
                    
                # åˆå¹¶æ•°æ®åº“ï¼ˆè¿™é‡Œç°åœ¨æ˜¯å®‰å…¨çš„ï¼Œå› ä¸ºåªæœ‰ä¸€ä¸ªå†™å…¥è€…ï¼‰
                merged_db = self._merge_databases(existing_db, aggregated_db)
            except Exception as e:
                logger.warning(f"è¯»å–ç°æœ‰æ•°æ®åº“å¤±è´¥ï¼Œå°†åˆ›å»ºæ–°æ•°æ®åº“: {e}")
                merged_db = aggregated_db
        else:
            merged_db = aggregated_db
        
        # åŸå­å†™å…¥
        temp_file = db_file.with_suffix('.tmp')
        with open(temp_file, 'w', encoding='utf-8') as f:
            json.dump(merged_db, f, indent=2, ensure_ascii=False)
        
        temp_file.replace(db_file)
        logger.info(f"ğŸ’¾ æ•°æ®åº“å·²ä¿å­˜åˆ°: {db_file}")
        
    def _merge_databases(self, existing: Dict, new: Dict) -> Dict:
        """åˆå¹¶ä¸¤ä¸ªæ•°æ®åº“ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„åˆå¹¶é€»è¾‘
        # ç°åœ¨å…ˆåšç®€å•çš„æ¨¡å‹çº§åˆ«åˆå¹¶
        merged = existing.copy()
        
        if 'models' in new:
            if 'models' not in merged:
                merged['models'] = {}
            merged['models'].update(new['models'])
        
        merged['last_updated'] = new.get('last_updated', existing.get('last_updated'))
        return merged
        
    def _update_performance_score(self, instance_name: str, success: bool):
        """æ›´æ–°å®ä¾‹æ€§èƒ½è¯„åˆ†"""
        if instance_name not in self.performance_stats:
            self.performance_stats[instance_name] = {'success': 0, 'total': 0}
            
        stats = self.performance_stats[instance_name]
        stats['total'] += 1
        if success:
            stats['success'] += 1
            
        # è®¡ç®—æˆåŠŸç‡ä½œä¸ºæ€§èƒ½è¯„åˆ†
        success_rate = stats['success'] / stats['total']
        self.instance_pool[instance_name].performance_score = success_rate
        
    def _run_qwen_test_internal(self, model: str, prompt_types: str, difficulty: str,
                               task_types: str, num_instances: int, rate_mode: str,
                               result_suffix: str, silent: bool, tool_success_rate: float,
                               max_workers: int) -> bool:
        """å†…éƒ¨æ–¹æ³•ï¼šå®é™…æ‰§è¡Œqwenæµ‹è¯•ï¼ˆåœ¨é˜Ÿåˆ—ä¸­è¿è¡Œï¼‰
        
        ç°åœ¨æ”¯æŒå¤šåˆ†ç‰‡å¹¶å‘æ‰§è¡Œï¼
        """
        
        # åˆ›å»ºä»»åŠ¡åˆ†ç‰‡ï¼ˆç°åœ¨æ”¯æŒå¤šä¸ªåˆ†ç‰‡ï¼ï¼‰
        shards = self.create_task_shards(model, prompt_types, difficulty,
                                        task_types, num_instances, tool_success_rate)
        
        if not shards:
            logger.error(f"æ— æ³•åˆ›å»ºä»»åŠ¡åˆ†ç‰‡: {model}")
            return False
        
        logger.info(f"ğŸš€ å¯åŠ¨{len(shards)}ä¸ªåˆ†ç‰‡å¹¶å‘æ‰§è¡Œ")
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰åˆ†ç‰‡
        processes = []
        for i, shard in enumerate(shards):
            process = self.execute_shard_async(shard, rate_mode=rate_mode,
                                              result_suffix=result_suffix,
                                              silent=silent, max_workers=max_workers,
                                              shard_index=i+1)
            processes.append(process)
            logger.info(f"   åˆ†ç‰‡{i+1}: {shard.instance_name} ({shard.num_instances}ä¸ªå®ä¾‹)")
        
        # ç­‰å¾…æ‰€æœ‰åˆ†ç‰‡å®Œæˆ - æ·»åŠ è¶…æ—¶ä¿æŠ¤é˜²æ­¢æ— é™ç­‰å¾…
        success_count = 0
        
        for i, process in enumerate(processes):
            try:
                # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨æ›´å¯é çš„è¶…æ—¶æœºåˆ¶ï¼ˆåŸºäºpollè€Œä¸æ˜¯signalï¼‰
                import time
                
                # ğŸ¯ åŠ¨æ€è®¡ç®—è¶…æ—¶æ—¶é—´ï¼ŒåŸºäºæµ‹è¯•è§„æ¨¡
                base_timeout = 30  # åŸºç¡€30åˆ†é’Ÿ
                
                # è·å–æµ‹è¯•è§„æ¨¡å‚æ•°
                num_instances = int(os.environ.get('NUM_INSTANCES', '20'))
                max_workers = int(os.environ.get('CUSTOM_WORKERS', '50'))
                
                # æ ¹æ®å®ä¾‹æ•°é‡è°ƒæ•´ï¼šæ¯ä¸ªå®ä¾‹å¹³å‡1åˆ†é’Ÿ
                instance_timeout = num_instances * 1  
                # æ ¹æ®workeræ•°é‡è°ƒæ•´ï¼šworkerå°‘åˆ™éœ€è¦æ›´å¤šæ—¶é—´
                worker_factor = max(1.0, 50.0 / max_workers)  
                
                timeout_minutes = int(base_timeout + instance_timeout * worker_factor)
                timeout_minutes = max(30, min(timeout_minutes, 120))  # é™åˆ¶åœ¨30-120åˆ†é’Ÿä¹‹é—´
                timeout_seconds = timeout_minutes * 60
                start_time = time.time()
                
                logger.info(f"ç­‰å¾…åˆ†ç‰‡{i+1}å®Œæˆï¼ˆ{num_instances}å®ä¾‹Ã—{max_workers}workersï¼Œæœ€å¤šç­‰å¾…{timeout_minutes}åˆ†é’Ÿï¼‰...")
                
                # è½®è¯¢ç­‰å¾…ï¼Œé¿å…æ— é™é˜»å¡
                while True:
                    # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦ç»“æŸ
                    return_code = process.poll()
                    if return_code is not None:
                        # è¿›ç¨‹å·²ç»“æŸ
                        if return_code == 0:
                            success_count += 1
                            logger.info(f"âœ… åˆ†ç‰‡{i+1}å®Œæˆ")
                        else:
                            logger.error(f"âŒ åˆ†ç‰‡{i+1}å¤±è´¥ (é€€å‡ºç : {return_code})")
                        break
                    
                    # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
                    elapsed = time.time() - start_time
                    if elapsed > timeout_seconds:
                        logger.warning(f"â° åˆ†ç‰‡{i+1}æ‰§è¡Œè¶…æ—¶{timeout_minutes}åˆ†é’Ÿï¼Œå¼ºåˆ¶ç»ˆæ­¢")
                        # å…ˆå°è¯•ä¼˜é›…ç»ˆæ­¢
                        process.terminate()
                        # ç­‰å¾…3ç§’è®©è¿›ç¨‹æœ‰æœºä¼šæ¸…ç†
                        for _ in range(30):  # 3ç§’
                            if process.poll() is not None:
                                break
                            time.sleep(0.1)
                        
                        # å¦‚æœè¿˜æ²¡ç»“æŸå°±å¼ºåˆ¶æ€æ­»
                        if process.poll() is None:
                            logger.warning(f"åˆ†ç‰‡{i+1}æœªå“åº”SIGTERMï¼Œä½¿ç”¨SIGKILLå¼ºåˆ¶ç»“æŸ")
                            process.kill()
                            process.wait()  # ç¡®ä¿è¿›ç¨‹å®Œå…¨ç»“æŸ
                        
                        logger.error(f"âŒ åˆ†ç‰‡{i+1}è¶…æ—¶ç»ˆæ­¢")
                        break
                    
                    # çŸ­æš‚ä¼‘çœ é¿å…CPUå ç”¨è¿‡é«˜
                    time.sleep(1)
                    
            except Exception as e:
                logger.error(f"âŒ åˆ†ç‰‡{i+1}ç­‰å¾…è¿‡ç¨‹ä¸­å‡ºç°å¼‚å¸¸: {e}")
                # ç¡®ä¿å¼‚å¸¸æƒ…å†µä¸‹ä¹Ÿå°è¯•æ¸…ç†è¿›ç¨‹
                try:
                    if process.poll() is None:
                        process.kill()
                        process.wait()
                except:
                    pass
        
        logger.info(f"ğŸ“Š å¹¶å‘æ‰§è¡Œç»“æœ: {success_count}/{len(processes)} åˆ†ç‰‡æˆåŠŸ")
        
        return success_count == len(processes)
    
    def run_batch_qwen_tests(self, models: List[str], prompt_types: str,
                            difficulties: List[str], task_types: str = "all",
                            num_instances: int = 20, rate_mode: str = "fixed",
                            result_suffix: str = "", silent: bool = False,
                            tool_success_rate: float = 0.8, max_workers: int = None) -> bool:
        """æ‰¹é‡è¿è¡Œqwenæµ‹è¯• - ä¸“ä¸ºPhase 5.2ç­‰åœºæ™¯è®¾è®¡
        
        ä½¿ç”¨é˜Ÿåˆ—è°ƒåº¦å™¨ç¡®ä¿ï¼š
        1. åŒä¸€keyçš„ä»»åŠ¡ä¸²è¡Œæ‰§è¡Œ
        2. ä¸åŒkeyä¹‹é—´å¹¶è¡Œæ‰§è¡Œ
        3. æ²¡æœ‰API keyå†²çª
        """
        
        logger.info(f"\nğŸš€ æ‰¹é‡Qwenæµ‹è¯•ï¼ˆé˜Ÿåˆ—è°ƒåº¦æ¨¡å¼ï¼‰")
        logger.info(f"   æ¨¡å‹æ•°: {len(models)}")
        logger.info(f"   éš¾åº¦: {difficulties}")
        logger.info(f"   æ€»ä»»åŠ¡æ•°: {len(models) * len(difficulties)}")
        
        # è®¾ç½®æ‰¹é‡æ¨¡å¼
        self._qwen_batch_mode = True
        
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        task_count = 0
        for difficulty in difficulties:
            for model in models:
                if "qwen" in model.lower():
                    task_count += 1
                    self.run_ultra_parallel_test(
                        model=model,
                        prompt_types=prompt_types,
                        difficulty=difficulty,
                        task_types=task_types,
                        num_instances=num_instances,
                        rate_mode=rate_mode,
                        result_suffix=result_suffix,
                        silent=silent,
                        tool_success_rate=tool_success_rate,
                        max_workers=max_workers
                    )
        
        logger.info(f"ğŸ“‹ å·²æäº¤ {task_count} ä¸ªä»»åŠ¡åˆ°é˜Ÿåˆ—")
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        logger.info(f"â³ ç­‰å¾…æ‰€æœ‰é˜Ÿåˆ—ä»»åŠ¡å®Œæˆ...")
        self.qwen_scheduler.wait_all()
        
        # æ¸…é™¤æ‰¹é‡æ¨¡å¼
        self._qwen_batch_mode = False
        
        logger.info(f"âœ… æ‰¹é‡Qwenæµ‹è¯•å®Œæˆ")
        return True
    
    def get_resource_utilization(self) -> Dict:
        """è·å–èµ„æºåˆ©ç”¨ç‡ç»Ÿè®¡"""
        total_capacity = sum(inst.max_workers for inst in self.instance_pool.values())
        current_load = sum(inst.current_load for inst in self.instance_pool.values())
        busy_instances = sum(1 for inst in self.instance_pool.values() if inst.is_busy)
        
        return {
            "total_instances": len(self.instance_pool),
            "busy_instances": busy_instances,
            "total_capacity": total_capacity,
            "current_load": current_load,
            "utilization_rate": current_load / total_capacity if total_capacity > 0 else 0,
            "active_tasks": len(self.active_tasks)
        }

def main():
    """å‘½ä»¤è¡Œæ¥å£"""
    import argparse
    import os
    
    parser = argparse.ArgumentParser(description="Ultra Parallel Test Runner")
    parser.add_argument("--model", required=True, help="æ¨¡å‹åç§°")
    parser.add_argument("--prompt-types", required=True, help="Promptç±»å‹")
    parser.add_argument("--difficulty", default="easy", help="éš¾åº¦")
    parser.add_argument("--task-types", default="all", help="ä»»åŠ¡ç±»å‹")
    parser.add_argument("--num-instances", type=int, default=20, help="å®ä¾‹æ•°é‡")
    parser.add_argument("--rate-mode", default="adaptive", 
                       choices=["adaptive", "fixed"],
                       help="é€Ÿç‡æ¨¡å¼: adaptive(åŠ¨æ€è°ƒæ•´) æˆ– fixed(å›ºå®šé€Ÿç‡)")
    parser.add_argument("--result-suffix", default="", 
                       help="ç»“æœæ–‡ä»¶åç¼€(ç”¨äºåŒºåˆ†é—­æº/å¼€æºæ¨¡å‹)")
    parser.add_argument("--silent", action="store_true",
                       help="é™é»˜æ¨¡å¼ï¼Œå‡å°‘è¾“å‡º")
    parser.add_argument("--tool-success-rate", type=float, default=0.8,
                       help="å·¥å…·æˆåŠŸç‡(0.0-1.0)")
    parser.add_argument("--max-workers", type=int, default=None,
                       help="æ¯ä¸ªåˆ†ç‰‡çš„æœ€å¤§å¹¶å‘workersæ•°")
    
    args = parser.parse_args()
    
    # ä¹Ÿå¯ä»¥ä»ç¯å¢ƒå˜é‡è¯»å–rate_mode
    rate_mode = args.rate_mode
    if os.environ.get("RATE_MODE"):
        rate_mode = os.environ.get("RATE_MODE")
        logger.info(f"ä½¿ç”¨ç¯å¢ƒå˜é‡ RATE_MODE: {rate_mode}")
    
    # æ³¨æ„ï¼šä¿ç•™loggerçš„INFOçº§åˆ«ä»¥æ˜¾ç¤ºé«˜å±‚çº§è¿›åº¦ä¿¡æ¯
    # silentå‚æ•°åªæ§åˆ¶è¯¦ç»†çš„æ‰§è¡Œè¾“å‡ºï¼Œä¸å½±å“ä¸»è¦è¿›åº¦æ˜¾ç¤º
    
    runner = UltraParallelRunner()
    
    # æ˜¾ç¤ºèµ„æºçŠ¶æ€
    util = runner.get_resource_utilization()
    logger.info(f"èµ„æºæ± çŠ¶æ€: {util['total_instances']}ä¸ªå®ä¾‹, å®¹é‡{util['total_capacity']}")
    
    # æ‰§è¡Œæµ‹è¯•
    success = runner.run_ultra_parallel_test(
        model=args.model,
        prompt_types=args.prompt_types,
        difficulty=args.difficulty,
        task_types=args.task_types,
        num_instances=args.num_instances,
        rate_mode=rate_mode,
        result_suffix=args.result_suffix,
        silent=args.silent,
        tool_success_rate=args.tool_success_rate,
        max_workers=args.max_workers
    )
    
    # æœ€ç»ˆç»Ÿè®¡
    final_util = runner.get_resource_utilization()
    logger.info(f"æœ€ç»ˆåˆ©ç”¨ç‡: {final_util['utilization_rate']:.1%}")
    
    return 0 if success else 1

if __name__ == "__main__":
    exit(main())