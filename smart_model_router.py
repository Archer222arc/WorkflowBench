#!/usr/bin/env python3
"""
æ™ºèƒ½æ¨¡åž‹è·¯ç”±å™¨
==============
è‡ªåŠ¨é€‰æ‹©æœ€ä¼˜çš„APIç«¯ç‚¹ï¼Œä¼˜å…ˆä½¿ç”¨ç”¨æˆ·çš„Azureç«¯ç‚¹
"""

from typing import Dict, Optional, List, Tuple
import logging

logger = logging.getLogger(__name__)

# ç”¨æˆ·Azureç«¯ç‚¹å¯ç”¨çš„æ¨¡åž‹
USER_AZURE_MODELS = {
    # åŸºç¡€æ¨¡åž‹
    "DeepSeek-R1-0528",
    "DeepSeek-V3-0324",
    "gpt-5-mini",
    "gpt-5-nano",
    "gpt-oss-120b",
    "Llama-3.3-70B-Instruct",
    "grok-3",         # Azure AI Foundry
    "grok-3-mini",    # Azure AI Foundry
    
    # å¹¶è¡Œéƒ¨ç½²å®žä¾‹ï¼ˆä¿æŒåŽŸæ ·ä¼ é€’ï¼Œä¸åšè½¬æ¢ï¼‰
    "DeepSeek-V3-0324-2",      # DeepSeek V3 å¹¶è¡Œéƒ¨ç½²2
    "DeepSeek-V3-0324-3",      # DeepSeek V3 å¹¶è¡Œéƒ¨ç½²3
    "DeepSeek-R1-0528-2",      # DeepSeek R1 å¹¶è¡Œéƒ¨ç½²2
    "DeepSeek-R1-0528-3",      # DeepSeek R1 å¹¶è¡Œéƒ¨ç½²3
    "Llama-3.3-70B-Instruct-2", # Llama å¹¶è¡Œéƒ¨ç½²2
    "Llama-3.3-70B-Instruct-3"  # Llama å¹¶è¡Œéƒ¨ç½²3
}

# æ¨¡åž‹åˆ«åæ˜ å°„ï¼ˆæ”¯æŒå¤šç§åç§°æ ¼å¼ï¼‰
MODEL_ALIASES = {
    # DeepSeekç³»åˆ— - æ˜ å°„åˆ°æ­£ç¡®çš„deployment name
    "deepseek-r1": "DeepSeek-R1-0528",
    "deepseek-r1-0528": "DeepSeek-R1-0528",
    "deepseek-r1-671b": "DeepSeek-R1-0528",
    "DeepSeek-R1": "DeepSeek-R1-0528",
    "DeepSeek-R1-671B": "DeepSeek-R1-0528",
    
    "deepseek-v3": "DeepSeek-V3-0324",
    "deepseek-v3-0324": "DeepSeek-V3-0324",
    "deepseek-v3-671b": "DeepSeek-V3-0324",
    "DeepSeek-V3": "DeepSeek-V3-0324",
    "DeepSeek-V3-671B": "DeepSeek-V3-0324",
    
    # GPT-5ç³»åˆ—
    "gpt5-mini": "gpt-5-mini",
    "GPT-5-mini": "gpt-5-mini",
    "gpt5-nano": "gpt-5-nano",
    "GPT-5-nano": "gpt-5-nano",
    
    # GPT-OSS
    "gpt-oss": "gpt-oss-120b",
    "GPT-OSS": "gpt-oss-120b",
    "gpt-oss-120": "gpt-oss-120b",
    
    # Grok
    "grok3": "grok-3",
    "Grok-3": "grok-3",
    "GROK-3": "grok-3",
    "grok3-mini": "grok-3-mini",
    "Grok-3-mini": "grok-3-mini",
    
    # Llama
    "llama-3.3": "Llama-3.3-70B-Instruct",
    "llama-3.3-70b": "Llama-3.3-70B-Instruct",
    "llama-3.3-70b-instruct": "Llama-3.3-70B-Instruct",
    "Llama-3.3": "Llama-3.3-70B-Instruct",
    "llama-3.1-nemotron-70b-instruct": "Llama-3.3-70B-Instruct"  # æ˜ å°„åˆ°æ­£ç¡®çš„æ¨¡åž‹
}

# idealabå¯ç”¨çš„æ¨¡åž‹ï¼ˆä½œä¸ºå¤‡é€‰ï¼‰
IDEALAB_MODELS = {
    # GPTç³»åˆ—
    "gpt-41-0414-global", "o1-1217-global", "o3-0416-global", "o4-mini-0416-global",
    
    # Claudeç³»åˆ—
    "claude37_sonnet", "claude_sonnet4", "claude_opus4",
    
    # Geminiç³»åˆ—
    "gemini-2.5-pro-06-17", "gemini-2.5-flash-06-17", "gemini-1.5-pro", "gemini-2.0-flash",
    
    # DeepSeekç³»åˆ—ï¼ˆæ—§ç‰ˆï¼Œä½†ä»ç„¶å¯ç”¨ï¼‰
    "deepseek-v3-671b", "deepseek-r1-671b", "DeepSeek-V3-671B", "DeepSeek-R1-671B",
    
    # Qwenç³»åˆ—
    "qwen2.5-max", "qwen2.5-72b-instruct", "qwen2.5-32b-instruct", 
    "qwen2.5-14b-instruct", "qwen2.5-7b-instruct", "qwen2.5-3b-instruct",
    
    # Kimi
    "kimi-k2"
}

# Azureå¯ç”¨çš„æ¨¡åž‹
AZURE_MODELS = {"gpt-4o-mini"}


class SmartModelRouter:
    """æ™ºèƒ½æ¨¡åž‹è·¯ç”±å™¨ï¼Œè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜APIç«¯ç‚¹"""
    
    def __init__(self):
        self.routing_cache = {}
        
    def resolve_model_name(self, model_name: str) -> str:
        """è§£æžæ¨¡åž‹åç§°ï¼Œå¤„ç†åˆ«å"""
        # å¦‚æžœæ˜¯å·²çŸ¥çš„åˆ«åï¼Œè½¬æ¢ä¸ºæ ‡å‡†åç§°
        if model_name in MODEL_ALIASES:
            resolved = MODEL_ALIASES[model_name]
            logger.info(f"Resolved alias: {model_name} -> {resolved}")
            return resolved
        return model_name
    
    def get_best_provider(self, model_name: str) -> Tuple[str, str]:
        """
        èŽ·å–æ¨¡åž‹çš„æœ€ä½³æä¾›å•†
        
        Returns:
            (provider, resolved_model_name)
        """
        # å…ˆè§£æžæ¨¡åž‹åç§°
        resolved_name = self.resolve_model_name(model_name)
        
        # æ£€æŸ¥ç¼“å­˜
        if resolved_name in self.routing_cache:
            return self.routing_cache[resolved_name]
        
        # å…ˆæ£€æŸ¥æ˜¯å¦æœ‰å¯æ›¿ä»£çš„æ¨¡åž‹ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰
        if self._find_alternative(resolved_name):
            alt_provider, alt_model = self._find_alternative(resolved_name)
            provider = (alt_provider, alt_model)
            logger.info(f"âœ¨ Routing {resolved_name} -> {alt_model} via {alt_provider}")
            
        # ä¼˜å…ˆçº§2ï¼šç”¨æˆ·çš„Azureç«¯ç‚¹ï¼ˆæœ€æ–°æœ€å¼ºçš„æ¨¡åž‹ï¼‰
        elif resolved_name in USER_AZURE_MODELS:
            provider = ("user_azure", resolved_name)
            logger.info(f"âœ¨ Using USER's Azure endpoint for {resolved_name}")
            
        # ä¼˜å…ˆçº§4ï¼šæ ‡å‡†Azureç«¯ç‚¹
        elif resolved_name in AZURE_MODELS:
            provider = ("azure", resolved_name)
            logger.info(f"Using Azure for {resolved_name}")
            
        # ä¼˜å…ˆçº§5ï¼šidealab API
        elif resolved_name in IDEALAB_MODELS:
            provider = ("idealab", resolved_name)
            logger.info(f"Using idealab for {resolved_name}")
            
        else:
            # é»˜è®¤å°è¯•idealab
            provider = ("idealab", resolved_name)
            logger.warning(f"Model {resolved_name} not in known lists, defaulting to idealab")
        
        # ç¼“å­˜ç»“æžœ
        self.routing_cache[resolved_name] = provider
        return provider
    
    def _find_alternative(self, model_name: str) -> Optional[Tuple[str, str]]:
        """å¯»æ‰¾æ›¿ä»£æ¨¡åž‹"""
        alternatives = {
            # DeepSeekæ—§ç‰ˆæœ¬ -> æ–°ç‰ˆæœ¬ (V3-0324å¯ä»¥æ›¿ä»£æ‰€æœ‰V3å˜ä½“)
            "deepseek-v3-671b": ("user_azure", "DeepSeek-V3-0324"),
            "DeepSeek-V3-671B": ("user_azure", "DeepSeek-V3-0324"),
            "deepseek-v3": ("user_azure", "DeepSeek-V3-0324"),
            
            # DeepSeek R1ç³»åˆ—
            "deepseek-r1-671b": ("user_azure", "DeepSeek-R1-0528"),
            "DeepSeek-R1-671B": ("user_azure", "DeepSeek-R1-0528"),
            "deepseek-r1": ("user_azure", "DeepSeek-R1-0528"),
            
            # å…¶ä»–GPTæ¨¡åž‹çš„å¤‡é€‰
            "gpt-4o": ("idealab", "gpt-41-0414-global"),
            "gpt-o1": ("idealab", "o1-1217-global"),
            "gpt-o3": ("idealab", "o3-0416-global"),
            "gpt-o4-mini": ("idealab", "o4-mini-0416-global"),
        }
        
        if model_name in alternatives:
            return alternatives[model_name]
        return None
    
    def get_routing_summary(self) -> Dict[str, List[str]]:
        """èŽ·å–è·¯ç”±æ‘˜è¦"""
        summary = {
            "user_azure": list(USER_AZURE_MODELS),
            "azure": list(AZURE_MODELS),
            "idealab": list(IDEALAB_MODELS)
        }
        return summary
    
    def batch_route(self, model_names: List[str]) -> Dict[str, Tuple[str, str]]:
        """æ‰¹é‡è·¯ç”±å¤šä¸ªæ¨¡åž‹"""
        results = {}
        for model in model_names:
            provider, resolved = self.get_best_provider(model)
            results[model] = (provider, resolved)
        return results


# å…¨å±€è·¯ç”±å™¨å®žä¾‹
_router = None

def get_router() -> SmartModelRouter:
    """èŽ·å–å…¨å±€è·¯ç”±å™¨å®žä¾‹"""
    global _router
    if _router is None:
        _router = SmartModelRouter()
    return _router


def route_model(model_name: str) -> Tuple[str, str]:
    """ä¾¿æ·å‡½æ•°ï¼šè·¯ç”±å•ä¸ªæ¨¡åž‹"""
    router = get_router()
    return router.get_best_provider(model_name)


def print_routing_report(models: List[str]):
    """æ‰“å°è·¯ç”±æŠ¥å‘Š"""
    router = get_router()
    results = router.batch_route(models)
    
    print("\n" + "="*60)
    print("æ¨¡åž‹è·¯ç”±æŠ¥å‘Š")
    print("="*60)
    
    by_provider = {}
    for model, (provider, resolved) in results.items():
        if provider not in by_provider:
            by_provider[provider] = []
        by_provider[provider].append((model, resolved))
    
    for provider, models in by_provider.items():
        if provider == "user_azure":
            print(f"\nâœ¨ ä½¿ç”¨æ‚¨çš„Azureç«¯ç‚¹ ({len(models)}ä¸ªæ¨¡åž‹):")
        elif provider == "azure":
            print(f"\nâ˜ï¸ ä½¿ç”¨æ ‡å‡†Azure ({len(models)}ä¸ªæ¨¡åž‹):")
        else:
            print(f"\nðŸ”§ ä½¿ç”¨{provider} ({len(models)}ä¸ªæ¨¡åž‹):")
        
        for original, resolved in models:
            if original != resolved:
                print(f"  â€¢ {original} -> {resolved}")
            else:
                print(f"  â€¢ {original}")
    
    print("\n" + "="*60)


if __name__ == "__main__":
    # æµ‹è¯•è·¯ç”±å™¨
    test_models = [
        "gpt-5-mini",
        "deepseek-v3",
        "deepseek-v3-671b",  # æ—§ç‰ˆæœ¬ï¼Œåº”è¯¥è·¯ç”±åˆ°æ–°ç‰ˆ
        "gpt-4o-mini",
        "llama-3.3",
        "qwen2.5-72b-instruct",
        "grok-3"
    ]
    
    print_routing_report(test_models)