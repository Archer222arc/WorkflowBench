#!/usr/bin/env python3
"""
æµ‹è¯•é”™è¯¯è®¡æ•°ä¿®å¤æ•ˆæœ
éªŒè¯æ‰€æœ‰éfull_successçš„æµ‹è¯•éƒ½ä¼šè¢«æ­£ç¡®è®¡å…¥é”™è¯¯
"""

import os
os.environ['STORAGE_FORMAT'] = 'parquet'

from parquet_cumulative_manager import ParquetCumulativeManager
from cumulative_test_manager import TestRecord

def test_error_counting():
    """æµ‹è¯•é”™è¯¯è®¡æ•°æ˜¯å¦æ­£ç¡®"""
    
    print("=" * 60)
    print("æµ‹è¯•é”™è¯¯è®¡æ•°ä¿®å¤")
    print("=" * 60)
    
    # åˆ›å»ºç®¡ç†å™¨
    manager = ParquetCumulativeManager()
    
    # æµ‹è¯•åœºæ™¯1ï¼šå¤±è´¥çš„æµ‹è¯•ï¼ˆæ²¡æœ‰error_messageï¼‰
    print("\nåœºæ™¯1ï¼šå¤±è´¥çš„æµ‹è¯•ï¼ˆæ— error_messageï¼‰")
    record1 = TestRecord(
        model='test-error-counting',
        task_type='api_integration',
        prompt_type='optimal',
        difficulty='easy'
    )
    record1.tool_success_rate = 0.8
    record1.success = False
    record1.partial_success = False
    record1.success_level = 'failure'
    record1.execution_time = 10.0
    record1.turns = 10
    record1.tool_calls = 0  # æ²¡æœ‰å·¥å…·è°ƒç”¨
    record1.executed_tools = []
    # æ³¨æ„ï¼šæ²¡æœ‰error_message
    
    success = manager.add_test_result_with_classification(record1)
    print(f"  æ·»åŠ è®°å½•: {'âœ…' if success else 'âŒ'}")
    
    # æµ‹è¯•åœºæ™¯2ï¼šéƒ¨åˆ†æˆåŠŸï¼ˆæœ‰è¾…åŠ©ï¼‰
    print("\nåœºæ™¯2ï¼šéƒ¨åˆ†æˆåŠŸï¼ˆæœ‰è¾…åŠ©ï¼Œåº”è®¡å…¥é”™è¯¯ï¼‰")
    record2 = TestRecord(
        model='test-error-counting',
        task_type='api_integration',
        prompt_type='optimal',
        difficulty='easy'
    )
    record2.tool_success_rate = 0.8
    record2.success = True
    record2.partial_success = True
    record2.success_level = 'partial_success'
    record2.execution_time = 10.0
    record2.turns = 10
    record2.tool_calls = 1
    record2.format_error_count = 3  # æœ‰è¾…åŠ©
    record2.error_message = None  # æ²¡æœ‰æ˜ç¡®çš„é”™è¯¯æ¶ˆæ¯
    
    success = manager.add_test_result_with_classification(record2)
    print(f"  æ·»åŠ è®°å½•: {'âœ…' if success else 'âŒ'}")
    
    # æµ‹è¯•åœºæ™¯3ï¼šå®Œå…¨æˆåŠŸï¼ˆä¸åº”è®¡å…¥é”™è¯¯ï¼‰
    print("\nåœºæ™¯3ï¼šå®Œå…¨æˆåŠŸï¼ˆä¸åº”è®¡å…¥é”™è¯¯ï¼‰")
    record3 = TestRecord(
        model='test-error-counting',
        task_type='api_integration',
        prompt_type='optimal',
        difficulty='easy'
    )
    record3.tool_success_rate = 0.8
    record3.success = True
    record3.partial_success = False
    record3.success_level = 'full_success'
    record3.execution_time = 5.0
    record3.turns = 5
    record3.tool_calls = 3
    
    success = manager.add_test_result_with_classification(record3)
    print(f"  æ·»åŠ è®°å½•: {'âœ…' if success else 'âŒ'}")
    
    # åˆ·æ–°ç¼“å†²åŒº
    manager._flush_buffer()
    print("\nâœ… åˆ·æ–°ç¼“å†²åŒºæˆåŠŸ")
    
    # éªŒè¯ç»“æœ
    print("\néªŒè¯ç»Ÿè®¡ç»“æœ...")
    
    # æŸ¥è¯¢ç»Ÿè®¡
    from parquet_data_manager import ParquetDataManager
    pdm = ParquetDataManager()
    df = pdm.query_model_stats(model_name='test-error-counting')
    
    if not df.empty:
        latest = df.iloc[-1]
        
        # æ£€æŸ¥å…³é”®æŒ‡æ ‡
        total = latest.get('total', 0)
        success = latest.get('success', 0)
        full_success = latest.get('full_success', 0)
        partial_success = latest.get('partial_success', 0)
        failed = latest.get('failed', 0)
        total_errors = latest.get('total_errors', 0)
        format_errors = latest.get('tool_call_format_errors', 0)
        other_errors = latest.get('other_errors', 0)
        tests_with_assistance = latest.get('tests_with_assistance', 0)
        assisted_success = latest.get('assisted_success', 0)
        assisted_failure = latest.get('assisted_failure', 0)
        
        print(f"\nç»Ÿè®¡ç»“æœ:")
        print(f"  æ€»æµ‹è¯•æ•°: {total} (æœŸæœ›: 3)")
        print(f"  æˆåŠŸæ•°: {success} (æœŸæœ›: 2)")
        print(f"  å®Œå…¨æˆåŠŸ: {full_success} (æœŸæœ›: 1)")
        print(f"  éƒ¨åˆ†æˆåŠŸ: {partial_success} (æœŸæœ›: 1)")
        print(f"  å¤±è´¥æ•°: {failed} (æœŸæœ›: 1)")
        print(f"  æ€»é”™è¯¯æ•°: {total_errors} (æœŸæœ›: 2) â† å…³é”®æŒ‡æ ‡")
        print(f"  æ ¼å¼é”™è¯¯: {format_errors} (æœŸæœ›: 1)")
        print(f"  å…¶ä»–é”™è¯¯: {other_errors} (æœŸæœ›: 1)")
        print(f"  æœ‰è¾…åŠ©çš„æµ‹è¯•: {tests_with_assistance} (æœŸæœ›: 1)")
        print(f"  è¾…åŠ©æˆåŠŸ: {assisted_success} (æœŸæœ›: 1)")
        print(f"  è¾…åŠ©å¤±è´¥: {assisted_failure} (æœŸæœ›: 0)")
        
        # éªŒè¯å…³é”®ç‚¹
        checks = []
        checks.append(("æ€»æ•°æ­£ç¡®", total == 3))
        checks.append(("æˆåŠŸæ•°æ­£ç¡®", success == 2))
        checks.append(("å¤±è´¥æ•°æ­£ç¡®", failed == 1))
        checks.append(("æ€»é”™è¯¯æ•°æ­£ç¡®", total_errors == 2))  # æœ€é‡è¦ï¼
        checks.append(("æ ¼å¼é”™è¯¯æ­£ç¡®", format_errors == 1))
        checks.append(("å…¶ä»–é”™è¯¯æ­£ç¡®", other_errors == 1))
        checks.append(("è¾…åŠ©ç»Ÿè®¡æ­£ç¡®", tests_with_assistance == 1))
        
        print("\néªŒè¯ç»“æœ:")
        all_passed = True
        for check_name, passed in checks:
            status = "âœ…" if passed else "âŒ"
            print(f"  {status} {check_name}")
            if not passed:
                all_passed = False
        
        if all_passed:
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼é”™è¯¯è®¡æ•°é€»è¾‘å·²ä¿®å¤")
            print("   - å¤±è´¥çš„æµ‹è¯•å³ä½¿æ²¡æœ‰error_messageä¹Ÿä¼šè®¡å…¥é”™è¯¯")
            print("   - éƒ¨åˆ†æˆåŠŸçš„æµ‹è¯•ä¹Ÿä¼šè®¡å…¥é”™è¯¯")
            print("   - åªæœ‰å®Œå…¨æˆåŠŸçš„æµ‹è¯•ä¸è®¡å…¥é”™è¯¯")
        else:
            print("\nâš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä¿®å¤")
        
        # æ˜¾ç¤ºå®Œæ•´æ•°æ®ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        if not all_passed:
            print("\nå®Œæ•´æ•°æ®ï¼ˆè°ƒè¯•ç”¨ï¼‰:")
            important_fields = [
                'total', 'success', 'full_success', 'partial_success', 'failed',
                'total_errors', 'tool_call_format_errors', 'timeout_errors',
                'parameter_config_errors', 'tool_selection_errors', 'sequence_order_errors',
                'max_turns_errors', 'other_errors',
                'tests_with_assistance', 'assisted_success', 'assisted_failure'
            ]
            for field in important_fields:
                if field in latest:
                    print(f"  {field}: {latest[field]}")
        
        return all_passed
    else:
        print("âŒ æœªæ‰¾åˆ°æµ‹è¯•æ•°æ®")
        return False

if __name__ == "__main__":
    import sys
    success = test_error_counting()
    sys.exit(0 if success else 1)