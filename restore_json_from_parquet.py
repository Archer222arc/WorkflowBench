#!/usr/bin/env python3
"""ä»Parquetæ–‡ä»¶æ¢å¤JSONæ•°æ®åº“"""

import json
import pandas as pd
from pathlib import Path
from datetime import datetime
from collections import defaultdict

def restore_json_from_parquet():
    """ä»Parquetæ–‡ä»¶æ¢å¤JSONæ•°æ®åº“"""
    
    # å¤‡ä»½å½“å‰çš„JSONæ–‡ä»¶
    json_path = Path('pilot_bench_cumulative_results/master_database.json')
    if json_path.exists():
        backup_path = json_path.with_suffix(f'.backup_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json')
        json_path.rename(backup_path)
        print(f"âœ… å¤‡ä»½å½“å‰JSONåˆ°: {backup_path}")
    
    # è¯»å–Parquetæ–‡ä»¶
    parquet_path = Path('pilot_bench_parquet_data/test_results.parquet')
    if not parquet_path.exists():
        print("âŒ Parquetæ–‡ä»¶ä¸å­˜åœ¨")
        return False
    
    df = pd.read_parquet(parquet_path)
    print(f"ğŸ“Š ä»Parquetè¯»å– {len(df)} æ¡è®°å½•")
    
    # æ„å»ºJSONæ•°æ®åº“ç»“æ„
    db = {
        "version": "3.0",
        "created_at": datetime.now().isoformat(),
        "last_updated": datetime.now().isoformat(),
        "test_groups": {},
        "models": defaultdict(lambda: {
            "overall_stats": {
                "total_tests": 0,
                "successful_tests": 0,
                "partial_tests": 0,
                "failed_tests": 0,
                "success_rate": 0.0,
                "partial_rate": 0.0,
                "failure_rate": 0.0,
                "weighted_success_score": 0.0,
                "avg_execution_time": 0.0,
                "avg_turns": 0.0,
                "avg_tool_calls": 0.0,
                "tool_coverage_rate": 0.0
            },
            "experiment_metrics": {},
            "by_prompt_type": defaultdict(lambda: {
                "total_tests": 0,
                "success_rate": 0.0,
                "by_tool_success_rate": defaultdict(lambda: {
                    "by_difficulty": defaultdict(lambda: {
                        "by_task_type": defaultdict(lambda: {
                            "total": 0,
                            "successful": 0,
                            "partial": 0,
                            "failed": 0,
                            "success_rate": 0.0,
                            "partial_rate": 0.0,
                            "failure_rate": 0.0,
                            "weighted_success_score": 0.0,
                            "avg_execution_time": 0.0,
                            "avg_turns": 0.0,
                            "tool_coverage_rate": 0.0,
                            "avg_tool_calls": 0.0
                        })
                    })
                })
            })
        }),
        "summary": {
            "total_tests": 0,
            "total_success": 0,
            "total_partial": 0,
            "total_failure": 0,
            "models_tested": [],
            "last_test_time": None
        }
    }
    
    # å¤„ç†æ¯æ¡è®°å½•
    for _, row in df.iterrows():
        model = row.get('model', 'unknown')
        prompt_type = row.get('prompt_type', 'baseline')
        difficulty = row.get('difficulty', 'easy')
        task_type = row.get('task_type', 'unknown')
        tool_success_rate = str(round(row.get('tool_success_rate', 0.8), 4))
        
        # ç¡®ä¿æ¨¡å‹å­˜åœ¨
        if model not in db['models']:
            db['models'][model] = {
                "overall_stats": {
                    "total_tests": 0,
                    "successful_tests": 0,
                    "partial_tests": 0,
                    "failed_tests": 0,
                    "success_rate": 0.0,
                    "partial_rate": 0.0,
                    "failure_rate": 0.0,
                    "weighted_success_score": 0.0,
                    "avg_execution_time": 0.0,
                    "avg_turns": 0.0,
                    "avg_tool_calls": 0.0,
                    "tool_coverage_rate": 0.0
                },
                "experiment_metrics": {},
                "by_prompt_type": {}
            }
        
        # ç¡®ä¿prompt_typeå±‚çº§å­˜åœ¨
        if prompt_type not in db['models'][model]['by_prompt_type']:
            db['models'][model]['by_prompt_type'][prompt_type] = {
                "total_tests": 0,
                "success_rate": 0.0,
                "by_tool_success_rate": {}
            }
        
        # ç¡®ä¿tool_success_rateå±‚çº§å­˜åœ¨
        if tool_success_rate not in db['models'][model]['by_prompt_type'][prompt_type]['by_tool_success_rate']:
            db['models'][model]['by_prompt_type'][prompt_type]['by_tool_success_rate'][tool_success_rate] = {
                "by_difficulty": {}
            }
        
        # ç¡®ä¿difficultyå±‚çº§å­˜åœ¨
        if difficulty not in db['models'][model]['by_prompt_type'][prompt_type]['by_tool_success_rate'][tool_success_rate]['by_difficulty']:
            db['models'][model]['by_prompt_type'][prompt_type]['by_tool_success_rate'][tool_success_rate]['by_difficulty'][difficulty] = {
                "by_task_type": {}
            }
        
        # ç¡®ä¿task_typeå±‚çº§å­˜åœ¨
        if task_type not in db['models'][model]['by_prompt_type'][prompt_type]['by_tool_success_rate'][tool_success_rate]['by_difficulty'][difficulty]['by_task_type']:
            db['models'][model]['by_prompt_type'][prompt_type]['by_tool_success_rate'][tool_success_rate]['by_difficulty'][difficulty]['by_task_type'][task_type] = {
                "total": 0,
                "successful": 0,
                "partial": 0,
                "failed": 0,
                "success_rate": 0.0,
                "partial_rate": 0.0,
                "failure_rate": 0.0,
                "weighted_success_score": 0.0,
                "avg_execution_time": 0.0,
                "avg_turns": 0.0,
                "tool_coverage_rate": 0.0,
                "avg_tool_calls": 0.0
            }
        
        # è·å–ä»»åŠ¡ç±»å‹æ•°æ®çš„å¼•ç”¨
        task_data = db['models'][model]['by_prompt_type'][prompt_type]['by_tool_success_rate'][tool_success_rate]['by_difficulty'][difficulty]['by_task_type'][task_type]
        
        # æ›´æ–°ç»Ÿè®¡
        task_data['total'] += 1
        
        success_value = row.get('success', False)
        if pd.isna(success_value):
            success_value = False
        
        if success_value:
            task_data['successful'] += 1
            db['models'][model]['overall_stats']['successful_tests'] += 1
            db['summary']['total_success'] += 1
        else:
            # æ£€æŸ¥æ˜¯å¦æ˜¯éƒ¨åˆ†æˆåŠŸ
            success_level = row.get('success_level', 'failure')
            if success_level == 'partial':
                task_data['partial'] += 1
                db['models'][model]['overall_stats']['partial_tests'] += 1
                db['summary']['total_partial'] += 1
            else:
                task_data['failed'] += 1
                db['models'][model]['overall_stats']['failed_tests'] += 1
                db['summary']['total_failure'] += 1
        
        # æ›´æ–°æ€»æ•°
        db['models'][model]['overall_stats']['total_tests'] += 1
        db['summary']['total_tests'] += 1
        
        # ç´¯åŠ å…¶ä»–æŒ‡æ ‡ï¼ˆç”¨äºè®¡ç®—å¹³å‡å€¼ï¼‰
        exec_time = row.get('execution_time', 0)
        if pd.notna(exec_time):
            task_data['avg_execution_time'] += exec_time
        
        turns = row.get('turns', 0)
        if pd.notna(turns):
            task_data['avg_turns'] += turns
            db['models'][model]['overall_stats']['avg_turns'] += turns
        
        tool_calls = row.get('tool_calls', 0)
        if pd.notna(tool_calls):
            task_data['avg_tool_calls'] += tool_calls
            db['models'][model]['overall_stats']['avg_tool_calls'] += tool_calls
        
        # æ›´æ–°tool_coverage_rate
        tool_coverage = row.get('tool_coverage_rate', 0)
        if pd.notna(tool_coverage):
            task_data['tool_coverage_rate'] += tool_coverage
            db['models'][model]['overall_stats']['tool_coverage_rate'] += tool_coverage
    
    # è®¡ç®—å¹³å‡å€¼å’Œæ¯”ç‡
    for model, model_data in db['models'].items():
        total = model_data['overall_stats']['total_tests']
        if total > 0:
            model_data['overall_stats']['success_rate'] = model_data['overall_stats']['successful_tests'] / total
            model_data['overall_stats']['partial_rate'] = model_data['overall_stats']['partial_tests'] / total
            model_data['overall_stats']['failure_rate'] = model_data['overall_stats']['failed_tests'] / total
            model_data['overall_stats']['avg_turns'] /= total
            model_data['overall_stats']['avg_tool_calls'] /= total
            model_data['overall_stats']['tool_coverage_rate'] /= total
        
        # å¤„ç†æ¯ä¸ªprompt_type
        for prompt_type, prompt_data in model_data['by_prompt_type'].items():
            prompt_total = 0
            prompt_success = 0
            
            for rate_key, rate_data in prompt_data['by_tool_success_rate'].items():
                for diff_key, diff_data in rate_data['by_difficulty'].items():
                    for task_key, task_data in diff_data['by_task_type'].items():
                        if task_data['total'] > 0:
                            # è®¡ç®—æ¯”ç‡
                            task_data['success_rate'] = task_data['successful'] / task_data['total']
                            task_data['partial_rate'] = task_data['partial'] / task_data['total']
                            task_data['failure_rate'] = task_data['failed'] / task_data['total']
                            
                            # è®¡ç®—å¹³å‡å€¼
                            task_data['avg_execution_time'] /= task_data['total']
                            task_data['avg_turns'] /= task_data['total']
                            task_data['avg_tool_calls'] /= task_data['total']
                            task_data['tool_coverage_rate'] /= task_data['total']
                            
                            # ç´¯è®¡promptçº§åˆ«ç»Ÿè®¡
                            prompt_total += task_data['total']
                            prompt_success += task_data['successful']
            
            # æ›´æ–°promptçº§åˆ«ç»Ÿè®¡
            prompt_data['total_tests'] = prompt_total
            if prompt_total > 0:
                prompt_data['success_rate'] = prompt_success / prompt_total
    
    # æ›´æ–°æ¨¡å‹åˆ—è¡¨
    db['summary']['models_tested'] = list(db['models'].keys())
    
    # ä¿å­˜åˆ°JSONæ–‡ä»¶
    json_path = Path('pilot_bench_cumulative_results/master_database.json')
    json_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(dict(db), f, indent=2, ensure_ascii=False, default=str)
    
    print(f"âœ… æˆåŠŸæ¢å¤JSONæ•°æ®åº“")
    print(f"   æ€»æµ‹è¯•æ•°: {db['summary']['total_tests']}")
    print(f"   æˆåŠŸ: {db['summary']['total_success']}")
    print(f"   éƒ¨åˆ†: {db['summary']['total_partial']}")
    print(f"   å¤±è´¥: {db['summary']['total_failure']}")
    print(f"   æ¨¡å‹æ•°: {len(db['models'])}")
    
    return True

if __name__ == "__main__":
    restore_json_from_parquet()