#!/usr/bin/env python3
"""
Workflowé¢„ç”Ÿæˆè„šæœ¬ - ä¸ºæ‰€æœ‰ä»»åŠ¡åº“æ·»åŠ workflowå­—æ®µ
============================================
åŠŸèƒ½ï¼š
1. è¯»å–æ‰€æœ‰éš¾åº¦çº§åˆ«çš„ä»»åŠ¡åº“æ–‡ä»¶
2. ä¸ºæ¯ä¸ªä»»åŠ¡ç”Ÿæˆworkflowï¼ˆä½¿ç”¨MDPWorkflowGeneratorï¼‰
3. å°†workflowä¿å­˜åˆ°task_instance['workflow']å­—æ®µ
4. è¾“å‡ºå¢å¼ºç‰ˆä»»åŠ¡åº“æ–‡ä»¶

è¿™æ ·å¯ä»¥é¿å…åœ¨å¹¶å‘æµ‹è¯•æ—¶é‡å¤åŠ è½½å¤§å‹æ¨¡å‹ï¼ˆ250-350MBæ¯ä¸ªè¿›ç¨‹ï¼‰
"""

import json
import argparse
import time
from pathlib import Path
from typing import Dict, List, Optional, Any
from datetime import datetime
import sys
from enum import Enum

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from mdp_workflow_generator import MDPWorkflowGenerator


def make_json_serializable(obj: Any) -> Any:
    """
    é€’å½’è½¬æ¢å¯¹è±¡ä¸ºJSONå¯åºåˆ—åŒ–æ ¼å¼
    """
    if isinstance(obj, dict):
        return {k: make_json_serializable(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [make_json_serializable(item) for item in obj]
    elif isinstance(obj, tuple):
        return [make_json_serializable(item) for item in obj]
    elif isinstance(obj, Enum):
        return obj.value
    elif hasattr(obj, '__dict__'):
        # è‡ªå®šä¹‰å¯¹è±¡è½¬æ¢ä¸ºå­—å…¸
        return make_json_serializable(obj.__dict__)
    elif isinstance(obj, (str, int, float, bool, type(None))):
        return obj
    else:
        # å…¶ä»–ç±»å‹è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        return str(obj)


class WorkflowAugmenter:
    """ä»»åŠ¡åº“workflowå¢å¼ºå™¨"""
    
    def __init__(self, force_regenerate: bool = False, verbose: bool = True):
        """
        åˆå§‹åŒ–augmenter
        
        Args:
            force_regenerate: æ˜¯å¦å¼ºåˆ¶é‡æ–°ç”Ÿæˆå·²æœ‰workflowçš„ä»»åŠ¡
            verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†è¾“å‡º
        """
        self.force_regenerate = force_regenerate
        self.verbose = verbose
        self.generator = None
        self.stats = {
            'total_tasks': 0,
            'workflows_generated': 0,
            'workflows_skipped': 0,
            'errors': 0
        }
        
    def initialize_generator(self):
        """åˆå§‹åŒ–MDPWorkflowGeneratorï¼ˆåªéœ€ä¸€æ¬¡ï¼‰"""
        if self.verbose:
            print("\n" + "="*60)
            print("åˆå§‹åŒ–MDPWorkflowGenerator...")
            print("="*60)
            
        start_time = time.time()
        
        # è®¾ç½®æ—¥å¿—çº§åˆ«ä»¥å‡å°‘è¾“å‡º
        import logging
        logging.getLogger('mdp_workflow_generator').setLevel(logging.ERROR)
        logging.getLogger('mcp_embedding_manager').setLevel(logging.ERROR)
        logging.getLogger('unified_training_manager').setLevel(logging.ERROR)
        logging.getLogger('tool_capability_manager').setLevel(logging.ERROR)
        
        # åˆå§‹åŒ–generator - è¿™ä¼šåŠ è½½å¤§å‹æ¨¡å‹
        self.generator = MDPWorkflowGenerator(
            model_path="checkpoints/best_model.pt",
            use_embeddings=True  # ä½¿ç”¨FAISSåµŒå…¥æœç´¢
        )
        
        elapsed = time.time() - start_time
        if self.verbose:
            print(f"âœ… Generatoråˆå§‹åŒ–å®Œæˆï¼Œè€—æ—¶: {elapsed:.2f}ç§’")
            print(f"   - åŠ è½½äº†ç¥ç»ç½‘ç»œæ¨¡å‹")
            print(f"   - åŠ è½½äº†FAISSç´¢å¼•")
            print(f"   - å‡†å¤‡å°±ç»ª")
            
    def augment_task(self, task: Dict) -> Dict:
        """
        ä¸ºå•ä¸ªä»»åŠ¡æ·»åŠ workflow
        
        Args:
            task: ä»»åŠ¡å®ä¾‹
            
        Returns:
            å¢å¼ºåçš„ä»»åŠ¡ï¼ˆåŒ…å«workflowå­—æ®µï¼‰
        """
        task_id = task.get('instance_id') or task.get('id', 'unknown')
        
        # æ£€æŸ¥æ˜¯å¦å·²æœ‰workflow
        if 'workflow' in task and not self.force_regenerate:
            if self.verbose:
                print(f"   â­ï¸  è·³è¿‡ {task_id} (å·²æœ‰workflow)")
            self.stats['workflows_skipped'] += 1
            return task
            
        try:
            # ç”Ÿæˆworkflow
            if self.verbose:
                print(f"   ğŸ”§ ç”Ÿæˆ {task_id}...", end='')
                
            workflow = self.generator.generate_workflow_for_instance(
                task_instance=task,
                max_depth=20
            )
            
            # ç¡®ä¿workflowæ˜¯JSONå¯åºåˆ—åŒ–çš„
            workflow = make_json_serializable(workflow)
            
            # å°†workflowæ·»åŠ åˆ°ä»»åŠ¡
            task['workflow'] = workflow
            
            if self.verbose:
                sequence_len = len(workflow.get('optimal_sequence', []))
                print(f" âœ… (åºåˆ—é•¿åº¦: {sequence_len})")
                
            self.stats['workflows_generated'] += 1
            
        except Exception as e:
            if self.verbose:
                print(f" âŒ é”™è¯¯: {str(e)}")
            self.stats['errors'] += 1
            # å³ä½¿å‡ºé”™ä¹Ÿè¿”å›åŸä»»åŠ¡ï¼Œé¿å…æ•°æ®ä¸¢å¤±
            
        return task
        
    def augment_task_library(self, input_path: Path, output_path: Optional[Path] = None) -> bool:
        """
        å¢å¼ºæ•´ä¸ªä»»åŠ¡åº“æ–‡ä»¶
        
        Args:
            input_path: è¾“å…¥ä»»åŠ¡åº“è·¯å¾„
            output_path: è¾“å‡ºè·¯å¾„ï¼ˆé»˜è®¤ä¸ºåŒç›®å½•ä¸‹çš„_with_workflowsç‰ˆæœ¬ï¼‰
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        if not input_path.exists():
            print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {input_path}")
            return False
            
        # ç¡®å®šè¾“å‡ºè·¯å¾„
        if output_path is None:
            # åœ¨æ–‡ä»¶åä¸­æ’å…¥_with_workflows
            stem = input_path.stem  # ä¸å«æ‰©å±•åçš„æ–‡ä»¶å
            output_path = input_path.parent / f"{stem}_with_workflows.json"
            
        print(f"\nå¤„ç†: {input_path.name}")
        print(f"è¾“å‡ºåˆ°: {output_path.name}")
        
        # è¯»å–ä»»åŠ¡åº“
        with open(input_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            
        # å¤„ç†ä¸åŒçš„æ•°æ®æ ¼å¼
        if isinstance(data, dict) and 'tasks' in data:
            tasks = data['tasks']
            is_wrapped = True
        elif isinstance(data, list):
            tasks = data
            is_wrapped = False
        else:
            print(f"âŒ æœªçŸ¥çš„æ•°æ®æ ¼å¼")
            return False
            
        print(f"   å‘ç° {len(tasks)} ä¸ªä»»åŠ¡")
        
        # ä¸ºæ¯ä¸ªä»»åŠ¡ç”Ÿæˆworkflow
        augmented_tasks = []
        for i, task in enumerate(tasks):
            if self.verbose and i % 10 == 0 and i > 0:
                print(f"   è¿›åº¦: {i}/{len(tasks)}")
                
            self.stats['total_tasks'] += 1
            augmented_task = self.augment_task(task)
            augmented_tasks.append(augmented_task)
            
        # æ„å»ºè¾“å‡ºæ•°æ®
        if is_wrapped:
            output_data = {
                'tasks': augmented_tasks,
                'metadata': {
                    'original_file': input_path.name,
                    'augmented_at': datetime.now().isoformat(),
                    'workflows_generated': self.stats['workflows_generated'],
                    'generator_version': '1.0'
                }
            }
        else:
            output_data = augmented_tasks
            
        # ä¿å­˜å¢å¼ºåçš„ä»»åŠ¡åº“
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, indent=2, ensure_ascii=False)
            
        print(f"   âœ… ä¿å­˜åˆ°: {output_path}")
        return True
        
    def process_all_libraries(self, directory: Path):
        """
        å¤„ç†ç›®å½•ä¸‹çš„æ‰€æœ‰ä»»åŠ¡åº“æ–‡ä»¶
        
        Args:
            directory: åŒ…å«ä»»åŠ¡åº“çš„ç›®å½•
        """
        # æŸ¥æ‰¾æ‰€æœ‰ä»»åŠ¡åº“æ–‡ä»¶ï¼ˆä¸åŒ…å«å·²ç»æœ‰workflowçš„ç‰ˆæœ¬ï¼‰
        task_files = sorted([
            f for f in directory.glob("task_library_enhanced_v3_*.json")
            if "_with_workflows" not in f.name
        ])
        
        if not task_files:
            print(f"âŒ åœ¨ {directory} ä¸­æœªæ‰¾åˆ°ä»»åŠ¡åº“æ–‡ä»¶")
            return
            
        print(f"\næ‰¾åˆ° {len(task_files)} ä¸ªä»»åŠ¡åº“æ–‡ä»¶:")
        for f in task_files:
            print(f"  - {f.name}")
            
        # åˆå§‹åŒ–generatorï¼ˆåªéœ€ä¸€æ¬¡ï¼‰
        self.initialize_generator()
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        print("\n" + "="*60)
        print("å¼€å§‹å¤„ç†ä»»åŠ¡åº“")
        print("="*60)
        
        for task_file in task_files:
            success = self.augment_task_library(task_file)
            if not success:
                print(f"âš ï¸  å¤„ç† {task_file.name} æ—¶å‡ºé”™")
                
        # æ˜¾ç¤ºç»Ÿè®¡
        self.print_statistics()
        
    def print_statistics(self):
        """æ‰“å°å¤„ç†ç»Ÿè®¡"""
        print("\n" + "="*60)
        print("å¤„ç†å®Œæˆ - ç»Ÿè®¡ä¿¡æ¯")
        print("="*60)
        print(f"æ€»ä»»åŠ¡æ•°: {self.stats['total_tasks']}")
        print(f"ç”Ÿæˆworkflow: {self.stats['workflows_generated']}")
        print(f"è·³è¿‡ï¼ˆå·²å­˜åœ¨ï¼‰: {self.stats['workflows_skipped']}")
        print(f"é”™è¯¯: {self.stats['errors']}")
        
        if self.stats['workflows_generated'] > 0:
            print(f"\nâœ… æˆåŠŸä¸º {self.stats['workflows_generated']} ä¸ªä»»åŠ¡ç”Ÿæˆäº†workflow!")
            print("   è¿™å°†æ˜¾è‘—å‡å°‘å¹¶å‘æµ‹è¯•æ—¶çš„å†…å­˜ä½¿ç”¨ï¼ˆä»8.75GBé™åˆ°<2GBï¼‰")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='ä¸ºä»»åŠ¡åº“ç”Ÿæˆworkflowï¼Œé¿å…å¹¶å‘æµ‹è¯•æ—¶é‡å¤åŠ è½½å¤§å‹æ¨¡å‹'
    )
    
    parser.add_argument(
        '--directory', '-d',
        type=str,
        default='mcp_generated_library/difficulty_versions',
        help='ä»»åŠ¡åº“ç›®å½•è·¯å¾„'
    )
    
    parser.add_argument(
        '--file', '-f',
        type=str,
        help='å¤„ç†å•ä¸ªæ–‡ä»¶ï¼ˆå¯é€‰ï¼‰'
    )
    
    parser.add_argument(
        '--force-regenerate',
        action='store_true',
        help='å¼ºåˆ¶é‡æ–°ç”Ÿæˆå·²æœ‰workflowçš„ä»»åŠ¡'
    )
    
    parser.add_argument(
        '--quiet', '-q',
        action='store_true',
        help='å‡å°‘è¾“å‡ºä¿¡æ¯'
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºaugmenter
    augmenter = WorkflowAugmenter(
        force_regenerate=args.force_regenerate,
        verbose=not args.quiet
    )
    
    # å¤„ç†å•ä¸ªæ–‡ä»¶æˆ–æ•´ä¸ªç›®å½•
    if args.file:
        file_path = Path(args.file)
        if not augmenter.initialize_generator():
            augmenter.initialize_generator()
        success = augmenter.augment_task_library(file_path)
        if success:
            augmenter.print_statistics()
    else:
        directory = Path(args.directory)
        augmenter.process_all_libraries(directory)
        
    print("\nğŸ‰ Workflowé¢„ç”Ÿæˆå®Œæˆï¼")
    print("   ä¸‹æ¬¡è¿è¡Œæµ‹è¯•æ—¶å°†ç›´æ¥ä½¿ç”¨è¿™äº›workflowï¼Œæ— éœ€åŠ è½½å¤§å‹æ¨¡å‹")


if __name__ == "__main__":
    main()