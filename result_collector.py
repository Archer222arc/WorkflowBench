#!/usr/bin/env python3
"""
Result Collector - æ¶ˆæ¯é˜Ÿåˆ—å¼ç»“æœæ”¶é›†å™¨
æ”¯æŒè¶…å¹¶å‘æ¨¡å¼ä¸‹çš„æ— å†²çªæ•°æ®èšåˆ

è®¾è®¡ç†å¿µï¼š
- æµ‹è¯•è¿›ç¨‹ï¼šåªè´Ÿè´£ç”Ÿæˆç»“æœï¼Œå‘é€åˆ°æ”¶é›†å™¨
- æ”¶é›†å™¨ï¼šè´Ÿè´£èšåˆæ‰€æœ‰ç»“æœï¼Œç»Ÿä¸€å†™å…¥æ•°æ®åº“
- é›¶å¹¶å‘å†²çªï¼šåªæœ‰ä¸€ä¸ªå†™å…¥è€…
"""

import json
import os
import time
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
import logging

logger = logging.getLogger(__name__)

class ResultCollector:
    """åŸºäºæ–‡ä»¶çš„ç»“æœæ”¶é›†å™¨"""
    
    def __init__(self, temp_dir: str = "temp_results"):
        """
        åˆå§‹åŒ–ç»“æœæ”¶é›†å™¨
        
        Args:
            temp_dir: ä¸´æ—¶ç»“æœæ–‡ä»¶å­˜å‚¨ç›®å½•
        """
        self.temp_dir = Path(temp_dir)
        self.temp_dir.mkdir(exist_ok=True)
        logger.info(f"ResultCollectoråˆå§‹åŒ–ï¼Œä¸´æ—¶ç›®å½•: {self.temp_dir}")
    
    def add_batch_result(self, model: str, results: List[Dict], 
                        process_info: Optional[Dict] = None) -> str:
        """
        æ·»åŠ ä¸€ä¸ªæ‰¹æ¬¡çš„æµ‹è¯•ç»“æœ
        
        Args:
            model: æ¨¡å‹åç§°
            results: æµ‹è¯•ç»“æœåˆ—è¡¨
            process_info: è¿›ç¨‹ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            ç»“æœæ–‡ä»¶è·¯å¾„
        """
        # åˆ›å»ºå”¯ä¸€çš„ç»“æœæ–‡ä»¶å
        timestamp = int(time.time() * 1000000)  # å¾®ç§’ç²¾åº¦
        pid = os.getpid()
        filename = f"{model}_{pid}_{timestamp}.json"
        result_file = self.temp_dir / filename
        
        # å‡†å¤‡ç»“æœæ•°æ®
        result_data = {
            'model': model,
            'results': results,
            'timestamp': datetime.now().isoformat(),
            'pid': pid,
            'process_info': process_info or {},
            'result_count': len(results)
        }
        
        # å†™å…¥ç»“æœæ–‡ä»¶
        try:
            # ç¡®ä¿æ‰€æœ‰æ•°æ®éƒ½æ˜¯JSONå¯åºåˆ—åŒ–çš„
            json_str = json.dumps(result_data, indent=2, ensure_ascii=False, default=str)
            
            with open(result_file, 'w', encoding='utf-8') as f:
                f.write(json_str)
            
            logger.info(f"ğŸ“¤ å·²æäº¤ {model} çš„ {len(results)} ä¸ªç»“æœåˆ°æ”¶é›†å™¨: {filename}")
            return str(result_file)
            
        except Exception as e:
            logger.error(f"âŒ æäº¤ç»“æœå¤±è´¥: {e}")
            # å°è¯•è¯Šæ–­é—®é¢˜
            try:
                for i, result in enumerate(results):
                    json.dumps(result, default=str)
            except Exception as inner_e:
                logger.error(f"âŒ ç¬¬ {i} ä¸ªç»“æœæ— æ³•åºåˆ—åŒ–: {inner_e}")
                logger.error(f"é—®é¢˜æ•°æ®ç±»å‹: {type(result)}")
            raise
    
    def collect_all_results(self, cleanup: bool = True) -> List[Dict]:
        """
        æ”¶é›†æ‰€æœ‰å¾…å¤„ç†çš„ç»“æœ
        
        Args:
            cleanup: æ˜¯å¦åˆ é™¤å·²å¤„ç†çš„ä¸´æ—¶æ–‡ä»¶
            
        Returns:
            æ‰€æœ‰ç»“æœçš„åˆ—è¡¨
        """
        all_results = []
        result_files = list(self.temp_dir.glob("*.json"))
        
        logger.info(f"ğŸ”„ å¼€å§‹æ”¶é›†ç»“æœï¼Œå‘ç° {len(result_files)} ä¸ªç»“æœæ–‡ä»¶")
        
        for result_file in result_files:
            try:
                with open(result_file, 'r', encoding='utf-8') as f:
                    result_data = json.load(f)
                
                all_results.append(result_data)
                
                # æ—¥å¿—æ˜¾ç¤ºæ”¶é›†è¿›åº¦
                model = result_data.get('model', 'unknown')
                count = result_data.get('result_count', 0)
                pid = result_data.get('pid', 'unknown')
                logger.info(f"ğŸ“¥ æ”¶é›†åˆ° {model} çš„ {count} ä¸ªç»“æœ (PID: {pid})")
                
                # æ¸…ç†å·²å¤„ç†çš„æ–‡ä»¶
                if cleanup:
                    result_file.unlink()
                    
            except Exception as e:
                logger.warning(f"âš ï¸ è¯»å–ç»“æœæ–‡ä»¶å¤±è´¥: {result_file}, é”™è¯¯: {e}")
                # ä¸åˆ é™¤å¤±è´¥çš„æ–‡ä»¶ï¼Œä¾¿äºåç»­æ’æŸ¥
                continue
        
        # æŒ‰æ¨¡å‹åˆ†ç»„ç»Ÿè®¡
        model_stats = {}
        for result in all_results:
            model = result.get('model', 'unknown')
            count = result.get('result_count', 0)
            model_stats[model] = model_stats.get(model, 0) + count
        
        logger.info("ğŸ“Š æ”¶é›†ç»Ÿè®¡:")
        total_results = 0
        for model, count in model_stats.items():
            logger.info(f"  {model}: {count} ä¸ªç»“æœ")
            total_results += count
        
        logger.info(f"âœ… æ€»è®¡æ”¶é›† {total_results} ä¸ªæµ‹è¯•ç»“æœï¼Œæ¥è‡ª {len(model_stats)} ä¸ªæ¨¡å‹")
        
        return all_results
    
    def get_pending_count(self) -> int:
        """è·å–å¾…å¤„ç†çš„ç»“æœæ–‡ä»¶æ•°é‡"""
        return len(list(self.temp_dir.glob("*.json")))
    
    def clear_temp_files(self):
        """æ¸…ç†æ‰€æœ‰ä¸´æ—¶ç»“æœæ–‡ä»¶ï¼ˆæ…ç”¨ï¼ï¼‰"""
        result_files = list(self.temp_dir.glob("*.json"))
        for result_file in result_files:
            result_file.unlink()
        logger.info(f"ğŸ§¹ å·²æ¸…ç† {len(result_files)} ä¸ªä¸´æ—¶ç»“æœæ–‡ä»¶")


class ResultAggregator:
    """ç»“æœèšåˆå™¨ - å°†æ”¶é›†åˆ°çš„ç»“æœèšåˆä¸ºæ•°æ®åº“æ ¼å¼"""
    
    def __init__(self):
        logger.info("ResultAggregatoråˆå§‹åŒ–")
    
    def aggregate_results(self, collected_results: List[Dict]) -> Dict:
        """
        èšåˆæ‰€æœ‰æ”¶é›†åˆ°çš„ç»“æœ
        
        Args:
            collected_results: ResultCollector.collect_all_results()çš„è¿”å›å€¼
            
        Returns:
            èšåˆåçš„æ•°æ®åº“æ ¼å¼
        """
        aggregated_db = {
            "version": "3.0",
            "created_at": datetime.now().isoformat(),
            "last_updated": datetime.now().isoformat(),
            "models": {},
            "summary": {
                "total_tests": 0,
                "total_success": 0,
                "total_failure": 0,
                "models_tested": []
            }
        }
        
        logger.info(f"ğŸ”„ å¼€å§‹èšåˆ {len(collected_results)} ä¸ªç»“æœæ‰¹æ¬¡")
        
        for batch in collected_results:
            model = batch.get('model')
            results = batch.get('results', [])
            
            if not model or not results:
                logger.warning(f"âš ï¸ è·³è¿‡æ— æ•ˆæ‰¹æ¬¡: model={model}, results_count={len(results)}")
                continue
            
            # ä¸ºæ¨¡å‹åˆ›å»ºæ¡ç›®ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            if model not in aggregated_db["models"]:
                aggregated_db["models"][model] = self._create_empty_model_entry(model)
                aggregated_db["summary"]["models_tested"].append(model)
            
            # èšåˆç»“æœåˆ°æ¨¡å‹æ¡ç›®
            self._aggregate_model_results(aggregated_db["models"][model], results)
            
            logger.info(f"ğŸ“Š å·²èšåˆ {model} çš„ {len(results)} ä¸ªç»“æœ")
        
        # æ›´æ–°æ€»ä½“ç»Ÿè®¡
        self._update_summary_stats(aggregated_db)
        
        logger.info("âœ… ç»“æœèšåˆå®Œæˆ")
        return aggregated_db
    
    def _create_empty_model_entry(self, model: str) -> Dict:
        """åˆ›å»ºç©ºçš„æ¨¡å‹æ¡ç›®"""
        return {
            "model_name": model,
            "first_test_time": datetime.now().isoformat(),
            "last_test_time": datetime.now().isoformat(),
            "total_tests": 0,
            "overall_stats": {},
            "by_prompt_type": {}
        }
    
    def _aggregate_model_results(self, model_entry: Dict, results: List[Dict]):
        """å°†ç»“æœèšåˆåˆ°æ¨¡å‹æ¡ç›®ä¸­"""
        # è¿™é‡Œéœ€è¦æ ¹æ®ä½ çš„æ•°æ®ç»“æ„æ¥å®ç°å…·ä½“çš„èšåˆé€»è¾‘
        # ç°åœ¨å…ˆåšç®€å•çš„è®¡æ•°
        model_entry["total_tests"] += len(results)
        model_entry["last_test_time"] = datetime.now().isoformat()
        
        # TODO: å®ç°è¯¦ç»†çš„æŒ‰task_typeã€prompt_typeç­‰ç»´åº¦çš„èšåˆ
        # è¿™éƒ¨åˆ†é€»è¾‘å¯ä»¥ä»ç°æœ‰çš„CumulativeTestManagerä¸­æå–
        
    def _update_summary_stats(self, aggregated_db: Dict):
        """æ›´æ–°æ€»ä½“ç»Ÿè®¡ä¿¡æ¯"""
        total_tests = 0
        for model_data in aggregated_db["models"].values():
            total_tests += model_data.get("total_tests", 0)
        
        aggregated_db["summary"]["total_tests"] = total_tests


# å…¼å®¹æ€§æ¥å£ï¼šå¯ä»¥ä½œä¸ºç°æœ‰ç³»ç»Ÿçš„æ’ä»¶
class CumulativeTestManagerAdapter:
    """é€‚é…å™¨ï¼šè®©ResultCollectorå…¼å®¹ç°æœ‰çš„CumulativeTestManageræ¥å£"""
    
    def __init__(self, use_collector: bool = False):
        """
        Args:
            use_collector: æ˜¯å¦å¯ç”¨æ–°çš„collectoræ¨¡å¼
        """
        self.use_collector = use_collector
        
        if use_collector:
            self.collector = ResultCollector()
            self.aggregator = ResultAggregator()
            logger.info("ğŸ†• å¯ç”¨ResultCollectoræ¨¡å¼")
        else:
            # ä½¿ç”¨åŸæœ‰çš„CumulativeTestManager
            from cumulative_test_manager import CumulativeTestManager
            self.legacy_manager = CumulativeTestManager()
            logger.info("ğŸ“œ ä½¿ç”¨ä¼ ç»ŸCumulativeTestManageræ¨¡å¼")
    
    def append_test_result(self, result: Dict) -> bool:
        """å…¼å®¹ç°æœ‰æ¥å£ï¼šæ·»åŠ å•ä¸ªæµ‹è¯•ç»“æœ"""
        if self.use_collector:
            # æ–°æ¨¡å¼ï¼šç¼“å­˜ç»“æœï¼Œç¨åæ‰¹é‡æäº¤
            if not hasattr(self, '_batch_cache'):
                self._batch_cache = []
            self._batch_cache.append(result)
            return True
        else:
            # ä¼ ç»Ÿæ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨åŸæœ‰ç®¡ç†å™¨
            return self.legacy_manager.append_test_result(result)
    
    def save_database(self):
        """å…¼å®¹ç°æœ‰æ¥å£ï¼šä¿å­˜æ•°æ®åº“"""
        if self.use_collector:
            # æ–°æ¨¡å¼ï¼šæäº¤ç¼“å­˜çš„ç»“æœåˆ°collector
            if hasattr(self, '_batch_cache') and self._batch_cache:
                model = self._batch_cache[0].get('model', 'unknown')
                self.collector.add_batch_result(model, self._batch_cache)
                self._batch_cache = []
                logger.info("ğŸ“¤ å·²æäº¤æ‰¹æ¬¡ç»“æœåˆ°collector")
        else:
            # ä¼ ç»Ÿæ¨¡å¼
            self.legacy_manager.save_database()