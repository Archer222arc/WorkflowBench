#!/usr/bin/env python3
"""
å®Œæ•´é›†æˆçš„æ‰¹é‡æµ‹è¯•è„šæœ¬
æ”¯æŒç´¯ç§¯æµ‹è¯•ã€æ–­ç‚¹ç»­ä¼ ã€å¤šéš¾åº¦çº§åˆ«ã€å¹¶è¡Œæ‰§è¡Œ
"""

import argparse
import json
import os
import sys
import time
import subprocess
import threading
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Optional, Tuple
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass, asdict


# ===================== é…ç½® =====================
CUMULATIVE_RESULTS_DIR = Path("cumulative_test_results")
CUMULATIVE_RESULTS_DIR.mkdir(exist_ok=True)


# ===================== æ•°æ®ç±» =====================
@dataclass
class TestResult:
    """å•ä¸ªæµ‹è¯•ç»“æœ"""
    model: str
    task_type: str
    prompt_type: str
    difficulty_level: str
    task_complexity: str
    success: bool
    success_level: str = "failed"
    execution_time: float = 0.0
    error: str = ""
    workflow_id: str = ""
    timestamp: str = ""
    
    def to_dict(self):
        return asdict(self)


@dataclass
class TestConfig:
    """æµ‹è¯•é…ç½®"""
    model: str
    task_types: List[str]
    prompt_types: List[str]
    difficulty: str = "easy"
    instances_per_type: int = 1
    test_flawed: bool = False
    save_logs: bool = True
    parallel: int = 4
    repeat_target: int = 100
    continue_test: bool = False


# ===================== ç´¯ç§¯ç»“æœç®¡ç†å™¨ =====================
class CumulativeResultsManager:
    """ç®¡ç†ç´¯ç§¯æµ‹è¯•ç»“æœ"""
    
    def __init__(self, results_dir: Path = CUMULATIVE_RESULTS_DIR):
        self.results_dir = results_dir
        self.results_db_path = results_dir / "results_database.json"
        self.lock = threading.Lock()
        self.results_db = self._load_database()
    
    def _load_database(self) -> Dict:
        """åŠ è½½æˆ–åˆ›å»ºç»“æœæ•°æ®åº“"""
        if self.results_db_path.exists():
            with open(self.results_db_path, 'r') as f:
                return json.load(f)
        else:
            return {
                "created_at": datetime.now().isoformat(),
                "total_tests": 0,
                "models": {},
                "test_sessions": []
            }
    
    def save_database(self):
        """ä¿å­˜æ•°æ®åº“ï¼ˆçº¿ç¨‹å®‰å…¨ï¼‰"""
        with self.lock:
            with open(self.results_db_path, 'w') as f:
                json.dump(self.results_db, f, indent=2)
    
    def add_result(self, result: TestResult):
        """æ·»åŠ å•ä¸ªæµ‹è¯•ç»“æœ"""
        with self.lock:
            model = result.model
            if model not in self.results_db["models"]:
                self.results_db["models"][model] = {
                    "first_tested": datetime.now().isoformat(),
                    "total_tests": 0,
                    "results": {}
                }
            
            # æ„å»ºç»“æœé”®
            key = f"{result.task_type}_{result.prompt_type}_{result.difficulty_level}"
            if result.task_type == "flawed":
                key = f"{result.task_type}_flawed_{result.prompt_type}"
            
            # æ·»åŠ ç»“æœ
            if key not in self.results_db["models"][model]["results"]:
                self.results_db["models"][model]["results"][key] = []
            
            self.results_db["models"][model]["results"][key].append(result.to_dict())
            self.results_db["models"][model]["total_tests"] += 1
            self.results_db["total_tests"] += 1
            
            # å®šæœŸä¿å­˜
            if self.results_db["total_tests"] % 10 == 0:
                self.save_database()
    
    def get_progress(self, model: str, target_per_group: int) -> Dict:
        """è·å–æ¨¡å‹çš„æµ‹è¯•è¿›åº¦"""
        if model not in self.results_db["models"]:
            return {"groups": {}, "total": 0}
        
        model_data = self.results_db["models"][model]
        progress = {"groups": {}, "total": 0}
        
        for key, results in model_data["results"].items():
            completed = len(results)
            progress["groups"][key] = {
                "completed": completed,
                "target": target_per_group,
                "percentage": (completed / target_per_group * 100) if target_per_group > 0 else 0
            }
            progress["total"] += completed
        
        return progress


# ===================== å­è¿›ç¨‹æµ‹è¯•è¿è¡Œå™¨ =====================
class SubprocessTestRunner:
    """ä½¿ç”¨å­è¿›ç¨‹è¿è¡Œæµ‹è¯•ä»¥é¿å…å†…å­˜æ³„æ¼å’Œè¿›ç¨‹æŒ‚èµ·"""
    
    def __init__(self, results_dir: Path):
        self.results_dir = results_dir
        self.temp_dir = results_dir / "temp"
        self.temp_dir.mkdir(exist_ok=True)
    
    def create_test_script(self, config: TestConfig, batch_id: str) -> str:
        """åˆ›å»ºåœ¨å­è¿›ç¨‹ä¸­è¿è¡Œçš„æµ‹è¯•è„šæœ¬"""
        return f'''#!/usr/bin/env python3
import os
import sys
import json
import random
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, "{Path(__file__).parent}")

def run_tests():
    """è¿è¡Œæµ‹è¯•æ‰¹æ¬¡"""
    from mdp_workflow_generator import MDPWorkflowGenerator
    from interactive_executor import InteractiveExecutor
    from flawed_workflow_generator import FlawedWorkflowGenerator
    from workflow_quality_test_flawed import ExecutionResult
    from dataclasses import asdict
    
    results = []
    
    try:
        # åˆå§‹åŒ–ç”Ÿæˆå™¨
        generator = MDPWorkflowGenerator(
            model_path="checkpoints/best_model.pt"
        )
        
        # åŠ è½½ä»»åŠ¡åº“ - æ ¹æ®éš¾åº¦çº§åˆ«
        difficulty = "{config.difficulty}"
        task_lib_path = f"mcp_generated_library/difficulty_versions/task_library_enhanced_v3_{{difficulty}}.json"
        
        if not Path(task_lib_path).exists():
            # å›é€€åˆ°é»˜è®¤
            task_lib_path = "mcp_generated_library/difficulty_versions/task_library_enhanced_v3_easy.json"
        
        with open(task_lib_path, 'r') as f:
            task_data = json.load(f)
        
        # å¤„ç†ä»»åŠ¡æ•°æ®
        tasks = task_data.get('tasks', task_data) if isinstance(task_data, dict) else task_data
        
        # æŒ‰ä»»åŠ¡ç±»å‹åˆ†ç»„
        tasks_by_type = {{}}
        for task in tasks:
            task_type = task.get('task_type', 'unknown')
            if task_type not in tasks_by_type:
                tasks_by_type[task_type] = []
            tasks_by_type[task_type].append(task)
        
        # è¿è¡Œæµ‹è¯•
        task_types = {config.task_types}
        prompt_types = {config.prompt_types}
        instances_per_type = {config.instances_per_type}
        test_flawed = {config.test_flawed}
        
        # ç¼ºé™·ç”Ÿæˆå™¨
        flawed_generator = None
        if test_flawed:
            flawed_generator = FlawedWorkflowGenerator(
                generator.tool_registry,
                tool_capability_manager=generator.tool_capability_manager
            )
        
        for task_type in task_types:
            # è·å–ä»»åŠ¡å®ä¾‹
            available_tasks = tasks_by_type.get(task_type, [])
            if not available_tasks:
                print(f"è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°ä»»åŠ¡ç±»å‹ {{task_type}}")
                continue
            
            # éšæœºé€‰æ‹©ä»»åŠ¡
            selected_tasks = random.sample(
                available_tasks, 
                min(instances_per_type, len(available_tasks))
            )
            
            for task_instance in selected_tasks:
                for prompt_type in prompt_types:
                    try:
                        # ç”Ÿæˆå·¥ä½œæµ
                        workflow = generator.generate_workflow(
                            task_type=task_type,
                            task_instance=task_instance
                        )
                        
                        if not workflow:
                            continue
                        
                        # å¦‚æœéœ€è¦ï¼Œæ³¨å…¥ç¼ºé™·
                        if test_flawed and flawed_generator:
                            flaw_types = ['missing_step', 'wrong_order', 'infinite_loop']
                            flaw_type = random.choice(flaw_types)
                            workflow = flawed_generator.inject_specific_flaw(
                                workflow, flaw_type
                            )
                        
                        # æ‰§è¡Œå·¥ä½œæµ
                        executor = InteractiveExecutor(
                            tool_registry=generator.tool_registry,
                            model="{config.model}"
                        )
                        
                        start_time = datetime.now()
                        execution_result = executor.execute_interactive(
                            initial_prompt=f"Execute {{task_type}}: {{task_instance.get('description', '')}}",
                            task_instance=task_instance,
                            workflow=workflow,
                            prompt_type=prompt_type
                        )
                        execution_time = (datetime.now() - start_time).total_seconds()
                        
                        # æ„å»ºç»“æœ
                        result = {{
                            "model": "{config.model}",
                            "task_type": task_type,
                            "prompt_type": prompt_type,
                            "difficulty_level": difficulty,
                            "task_complexity": task_instance.get('complexity', 'unknown'),
                            "success": execution_result.get('success', False) if isinstance(execution_result, dict) else False,
                            "success_level": execution_result.get('success_level', 'failed') if isinstance(execution_result, dict) else 'failed',
                            "execution_time": execution_time,
                            "workflow_id": workflow.get('workflow_id', 'unknown'),
                            "timestamp": datetime.now().isoformat()
                        }}
                        
                        results.append(result)
                        
                    except Exception as e:
                        print(f"æµ‹è¯•å¤±è´¥: {{e}}")
                        results.append({{
                            "model": "{config.model}",
                            "task_type": task_type,
                            "prompt_type": prompt_type,
                            "difficulty_level": difficulty,
                            "task_complexity": task_instance.get('complexity', 'unknown'),
                            "success": False,
                            "error": str(e),
                            "timestamp": datetime.now().isoformat()
                        }})
        
    except Exception as e:
        print(f"æ‰¹æ¬¡æ‰§è¡Œå¤±è´¥: {{e}}")
    
    # ä¿å­˜ç»“æœ
    output_file = Path("{self.temp_dir}") / f"batch_{{'{batch_id}'}}_results.json"
    with open(output_file, 'w') as f:
        json.dump({{
            "batch_id": "{batch_id}",
            "results": results,
            "timestamp": datetime.now().isoformat()
        }}, f, indent=2)
    
    print(f"æ‰¹æ¬¡å®Œæˆ: {{len(results)}} ä¸ªæµ‹è¯•")

if __name__ == "__main__":
    run_tests()
'''
    
    def run_batch(self, config: TestConfig, batch_id: str) -> Optional[List[TestResult]]:
        """åœ¨å­è¿›ç¨‹ä¸­è¿è¡Œæµ‹è¯•æ‰¹æ¬¡"""
        # åˆ›å»ºæµ‹è¯•è„šæœ¬
        script_content = self.create_test_script(config, batch_id)
        script_path = self.temp_dir / f"test_script_{batch_id}.py"
        
        with open(script_path, 'w') as f:
            f.write(script_content)
        
        try:
            # è¿è¡Œå­è¿›ç¨‹
            result = subprocess.run(
                [sys.executable, str(script_path)],
                capture_output=True,
                text=True,
                timeout=600  # 10åˆ†é’Ÿè¶…æ—¶
            )
            
            if result.returncode != 0:
                print(f"æ‰¹æ¬¡ {batch_id} æ‰§è¡Œå¤±è´¥")
                if result.stderr:
                    print(f"é”™è¯¯: {result.stderr}")
                return None
            
            # è¯»å–ç»“æœ
            result_file = self.temp_dir / f"batch_{batch_id}_results.json"
            if result_file.exists():
                with open(result_file, 'r') as f:
                    data = json.load(f)
                
                # è½¬æ¢ä¸º TestResult å¯¹è±¡
                results = []
                for r in data.get('results', []):
                    results.append(TestResult(**r))
                
                # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                os.unlink(result_file)
                
                return results
            
            return None
            
        except subprocess.TimeoutExpired:
            print(f"æ‰¹æ¬¡ {batch_id} è¶…æ—¶")
            return None
        finally:
            # æ¸…ç†è„šæœ¬
            if script_path.exists():
                os.unlink(script_path)


# ===================== ä¸»æµ‹è¯•åè°ƒå™¨ =====================
class BatchTestCoordinator:
    """åè°ƒæ‰¹é‡æµ‹è¯•æ‰§è¡Œ"""
    
    def __init__(self, config: TestConfig):
        self.config = config
        self.manager = CumulativeResultsManager()
        self.runner = SubprocessTestRunner(CUMULATIVE_RESULTS_DIR)
        self.session_id = f"{config.model}_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    
    def calculate_needed_tests(self) -> Dict[str, int]:
        """è®¡ç®—éœ€è¦çš„æµ‹è¯•æ•°é‡"""
        progress = self.manager.get_progress(self.config.model, self.config.repeat_target)
        needed = {}
        
        # è®¡ç®—æ¯ä¸ªç»„åˆéœ€è¦çš„æµ‹è¯•
        for task_type in self.config.task_types:
            for prompt_type in self.config.prompt_types:
                key = f"{task_type}_{prompt_type}_{self.config.difficulty}"
                existing = progress["groups"].get(key, {}).get("completed", 0)
                remaining = max(0, self.config.repeat_target - existing)
                if remaining > 0:
                    needed[key] = remaining
        
        # å¦‚æœæµ‹è¯•ç¼ºé™·
        if self.config.test_flawed:
            for task_type in self.config.task_types:
                for flaw_type in ['missing_step', 'wrong_order', 'infinite_loop']:
                    key = f"{task_type}_flawed_{flaw_type}"
                    existing = progress["groups"].get(key, {}).get("completed", 0)
                    remaining = max(0, self.config.repeat_target - existing)
                    if remaining > 0:
                        needed[key] = remaining
        
        return needed
    
    def create_batches(self, needed: Dict[str, int]) -> List[TestConfig]:
        """åˆ›å»ºæµ‹è¯•æ‰¹æ¬¡"""
        batches = []
        
        # æŒ‰ä»»åŠ¡ç±»å‹åˆ†ç»„åˆ›å»ºæ‰¹æ¬¡
        while sum(needed.values()) > 0:
            batch_task_types = []
            batch_size = 0
            
            for key, count in list(needed.items()):
                if count > 0 and batch_size < self.config.instances_per_type:
                    task_type = key.split('_')[0]
                    if task_type not in batch_task_types:
                        batch_task_types.append(task_type)
                        needed[key] -= 1
                        batch_size += 1
            
            if batch_task_types:
                batch_config = TestConfig(
                    model=self.config.model,
                    task_types=batch_task_types,
                    prompt_types=self.config.prompt_types,
                    difficulty=self.config.difficulty,
                    instances_per_type=1,
                    test_flawed=self.config.test_flawed,
                    save_logs=self.config.save_logs,
                    parallel=1
                )
                batches.append(batch_config)
        
        return batches
    
    def run(self):
        """è¿è¡Œæ‰¹é‡æµ‹è¯•"""
        print(f"\n{'='*60}")
        print(f"æ‰¹é‡æµ‹è¯•: {self.config.model}")
        print(f"{'='*60}")
        print(f"éš¾åº¦çº§åˆ«: {self.config.difficulty}")
        print(f"ä»»åŠ¡ç±»å‹: {', '.join(self.config.task_types)}")
        print(f"æç¤ºç±»å‹: {', '.join(self.config.prompt_types)}")
        print(f"ç›®æ ‡æ¬¡æ•°/ç»„: {self.config.repeat_target}")
        print(f"å¹¶è¡Œåº¦: {self.config.parallel}")
        
        # æ˜¾ç¤ºå½“å‰è¿›åº¦
        progress = self.manager.get_progress(self.config.model, self.config.repeat_target)
        print(f"\nå½“å‰è¿›åº¦:")
        for key, info in sorted(progress["groups"].items()):
            print(f"  {key}: {info['completed']}/{info['target']} ({info['percentage']:.1f}%)")
        
        # è®¡ç®—éœ€è¦çš„æµ‹è¯•
        needed = self.calculate_needed_tests()
        total_needed = sum(needed.values())
        
        if total_needed == 0:
            print(f"\nâœ… æ‰€æœ‰æµ‹è¯•ç»„å·²è¾¾åˆ°ç›®æ ‡!")
            return
        
        print(f"\néœ€è¦è¡¥å……: {total_needed} ä¸ªæµ‹è¯•")
        
        # ç¡®è®¤
        if not self.config.continue_test:
            confirm = input("\nå¼€å§‹æµ‹è¯•? (y/n) [y]: ").strip().lower() or "y"
            if confirm != "y":
                print("æµ‹è¯•å·²å–æ¶ˆ")
                return
        
        # åˆ›å»ºæ‰¹æ¬¡
        batches = self.create_batches(needed)
        print(f"\nåˆ›å»ºäº† {len(batches)} ä¸ªæµ‹è¯•æ‰¹æ¬¡")
        
        # å¹¶è¡Œæ‰§è¡Œæ‰¹æ¬¡
        start_time = time.time()
        completed = 0
        failed = 0
        
        with ThreadPoolExecutor(max_workers=self.config.parallel) as executor:
            # æäº¤æ‰¹æ¬¡
            future_to_batch = {}
            for i, batch_config in enumerate(batches):
                batch_id = f"{self.session_id}_batch_{i:04d}"
                future = executor.submit(self.runner.run_batch, batch_config, batch_id)
                future_to_batch[future] = (batch_id, batch_config)
            
            # å¤„ç†ç»“æœ
            for future in as_completed(future_to_batch):
                batch_id, batch_config = future_to_batch[future]
                try:
                    results = future.result()
                    if results:
                        # ä¿å­˜ç»“æœ
                        for result in results:
                            self.manager.add_result(result)
                        completed += len(results)
                        print(f"âœ“ æ‰¹æ¬¡ {batch_id}: {len(results)} ä¸ªæµ‹è¯•")
                    else:
                        failed += 1
                        print(f"âœ— æ‰¹æ¬¡ {batch_id}: å¤±è´¥")
                except Exception as e:
                    failed += 1
                    print(f"âœ— æ‰¹æ¬¡ {batch_id}: {e}")
        
        # æœ€ç»ˆä¿å­˜
        self.manager.save_database()
        
        # è®°å½•ä¼šè¯
        self.manager.results_db["test_sessions"].append({
            "session_id": self.session_id,
            "model": self.config.model,
            "timestamp": datetime.now().isoformat(),
            "num_tests": completed,
            "difficulty": self.config.difficulty
        })
        self.manager.save_database()
        
        # æ€»ç»“
        elapsed = time.time() - start_time
        print(f"\n{'='*60}")
        print(f"æµ‹è¯•å®Œæˆ")
        print(f"{'='*60}")
        print(f"è€—æ—¶: {elapsed/60:.1f} åˆ†é’Ÿ")
        print(f"æˆåŠŸ: {completed} ä¸ªæµ‹è¯•")
        print(f"å¤±è´¥: {failed} ä¸ªæ‰¹æ¬¡")
        print(f"å¹³å‡: {elapsed/max(completed, 1):.1f} ç§’/æµ‹è¯•")
        
        # æ˜¾ç¤ºæœ€æ–°è¿›åº¦
        progress = self.manager.get_progress(self.config.model, self.config.repeat_target)
        print(f"\næœ€æ–°è¿›åº¦:")
        for key, info in sorted(progress["groups"].items()):
            print(f"  {key}: {info['completed']}/{info['target']} ({info['percentage']:.1f}%)")


# ===================== è¿›åº¦æŸ¥çœ‹å™¨ =====================
def view_progress(model: Optional[str] = None, target: int = 100):
    """æŸ¥çœ‹æµ‹è¯•è¿›åº¦"""
    manager = CumulativeResultsManager()
    
    print(f"\n{'='*80}")
    print("ç´¯ç§¯æµ‹è¯•è¿›åº¦æŠ¥å‘Š")
    print(f"{'='*80}")
    
    db = manager.results_db
    print(f"æ•°æ®åº“åˆ›å»º: {db.get('created_at', 'N/A')}")
    print(f"æ€»æµ‹è¯•æ•°: {db.get('total_tests', 0)}")
    print(f"æ¨¡å‹æ•°: {len(db.get('models', {}))}")
    
    # æ˜¾ç¤ºæ¯ä¸ªæ¨¡å‹
    models = db.get('models', {})
    for model_name, model_data in models.items():
        if model and model not in model_name:
            continue
        
        print(f"\n{'='*80}")
        print(f"æ¨¡å‹: {model_name}")
        print(f"{'='*80}")
        
        results = model_data.get('results', {})
        
        # ç»Ÿè®¡
        total_tests = 0
        total_success = 0
        
        print(f"\nè¿›åº¦ (ç›®æ ‡: {target}/ç»„):")
        print(f"{'ç»„åˆ':<50} {'å®Œæˆ':<10} {'è¿›åº¦':<15} {'æˆåŠŸç‡'}")
        print("-" * 80)
        
        for key in sorted(results.keys()):
            tests = results[key]
            count = len(tests)
            success = sum(1 for t in tests if t.get('success', False))
            percent = (count / target * 100) if target > 0 else 0
            success_rate = (success / count * 100) if count > 0 else 0
            
            total_tests += count
            total_success += success
            
            status = "âœ…" if count >= target else "ğŸ”„"
            print(f"{key:<50} {count:>3}/{target:<3} {percent:>6.1f}% {status}  {success_rate:>5.1f}%")
        
        print("-" * 80)
        overall_success = (total_success / total_tests * 100) if total_tests > 0 else 0
        print(f"{'æ€»è®¡':<50} {total_tests:<10} {'':15} {overall_success:>5.1f}%")


# ===================== ä¸»å‡½æ•° =====================
def main():
    parser = argparse.ArgumentParser(description='å®Œæ•´é›†æˆçš„æ‰¹é‡æµ‹è¯•è„šæœ¬')
    
    # åŸºç¡€å‚æ•°
    parser.add_argument('--model', type=str, default='qwen2.5-3b-instruct',
                       help='æ¨¡å‹åç§°')
    parser.add_argument('--task-types', nargs='+',
                       default=['simple_task', 'data_pipeline', 'api_integration'],
                       help='è¦æµ‹è¯•çš„ä»»åŠ¡ç±»å‹')
    parser.add_argument('--prompt-types', nargs='+',
                       default=['baseline', 'optimal', 'cot'],
                       help='è¦æµ‹è¯•çš„æç¤ºç±»å‹')
    parser.add_argument('--difficulty', type=str, default='easy',
                       choices=['very_easy', 'easy', 'medium', 'hard', 'very_hard'],
                       help='ä»»åŠ¡æè¿°éš¾åº¦çº§åˆ«')
    
    # æµ‹è¯•æ§åˆ¶
    parser.add_argument('--repeat', type=int, default=100,
                       help='æ¯ç»„ç›®æ ‡æµ‹è¯•æ•°')
    parser.add_argument('--instances', type=int, default=10,
                       help='æ¯æ‰¹è¿è¡Œçš„å®ä¾‹æ•°')
    parser.add_argument('--parallel', type=int, default=4,
                       help='å¹¶è¡Œæµ‹è¯•æ•°')
    parser.add_argument('--continue', dest='continue_test', action='store_true',
                       help='ç»§ç»­ä¹‹å‰çš„æµ‹è¯•')
    
    # é€‰é¡¹
    parser.add_argument('--test-flawed', action='store_true',
                       help='åŒ…å«ç¼ºé™·æµ‹è¯•')
    parser.add_argument('--no-save-logs', action='store_true',
                       help='ä¸ä¿å­˜è¯¦ç»†æ—¥å¿—')
    
    # æŸ¥çœ‹è¿›åº¦
    parser.add_argument('--view-progress', action='store_true',
                       help='ä»…æŸ¥çœ‹è¿›åº¦')
    parser.add_argument('--target', type=int, default=100,
                       help='æŸ¥çœ‹è¿›åº¦æ—¶çš„ç›®æ ‡å€¼')
    
    args = parser.parse_args()
    
    # å¦‚æœæ˜¯æŸ¥çœ‹è¿›åº¦
    if args.view_progress:
        view_progress(args.model, args.target)
        return
    
    # åˆ›å»ºæµ‹è¯•é…ç½®
    config = TestConfig(
        model=args.model,
        task_types=args.task_types,
        prompt_types=args.prompt_types,
        difficulty=args.difficulty,
        instances_per_type=args.instances,
        test_flawed=args.test_flawed,
        save_logs=not args.no_save_logs,
        parallel=args.parallel,
        repeat_target=args.repeat,
        continue_test=args.continue_test
    )
    
    # è¿è¡Œæµ‹è¯•
    coordinator = BatchTestCoordinator(config)
    coordinator.run()


if __name__ == "__main__":
    main()