{
  "task_type": "multi_stage_pipeline",
  "severity": "light",
  "timestamp": "20250624_021126",
  "flaw_test_results": {
    "order_flaw_swap": {
      "flaw_info": {
        "type": "unknown",
        "severity": "light",
        "description": "Unknown flaw type: unknown"
      },
      "performance_comparison": {
        "optimal_prompt": {
          "success_rate": 0.66,
          "avg_execution_time": 2.5444846105575563,
          "tool_accuracy": 0.9423076923076923,
          "error_rate": 0.33999999999999997,
          "total_tests": 50
        },
        "baseline_prompt": {
          "success_rate": 0.6,
          "avg_execution_time": 2.0383123636245726,
          "tool_accuracy": 0.5949367088607594,
          "error_rate": 0.4,
          "total_tests": 50
        },
        "flawed_optimal_prompt": {
          "success_rate": 0.12,
          "avg_execution_time": 2.4963055515289305,
          "tool_accuracy": 0.13452914798206278,
          "error_rate": 0.88,
          "total_tests": 50
        }
      }
    },
    "order_flaw_dependency": {
      "flaw_info": {
        "type": "unknown",
        "severity": "light",
        "description": "Unknown flaw type: unknown"
      },
      "performance_comparison": {
        "optimal_prompt": {
          "success_rate": 0.56,
          "avg_execution_time": 2.200698928833008,
          "tool_accuracy": 0.916030534351145,
          "error_rate": 0.43999999999999995,
          "total_tests": 50
        },
        "baseline_prompt": {
          "success_rate": 0.56,
          "avg_execution_time": 2.3489542198181153,
          "tool_accuracy": 0.5569620253164557,
          "error_rate": 0.43999999999999995,
          "total_tests": 50
        },
        "flawed_optimal_prompt": {
          "success_rate": 0.06,
          "avg_execution_time": 3.292285671234131,
          "tool_accuracy": 0.07211538461538461,
          "error_rate": 0.94,
          "total_tests": 50
        }
      }
    },
    "tool_misuse_similar": {
      "flaw_info": {
        "type": "tool_similar",
        "severity": "light",
        "description": "Tools replaced with similar but incorrect ones (light severity)"
      },
      "performance_comparison": {
        "optimal_prompt": {
          "success_rate": 0.5,
          "avg_execution_time": 2.3691026401519775,
          "tool_accuracy": 0.8321167883211679,
          "error_rate": 0.5,
          "total_tests": 50
        },
        "baseline_prompt": {
          "success_rate": 0.62,
          "avg_execution_time": 2.3764938735961914,
          "tool_accuracy": 0.6293103448275862,
          "error_rate": 0.38,
          "total_tests": 50
        },
        "flawed_optimal_prompt": {
          "success_rate": 0.02,
          "avg_execution_time": 2.419395236968994,
          "tool_accuracy": 0.02976190476190476,
          "error_rate": 0.98,
          "total_tests": 50
        }
      }
    },
    "tool_misuse_wrong_category": {
      "flaw_info": {
        "type": "tool_wrong_category",
        "severity": "light",
        "description": "Tools replaced with wrong category (light severity)"
      },
      "performance_comparison": {
        "optimal_prompt": {
          "success_rate": 0.58,
          "avg_execution_time": 2.3644294691085816,
          "tool_accuracy": 0.896551724137931,
          "error_rate": 0.42000000000000004,
          "total_tests": 50
        },
        "baseline_prompt": {
          "success_rate": 0.52,
          "avg_execution_time": 2.1339981985092162,
          "tool_accuracy": 0.5170940170940171,
          "error_rate": 0.48,
          "total_tests": 50
        },
        "flawed_optimal_prompt": {
          "success_rate": 0.0,
          "avg_execution_time": 2.633262538909912,
          "tool_accuracy": 0.0,
          "error_rate": 1.0,
          "total_tests": 50
        }
      }
    },
    "param_missing": {
      "flaw_info": {
        "type": "parameter_missing",
        "severity": "light",
        "description": "Required parameters missing (light severity)"
      },
      "performance_comparison": {
        "optimal_prompt": {
          "success_rate": 0.62,
          "avg_execution_time": 2.321013855934143,
          "tool_accuracy": 0.9324324324324325,
          "error_rate": 0.38,
          "total_tests": 50
        },
        "baseline_prompt": {
          "success_rate": 0.52,
          "avg_execution_time": 1.9891107416152953,
          "tool_accuracy": 0.5309734513274337,
          "error_rate": 0.48,
          "total_tests": 50
        },
        "flawed_optimal_prompt": {
          "success_rate": 0.12,
          "avg_execution_time": 2.3937687778472903,
          "tool_accuracy": 0.1382488479262673,
          "error_rate": 0.88,
          "total_tests": 50
        }
      }
    },
    "param_wrong_type": {
      "flaw_info": {
        "type": "parameter_wrong_type",
        "severity": "light",
        "description": "Wrong parameter types (light severity)"
      },
      "performance_comparison": {
        "optimal_prompt": {
          "success_rate": 0.6,
          "avg_execution_time": 2.2819866847991945,
          "tool_accuracy": 0.9230769230769231,
          "error_rate": 0.4,
          "total_tests": 50
        },
        "baseline_prompt": {
          "success_rate": 0.64,
          "avg_execution_time": 2.060892825126648,
          "tool_accuracy": 0.6550218340611353,
          "error_rate": 0.36,
          "total_tests": 50
        },
        "flawed_optimal_prompt": {
          "success_rate": 0.06,
          "avg_execution_time": 2.3745061111450196,
          "tool_accuracy": 0.08426966292134831,
          "error_rate": 0.94,
          "total_tests": 50
        }
      }
    },
    "missing_middle": {
      "flaw_info": {
        "type": "unknown",
        "severity": "light",
        "description": "Unknown flaw type: unknown"
      },
      "performance_comparison": {
        "optimal_prompt": {
          "success_rate": 0.52,
          "avg_execution_time": 2.1772302865982054,
          "tool_accuracy": 0.9112903225806451,
          "error_rate": 0.48,
          "total_tests": 50
        },
        "baseline_prompt": {
          "success_rate": 0.62,
          "avg_execution_time": 1.8481393098831176,
          "tool_accuracy": 0.6548672566371682,
          "error_rate": 0.38,
          "total_tests": 50
        },
        "flawed_optimal_prompt": {
          "success_rate": 0.1,
          "avg_execution_time": 2.393985652923584,
          "tool_accuracy": 0.12315270935960591,
          "error_rate": 0.9,
          "total_tests": 50
        }
      }
    },
    "missing_validation": {
      "flaw_info": {
        "type": "unknown",
        "severity": "light",
        "description": "Unknown flaw type: unknown"
      },
      "performance_comparison": {
        "optimal_prompt": {
          "success_rate": 0.54,
          "avg_execution_time": 2.3038620138168335,
          "tool_accuracy": 0.8854961832061069,
          "error_rate": 0.45999999999999996,
          "total_tests": 50
        },
        "baseline_prompt": {
          "success_rate": 0.52,
          "avg_execution_time": 1.8860336875915527,
          "tool_accuracy": 0.521551724137931,
          "error_rate": 0.48,
          "total_tests": 50
        },
        "flawed_optimal_prompt": {
          "success_rate": 0.16,
          "avg_execution_time": 2.2327764654159545,
          "tool_accuracy": 0.1951219512195122,
          "error_rate": 0.84,
          "total_tests": 50
        }
      }
    },
    "redundant_duplicate": {
      "flaw_info": {
        "type": "redundancy_duplicate",
        "severity": "light",
        "description": "Steps duplicated (light severity)"
      },
      "performance_comparison": {
        "optimal_prompt": {
          "success_rate": 0.56,
          "avg_execution_time": 2.3117402315139772,
          "tool_accuracy": 0.8928571428571429,
          "error_rate": 0.43999999999999995,
          "total_tests": 50
        },
        "baseline_prompt": {
          "success_rate": 0.5,
          "avg_execution_time": 2.0462041330337524,
          "tool_accuracy": 0.4978723404255319,
          "error_rate": 0.5,
          "total_tests": 50
        },
        "flawed_optimal_prompt": {
          "success_rate": 0.0,
          "avg_execution_time": 2.403630609512329,
          "tool_accuracy": 0.0,
          "error_rate": 1.0,
          "total_tests": 50
        }
      }
    },
    "redundant_unnecessary": {
      "flaw_info": {
        "type": "redundancy_unnecessary",
        "severity": "light",
        "description": "Unnecessary steps added (light severity)"
      },
      "performance_comparison": {
        "optimal_prompt": {
          "success_rate": 0.6,
          "avg_execution_time": 2.545558662414551,
          "tool_accuracy": 0.9148936170212766,
          "error_rate": 0.4,
          "total_tests": 50
        },
        "baseline_prompt": {
          "success_rate": 0.6,
          "avg_execution_time": 2.060223727226257,
          "tool_accuracy": 0.6086956521739131,
          "error_rate": 0.4,
          "total_tests": 50
        },
        "flawed_optimal_prompt": {
          "success_rate": 0.0,
          "avg_execution_time": 2.3124151945114138,
          "tool_accuracy": 0.0,
          "error_rate": 1.0,
          "total_tests": 50
        }
      }
    },
    "logic_break_format": {
      "flaw_info": {
        "type": "logic_format",
        "severity": "light",
        "description": "Format mismatches between steps (light severity)"
      },
      "performance_comparison": {
        "optimal_prompt": {
          "success_rate": 0.6,
          "avg_execution_time": 2.2818558645248412,
          "tool_accuracy": 0.9014084507042254,
          "error_rate": 0.4,
          "total_tests": 50
        },
        "baseline_prompt": {
          "success_rate": 0.62,
          "avg_execution_time": 3.1324885368347166,
          "tool_accuracy": 0.6410256410256411,
          "error_rate": 0.38,
          "total_tests": 50
        },
        "flawed_optimal_prompt": {
          "success_rate": 0.0,
          "avg_execution_time": 2.6020051765441896,
          "tool_accuracy": 0.0,
          "error_rate": 1.0,
          "total_tests": 50
        }
      }
    },
    "logic_break_unrelated": {
      "flaw_info": {
        "type": "logic_unrelated",
        "severity": "light",
        "description": "Unrelated steps inserted (light severity)"
      },
      "performance_comparison": {
        "optimal_prompt": {
          "success_rate": 0.48,
          "avg_execution_time": 2.454344005584717,
          "tool_accuracy": 0.8429752066115702,
          "error_rate": 0.52,
          "total_tests": 50
        },
        "baseline_prompt": {
          "success_rate": 0.66,
          "avg_execution_time": 1.8469521808624267,
          "tool_accuracy": 0.6569037656903766,
          "error_rate": 0.33999999999999997,
          "total_tests": 50
        },
        "flawed_optimal_prompt": {
          "success_rate": 0.02,
          "avg_execution_time": 2.2944959926605226,
          "tool_accuracy": 0.02702702702702703,
          "error_rate": 0.98,
          "total_tests": 50
        }
      }
    }
  }
}