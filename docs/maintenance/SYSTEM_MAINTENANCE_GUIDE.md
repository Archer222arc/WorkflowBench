# ç³»ç»Ÿç»´æŠ¤å®Œæ•´æŒ‡å—

> ç‰ˆæœ¬: 1.0  
> åˆ›å»ºæ—¶é—´: 2025-08-17  
> çŠ¶æ€: ğŸŸ¢ Active

## ğŸ“‹ ç›®å½•
1. [æ—¥å¸¸ç»´æŠ¤](#æ—¥å¸¸ç»´æŠ¤)
2. [å®šæœŸä»»åŠ¡](#å®šæœŸä»»åŠ¡)
3. [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)
4. [æ•°æ®ç®¡ç†](#æ•°æ®ç®¡ç†)
5. [æ•…éšœæ¢å¤](#æ•…éšœæ¢å¤)
6. [ç³»ç»Ÿç›‘æ§](#ç³»ç»Ÿç›‘æ§)
7. [ç»´æŠ¤è„šæœ¬](#ç»´æŠ¤è„šæœ¬)

---

## ğŸ“… æ—¥å¸¸ç»´æŠ¤

### æ¯æ—¥æ£€æŸ¥æ¸…å•
```bash
#!/bin/bash
# daily_check.sh - æ¯æ—¥ç³»ç»Ÿæ£€æŸ¥

echo "=== æ¯æ—¥ç³»ç»Ÿæ£€æŸ¥ $(date) ==="

# 1. æ£€æŸ¥ç³»ç»ŸçŠ¶æ€
echo "[1/5] ç³»ç»ŸçŠ¶æ€..."
ps aux | grep python | grep -c batch_test && echo "âœ… æµ‹è¯•è¿›ç¨‹è¿è¡Œä¸­" || echo "âš ï¸ æ— æµ‹è¯•è¿›ç¨‹"

# 2. æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
echo "[2/5] æ•°æ®å®Œæ•´æ€§..."
python -c "
import json
try:
    with open('pilot_bench_cumulative_results/master_database.json') as f:
        data = json.load(f)
    print(f'âœ… JSONæ•°æ®æ­£å¸¸: {len(data.get(\"models\", {}))} ä¸ªæ¨¡å‹')
except:
    print('âŒ JSONæ•°æ®å¼‚å¸¸')
"

# 3. æ£€æŸ¥ç£ç›˜ç©ºé—´
echo "[3/5] ç£ç›˜ç©ºé—´..."
USAGE=$(df -h . | tail -1 | awk '{print $5}' | sed 's/%//')
if [ $USAGE -lt 80 ]; then
    echo "âœ… ç£ç›˜ä½¿ç”¨ç‡: ${USAGE}%"
else
    echo "âš ï¸ ç£ç›˜ä½¿ç”¨ç‡è¿‡é«˜: ${USAGE}%"
fi

# 4. æ£€æŸ¥æ—¥å¿—é”™è¯¯
echo "[4/5] é”™è¯¯æ—¥å¿—..."
ERROR_COUNT=$(grep -c ERROR logs/*.log 2>/dev/null || echo 0)
echo "é”™è¯¯æ•°é‡: $ERROR_COUNT"

# 5. æ£€æŸ¥APIè¿æ¥
echo "[5/5] APIè¿æ¥..."
python -c "
import requests
try:
    r = requests.get('https://api.openai.com', timeout=5)
    print('âœ… APIå¯è®¿é—®')
except:
    print('âŒ APIä¸å¯è®¿é—®')
"

echo "=== æ£€æŸ¥å®Œæˆ ==="
```

### å®æ—¶ç›‘æ§é¢æ¿
```python
#!/usr/bin/env python3
# monitor.py - å®æ—¶ç›‘æ§é¢æ¿

import time
import os
from datetime import datetime
from pathlib import Path
import pandas as pd

def clear_screen():
    os.system('clear' if os.name == 'posix' else 'cls')

def get_system_stats():
    """è·å–ç³»ç»Ÿç»Ÿè®¡"""
    stats = {}
    
    # è¿›ç¨‹æ•°
    import subprocess
    result = subprocess.run(['ps', 'aux'], capture_output=True, text=True)
    stats['processes'] = len([l for l in result.stdout.split('\n') if 'python' in l and 'batch' in l])
    
    # æ•°æ®ç»Ÿè®¡
    try:
        if Path('pilot_bench_parquet_data/test_results.parquet').exists():
            df = pd.read_parquet('pilot_bench_parquet_data/test_results.parquet')
            stats['total_tests'] = len(df)
            stats['success_rate'] = f"{df['success'].mean():.1%}"
        else:
            stats['total_tests'] = 0
            stats['success_rate'] = "N/A"
    except:
        stats['total_tests'] = 0
        stats['success_rate'] = "N/A"
    
    # å¢é‡æ–‡ä»¶
    inc_dir = Path('pilot_bench_parquet_data/incremental')
    if inc_dir.exists():
        stats['incremental_files'] = len(list(inc_dir.glob('*.parquet')))
    else:
        stats['incremental_files'] = 0
    
    return stats

def display_dashboard():
    """æ˜¾ç¤ºç›‘æ§é¢æ¿"""
    while True:
        clear_screen()
        stats = get_system_stats()
        
        print("="*60)
        print(f"ğŸ“Š PILOT-Bench ç³»ç»Ÿç›‘æ§é¢æ¿")
        print(f"ğŸ• {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print("="*60)
        
        print(f"\nğŸ“ˆ æ•°æ®ç»Ÿè®¡:")
        print(f"  æ€»æµ‹è¯•æ•°: {stats['total_tests']}")
        print(f"  æˆåŠŸç‡: {stats['success_rate']}")
        print(f"  å¢é‡æ–‡ä»¶: {stats['incremental_files']}")
        
        print(f"\nâš™ï¸ ç³»ç»ŸçŠ¶æ€:")
        print(f"  è¿è¡Œè¿›ç¨‹: {stats['processes']}")
        
        print("\næŒ‰ Ctrl+C é€€å‡º")
        
        time.sleep(5)  # 5ç§’åˆ·æ–°ä¸€æ¬¡

if __name__ == "__main__":
    try:
        display_dashboard()
    except KeyboardInterrupt:
        print("\nç›‘æ§å·²åœæ­¢")
```

---

## â° å®šæœŸä»»åŠ¡

### Crontabé…ç½®
```bash
# ç¼–è¾‘crontab: crontab -e

# æ¯å°æ—¶åˆå¹¶Parquetå¢é‡æ•°æ®
0 * * * * cd /path/to/project && python -c "from parquet_data_manager import ParquetDataManager; m=ParquetDataManager(); m.consolidate_incremental_data()" >> logs/cron.log 2>&1

# æ¯å¤©å‡Œæ™¨2ç‚¹å¤‡ä»½æ•°æ®
0 2 * * * cd /path/to/project && ./backup_data.sh >> logs/backup.log 2>&1

# æ¯å¤©å‡Œæ™¨3ç‚¹æ¸…ç†æ—§æ—¥å¿—
0 3 * * * cd /path/to/project && find logs/ -name "*.log" -mtime +7 -delete

# æ¯å‘¨ä¸€å‡Œæ™¨4ç‚¹ç”ŸæˆæŠ¥å‘Š
0 4 * * 1 cd /path/to/project && python generate_weekly_report.py >> logs/report.log 2>&1

# æ¯å¤©æ—©ä¸Š9ç‚¹å‘é€å¥åº·æ£€æŸ¥é‚®ä»¶
0 9 * * * cd /path/to/project && ./health_check.sh | mail -s "System Health Report" admin@example.com
```

### è‡ªåŠ¨å¤‡ä»½è„šæœ¬
```bash
#!/bin/bash
# backup_data.sh - è‡ªåŠ¨å¤‡ä»½è„šæœ¬

BACKUP_DIR="backups/$(date +%Y%m%d)"
mkdir -p $BACKUP_DIR

echo "å¼€å§‹å¤‡ä»½ $(date)"

# 1. å¤‡ä»½JSONæ•°æ®
if [ -f "pilot_bench_cumulative_results/master_database.json" ]; then
    cp pilot_bench_cumulative_results/master_database.json "$BACKUP_DIR/"
    echo "âœ… JSONæ•°æ®å·²å¤‡ä»½"
fi

# 2. å¤‡ä»½Parquetæ•°æ®
if [ -d "pilot_bench_parquet_data" ]; then
    tar -czf "$BACKUP_DIR/parquet_data.tar.gz" pilot_bench_parquet_data/
    echo "âœ… Parquetæ•°æ®å·²å¤‡ä»½"
fi

# 3. å¤‡ä»½é…ç½®æ–‡ä»¶
cp -r config/ "$BACKUP_DIR/"
echo "âœ… é…ç½®æ–‡ä»¶å·²å¤‡ä»½"

# 4. æ¸…ç†æ—§å¤‡ä»½ï¼ˆä¿ç•™30å¤©ï¼‰
find backups/ -type d -mtime +30 -exec rm -rf {} \; 2>/dev/null

echo "å¤‡ä»½å®Œæˆ $(date)"
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–

### å†…å­˜ä¼˜åŒ–
```python
# memory_optimizer.py - å†…å­˜ä¼˜åŒ–å·¥å…·

import gc
import psutil
import os

def get_memory_usage():
    """è·å–å½“å‰å†…å­˜ä½¿ç”¨"""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024  # MB

def optimize_memory():
    """ä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
    before = get_memory_usage()
    
    # 1. å¼ºåˆ¶åƒåœ¾å›æ”¶
    gc.collect()
    
    # 2. æ¸…ç†ç¼“å­˜
    import functools
    functools.lru_cache().cache_clear()
    
    # 3. é‡Šæ”¾æœªä½¿ç”¨çš„å†…å­˜
    import ctypes
    libc = ctypes.CDLL("libc.so.6")
    libc.malloc_trim(0)
    
    after = get_memory_usage()
    
    print(f"å†…å­˜ä¼˜åŒ–: {before:.1f}MB -> {after:.1f}MB (é‡Šæ”¾ {before-after:.1f}MB)")
    
    return before - after

# å®šæœŸä¼˜åŒ–
import schedule
schedule.every(30).minutes.do(optimize_memory)
```

### å¹¶å‘ä¼˜åŒ–
```python
# concurrency_optimizer.py - å¹¶å‘ä¼˜åŒ–

import os
import multiprocessing

def get_optimal_workers():
    """è®¡ç®—æœ€ä¼˜å·¥ä½œè¿›ç¨‹æ•°"""
    cpu_count = multiprocessing.cpu_count()
    
    # æ ¹æ®ä¸åŒåœºæ™¯ä¼˜åŒ–
    if os.environ.get('STORAGE_FORMAT') == 'parquet':
        # Parquetæ¨¡å¼å¯ä»¥æ›´é«˜å¹¶å‘
        return min(cpu_count * 2, 50)
    else:
        # JSONæ¨¡å¼éœ€è¦é™åˆ¶å¹¶å‘é¿å…å†²çª
        return min(cpu_count, 10)

def optimize_batch_size(total_tasks, workers):
    """ä¼˜åŒ–æ‰¹é‡å¤§å°"""
    if total_tasks < 100:
        return max(1, total_tasks // workers)
    elif total_tasks < 1000:
        return 10
    else:
        return 20

# åº”ç”¨ä¼˜åŒ–
workers = get_optimal_workers()
batch_size = optimize_batch_size(1000, workers)
print(f"ä¼˜åŒ–é…ç½®: {workers} workers, batch_size={batch_size}")
```

---

## ğŸ’¾ æ•°æ®ç®¡ç†

### æ•°æ®æ¸…ç†
```python
#!/usr/bin/env python3
# data_cleanup.py - æ•°æ®æ¸…ç†å·¥å…·

from pathlib import Path
from datetime import datetime, timedelta
import pandas as pd

def cleanup_old_incremental(days=7):
    """æ¸…ç†æ—§çš„å¢é‡æ–‡ä»¶"""
    inc_dir = Path('pilot_bench_parquet_data/incremental')
    if not inc_dir.exists():
        return
    
    cutoff = datetime.now() - timedelta(days=days)
    removed = 0
    
    for file in inc_dir.glob('*.parquet'):
        if datetime.fromtimestamp(file.stat().st_mtime) < cutoff:
            file.unlink()
            removed += 1
    
    print(f"æ¸…ç†äº† {removed} ä¸ªæ—§å¢é‡æ–‡ä»¶")

def deduplicate_data():
    """å»é‡æ•°æ®"""
    parquet_file = Path('pilot_bench_parquet_data/test_results.parquet')
    if not parquet_file.exists():
        return
    
    df = pd.read_parquet(parquet_file)
    before = len(df)
    
    # åŸºäºtest_idå»é‡
    df = df.drop_duplicates(subset=['test_id'], keep='last')
    
    after = len(df)
    
    if before > after:
        df.to_parquet(parquet_file, index=False)
        print(f"å»é‡: {before} -> {after} (åˆ é™¤ {before-after} æ¡)")
    else:
        print("æ²¡æœ‰é‡å¤æ•°æ®")

def compress_logs():
    """å‹ç¼©æ—¥å¿—æ–‡ä»¶"""
    import gzip
    import shutil
    
    log_dir = Path('logs')
    compressed = 0
    
    for log_file in log_dir.glob('*.log'):
        # åªå‹ç¼©è¶…è¿‡1å¤©çš„æ—¥å¿—
        if datetime.fromtimestamp(log_file.stat().st_mtime) < datetime.now() - timedelta(days=1):
            with open(log_file, 'rb') as f_in:
                with gzip.open(f"{log_file}.gz", 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
            log_file.unlink()
            compressed += 1
    
    print(f"å‹ç¼©äº† {compressed} ä¸ªæ—¥å¿—æ–‡ä»¶")

if __name__ == "__main__":
    print("å¼€å§‹æ•°æ®æ¸…ç†...")
    cleanup_old_incremental()
    deduplicate_data()
    compress_logs()
    print("æ¸…ç†å®Œæˆ")
```

### æ•°æ®è¿ç§»
```python
#!/usr/bin/env python3
# data_migration.py - æ•°æ®è¿ç§»å·¥å…·

def migrate_json_to_parquet():
    """JSONåˆ°Parquetè¿ç§»"""
    import json
    import pandas as pd
    from pathlib import Path
    
    json_path = Path("pilot_bench_cumulative_results/master_database.json")
    parquet_path = Path("pilot_bench_parquet_data/test_results.parquet")
    
    if not json_path.exists():
        print("JSONæ–‡ä»¶ä¸å­˜åœ¨")
        return False
    
    # è¿è¡Œè½¬æ¢è„šæœ¬
    import subprocess
    result = subprocess.run(['python', 'json_to_parquet_converter.py'], 
                          capture_output=True, text=True)
    
    if result.returncode == 0:
        print("âœ… è¿ç§»æˆåŠŸ")
        return True
    else:
        print(f"âŒ è¿ç§»å¤±è´¥: {result.stderr}")
        return False

def export_for_analysis():
    """å¯¼å‡ºæ•°æ®ç”¨äºåˆ†æ"""
    import pandas as pd
    
    # è¯»å–Parquetæ•°æ®
    df = pd.read_parquet('pilot_bench_parquet_data/test_results.parquet')
    
    # å¯¼å‡ºä¸åŒæ ¼å¼
    df.to_csv('export/test_results.csv', index=False)
    df.to_excel('export/test_results.xlsx', index=False)
    df.to_json('export/test_results.json', orient='records', indent=2)
    
    print(f"å¯¼å‡º {len(df)} æ¡è®°å½•åˆ° export/ ç›®å½•")
```

---

## ğŸš¨ æ•…éšœæ¢å¤

### ç´§æ€¥æ¢å¤æµç¨‹
```bash
#!/bin/bash
# emergency_recovery.sh - ç´§æ€¥æ¢å¤è„šæœ¬

echo "=== ç´§æ€¥æ¢å¤æµç¨‹ ==="

# 1. åœæ­¢æ‰€æœ‰è¿›ç¨‹
echo "[1/5] åœæ­¢æ‰€æœ‰æµ‹è¯•è¿›ç¨‹..."
pkill -f "batch_test"
pkill -f "smart_batch"
pkill -f "ultra_parallel"
sleep 2

# 2. å¤‡ä»½å½“å‰æ•°æ®
echo "[2/5] å¤‡ä»½å½“å‰æ•°æ®..."
EMERGENCY_BACKUP="emergency_backup_$(date +%Y%m%d_%H%M%S)"
mkdir -p $EMERGENCY_BACKUP
cp -r pilot_bench_cumulative_results/ $EMERGENCY_BACKUP/ 2>/dev/null
cp -r pilot_bench_parquet_data/ $EMERGENCY_BACKUP/ 2>/dev/null

# 3. æ£€æŸ¥å¤‡ä»½
echo "[3/5] æŸ¥æ‰¾å¯ç”¨å¤‡ä»½..."
LATEST_BACKUP=$(ls -t backups/*/master_database.json 2>/dev/null | head -1)
if [ -n "$LATEST_BACKUP" ]; then
    echo "æ‰¾åˆ°å¤‡ä»½: $LATEST_BACKUP"
    cp $LATEST_BACKUP pilot_bench_cumulative_results/master_database.json
    echo "âœ… å·²æ¢å¤JSONæ•°æ®"
else
    echo "âš ï¸ æ²¡æœ‰æ‰¾åˆ°å¤‡ä»½"
fi

# 4. éªŒè¯æ•°æ®
echo "[4/5] éªŒè¯æ•°æ®..."
python -c "
import json
try:
    with open('pilot_bench_cumulative_results/master_database.json') as f:
        data = json.load(f)
    print('âœ… æ•°æ®éªŒè¯é€šè¿‡')
except Exception as e:
    print(f'âŒ æ•°æ®éªŒè¯å¤±è´¥: {e}')
"

# 5. é‡å»ºç´¢å¼•
echo "[5/5] é‡å»ºParquetç´¢å¼•..."
python -c "
from parquet_data_manager import ParquetDataManager
m = ParquetDataManager()
m.consolidate_incremental_data()
print('âœ… ç´¢å¼•é‡å»ºå®Œæˆ')
"

echo "=== æ¢å¤å®Œæˆ ==="
```

### æ•°æ®ä¿®å¤å·¥å…·
```python
#!/usr/bin/env python3
# repair_tool.py - æ•°æ®ä¿®å¤å·¥å…·

import json
from pathlib import Path
from datetime import datetime

class DataRepairTool:
    def __init__(self):
        self.db_path = Path("pilot_bench_cumulative_results/master_database.json")
        self.backup_dir = Path("backups")
    
    def diagnose(self):
        """è¯Šæ–­æ•°æ®é—®é¢˜"""
        issues = []
        
        try:
            with open(self.db_path) as f:
                data = json.load(f)
        except json.JSONDecodeError as e:
            issues.append(f"JSONè§£æé”™è¯¯: {e}")
            return issues
        except FileNotFoundError:
            issues.append("æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨")
            return issues
        
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        required = ['version', 'models', 'summary']
        for field in required:
            if field not in data:
                issues.append(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
        
        # æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§
        if 'models' in data and 'summary' in data:
            actual_total = sum(m.get('total_tests', 0) for m in data['models'].values())
            summary_total = data['summary'].get('total_tests', 0)
            if actual_total != summary_total:
                issues.append(f"æ•°æ®ä¸ä¸€è‡´: å®é™…={actual_total}, æ±‡æ€»={summary_total}")
        
        return issues
    
    def repair(self):
        """ä¿®å¤æ•°æ®"""
        issues = self.diagnose()
        
        if not issues:
            print("âœ… æ•°æ®æ­£å¸¸ï¼Œæ— éœ€ä¿®å¤")
            return True
        
        print(f"å‘ç° {len(issues)} ä¸ªé—®é¢˜:")
        for issue in issues:
            print(f"  - {issue}")
        
        # å°è¯•è‡ªåŠ¨ä¿®å¤
        try:
            with open(self.db_path) as f:
                data = json.load(f)
            
            # ä¿®å¤ç¼ºå¤±å­—æ®µ
            if 'version' not in data:
                data['version'] = '3.0'
            if 'models' not in data:
                data['models'] = {}
            if 'summary' not in data:
                data['summary'] = {}
            
            # ä¿®å¤æ•°æ®ä¸€è‡´æ€§
            actual_total = sum(m.get('total_tests', 0) for m in data['models'].values())
            data['summary']['total_tests'] = actual_total
            data['last_updated'] = datetime.now().isoformat()
            
            # ä¿å­˜ä¿®å¤åçš„æ•°æ®
            with open(self.db_path, 'w') as f:
                json.dump(data, f, indent=2)
            
            print("âœ… æ•°æ®ä¿®å¤å®Œæˆ")
            return True
            
        except Exception as e:
            print(f"âŒ ä¿®å¤å¤±è´¥: {e}")
            return False

if __name__ == "__main__":
    tool = DataRepairTool()
    tool.repair()
```

---

## ğŸ“Š ç³»ç»Ÿç›‘æ§

### Prometheusé…ç½®
```yaml
# prometheus.yml - Prometheusç›‘æ§é…ç½®

global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'pilot_bench'
    static_configs:
      - targets: ['localhost:8000']
    
  - job_name: 'node_exporter'
    static_configs:
      - targets: ['localhost:9100']
```

### ç›‘æ§æŒ‡æ ‡å¯¼å‡º
```python
#!/usr/bin/env python3
# metrics_exporter.py - å¯¼å‡ºPrometheusæŒ‡æ ‡

from prometheus_client import start_http_server, Gauge, Counter
import time
from pathlib import Path
import pandas as pd

# å®šä¹‰æŒ‡æ ‡
total_tests = Gauge('pilot_bench_total_tests', 'Total number of tests')
success_rate = Gauge('pilot_bench_success_rate', 'Overall success rate')
incremental_files = Gauge('pilot_bench_incremental_files', 'Number of incremental files')
api_errors = Counter('pilot_bench_api_errors', 'Total API errors')

def update_metrics():
    """æ›´æ–°æŒ‡æ ‡"""
    # æ›´æ–°æµ‹è¯•æ€»æ•°
    try:
        df = pd.read_parquet('pilot_bench_parquet_data/test_results.parquet')
        total_tests.set(len(df))
        success_rate.set(df['success'].mean())
    except:
        pass
    
    # æ›´æ–°å¢é‡æ–‡ä»¶æ•°
    inc_dir = Path('pilot_bench_parquet_data/incremental')
    if inc_dir.exists():
        incremental_files.set(len(list(inc_dir.glob('*.parquet'))))

if __name__ == '__main__':
    # å¯åŠ¨HTTPæœåŠ¡å™¨
    start_http_server(8000)
    
    # å®šæœŸæ›´æ–°æŒ‡æ ‡
    while True:
        update_metrics()
        time.sleep(30)
```

---

## ğŸ”§ ç»´æŠ¤è„šæœ¬

### ä¸€é”®ç»´æŠ¤è„šæœ¬
```bash
#!/bin/bash
# maintenance.sh - ä¸€é”®ç»´æŠ¤è„šæœ¬

function show_menu() {
    echo "================================"
    echo "    ç³»ç»Ÿç»´æŠ¤èœå•"
    echo "================================"
    echo "1. æ—¥å¸¸æ£€æŸ¥"
    echo "2. æ•°æ®å¤‡ä»½"
    echo "3. æ•°æ®æ¸…ç†"
    echo "4. æ€§èƒ½ä¼˜åŒ–"
    echo "5. æ•…éšœæ¢å¤"
    echo "6. ç”ŸæˆæŠ¥å‘Š"
    echo "7. æŸ¥çœ‹ç›‘æ§"
    echo "0. é€€å‡º"
    echo "================================"
}

function daily_check() {
    ./daily_check.sh
}

function backup_data() {
    ./backup_data.sh
}

function cleanup_data() {
    python data_cleanup.py
}

function optimize_performance() {
    python memory_optimizer.py
    echo "æ€§èƒ½ä¼˜åŒ–å®Œæˆ"
}

function emergency_recovery() {
    ./emergency_recovery.sh
}

function generate_report() {
    python generate_report.py
}

function show_monitoring() {
    python monitor.py
}

# ä¸»å¾ªç¯
while true; do
    show_menu
    read -p "è¯·é€‰æ‹©æ“ä½œ [0-7]: " choice
    
    case $choice in
        1) daily_check ;;
        2) backup_data ;;
        3) cleanup_data ;;
        4) optimize_performance ;;
        5) emergency_recovery ;;
        6) generate_report ;;
        7) show_monitoring ;;
        0) echo "é€€å‡ºç»´æŠ¤ç³»ç»Ÿ"; exit 0 ;;
        *) echo "æ— æ•ˆé€‰æ‹©" ;;
    esac
    
    echo ""
    read -p "æŒ‰å›è½¦ç»§ç»­..."
done
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

### å†…éƒ¨æ–‡æ¡£
- [DEBUG_KNOWLEDGE_BASE_V2.md](./DEBUG_KNOWLEDGE_BASE_V2.md) - è°ƒè¯•çŸ¥è¯†åº“
- [COMMON_ISSUES_V2.md](./COMMON_ISSUES_V2.md) - å¸¸è§é—®é¢˜
- [PARQUET_GUIDE.md](../guides/PARQUET_GUIDE.md) - ParquetæŒ‡å—

### å¤–éƒ¨å·¥å…·
- [Prometheus](https://prometheus.io/) - ç›‘æ§ç³»ç»Ÿ
- [Grafana](https://grafana.com/) - å¯è§†åŒ–é¢æ¿
- [Sentry](https://sentry.io/) - é”™è¯¯è¿½è¸ª

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0  
**åˆ›å»ºæ—¶é—´**: 2025-08-17  
**ç»´æŠ¤è€…**: System Administrator  
**çŠ¶æ€**: ğŸŸ¢ Active | âœ… å®Œæ•´