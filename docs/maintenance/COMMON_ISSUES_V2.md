# å¸¸è§é—®é¢˜è§£å†³æ–¹æ¡ˆ V2.0

> æœ€åæ›´æ–°: 2025-08-17  
> ç‰ˆæœ¬: 2.0  
> çŠ¶æ€: ğŸŸ¢ Active

## ğŸ“‹ å¿«é€Ÿå¯¼èˆª

### æŒ‰é”™è¯¯ç±»å‹
- [ğŸ”´ å¯åŠ¨é”™è¯¯](#å¯åŠ¨é”™è¯¯)
- [ğŸŸ¡ è¿è¡Œæ—¶é”™è¯¯](#è¿è¡Œæ—¶é”™è¯¯)  
- [ğŸ”µ APIé”™è¯¯](#apié”™è¯¯)
- [ğŸŸ¢ æ•°æ®é”™è¯¯](#æ•°æ®é”™è¯¯)
- [âš« ç³»ç»Ÿé”™è¯¯](#ç³»ç»Ÿé”™è¯¯)

### æŒ‰ç´§æ€¥ç¨‹åº¦
- [ğŸš¨ ç´§æ€¥](#ç´§æ€¥é—®é¢˜)
- [âš ï¸ é‡è¦](#é‡è¦é—®é¢˜)
- [â„¹ï¸ ä¸€èˆ¬](#ä¸€èˆ¬é—®é¢˜)

---

## ğŸš¨ ç´§æ€¥é—®é¢˜

### 1. æ•°æ®å®Œå…¨ä¸¢å¤±
**ç—‡çŠ¶**: master_database.jsonè¢«æ¸…ç©ºæˆ–æŸå
```
JSONDecodeError: Expecting value: line 1 column 1 (char 0)
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. æ£€æŸ¥å¤‡ä»½
ls -la pilot_bench_cumulative_results/*.backup

# 2. ä»æœ€è¿‘çš„å¤‡ä»½æ¢å¤
cp pilot_bench_cumulative_results/master_database.backup master_database.json

# 3. å¦‚æœæœ‰Parquetæ•°æ®ï¼Œä»Parquetæ¢å¤
python -c "
from parquet_data_manager import ParquetDataManager
manager = ParquetDataManager()
manager.export_to_json(output_path='pilot_bench_cumulative_results/master_database.json')
"

# 4. ä»å¢é‡æ–‡ä»¶é‡å»ºï¼ˆæœ€åæ‰‹æ®µï¼‰
python recover_from_incremental.py
```

### 2. æ‰€æœ‰APIè°ƒç”¨å¤±è´¥
**ç—‡çŠ¶**: æ‰€æœ‰æ¨¡å‹éƒ½è¿”å›è¶…æ—¶æˆ–è¿æ¥é”™è¯¯

**è¯Šæ–­**:
```python
# æµ‹è¯•APIè¿æ¥
python test_deepseek_api.py

# æ£€æŸ¥ç½‘ç»œ
ping api.openai.com
curl -I https://api.openai.com
```

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. æ£€æŸ¥APIå¯†é’¥
import os
print("API Keys configured:", [k for k in os.environ if 'API' in k])

# 2. åˆ‡æ¢åˆ°å¤‡ç”¨ç«¯ç‚¹
export IDEALAB_API_BASE=https://backup.endpoint.com/v1

# 3. é™ä½å¹¶å‘
export MAX_WORKERS=1
export QPS_LIMIT=1

# 4. å¢åŠ è¶…æ—¶
# åœ¨ä»£ç ä¸­ä¿®æ”¹timeoutå‚æ•°
timeout=120  # å¢åŠ åˆ°120ç§’
```

---

## âš ï¸ é‡è¦é—®é¢˜

### 3. Bashè„šæœ¬è¯­æ³•é”™è¯¯
**ç—‡çŠ¶**: macOSä¸Šè¿è¡Œæ—¶æŠ¥é”™
```bash
${STORAGE_FORMAT^^}: bad substitution
```

**åŸå› **: macOSçš„bashç‰ˆæœ¬(3.2)ä¸æ”¯æŒ`^^`è¯­æ³•

**è§£å†³æ–¹æ¡ˆ**:
```bash
# âŒ é”™è¯¯ï¼ˆbash 4.0+ï¼‰
UPPER=${VAR^^}

# âœ… æ­£ç¡®ï¼ˆå…¼å®¹æ‰€æœ‰ç‰ˆæœ¬ï¼‰
UPPER=$(echo "$VAR" | tr '[:lower:]' '[:upper:]')

# æˆ–è€…å‡çº§bash
brew install bash
echo "/usr/local/bin/bash" | sudo tee -a /etc/shells
chsh -s /usr/local/bin/bash
```

### 4. Parquetæ–‡ä»¶æœªç”Ÿæˆ
**ç—‡çŠ¶**: å¯ç”¨Parquetæ¨¡å¼ä½†çœ‹ä¸åˆ°.parquetæ–‡ä»¶

**è¯Šæ–­**:
```bash
# æ£€æŸ¥ç¯å¢ƒå˜é‡
echo $STORAGE_FORMAT

# æ£€æŸ¥å¢é‡ç›®å½•
ls -la pilot_bench_parquet_data/incremental/

# æŸ¥çœ‹Pythonä¸­çš„è®¾ç½®
python -c "import os; print('STORAGE_FORMAT:', os.environ.get('STORAGE_FORMAT', 'not set'))"
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. æ­£ç¡®è®¾ç½®ç¯å¢ƒå˜é‡
export STORAGE_FORMAT=parquet

# 2. åˆå¹¶å¢é‡æ–‡ä»¶
python -c "
from parquet_data_manager import ParquetDataManager
m = ParquetDataManager()
m.consolidate_incremental_data()
"

# 3. éªŒè¯æ–‡ä»¶
ls -la pilot_bench_parquet_data/*.parquet
```

### 5. å¹¶å‘å†™å…¥å†²çª
**ç—‡çŠ¶**: å¤šè¿›ç¨‹æµ‹è¯•æ—¶æ•°æ®ä¸ä¸€è‡´æˆ–ä¸¢å¤±

**JSONæ¨¡å¼çš„é—®é¢˜**:
```python
# âŒ å¤šè¿›ç¨‹åŒæ—¶å†™å…¥JSONä¼šå†²çª
with open('database.json', 'w') as f:
    json.dump(data, f)  # å¯èƒ½è¢«å…¶ä»–è¿›ç¨‹è¦†ç›–
```

**è§£å†³æ–¹æ¡ˆ - ä½¿ç”¨Parquet**:
```bash
# åˆ‡æ¢åˆ°Parquetæ¨¡å¼ï¼ˆè‡ªåŠ¨å¤„ç†å¹¶å‘ï¼‰
export STORAGE_FORMAT=parquet
```

**è§£å†³æ–¹æ¡ˆ - JSONåŠ é”**:
```python
import fcntl
import json

def safe_json_update(filepath, update_func):
    with open(filepath, 'r+') as f:
        fcntl.flock(f.fileno(), fcntl.LOCK_EX)
        try:
            data = json.load(f)
            update_func(data)
            f.seek(0)
            json.dump(data, f, indent=2)
            f.truncate()
        finally:
            fcntl.flock(f.fileno(), fcntl.LOCK_UN)
```

---

## ğŸ”µ APIé”™è¯¯

### 6. Model not configuredé”™è¯¯
**ç—‡çŠ¶**: 
```
ValueError: Model DeepSeek-V3-0324 is not configured for user_azure
```

**åŸå› **: æ¨¡å‹é…ç½®ç¼ºå¤±æˆ–ä¸æ­£ç¡®

**è§£å†³æ–¹æ¡ˆ**:
```json
// åœ¨config/config.jsonçš„model_configséƒ¨åˆ†æ·»åŠ 
"DeepSeek-V3-0324": {
    "provider": "user_azure",
    "azure_endpoint": "https://your-endpoint.services.ai.azure.com",
    "api_version": "2024-02-15-preview",
    "deployment_name": "DeepSeek-V3-0324",
    "max_tokens": 4096,
    "temperature": 0.1
}
```

### 7. Rate limité”™è¯¯
**ç—‡çŠ¶**: 
```
openai.RateLimitError: Rate limit exceeded
```

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. é™ä½QPS
export QPS_LIMIT=5  # é™åˆ°5 QPS

# 2. ä½¿ç”¨å¤šä¸ªAPIå¯†é’¥è½®è¯¢
# åœ¨config.jsonä¸­é…ç½®å¤šä¸ªkey

# 3. æ·»åŠ é‡è¯•é€»è¾‘
from tenacity import retry, wait_exponential, stop_after_attempt

@retry(
    wait=wait_exponential(multiplier=1, min=4, max=60),
    stop=stop_after_attempt(5)
)
def call_api():
    # APIè°ƒç”¨
    pass

# 4. ä½¿ç”¨ä¸åŒçš„prompt_typeåˆ†æ•£è´Ÿè½½
# baseline, cot, optimalä½¿ç”¨ä¸åŒçš„API key
```

### 8. è¶…æ—¶é”™è¯¯
**ç—‡çŠ¶**:
```
TimeoutError: Request timed out after 30 seconds
```

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. å¢åŠ è¶…æ—¶æ—¶é—´
response = client.chat.completions.create(
    model=model,
    messages=messages,
    timeout=120  # å¢åŠ åˆ°120ç§’
)

# 2. å‡å°æ‰¹é‡å¤§å°
--num-instances 5  # å‡å°‘å¹¶å‘è¯·æ±‚

# 3. ä½¿ç”¨æµå¼å“åº”
stream=True  # å¯ç”¨æµå¼å“åº”é¿å…è¶…æ—¶
```

---

## ğŸŸ¢ æ•°æ®é”™è¯¯

### 9. æ•°æ®ä¸ä¸€è‡´
**ç—‡çŠ¶**: ç»Ÿè®¡æ•°å­—ä¸åŒ¹é…ï¼Œæ€»æ•°ä¸å¯¹

**è¯Šæ–­è„šæœ¬**:
```python
def diagnose_data_consistency():
    import json
    
    with open('pilot_bench_cumulative_results/master_database.json') as f:
        data = json.load(f)
    
    # è®¡ç®—å®é™…æ€»æ•°
    actual_total = 0
    for model_data in data['models'].values():
        actual_total += model_data.get('total_tests', 0)
    
    # æ¯”è¾ƒæ±‡æ€»
    summary_total = data['summary'].get('total_tests', 0)
    
    print(f"å®é™…æ€»æ•°: {actual_total}")
    print(f"æ±‡æ€»æ€»æ•°: {summary_total}")
    
    if actual_total != summary_total:
        print("âŒ æ•°æ®ä¸ä¸€è‡´ï¼")
        # ä¿®å¤
        data['summary']['total_tests'] = actual_total
        with open('master_database.json', 'w') as f:
            json.dump(data, f, indent=2)
        print("âœ… å·²ä¿®å¤")

diagnose_data_consistency()
```

### 10. é‡å¤æ•°æ®
**ç—‡çŠ¶**: åŒä¸€æµ‹è¯•è¢«è®°å½•å¤šæ¬¡

**æ£€æµ‹**:
```python
import pandas as pd

# Parquetæ¨¡å¼
df = pd.read_parquet('pilot_bench_parquet_data/test_results.parquet')
duplicates = df[df.duplicated(subset=['test_id'])]
print(f"å‘ç° {len(duplicates)} æ¡é‡å¤è®°å½•")

# å»é‡
df_clean = df.drop_duplicates(subset=['test_id'], keep='last')
df_clean.to_parquet('test_results_clean.parquet')
```

---

## âš« ç³»ç»Ÿé”™è¯¯

### 11. å†…å­˜ä¸è¶³
**ç—‡çŠ¶**: 
```
MemoryError: Unable to allocate array
```

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. åˆ†æ‰¹å¤„ç†
def process_in_batches(data, batch_size=1000):
    for i in range(0, len(data), batch_size):
        batch = data[i:i+batch_size]
        process_batch(batch)
        
        # å¼ºåˆ¶åƒåœ¾å›æ”¶
        import gc
        gc.collect()

# 2. ä½¿ç”¨ç”Ÿæˆå™¨
def read_large_file(filepath):
    with open(filepath) as f:
        for line in f:
            yield json.loads(line)

# 3. é™åˆ¶å¹¶å‘
MAX_WORKERS=5  # å‡å°‘å¹¶å‘æ•°
```

### 12. ç£ç›˜ç©ºé—´ä¸è¶³
**ç—‡çŠ¶**: 
```
OSError: [Errno 28] No space left on device
```

**è¯Šæ–­**:
```bash
# æ£€æŸ¥ç£ç›˜ä½¿ç”¨
df -h

# æŸ¥æ‰¾å¤§æ–‡ä»¶
du -sh * | sort -h | tail -20

# æ¸…ç†æ—¥å¿—
find logs/ -name "*.log" -mtime +7 -delete

# æ¸…ç†æ—§çš„å¢é‡æ–‡ä»¶
find pilot_bench_parquet_data/incremental/ -name "*.parquet" -mtime +3 -delete
```

---

## â„¹ï¸ ä¸€èˆ¬é—®é¢˜

### 13. è¿›åº¦æ˜¾ç¤ºä¸å‡†ç¡®
**ç—‡çŠ¶**: è¿›åº¦æ¡å¡ä½æˆ–æ˜¾ç¤ºé”™è¯¯

**è§£å†³æ–¹æ¡ˆ**:
```python
# ä½¿ç”¨tqdmçš„åŠ¨æ€æ›´æ–°
from tqdm import tqdm

pbar = tqdm(total=total_tasks, dynamic_ncols=True)
pbar.set_description(f"å¤„ç†ä¸­")
pbar.update(1)
pbar.refresh()  # å¼ºåˆ¶åˆ·æ–°æ˜¾ç¤º
```

### 14. æ—¥å¿—æ–‡ä»¶è¿‡å¤§
**ç—‡çŠ¶**: logs/ç›®å½•å ç”¨å¤§é‡ç©ºé—´

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. å‹ç¼©æ—§æ—¥å¿—
gzip logs/*.log

# 2. è®¾ç½®æ—¥å¿—è½®è½¬
# åœ¨ä»£ç ä¸­ä½¿ç”¨RotatingFileHandler
from logging.handlers import RotatingFileHandler

handler = RotatingFileHandler(
    'app.log',
    maxBytes=10*1024*1024,  # 10MB
    backupCount=5
)

# 3. æ¸…ç†è„šæœ¬
#!/bin/bash
find logs/ -name "*.log" -mtime +7 -exec gzip {} \;
find logs/ -name "*.gz" -mtime +30 -delete
```

### 15. æµ‹è¯•ç»“æœä¸å¯é‡ç°
**ç—‡çŠ¶**: ç›¸åŒé…ç½®çš„æµ‹è¯•ç»“æœå·®å¼‚å¾ˆå¤§

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. è®¾ç½®éšæœºç§å­
import random
import numpy as np

random.seed(42)
np.random.seed(42)

# 2. å›ºå®šæ¨¡å‹å‚æ•°
temperature=0.0  # ä½¿ç”¨0æ¸©åº¦ç¡®ä¿ç¡®å®šæ€§

# 3. è®°å½•å®Œæ•´é…ç½®
test_config = {
    'model': model_name,
    'seed': 42,
    'temperature': 0.0,
    'timestamp': datetime.now().isoformat(),
    'environment': dict(os.environ)
}
```

---

## ğŸ› ï¸ å¿«é€Ÿä¿®å¤å·¥å…·ç®±

### ä¸€é”®è¯Šæ–­è„šæœ¬
```bash
#!/bin/bash
# diagnose.sh - ç³»ç»Ÿè¯Šæ–­è„šæœ¬

echo "=== ç³»ç»Ÿè¯Šæ–­ ==="

# æ£€æŸ¥ç¯å¢ƒå˜é‡
echo "ç¯å¢ƒå˜é‡:"
echo "  STORAGE_FORMAT: $STORAGE_FORMAT"
echo "  MAX_WORKERS: $MAX_WORKERS"

# æ£€æŸ¥æ–‡ä»¶
echo "æ•°æ®æ–‡ä»¶:"
ls -lh pilot_bench_cumulative_results/*.json 2>/dev/null || echo "  âŒ JSONæ–‡ä»¶ä¸å­˜åœ¨"
ls -lh pilot_bench_parquet_data/*.parquet 2>/dev/null || echo "  âŒ Parquetæ–‡ä»¶ä¸å­˜åœ¨"

# æ£€æŸ¥è¿›ç¨‹
echo "è¿è¡Œä¸­çš„è¿›ç¨‹:"
ps aux | grep python | grep -E "(batch|test)" || echo "  æ²¡æœ‰æµ‹è¯•è¿›ç¨‹"

# æ£€æŸ¥ç£ç›˜ç©ºé—´
echo "ç£ç›˜ç©ºé—´:"
df -h . | tail -1

# æ£€æŸ¥æœ€è¿‘é”™è¯¯
echo "æœ€è¿‘çš„é”™è¯¯:"
grep ERROR logs/*.log 2>/dev/null | tail -5 || echo "  æ²¡æœ‰é”™è¯¯æ—¥å¿—"
```

### æ•°æ®ä¿®å¤å·¥å…·
```python
#!/usr/bin/env python3
# repair_data.py - æ•°æ®ä¿®å¤å·¥å…·

def repair_json_database():
    """ä¿®å¤JSONæ•°æ®åº“"""
    import json
    from pathlib import Path
    
    db_path = Path("pilot_bench_cumulative_results/master_database.json")
    
    try:
        with open(db_path) as f:
            data = json.load(f)
    except json.JSONDecodeError:
        print("âŒ JSONæ–‡ä»¶æŸåï¼Œå°è¯•ä»å¤‡ä»½æ¢å¤...")
        # æŸ¥æ‰¾æœ€è¿‘çš„å¤‡ä»½
        backups = sorted(Path(".").glob("*.backup"))
        if backups:
            import shutil
            shutil.copy(backups[-1], db_path)
            print(f"âœ… ä» {backups[-1]} æ¢å¤")
        else:
            print("âŒ æ²¡æœ‰å¯ç”¨å¤‡ä»½")
            return False
    
    # ä¿®å¤æ•°æ®ä¸€è‡´æ€§
    # ... ä¿®å¤é€»è¾‘ ...
    
    return True

if __name__ == "__main__":
    repair_json_database()
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [DEBUG_KNOWLEDGE_BASE_V2.md](./DEBUG_KNOWLEDGE_BASE_V2.md) - è¯¦ç»†è°ƒè¯•æŒ‡å—
- [PARQUET_GUIDE.md](../guides/PARQUET_GUIDE.md) - Parquetä½¿ç”¨æŒ‡å—
- [API_TROUBLESHOOTING.md](../api/API_TROUBLESHOOTING.md) - APIé—®é¢˜è¯¦è§£

---

**æ–‡æ¡£ç‰ˆæœ¬**: 2.0  
**åˆ›å»ºæ—¶é—´**: 2025-08-17  
**ç»´æŠ¤è€…**: System Administrator  
**çŠ¶æ€**: ğŸŸ¢ Active | âœ… å·²æ›´æ–°