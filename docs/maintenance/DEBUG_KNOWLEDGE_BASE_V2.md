# è°ƒè¯•çŸ¥è¯†åº“ V2.0 (Debug Knowledge Base)

> æœ€åæ›´æ–°: 2025-08-17  
> ç‰ˆæœ¬: 2.0  
> çŠ¶æ€: ğŸŸ¢ Active

## ğŸ“‹ ç›®å½•
1. [Parquetæ¨¡å¼è°ƒè¯•](#parquetæ¨¡å¼è°ƒè¯•)
2. [å¸¸è§é”™è¯¯æ¨¡å¼](#å¸¸è§é”™è¯¯æ¨¡å¼)
3. [APIè¿æ¥é—®é¢˜](#apiè¿æ¥é—®é¢˜)
4. [æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥](#æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥)
5. [æ€§èƒ½è°ƒè¯•](#æ€§èƒ½è°ƒè¯•)
6. [ç´§æ€¥ä¿®å¤æµç¨‹](#ç´§æ€¥ä¿®å¤æµç¨‹)

---

## ğŸ†• Parquetæ¨¡å¼è°ƒè¯•

### é—®é¢˜1: Parquetæ–‡ä»¶æœªç”Ÿæˆ
**ç—‡çŠ¶**: å¯ç”¨Parquetæ¨¡å¼ä½†åªçœ‹åˆ°JSONæ›´æ–°
```bash
# è¯Šæ–­å‘½ä»¤
ls -la pilot_bench_parquet_data/incremental/
```

**åŸå› **: 
- æ•°æ®å†™å…¥å¢é‡ç›®å½•ï¼Œè€Œéä¸»æ–‡ä»¶
- ç¯å¢ƒå˜é‡æœªè®¾ç½®

**è§£å†³æ–¹æ¡ˆ**:
```bash
# 1. è®¾ç½®ç¯å¢ƒå˜é‡
export STORAGE_FORMAT=parquet

# 2. æ£€æŸ¥å¢é‡æ–‡ä»¶
ls -la pilot_bench_parquet_data/incremental/*.parquet

# 3. åˆå¹¶å¢é‡æ•°æ®
python -c "from parquet_data_manager import ParquetDataManager; m=ParquetDataManager(); m.consolidate_incremental_data()"
```

### é—®é¢˜2: åŒé‡å­˜å‚¨ï¼ˆJSONå’ŒParquetåŒæ—¶æ›´æ–°ï¼‰
**ç—‡çŠ¶**: master_database.jsonä»åœ¨æ›´æ–°
```python
# æ£€æŸ¥å­˜å‚¨æ¨¡å¼
import os
print(f"STORAGE_FORMAT: {os.environ.get('STORAGE_FORMAT', 'json')}")
```

**è§£å†³æ–¹æ¡ˆ**:
```python
# ç¡®ä¿åªä½¿ç”¨ä¸€ç§å­˜å‚¨
if os.environ.get('STORAGE_FORMAT') == 'parquet':
    from parquet_cumulative_manager import ParquetCumulativeManager as Manager
else:
    from cumulative_test_manager import CumulativeTestManager as Manager
```

---

## ğŸ”´ å¸¸è§é”™è¯¯æ¨¡å¼

### 1. 5.3æµ‹è¯•æ•°æ®æ±¡æŸ“é—®é¢˜ [NEW]

**ç—‡çŠ¶**: è¿è¡Œ5.3ç¼ºé™·å·¥ä½œæµæµ‹è¯•åï¼Œæ•°æ®åº“å‡ºç°ä¸ç›¸å…³çš„è®°å½•ï¼š
- ç®€åŒ–çš„"flawed"ï¼ˆåº”è¯¥æ˜¯å…·ä½“ç±»å‹å¦‚"flawed_sequence_disorder"ï¼‰  
- ä¸ç›¸å…³çš„"baseline"ã€"optimal"è®°å½•
- task_typeä¸º"unknown"çš„è®°å½•

**æ ¹æœ¬åŸå› **: smart_batch_runner.pyä¸­æœ‰å¤šå¤„å°†prompt_typeç®€åŒ–ï¼š
```python
# é”™è¯¯ä»£ç ï¼ˆå‡ºç°åœ¨ä¸¤å¤„ï¼‰
prompt_type="flawed" if is_flawed else prompt_type
```

**è§£å†³æ–¹æ¡ˆ**:
1. ä¿®å¤æ‰€æœ‰ç®€åŒ–ç‚¹ï¼ˆç¬¬220è¡Œå’Œç¬¬655è¡Œï¼‰ï¼š
```python
# æ­£ç¡®ä»£ç 
prompt_type=prompt_type  # ä¿æŒåŸå§‹å€¼
```

2. æ¸…ç†æ±¡æŸ“æ•°æ®ï¼š
```python
# æ¸…ç†Parquet
df = pd.read_parquet('pilot_bench_parquet_data/test_results.parquet')
bad_mask = (
    (df['model'] == 'DeepSeek-V3-0324') & 
    (df['prompt_type'] == 'flawed')  # ç®€åŒ–çš„flawed
)
df_clean = df[~bad_mask]
df_clean.to_parquet('pilot_bench_parquet_data/test_results.parquet')
```

3. éªŒè¯ä¿®å¤ï¼š
```bash
python smart_batch_runner.py --model gpt-4o-mini \
  --prompt-types flawed_sequence_disorder \
  --difficulty easy --task-types simple_task --num-instances 1
```

**ç›¸å…³æ–‡æ¡£**: [FIX-20250817-003](./debug_logs/2025-08-17_flawed_prompt_fix.md)

### 2. AttributeErrorç³»åˆ—

#### ExecutionStateç¼ºå°‘å±æ€§
```python
# âŒ é”™è¯¯
state.format_error_count  # AttributeError

# âœ… ä¿®å¤
count = getattr(state, 'format_error_count', 0)

# âœ… é˜²å¾¡æ€§ç¼–ç¨‹
if hasattr(state, 'format_error_count'):
    count = state.format_error_count
else:
    count = 0
```

#### TestRecordå­—æ®µç¼ºå¤±
```python
# âŒ é”™è¯¯
record.execution_status  # AttributeError

# âœ… ä¿®å¤ï¼šåˆå§‹åŒ–æ—¶è®¾ç½®æ‰€æœ‰å­—æ®µ
def create_test_record(**kwargs):
    record = TestRecord()
    # è®¾ç½®æ‰€æœ‰å¿…éœ€å­—æ®µ
    record.execution_status = kwargs.get('execution_status', 'unknown')
    record.format_error_count = kwargs.get('format_error_count', 0)
    record.execution_time = kwargs.get('execution_time', 0.0)
    return record
```

### 2. KeyErrorç³»åˆ—

#### å­—å…¸é”®ä¸å­˜åœ¨
```python
# âŒ é”™è¯¯
value = data['missing_key']  # KeyError

# âœ… ä¿®å¤1ï¼šä½¿ç”¨get()
value = data.get('missing_key', default_value)

# âœ… ä¿®å¤2ï¼šæ£€æŸ¥é”®å­˜åœ¨
if 'missing_key' in data:
    value = data['missing_key']
else:
    value = default_value

# âœ… ä¿®å¤3ï¼šä½¿ç”¨try-except
try:
    value = data['missing_key']
except KeyError:
    value = default_value
```

### 3. ImportErrorç³»åˆ—

#### Parquetä¾èµ–ç¼ºå¤±
```python
# âŒ é”™è¯¯
import pyarrow  # ImportError: No module named 'pyarrow'

# âœ… ä¿®å¤ï¼šæ¡ä»¶å¯¼å…¥
try:
    import pyarrow as pa
    import pyarrow.parquet as pq
    PARQUET_AVAILABLE = True
except ImportError:
    PARQUET_AVAILABLE = False
    print("Warning: Parquet support not available. Install: pip install pyarrow")
```

### 4. å¹¶å‘å†™å…¥å†²çª

#### JSONæ–‡ä»¶å¹¶å‘å†™å…¥
```python
# âŒ é”™è¯¯ï¼šå¤šè¿›ç¨‹åŒæ—¶å†™å…¥
with open('data.json', 'w') as f:
    json.dump(data, f)

# âœ… ä¿®å¤ï¼šä½¿ç”¨æ–‡ä»¶é”
import fcntl

def safe_json_write(filepath, data):
    with open(filepath, 'r+') as f:
        fcntl.flock(f.fileno(), fcntl.LOCK_EX)
        try:
            json.dump(data, f)
        finally:
            fcntl.flock(f.fileno(), fcntl.LOCK_UN)

# âœ… æ›´å¥½çš„æ–¹æ¡ˆï¼šä½¿ç”¨Parquetå¢é‡å†™å…¥
# æ¯ä¸ªè¿›ç¨‹å†™å…¥ç‹¬ç«‹çš„å¢é‡æ–‡ä»¶
```

---

## ğŸŒ APIè¿æ¥é—®é¢˜

### Azure OpenAIè¶…æ—¶
**ç—‡çŠ¶**: 
```
openai._base_client - INFO - Retrying request to /chat/completions
Batch execution timeout, cancelling remaining tasks
```

**è¯Šæ–­**:
```python
# æµ‹è¯•è¿æ¥
python test_deepseek_api.py

# æ£€æŸ¥é…ç½®
cat config/config.json | grep -A5 "DeepSeek"
```

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. å¢åŠ è¶…æ—¶æ—¶é—´
response = client.chat.completions.create(
    model=model,
    messages=messages,
    timeout=60  # å¢åŠ åˆ°60ç§’
)

# 2. æ·»åŠ é‡è¯•æœºåˆ¶
from tenacity import retry, stop_after_attempt, wait_exponential

@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))
def call_api(client, model, messages):
    return client.chat.completions.create(
        model=model,
        messages=messages,
        timeout=30
    )
```

### æ¨¡å‹é…ç½®é”™è¯¯
**ç—‡çŠ¶**: "Model XXX is not configured for user_azure"

**è¯Šæ–­**:
```bash
# æ£€æŸ¥æ¨¡å‹é…ç½®
python -c "
import json
with open('config/config.json') as f:
    config = json.load(f)
    print('Configured models:', list(config.get('model_configs', {}).keys())[:5])
"
```

**è§£å†³æ–¹æ¡ˆ**:
1. æ·»åŠ åˆ°config.jsonçš„model_configséƒ¨åˆ†
2. ç¡®ä¿providerå­—æ®µæ­£ç¡®è®¾ç½®
3. æ£€æŸ¥APIå¯†é’¥æ˜¯å¦é…ç½®

---

## ğŸ“Š æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥

### JSONæ•°æ®éªŒè¯
```python
# éªŒè¯master_database.json
def validate_json_database():
    import json
    from pathlib import Path
    
    db_path = Path("pilot_bench_cumulative_results/master_database.json")
    
    with open(db_path) as f:
        data = json.load(f)
    
    # æ£€æŸ¥å¿…éœ€å­—æ®µ
    assert 'version' in data
    assert 'models' in data
    assert 'summary' in data
    
    # éªŒè¯æ•°æ®ä¸€è‡´æ€§
    total_tests = 0
    for model_data in data['models'].values():
        total_tests += model_data.get('total_tests', 0)
    
    # æ¯”è¾ƒæ±‡æ€»
    summary_total = data['summary'].get('total_tests', 0)
    if total_tests != summary_total:
        print(f"âš ï¸ æ•°æ®ä¸ä¸€è‡´: è®¡ç®—={total_tests}, æ±‡æ€»={summary_total}")
    
    return data
```

### Parquetæ•°æ®éªŒè¯
```python
# éªŒè¯Parquetæ•°æ®å®Œæ•´æ€§
def validate_parquet_data():
    import pandas as pd
    from pathlib import Path
    
    parquet_file = Path("pilot_bench_parquet_data/test_results.parquet")
    
    if not parquet_file.exists():
        print("âŒ Parquetæ–‡ä»¶ä¸å­˜åœ¨")
        return False
    
    df = pd.read_parquet(parquet_file)
    
    # æ£€æŸ¥å¿…éœ€åˆ—
    required_cols = ['model', 'task_type', 'prompt_type', 'success']
    missing = [col for col in required_cols if col not in df.columns]
    if missing:
        print(f"âŒ ç¼ºå°‘åˆ—: {missing}")
        return False
    
    # æ£€æŸ¥æ•°æ®ç±»å‹
    if df['success'].dtype != bool:
        print(f"âš ï¸ successåˆ—ç±»å‹é”™è¯¯: {df['success'].dtype}")
    
    # æ£€æŸ¥ç©ºå€¼
    nulls = df[required_cols].isnull().sum()
    if nulls.any():
        print(f"âš ï¸ å‘ç°ç©ºå€¼:\n{nulls[nulls > 0]}")
    
    print(f"âœ… Parquetæ•°æ®éªŒè¯é€šè¿‡: {len(df)} æ¡è®°å½•")
    return True
```

---

## âš¡ æ€§èƒ½è°ƒè¯•

### å†…å­˜æ³„æ¼æ£€æµ‹
```python
import tracemalloc
import gc

# å¼€å§‹è·Ÿè¸ª
tracemalloc.start()

# ... è¿è¡Œæµ‹è¯• ...

# è·å–å†…å­˜å¿«ç…§
snapshot = tracemalloc.take_snapshot()
top_stats = snapshot.statistics('lineno')

# æ˜¾ç¤ºå‰10ä¸ªå†…å­˜æ¶ˆè€—
for stat in top_stats[:10]:
    print(stat)

# å¼ºåˆ¶åƒåœ¾å›æ”¶
gc.collect()
```

### å¹¶å‘æ€§èƒ½åˆ†æ
```python
import time
import concurrent.futures

def profile_concurrent_execution(func, tasks, max_workers):
    start = time.time()
    
    with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
        futures = [executor.submit(func, task) for task in tasks]
        results = [f.result() for f in concurrent.futures.as_completed(futures)]
    
    elapsed = time.time() - start
    throughput = len(tasks) / elapsed
    
    print(f"å¤„ç† {len(tasks)} ä¸ªä»»åŠ¡")
    print(f"è€—æ—¶: {elapsed:.2f}ç§’")
    print(f"ååé‡: {throughput:.1f} ä»»åŠ¡/ç§’")
    print(f"å¹³å‡å»¶è¿Ÿ: {elapsed/len(tasks)*1000:.1f}ms")
    
    return results
```

### ç“¶é¢ˆå®šä½
```bash
# ä½¿ç”¨cProfileåˆ†æ
python -m cProfile -s cumulative smart_batch_runner.py --model gpt-4o-mini ...

# ç”Ÿæˆç«ç„°å›¾
py-spy record -d 30 -f flamegraph.svg -- python smart_batch_runner.py ...

# ç›‘æ§èµ„æºä½¿ç”¨
htop  # CPUå’Œå†…å­˜
iotop  # ç£ç›˜I/O
```

---

## ğŸš¨ ç´§æ€¥ä¿®å¤æµç¨‹

### 1. æ•°æ®æŸåæ¢å¤
```bash
# æ­¥éª¤1: å¤‡ä»½å½“å‰æ•°æ®
cp pilot_bench_cumulative_results/master_database.json master_database.backup.$(date +%Y%m%d_%H%M%S).json

# æ­¥éª¤2: éªŒè¯å¤‡ä»½
python -c "import json; json.load(open('master_database.backup.*.json'))"

# æ­¥éª¤3: ä»å¤‡ä»½æ¢å¤
cp master_database.backup.20250817_123456.json pilot_bench_cumulative_results/master_database.json

# æ­¥éª¤4: è½¬æ¢åˆ°Parquetï¼ˆæ•°æ®å®‰å…¨ï¼‰
python json_to_parquet_converter.py
```

### 2. APIæ•…éšœåˆ‡æ¢
```python
# è‡ªåŠ¨æ•…éšœè½¬ç§»
class APIFailover:
    def __init__(self):
        self.endpoints = [
            {"name": "primary", "url": "https://primary.api.com"},
            {"name": "backup", "url": "https://backup.api.com"},
            {"name": "fallback", "url": "https://fallback.api.com"}
        ]
        self.current = 0
    
    def get_client(self):
        for _ in range(len(self.endpoints)):
            endpoint = self.endpoints[self.current]
            try:
                client = create_client(endpoint['url'])
                # æµ‹è¯•è¿æ¥
                test_connection(client)
                return client
            except Exception as e:
                print(f"âŒ {endpoint['name']} å¤±è´¥: {e}")
                self.current = (self.current + 1) % len(self.endpoints)
        
        raise Exception("æ‰€æœ‰ç«¯ç‚¹éƒ½å¤±è´¥")
```

### 3. è¿›ç¨‹æ¸…ç†
```bash
# æŸ¥æ‰¾å¡ä½çš„è¿›ç¨‹
ps aux | grep python | grep batch_test

# ç»ˆæ­¢æ‰€æœ‰æµ‹è¯•è¿›ç¨‹
pkill -f "batch_test"

# æ¸…ç†é”æ–‡ä»¶
rm -f /tmp/*.lock

# æ¸…ç†ä¸´æ—¶æ–‡ä»¶
rm -rf pilot_bench_parquet_data/incremental/temp_*
```

---

## ğŸ“ˆ ç›‘æ§å’Œå‘Šè­¦

### å®æ—¶ç›‘æ§è„šæœ¬
```python
#!/usr/bin/env python3
"""å®æ—¶ç›‘æ§æµ‹è¯•è¿›åº¦"""

import time
import pandas as pd
from pathlib import Path

def monitor_progress():
    while True:
        # æ£€æŸ¥å¢é‡æ–‡ä»¶
        incremental_dir = Path("pilot_bench_parquet_data/incremental")
        files = list(incremental_dir.glob("*.parquet"))
        
        if files:
            total_records = 0
            for f in files:
                df = pd.read_parquet(f)
                total_records += len(df)
            
            print(f"\rå¢é‡æ–‡ä»¶: {len(files)}, è®°å½•æ•°: {total_records}", end="")
        
        time.sleep(5)

if __name__ == "__main__":
    monitor_progress()
```

### å¥åº·æ£€æŸ¥
```python
def health_check():
    """ç³»ç»Ÿå¥åº·æ£€æŸ¥"""
    checks = {
        "JSONæ•°æ®åº“": check_json_database(),
        "Parquetæ•°æ®": check_parquet_data(),
        "APIè¿æ¥": check_api_connection(),
        "ç£ç›˜ç©ºé—´": check_disk_space(),
        "å†…å­˜ä½¿ç”¨": check_memory_usage()
    }
    
    for name, status in checks.items():
        emoji = "âœ…" if status else "âŒ"
        print(f"{emoji} {name}: {'æ­£å¸¸' if status else 'å¼‚å¸¸'}")
    
    return all(checks.values())
```

---

## ğŸ”§ è°ƒè¯•å·¥å…·ç®±

### å¿«é€Ÿè¯Šæ–­å‘½ä»¤
```bash
# æŸ¥çœ‹æœ€æ–°é”™è¯¯
tail -n 100 logs/batch_test_*.log | grep ERROR

# ç»Ÿè®¡é”™è¯¯ç±»å‹
grep "ERROR" logs/*.log | cut -d: -f4- | sort | uniq -c | sort -rn

# æŸ¥çœ‹è¿›ç¨‹çŠ¶æ€
ps aux | grep python | grep -E "(batch|test|runner)"

# æ£€æŸ¥ç«¯å£å ç”¨
lsof -i :8080

# æŸ¥çœ‹ç³»ç»Ÿèµ„æº
df -h  # ç£ç›˜ç©ºé—´
free -h  # å†…å­˜
top -b -n 1  # CPU
```

### è°ƒè¯•ç¯å¢ƒå˜é‡
```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
export DEBUG=1
export LOG_LEVEL=DEBUG

# ä½¿ç”¨Parquetæ¨¡å¼
export STORAGE_FORMAT=parquet

# ç¦ç”¨å¹¶å‘ï¼ˆè°ƒè¯•ç”¨ï¼‰
export MAX_WORKERS=1

# å¯ç”¨æ€§èƒ½åˆ†æ
export PROFILE=1
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [COMMON_ISSUES.md](./COMMON_ISSUES.md) - å¸¸è§é—®é¢˜è§£å†³æ–¹æ¡ˆ
- [PARQUET_GUIDE.md](../guides/PARQUET_GUIDE.md) - Parquetä½¿ç”¨æŒ‡å—
- [API_TROUBLESHOOTING.md](../api/API_TROUBLESHOOTING.md) - APIæ•…éšœæ’é™¤
- [PERFORMANCE_TUNING.md](./PERFORMANCE_TUNING.md) - æ€§èƒ½è°ƒä¼˜æŒ‡å—

---

**æ–‡æ¡£ç‰ˆæœ¬**: 2.0  
**åˆ›å»ºæ—¶é—´**: 2025-08-17  
**ç»´æŠ¤è€…**: System Administrator  
**çŠ¶æ€**: ğŸŸ¢ Active | âœ… å·²æ›´æ–°