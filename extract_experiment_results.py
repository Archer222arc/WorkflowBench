#!/usr/bin/env python3
"""
WorkflowBenchå®éªŒç»“æœæ•°æ®æå–è„šæœ¬ - é‡è¦æ ¸å¿ƒè„šæœ¬

åŠŸèƒ½: ä»master_database.jsonæå–å¹¶è®¡ç®—5.1/5.2/5.3å®éªŒè¡¨æ ¼æ•°æ®
ç‰¹ç‚¹: è‡ªåŠ¨æ’é™¤timeoutå¤±è´¥ï¼Œç¡®ä¿æ•°æ®å‡†ç¡®æ€§
ç”¨é€”: ç”Ÿæˆæ ‡å‡†åŒ–çš„å®éªŒç»“æœè¡¨æ ¼ç”¨äºè®ºæ–‡å’ŒæŠ¥å‘Š

ä½œè€…: Claude Assistant (WorkflowBenché¡¹ç›®)
åˆ›å»ºæ—¶é—´: 2025-08-30
æœ€åæ›´æ–°: 2025-08-30
ç‰ˆæœ¬: 1.0.0

é‡è¦æ€§çº§åˆ«: â˜…â˜…â˜…â˜…â˜… (æ ¸å¿ƒæ•°æ®å¤„ç†è„šæœ¬)
"""

import json
import sys
from pathlib import Path
from typing import Dict, List, Optional, Tuple


class WorkflowBenchDataExtractor:
    """WorkflowBenchæµ‹è¯•æ•°æ®æå–å™¨"""
    
    def __init__(self, db_path: str = "pilot_bench_cumulative_results/master_database.json"):
        """åˆå§‹åŒ–æ•°æ®æå–å™¨"""
        self.db_path = Path(db_path)
        self.db = self._load_database()
        
        # æ¨¡å‹åˆ†ç»„
        self.baseline_models = [
            'DeepSeek-V3-0324',
            'Llama-3.3-70B-Instruct', 
            'DeepSeek-R1-0528',
            'qwen2.5-3b-instruct',
            'qwen2.5-14b-instruct',
            'qwen2.5-7b-instruct',
            'qwen2.5-32b-instruct',
            'qwen2.5-72b-instruct'
        ]
        
        self.qwen_models = [
            'qwen2.5-3b-instruct',
            'qwen2.5-7b-instruct', 
            'qwen2.5-14b-instruct',
            'qwen2.5-32b-instruct',
            'qwen2.5-72b-instruct'
        ]
        
        # ç¼ºé™·ç±»å‹
        self.flawed_types = [
            'flawed_sequence_disorder',
            'flawed_tool_misuse', 
            'flawed_parameter_error',
            'flawed_missing_step',
            'flawed_redundant_operations',
            'flawed_logical_inconsistency',
            'flawed_semantic_drift'
        ]

    def _load_database(self) -> dict:
        """åŠ è½½æ•°æ®åº“"""
        if not self.db_path.exists():
            raise FileNotFoundError(f"Database not found: {self.db_path}")
        
        with open(self.db_path, 'r', encoding='utf-8') as f:
            return json.load(f)

    def _calculate_timeout_excluded_stats(self, model: str, prompt_type: str = 'optimal', 
                                        difficulty: str = 'easy', tool_rate: str = '0.8') -> Optional[Dict]:
        """è®¡ç®—æ’é™¤timeoutåçš„æ¨¡å‹ç»Ÿè®¡æ•°æ®
        
        å…³é”®é€»è¾‘:
        1. timeout_failures = min(timeout_errors, failed) - ç¡®ä¿ä¸è¶…è¿‡å®é™…å¤±è´¥æ•°
        2. effective_total = original_total - timeout_failures - ä»æ€»æ•°ä¸­æ’é™¤timeoutå¤±è´¥
        3. success_rate = success / effective_total - åŸºäºæœ‰æ•ˆæ€»æ•°è®¡ç®—
        """
        if model not in self.db['models']:
            return None
        
        model_data = self.db['models'][model]
        
        try:
            prompt_data = model_data['by_prompt_type'][prompt_type]
            rate_data = prompt_data['by_tool_success_rate'][tool_rate]
            diff_data = rate_data['by_difficulty'][difficulty]
            
            # èšåˆæ‰€æœ‰ä»»åŠ¡ç±»å‹çš„æ•°æ®
            total_tests = 0
            total_success = 0
            total_partial = 0
            total_failed = 0
            total_timeout_errors = 0
            
            task_breakdown = {}
            
            for task_type, task_data in diff_data['by_task_type'].items():
                task_total = task_data.get('total', 0)
                task_success = task_data.get('success', 0)
                task_partial = task_data.get('partial', 0)
                task_failed = task_data.get('failed', 0)
                task_timeout = task_data.get('timeout_errors', 0)
                
                # è®¡ç®—è¯¥ä»»åŠ¡çš„timeoutå¤±è´¥æ•°
                task_timeout_failures = min(task_timeout, task_failed)
                task_effective_total = task_total - task_timeout_failures
                
                task_success_rate = task_success / task_effective_total * 100 if task_effective_total > 0 else 0.0
                
                task_breakdown[task_type] = {
                    'original_total': task_total,
                    'effective_total': task_effective_total,
                    'success': task_success,
                    'success_rate': task_success_rate,
                    'timeout_failures': task_timeout_failures
                }
                
                total_tests += task_total
                total_success += task_success
                total_partial += task_partial
                total_failed += task_failed
                total_timeout_errors += task_timeout
            
            # è®¡ç®—æ¨¡å‹çº§åˆ«çš„ç»Ÿè®¡
            total_timeout_failures = min(total_timeout_errors, total_failed)
            effective_total = total_tests - total_timeout_failures
            non_timeout_failed = total_failed - total_timeout_failures
            
            if effective_total > 0:
                full_success = total_success - total_partial
                
                total_success_rate = total_success / effective_total * 100
                full_success_rate = full_success / effective_total * 100
                partial_success_rate = total_partial / effective_total * 100
                failure_rate = non_timeout_failed / effective_total * 100
                
                return {
                    'original_total': total_tests,
                    'effective_total': effective_total,
                    'total_success': total_success,
                    'partial_success': total_partial,
                    'full_success': full_success,
                    'non_timeout_failed': non_timeout_failed,
                    'timeout_failures': total_timeout_failures,
                    'total_success_rate': total_success_rate,
                    'full_success_rate': full_success_rate,
                    'partial_success_rate': partial_success_rate,
                    'failure_rate': failure_rate,
                    'task_breakdown': task_breakdown
                }
            
        except KeyError as e:
            print(f"Data not found for {model} - {prompt_type} - {difficulty}: {e}")
            return None

    def extract_5_1_baseline_results(self) -> Dict:
        """æå–5.1åŸºå‡†æµ‹è¯•ç»“æœ (optimal, easy, 0.8)"""
        print("ğŸ“Š æå–5.1åŸºå‡†æµ‹è¯•ç»“æœ (æ’é™¤timeoutå¤±è´¥)")
        print("=" * 60)
        
        results = []
        total_stats = {
            'original_total': 0,
            'effective_total': 0,
            'total_success': 0,
            'total_partial': 0,
            'total_failed': 0,
            'timeout_failures': 0
        }
        
        for model in self.baseline_models:
            stats = self._calculate_timeout_excluded_stats(model)
            if stats:
                print(f"âœ… {model}: {stats['total_success_rate']:.1f}% æˆåŠŸç‡ "
                      f"({stats['effective_total']} æœ‰æ•ˆæµ‹è¯•, æ’é™¤ {stats['timeout_failures']} timeout)")
                
                results.append({
                    'model': model,
                    'total_success_rate': stats['total_success_rate'],
                    'full_success_rate': stats['full_success_rate'],
                    'partial_success_rate': stats['partial_success_rate'],
                    'failure_rate': stats['failure_rate'],
                    'effective_total': stats['effective_total'],
                    'timeout_failures': stats['timeout_failures']
                })
                
                # ç´¯è®¡ç»Ÿè®¡
                total_stats['original_total'] += stats['original_total']
                total_stats['effective_total'] += stats['effective_total']
                total_stats['total_success'] += stats['total_success']
                total_stats['total_partial'] += stats['partial_success']
                total_stats['total_failed'] += stats['non_timeout_failed']
                total_stats['timeout_failures'] += stats['timeout_failures']
            else:
                print(f"âŒ {model}: æ•°æ®æœªæ‰¾åˆ°")
        
        # è®¡ç®—æ€»ä½“ç»Ÿè®¡
        if total_stats['effective_total'] > 0:
            total_stats['overall_success_rate'] = total_stats['total_success'] / total_stats['effective_total'] * 100
            total_stats['overall_full_success_rate'] = (total_stats['total_success'] - total_stats['total_partial']) / total_stats['effective_total'] * 100
            total_stats['overall_failure_rate'] = total_stats['total_failed'] / total_stats['effective_total'] * 100
        
        return {
            'results': results,
            'summary': total_stats
        }

    def extract_5_2_qwen_scale_results(self) -> Dict:
        """æå–5.2 Qwenè§„æ¨¡æ•ˆåº”ç»“æœ (optimal, very_easy/medium, 0.8)"""
        print("\nğŸ“Š æå–5.2 Qwenè§„æ¨¡æ•ˆåº”ç»“æœ (æ’é™¤timeoutå¤±è´¥)")
        print("=" * 60)
        
        param_counts = {
            'qwen2.5-3b-instruct': 3,
            'qwen2.5-7b-instruct': 7,
            'qwen2.5-14b-instruct': 14,
            'qwen2.5-32b-instruct': 32,
            'qwen2.5-72b-instruct': 72
        }
        
        task_types = ['simple_task', 'basic_task', 'data_pipeline', 'api_integration', 'multi_stage_pipeline']
        
        results = {}
        
        for difficulty in ['very_easy', 'medium']:
            print(f"\n### {difficulty.upper()} éš¾åº¦:")
            difficulty_results = []
            
            for model in self.qwen_models:
                stats = self._calculate_timeout_excluded_stats(model, difficulty=difficulty)
                if stats:
                    param_count = param_counts[model]
                    overall_rate = stats['total_success_rate']
                    efficiency_score = overall_rate / (param_count ** 0.5) if param_count > 0 else 0
                    
                    # æå–ä»»åŠ¡ç‰¹å®šæˆåŠŸç‡
                    task_rates = []
                    for task_type in task_types:
                        if task_type in stats['task_breakdown']:
                            rate = stats['task_breakdown'][task_type]['success_rate']
                        else:
                            rate = 0.0
                        task_rates.append(rate)
                    
                    difficulty_results.append({
                        'model': model,
                        'param_count': param_count,
                        'task_rates': task_rates,
                        'overall_rate': overall_rate,
                        'efficiency_score': efficiency_score,
                        'timeout_failures': stats['timeout_failures']
                    })
                    
                    print(f"âœ… {model}: {overall_rate:.1f}% æ•´ä½“æˆåŠŸç‡ "
                          f"(æ’é™¤ {stats['timeout_failures']} timeout)")
                else:
                    print(f"âŒ {model}: æ•°æ®æœªæ‰¾åˆ°")
            
            results[difficulty] = difficulty_results
        
        return results

    def extract_5_3_flawed_workflow_results(self) -> Dict:
        """æå–5.3ç¼ºé™·å·¥ä½œæµé€‚åº”æ€§ç»“æœ"""
        print("\nğŸ“Š æå–5.3ç¼ºé™·å·¥ä½œæµé€‚åº”æ€§ç»“æœ (æ’é™¤timeoutå¤±è´¥)")
        print("=" * 60)
        
        results = {}
        
        for model in self.baseline_models:
            print(f"\n### {model}:")
            model_results = {}
            model_total_tests = 0
            model_total_corrections = 0
            model_total_timeout_failures = 0
            
            for flawed_type in self.flawed_types:
                stats = self._calculate_timeout_excluded_stats(model, prompt_type=flawed_type)
                if stats:
                    correction_rate = stats['total_success_rate']
                    timeout_failures = stats['timeout_failures']
                    
                    if timeout_failures > 0:
                        print(f"  {flawed_type}: {correction_rate:.1f}% "
                              f"({stats['total_success']}/{stats['effective_total']}) "
                              f"[æ’é™¤{timeout_failures}ä¸ªtimeout]")
                    else:
                        print(f"  {flawed_type}: {correction_rate:.1f}% "
                              f"({stats['total_success']}/{stats['original_total']})")
                    
                    model_results[flawed_type] = correction_rate
                    model_total_tests += stats['effective_total']
                    model_total_corrections += stats['total_success']
                    model_total_timeout_failures += timeout_failures
                else:
                    model_results[flawed_type] = 0.0
                    print(f"  {flawed_type}: âŒ æ•°æ®æœªæ‰¾åˆ°")
            
            # è®¡ç®—å¹³å‡é€‚åº”æ€§å¾—åˆ†
            if model_total_tests > 0:
                avg_score = model_total_corrections / model_total_tests * 100
                model_results['average'] = avg_score
                
                if model_total_timeout_failures > 0:
                    print(f"  ğŸ”´ æ’é™¤timeoutå¤±è´¥: {model_total_timeout_failures}ä¸ª")
                    print(f"  å¹³å‡é€‚åº”æ€§å¾—åˆ†: {avg_score:.1f}% "
                          f"({model_total_corrections}/{model_total_tests}) [æ’é™¤timeoutå]")
                else:
                    print(f"  âœ… æ— timeoutéœ€æ’é™¤")
                    print(f"  å¹³å‡é€‚åº”æ€§å¾—åˆ†: {avg_score:.1f}% "
                          f"({model_total_corrections}/{model_total_tests})")
            else:
                model_results['average'] = 0.0
                print(f"  å¹³å‡é€‚åº”æ€§å¾—åˆ†: 0.0%")
            
            results[model] = model_results
        
        return results

    def generate_markdown_tables(self, output_dir: str = "docs/analysis/generated/"):
        """ç”Ÿæˆmarkdownæ ¼å¼çš„è¡¨æ ¼æ–‡ä»¶"""
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        # ç”Ÿæˆ5.1è¡¨æ ¼
        baseline_results = self.extract_5_1_baseline_results()
        self._write_5_1_table(baseline_results, output_path / "5.1_baseline_results_auto.md")
        
        # ç”Ÿæˆ5.2è¡¨æ ¼
        qwen_results = self.extract_5_2_qwen_scale_results()
        self._write_5_2_table(qwen_results, output_path / "5.2_qwen_scale_results_auto.md")
        
        # ç”Ÿæˆ5.3è¡¨æ ¼
        flawed_results = self.extract_5_3_flawed_workflow_results()
        self._write_5_3_table(flawed_results, output_path / "5.3_flawed_workflow_results_auto.md")
        
        print(f"\nâœ… æ‰€æœ‰è¡¨æ ¼å·²ç”Ÿæˆåˆ°: {output_path}")

    def _write_5_1_table(self, data: Dict, output_file: Path):
        """å†™å…¥5.1åŸºå‡†æµ‹è¯•è¡¨æ ¼"""
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("# 5.1 åŸºå‡†æµ‹è¯•ç»“æœè¡¨ (è‡ªåŠ¨ç”Ÿæˆ - æ’é™¤timeoutå¤±è´¥)\n\n")
            f.write("| æ¨¡å‹åç§° | æ€»ä½“æˆåŠŸç‡ | å®Œå…¨æˆåŠŸç‡ | éƒ¨åˆ†æˆåŠŸç‡ | å¤±è´¥ç‡ | æœ‰æ•ˆæµ‹è¯•æ•° | æ’é™¤timeoutæ•° |\n")
            f.write("|---------|-----------|-----------|-----------|-------|-----------|-------------|\n")
            
            for result in data['results']:
                f.write(f"| **{result['model']}** | "
                       f"{result['total_success_rate']:.1f}% | "
                       f"{result['full_success_rate']:.1f}% | "
                       f"{result['partial_success_rate']:.1f}% | "
                       f"{result['failure_rate']:.1f}% | "
                       f"{result['effective_total']} | "
                       f"{result['timeout_failures']} |\n")
            
            summary = data['summary']
            avg_total = len(data['results'])
            f.write(f"| **å¹³å‡å€¼** | "
                   f"{summary['overall_success_rate']:.1f}% | "
                   f"{summary['overall_full_success_rate']:.1f}% | "
                   f"{(summary['total_partial'] / summary['effective_total'] * 100):.1f}% | "
                   f"{summary['overall_failure_rate']:.1f}% | "
                   f"{summary['effective_total'] // avg_total} | "
                   f"{summary['timeout_failures'] // avg_total} |\n")

    def _write_5_2_table(self, data: Dict, output_file: Path):
        """å†™å…¥5.2 Qwenè§„æ¨¡æ•ˆåº”è¡¨æ ¼"""
        task_names = ['ç®€å•ä»»åŠ¡æˆåŠŸç‡', 'åŸºç¡€ä»»åŠ¡æˆåŠŸç‡', 'æ•°æ®ç®¡é“æˆåŠŸç‡', 'APIé›†æˆæˆåŠŸç‡', 'å¤šé˜¶æ®µç®¡é“æˆåŠŸç‡']
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("# 5.2 Qwenè§„æ¨¡æ•ˆåº”æµ‹è¯•è¡¨ (è‡ªåŠ¨ç”Ÿæˆ - æ’é™¤timeoutå¤±è´¥)\n\n")
            
            for difficulty, results in data.items():
                f.write(f"## {difficulty.title()}éš¾åº¦æ€§èƒ½\n\n")
                f.write("| æ¨¡å‹è§„æ¨¡ | å‚æ•°é‡ | " + " | ".join(task_names) + " | æ•´ä½“æˆåŠŸç‡ | æ¯å‚æ•°æ•ˆç‡å¾—åˆ† | æ’é™¤timeoutæ•° |\n")
                f.write("|---------|-------|" + "---|" * len(task_names) + "-----------|--------------|-------------|\n")
                
                for result in results:
                    param = result['param_count']
                    rates = [f"{rate:.1f}%" for rate in result['task_rates']]
                    f.write(f"| **Qwen2.5-{param}B-Instruct** | {param}B | " + 
                           " | ".join(rates) + f" | {result['overall_rate']:.1f}% | " +
                           f"{result['efficiency_score']:.4f} | {result['timeout_failures']} |\n")
                f.write("\n")

    def _write_5_3_table(self, data: Dict, output_file: Path):
        """å†™å…¥5.3ç¼ºé™·å·¥ä½œæµé€‚åº”æ€§è¡¨æ ¼"""
        flawed_names = ['é¡ºåºé”™è¯¯çº æ­£ç‡', 'å·¥å…·è¯¯ç”¨çº æ­£ç‡', 'å‚æ•°é”™è¯¯çº æ­£ç‡', 'ç¼ºå¤±æ­¥éª¤è¡¥å…¨ç‡', 
                       'å†—ä½™æ“ä½œè¯†åˆ«ç‡', 'é€»è¾‘ä¸è¿ç»­ä¿®å¤ç‡', 'è¯­ä¹‰æ¼‚ç§»çº æ­£ç‡']
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("# 5.3 ç¼ºé™·å·¥ä½œæµé€‚åº”æ€§æµ‹è¯•è¡¨ (è‡ªåŠ¨ç”Ÿæˆ - æ’é™¤timeoutå¤±è´¥)\n\n")
            f.write("| æ¨¡å‹åç§° | " + " | ".join(flawed_names) + " | å¹³å‡é€‚åº”æ€§å¾—åˆ† |\n")
            f.write("|---------|" + "-------------|" * len(flawed_names) + "-------------|\n")
            
            for model, results in data.items():
                rates = []
                for flawed_type in self.flawed_types:
                    rate = results.get(flawed_type, 0.0)
                    rates.append(f"{rate:.1f}%")
                
                avg_rate = results.get('average', 0.0)
                f.write(f"| **{model}** | " + " | ".join(rates) + f" | {avg_rate:.1f}% |\n")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ WorkflowBenchå®éªŒç»“æœæ•°æ®æå–å·¥å…·")
    print("=" * 60)
    
    # æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶
    db_path = "pilot_bench_cumulative_results/master_database.json"
    if not Path(db_path).exists():
        print(f"âŒ æ•°æ®åº“æ–‡ä»¶æœªæ‰¾åˆ°: {db_path}")
        sys.exit(1)
    
    # åˆ›å»ºæå–å™¨
    extractor = WorkflowBenchDataExtractor(db_path)
    
    if len(sys.argv) > 1:
        command = sys.argv[1]
        
        if command == "5.1":
            results = extractor.extract_5_1_baseline_results()
            print("\nğŸ“‹ 5.1åŸºå‡†æµ‹è¯•è¡¨æ ¼æ•°æ®:")
            for result in results['results']:
                print(f"| **{result['model']}** | {result['total_success_rate']:.1f}% | "
                      f"{result['full_success_rate']:.1f}% | {result['partial_success_rate']:.1f}% | "
                      f"{result['failure_rate']:.1f}% | {result['effective_total']} | {result['timeout_failures']} |")
                      
        elif command == "5.2":
            results = extractor.extract_5_2_qwen_scale_results()
            print("\nğŸ“‹ 5.2 Qwenè§„æ¨¡æ•ˆåº”è¡¨æ ¼æ•°æ®å·²æå–")
            
        elif command == "5.3":
            results = extractor.extract_5_3_flawed_workflow_results()
            print("\nğŸ“‹ 5.3ç¼ºé™·å·¥ä½œæµé€‚åº”æ€§è¡¨æ ¼æ•°æ®å·²æå–")
            
        elif command == "generate":
            extractor.generate_markdown_tables()
            
        else:
            print(f"âŒ æœªçŸ¥å‘½ä»¤: {command}")
            print("å¯ç”¨å‘½ä»¤: 5.1, 5.2, 5.3, generate")
    else:
        # é»˜è®¤æå–æ‰€æœ‰æ•°æ®
        print("ğŸ“Š æå–æ‰€æœ‰å®éªŒç»“æœ...")
        extractor.extract_5_1_baseline_results()
        extractor.extract_5_2_qwen_scale_results()
        extractor.extract_5_3_flawed_workflow_results()
        
        print("\nğŸ¯ å¦‚éœ€ç”Ÿæˆmarkdownè¡¨æ ¼ï¼Œä½¿ç”¨: python extract_experiment_results.py generate")


if __name__ == "__main__":
    main()