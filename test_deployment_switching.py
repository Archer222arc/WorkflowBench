#!/usr/bin/env python3
"""
æµ‹è¯•éƒ¨ç½²åˆ‡æ¢åŠŸèƒ½
==================
éªŒè¯æ™ºèƒ½éƒ¨ç½²ç®¡ç†å™¨æ˜¯å¦èƒ½åœ¨429é”™è¯¯æ—¶æ­£ç¡®åˆ‡æ¢éƒ¨ç½²
"""

import sys
import time
import logging
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def test_client_creation_with_deployment_switching():
    """æµ‹è¯•å®¢æˆ·ç«¯åˆ›å»ºæ—¶çš„æ™ºèƒ½éƒ¨ç½²é€‰æ‹©"""
    print("ğŸ§ª æµ‹è¯•1: å®¢æˆ·ç«¯åˆ›å»ºæ—¶çš„æ™ºèƒ½éƒ¨ç½²é€‰æ‹©")
    print("=" * 50)
    
    from api_client_manager import get_client_for_model
    
    # æµ‹è¯•Llama-3.3-70B-Instructï¼ˆæœ‰å¤šéƒ¨ç½²é…ç½®ï¼‰
    print("ğŸ“¡ æµ‹è¯•Llama-3.3-70B-Instructéƒ¨ç½²é€‰æ‹©...")
    client = get_client_for_model("Llama-3.3-70B-Instruct")
    
    if hasattr(client, 'current_deployment'):
        print(f"âœ… å½“å‰éƒ¨ç½²: {client.current_deployment}")
        print(f"âœ… éƒ¨ç½²åç§°: {client.deployment_name}")
    else:
        print("âŒ å®¢æˆ·ç«¯ç¼ºå°‘current_deploymentå±æ€§")
    
    # æµ‹è¯•DeepSeek-V3-0324ï¼ˆä¹Ÿæœ‰å¤šéƒ¨ç½²é…ç½®ï¼‰
    print("\nğŸ“¡ æµ‹è¯•DeepSeek-V3-0324éƒ¨ç½²é€‰æ‹©...")
    client2 = get_client_for_model("DeepSeek-V3-0324")
    
    if hasattr(client2, 'current_deployment'):
        print(f"âœ… å½“å‰éƒ¨ç½²: {client2.current_deployment}")
        print(f"âœ… éƒ¨ç½²åç§°: {client2.deployment_name}")
    else:
        print("âŒ å®¢æˆ·ç«¯ç¼ºå°‘current_deploymentå±æ€§")

def test_deployment_manager():
    """æµ‹è¯•æ™ºèƒ½éƒ¨ç½²ç®¡ç†å™¨åŸºæœ¬åŠŸèƒ½"""
    print("\nğŸ§ª æµ‹è¯•2: æ™ºèƒ½éƒ¨ç½²ç®¡ç†å™¨åŸºæœ¬åŠŸèƒ½")
    print("=" * 50)
    
    from smart_deployment_manager import get_deployment_manager
    
    manager = get_deployment_manager()
    
    # æ˜¾ç¤ºå½“å‰çŠ¶æ€
    manager.print_status()
    
    # æµ‹è¯•è·å–æœ€ä½³éƒ¨ç½²
    print(f"\nğŸ¯ æµ‹è¯•éƒ¨ç½²é€‰æ‹©:")
    for i in range(3):
        llama_deployment = manager.get_best_deployment("Llama-3.3-70B-Instruct")
        deepseek_deployment = manager.get_best_deployment("DeepSeek-V3-0324")
        print(f"  è½®æ¬¡{i+1}: Llama -> {llama_deployment}, DeepSeek -> {deepseek_deployment}")
        time.sleep(0.1)

def test_429_error_simulation():
    """æ¨¡æ‹Ÿ429é”™è¯¯å’Œéƒ¨ç½²åˆ‡æ¢"""
    print("\nğŸ§ª æµ‹è¯•3: æ¨¡æ‹Ÿ429é”™è¯¯å’Œéƒ¨ç½²åˆ‡æ¢")
    print("=" * 50)
    
    from smart_deployment_manager import get_deployment_manager
    
    manager = get_deployment_manager()
    
    # æ¨¡æ‹Ÿ429é”™è¯¯
    print("ğŸš¨ æ¨¡æ‹ŸLlama-3.3-70B-Instructé‡åˆ°429é”™è¯¯...")
    
    # è·å–å½“å‰æœ€ä½³éƒ¨ç½²
    current_deployment = manager.get_best_deployment("Llama-3.3-70B-Instruct")
    print(f"å½“å‰ä½¿ç”¨éƒ¨ç½²: {current_deployment}")
    
    # æ ‡è®°ä¸º429å¤±è´¥
    manager.mark_deployment_failed(current_deployment, "429")
    print(f"æ ‡è®° {current_deployment} ä¸º429å¤±è´¥")
    
    # è·å–æ–°çš„æœ€ä½³éƒ¨ç½²
    new_deployment = manager.get_best_deployment("Llama-3.3-70B-Instruct")
    print(f"åˆ‡æ¢åéƒ¨ç½²: {new_deployment}")
    
    if new_deployment != current_deployment:
        print(f"âœ… æˆåŠŸåˆ‡æ¢éƒ¨ç½²: {current_deployment} -> {new_deployment}")
    else:
        print(f"âš ï¸  æ²¡æœ‰åˆ‡æ¢éƒ¨ç½²ï¼ˆå¯èƒ½æ²¡æœ‰å¯ç”¨çš„æ›¿ä»£éƒ¨ç½²ï¼‰")
    
    # æ˜¾ç¤ºæ›´æ–°åçŠ¶æ€
    print(f"\nğŸ“Š æ›´æ–°åçš„çŠ¶æ€:")
    manager.print_status()

def test_interactive_executor_integration():
    """æµ‹è¯•InteractiveExecutorä¸éƒ¨ç½²åˆ‡æ¢çš„é›†æˆ"""
    print("\nğŸ§ª æµ‹è¯•4: InteractiveExecutoré›†æˆæµ‹è¯•")
    print("=" * 50)
    
    try:
        from interactive_executor import InteractiveExecutor
        from api_client_manager import get_client_for_model
        
        print("ğŸ“¡ åˆ›å»ºInteractiveExecutor...")
        
        # åˆ›å»ºä¸€ä¸ªç®€å•çš„å·¥å…·æ³¨å†Œè¡¨
        tool_registry = {
            "test_tool": {
                "description": "A test tool",
                "parameters": [],
                "category": "test"
            }
        }
        
        # ä½¿ç”¨Llamaæ¨¡å‹åˆ›å»ºexecutor
        executor = InteractiveExecutor(
            tool_registry=tool_registry,
            model="Llama-3.3-70B-Instruct",
            prompt_type="optimal",
            max_turns=1,
            success_rate=0.8,
            silent=True
        )
        
        print(f"âœ… InteractiveExecutoråˆ›å»ºæˆåŠŸ")
        print(f"   æ¨¡å‹: {executor.model}")
        print(f"   Promptç±»å‹: {getattr(executor, 'prompt_type', 'N/A')}")
        print(f"   IdealLab Keyç´¢å¼•: {getattr(executor, 'ideallab_key_index', 'N/A')}")
        
        # æ£€æŸ¥å®¢æˆ·ç«¯æ˜¯å¦æœ‰éƒ¨ç½²ä¿¡æ¯
        if hasattr(executor.llm_client, 'current_deployment'):
            print(f"   å½“å‰éƒ¨ç½²: {executor.llm_client.current_deployment}")
            print(f"   éƒ¨ç½²åç§°: {executor.llm_client.deployment_name}")
        else:
            print("   âš ï¸  å®¢æˆ·ç«¯ç¼ºå°‘éƒ¨ç½²ä¿¡æ¯")
            
    except Exception as e:
        print(f"âŒ InteractiveExecutoræµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ éƒ¨ç½²åˆ‡æ¢åŠŸèƒ½æµ‹è¯•å¥—ä»¶")
    print("=" * 60)
    
    try:
        # æµ‹è¯•1: å®¢æˆ·ç«¯åˆ›å»º
        test_client_creation_with_deployment_switching()
        
        # æµ‹è¯•2: éƒ¨ç½²ç®¡ç†å™¨
        test_deployment_manager()
        
        # æµ‹è¯•3: 429é”™è¯¯æ¨¡æ‹Ÿ
        test_429_error_simulation()
        
        # æµ‹è¯•4: InteractiveExecutoré›†æˆ
        test_interactive_executor_integration()
        
        print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
        print("ğŸ’¡ å»ºè®®: è¿è¡Œå®é™…çš„5.5æµ‹è¯•æ¥éªŒè¯429é”™è¯¯æ—¶çš„è‡ªåŠ¨åˆ‡æ¢")
        
    except KeyboardInterrupt:
        print(f"\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()