=== æµ‹è¯•å¼€å§‹æ—¶é—´: 2025å¹´ 8æœˆ31æ—¥ æ˜ŸæœŸæ—¥ 16æ—¶48åˆ†12ç§’ EDT ===
=== æ‰§è¡Œå‘½ä»¤: python3 ./ultra_parallel_runner.py --model qwen2.5-72b-instruct --prompt-types baseline --difficulty easy --task-types all --num-instances 20 --rate-mode fixed --max-workers 3 ===
INFO:__main__:åˆå§‹åŒ–å®ä¾‹æ± : 17ä¸ªå®ä¾‹ (2ä¸ªAzure + 6ä¸ªIdealLab)
INFO:__main__:ğŸ“œ ä½¿ç”¨ä¼ ç»Ÿæ•°æ®åº“å†™å…¥æ¨¡å¼
INFO:__main__:èµ„æºæ± çŠ¶æ€: 17ä¸ªå®ä¾‹, å®¹é‡1306
INFO:__main__:
ğŸ¯ æ£€æµ‹åˆ°Qwenæ¨¡å‹ï¼Œä½¿ç”¨é˜Ÿåˆ—è°ƒåº¦å™¨
INFO:__main__:   æ¨¡å‹: qwen2.5-72b-instruct â†’ Key0
INFO:__main__:   Promptç±»å‹: baseline
INFO:__main__:   éš¾åº¦: easy
INFO:__main__:ğŸ”„ Key0: æ‰§è¡Œ qwen2.5-72b-instruct-easy
INFO:__main__:ğŸ¯ ä½¿ç”¨qwenæ™ºèƒ½åˆ†ç‰‡ç­–ç•¥: qwen2.5-72b-instruct
INFO:__main__:ğŸ”„ çœŸæ­£å¤šKeyå¹¶å‘ç­–ç•¥:
INFO:__main__:   æ¨¡å‹: qwen2.5-72b-instruct (è§„æ¨¡: 72b)
INFO:__main__:   ä½¿ç”¨Keys: key0, key1, key2
INFO:__main__:   æ€»å®ä¾‹æ•°: 20
INFO:__main__:   åˆ†ç‰‡æ•°: 3 (æ¯ä¸ªkeyç‹¬ç«‹åˆ†ç‰‡)
INFO:__main__:   å®ä¾‹åˆ†é…: [7, 7, 6]
INFO:__main__:   ğŸš€ å¯ç”¨3å€APIå¹¶å‘ï¼
INFO:__main__:ğŸš€ å¯åŠ¨3ä¸ªåˆ†ç‰‡å¹¶å‘æ‰§è¡Œ
INFO:__main__:  IdealLab qwenæ¨¡å‹é™åˆ¶: qwen-key0 å¼ºåˆ¶ä½¿ç”¨ max_workers=1, qps=10
INFO:__main__:    æ³¨æ„: IdealLab APIå¹¶å‘é™åˆ¶ä¸¥æ ¼ï¼Œå¿½ç•¥--max-workersè®¾ç½®
INFO:__main__:  ä½¿ç”¨IdealLab API Key 0
INFO:__main__:ğŸš€ å¯åŠ¨åˆ†ç‰‡ qwen2.5-72b-instruct_easy_baseline_key0: qwen-key0
INFO:__main__:   å®ä¾‹æ•°: 7, æ¨¡å‹: qwen2.5-72b-instruct
INFO:__main__:   è®¾ç½®STORAGE_FORMAT=jsonç»™å­è¿›ç¨‹
INFO:__main__:   è®¾ç½®USE_PARTIAL_LOADING=trueç»™å­è¿›ç¨‹
INFO:__main__:   è®¾ç½®TASK_LOAD_COUNT=20ç»™å­è¿›ç¨‹
INFO:__main__:   è®¾ç½®SKIP_MODEL_LOADING=trueç»™å­è¿›ç¨‹
INFO:__main__:   è®¾ç½®USE_RESULT_COLLECTOR=trueç»™å­è¿›ç¨‹
INFO:__main__:   è®¾ç½®KMP_DUPLICATE_LIB_OK=TRUEç»™å­è¿›ç¨‹
INFO:__main__:   è®¾ç½®PYTHONMALLOC=mallocç»™å­è¿›ç¨‹
INFO:__main__:   åˆ†ç‰‡1: qwen-key0 (7ä¸ªå®ä¾‹)
INFO:__main__:  IdealLab qwenæ¨¡å‹é™åˆ¶: qwen-key1 å¼ºåˆ¶ä½¿ç”¨ max_workers=1, qps=10
INFO:__main__:    æ³¨æ„: IdealLab APIå¹¶å‘é™åˆ¶ä¸¥æ ¼ï¼Œå¿½ç•¥--max-workersè®¾ç½®
INFO:__main__:  ä½¿ç”¨IdealLab API Key 1
INFO:__main__:ğŸš€ å¯åŠ¨åˆ†ç‰‡ qwen2.5-72b-instruct_easy_baseline_key1: qwen-key1
INFO:__main__:   å®ä¾‹æ•°: 7, æ¨¡å‹: qwen2.5-72b-instruct
INFO:__main__:   è®¾ç½®STORAGE_FORMAT=jsonç»™å­è¿›ç¨‹
INFO:__main__:   è®¾ç½®USE_PARTIAL_LOADING=trueç»™å­è¿›ç¨‹
INFO:__main__:   è®¾ç½®TASK_LOAD_COUNT=20ç»™å­è¿›ç¨‹
INFO:__main__:   è®¾ç½®SKIP_MODEL_LOADING=trueç»™å­è¿›ç¨‹
INFO:__main__:   è®¾ç½®USE_RESULT_COLLECTOR=trueç»™å­è¿›ç¨‹
INFO:__main__:   è®¾ç½®KMP_DUPLICATE_LIB_OK=TRUEç»™å­è¿›ç¨‹
INFO:__main__:   è®¾ç½®PYTHONMALLOC=mallocç»™å­è¿›ç¨‹
INFO:__main__:   åˆ†ç‰‡2: qwen-key1 (7ä¸ªå®ä¾‹)
INFO:__main__:  ä½¿ç”¨IdealLab API Key 2
INFO:__main__:ğŸš€ å¯åŠ¨åˆ†ç‰‡ qwen2.5-72b-instruct_easy_baseline_key2: qwen-key2
INFO:__main__:   å®ä¾‹æ•°: 6, æ¨¡å‹: qwen2.5-72b-instruct
INFO:__main__:   è®¾ç½®STORAGE_FORMAT=jsonç»™å­è¿›ç¨‹
INFO:__main__:   è®¾ç½®USE_PARTIAL_LOADING=trueç»™å­è¿›ç¨‹
INFO:__main__:   è®¾ç½®TASK_LOAD_COUNT=20ç»™å­è¿›ç¨‹
INFO:__main__:   è®¾ç½®SKIP_MODEL_LOADING=trueç»™å­è¿›ç¨‹
INFO:__main__:   è®¾ç½®USE_RESULT_COLLECTOR=trueç»™å­è¿›ç¨‹
INFO:__main__:   è®¾ç½®KMP_DUPLICATE_LIB_OK=TRUEç»™å­è¿›ç¨‹
INFO:__main__:   è®¾ç½®PYTHONMALLOC=mallocç»™å­è¿›ç¨‹
INFO:__main__:   åˆ†ç‰‡3: qwen-key2 (6ä¸ªå®ä¾‹)
INFO:__main__:ç­‰å¾…åˆ†ç‰‡1å®Œæˆï¼ˆ20å®ä¾‹Ã—50workersï¼Œæœ€å¤šç­‰å¾…50åˆ†é’Ÿï¼‰...
2025-08-31 16:48:13,021 - faiss.loader - INFO - Loading faiss.
2025-08-31 16:48:13,021 - faiss.loader - INFO - Loading faiss.
2025-08-31 16:48:13,021 - faiss.loader - INFO - Loading faiss.
2025-08-31 16:48:13,044 - faiss.loader - INFO - Successfully loaded faiss.
2025-08-31 16:48:13,044 - faiss.loader - INFO - Successfully loaded faiss.
2025-08-31 16:48:13,044 - faiss.loader - INFO - Successfully loaded faiss.
2025-08-31 16:48:13,944 - smart_result_collector - INFO - è‡ªåŠ¨ä¿å­˜çº¿ç¨‹å·²å¯åŠ¨
2025-08-31 16:48:13,944 - smart_result_collector - INFO - SmartResultCollectoråˆå§‹åŒ–å®Œæˆ
2025-08-31 16:48:13,944 - smart_result_collector - INFO -   - ä¸´æ—¶ç›®å½•: temp_results
2025-08-31 16:48:13,944 - smart_result_collector - INFO -   - å†…å­˜é˜ˆå€¼: 20
2025-08-31 16:48:13,944 - smart_result_collector - INFO -   - æ—¶é—´é˜ˆå€¼: 300ç§’
2025-08-31 16:48:13,944 - smart_result_collector - INFO -   - è‡ªåŠ¨ä¿å­˜: 60ç§’
2025-08-31 16:48:13,944 - smart_result_collector - INFO -   - è‡ªé€‚åº”é˜ˆå€¼: True
2025-08-31 16:48:13,944 - result_collector_adapter - INFO - âœ… ä½¿ç”¨SmartResultCollector
2025-08-31 16:48:13,944 - result_collector_adapter - INFO - AdaptiveResultCollectoråˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨: smart
2025-08-31 16:48:13,945 - smart_result_collector - INFO - è‡ªåŠ¨ä¿å­˜çº¿ç¨‹å·²å¯åŠ¨
2025-08-31 16:48:13,945 - smart_result_collector - INFO - SmartResultCollectoråˆå§‹åŒ–å®Œæˆ
2025-08-31 16:48:13,945 - smart_result_collector - INFO -   - ä¸´æ—¶ç›®å½•: temp_results
2025-08-31 16:48:13,945 - smart_result_collector - INFO -   - å†…å­˜é˜ˆå€¼: 20
2025-08-31 16:48:13,945 - smart_result_collector - INFO -   - æ—¶é—´é˜ˆå€¼: 300ç§’
2025-08-31 16:48:13,945 - smart_result_collector - INFO -   - è‡ªåŠ¨ä¿å­˜: 60ç§’
2025-08-31 16:48:13,945 - smart_result_collector - INFO -   - è‡ªé€‚åº”é˜ˆå€¼: True
2025-08-31 16:48:13,945 - result_collector_adapter - INFO - âœ… ä½¿ç”¨SmartResultCollector
2025-08-31 16:48:13,945 - result_collector_adapter - INFO - AdaptiveResultCollectoråˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨: smart
2025-08-31 16:48:13,947 - smart_result_collector - INFO - è‡ªåŠ¨ä¿å­˜çº¿ç¨‹å·²å¯åŠ¨
2025-08-31 16:48:13,947 - smart_model_router - INFO - âœ¨ Using USER's Azure endpoint for gpt-5-nano
2025-08-31 16:48:13,948 - smart_model_router - INFO - âœ¨ Using USER's Azure endpoint for gpt-5-nano
2025-08-31 16:48:13,948 - smart_result_collector - INFO - SmartResultCollectoråˆå§‹åŒ–å®Œæˆ
2025-08-31 16:48:13,948 - smart_result_collector - INFO -   - ä¸´æ—¶ç›®å½•: temp_results
2025-08-31 16:48:13,948 - smart_result_collector - INFO -   - å†…å­˜é˜ˆå€¼: 20
2025-08-31 16:48:13,948 - smart_result_collector - INFO -   - æ—¶é—´é˜ˆå€¼: 300ç§’
2025-08-31 16:48:13,948 - smart_result_collector - INFO -   - è‡ªåŠ¨ä¿å­˜: 60ç§’
2025-08-31 16:48:13,948 - smart_result_collector - INFO -   - è‡ªé€‚åº”é˜ˆå€¼: True
2025-08-31 16:48:13,948 - result_collector_adapter - INFO - âœ… ä½¿ç”¨SmartResultCollector
2025-08-31 16:48:13,948 - result_collector_adapter - INFO - AdaptiveResultCollectoråˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨: smart
2025-08-31 16:48:13,949 - smart_model_router - INFO - âœ¨ Using USER's Azure endpoint for gpt-5-nano
2025-08-31 16:48:14,000 - txt_based_ai_classifier - INFO - TXT-based AI classifier initialized with gpt-5-nano (parameter-filtered)
2025-08-31 16:48:14,000 - txt_based_ai_classifier - INFO - TXT-based AI classifier initialized with gpt-5-nano (parameter-filtered)
2025-08-31 16:48:14,000 - batch_test_runner - INFO - åŸºäºTXTæ–‡ä»¶çš„AIé”™è¯¯åˆ†ç±»ç³»ç»Ÿå·²å¯ç”¨ (ä½¿ç”¨gpt-5-nano)
2025-08-31 16:48:14,000 - batch_test_runner - INFO - åŸºäºTXTæ–‡ä»¶çš„AIé”™è¯¯åˆ†ç±»ç³»ç»Ÿå·²å¯ç”¨ (ä½¿ç”¨gpt-5-nano)
2025-08-31 16:48:14,000 - batch_test_runner - INFO - ============================================================
2025-08-31 16:48:14,000 - batch_test_runner - INFO - ============================================================
2025-08-31 16:48:14,000 - batch_test_runner - INFO - Batch test runner initialized
2025-08-31 16:48:14,001 - batch_test_runner - INFO - Configuration: debug=False, silent=False, adaptive=False
2025-08-31 16:48:14,001 - batch_test_runner - INFO - Batch test runner initialized
2025-08-31 16:48:14,001 - batch_test_runner - INFO - Log file: logs/batch_test_20250831_164813.log
2025-08-31 16:48:14,001 - batch_test_runner - INFO - Configuration: debug=False, silent=False, adaptive=False
2025-08-31 16:48:14,001 - batch_test_runner - INFO - ============================================================
2025-08-31 16:48:14,001 - batch_test_runner - INFO - Log file: logs/batch_test_20250831_164813.log
2025-08-31 16:48:14,001 - batch_test_runner - INFO - ============================================================
2025-08-31 16:48:14,001 - batch_test_runner - INFO - Running 35 tests with 2 workers, QPS limit: None
2025-08-31 16:48:14,001 - batch_test_runner - INFO - Running 30 tests with 2 workers, QPS limit: None
2025-08-31 16:48:14,001 - batch_test_runner - INFO - Initializing test components...
2025-08-31 16:48:14,001 - batch_test_runner - INFO - Initializing test components...
2025-08-31 16:48:14,001 - txt_based_ai_classifier - INFO - TXT-based AI classifier initialized with gpt-5-nano (parameter-filtered)
2025-08-31 16:48:14,001 - batch_test_runner - INFO - åŸºäºTXTæ–‡ä»¶çš„AIé”™è¯¯åˆ†ç±»ç³»ç»Ÿå·²å¯ç”¨ (ä½¿ç”¨gpt-5-nano)
2025-08-31 16:48:14,001 - batch_test_runner - INFO - ============================================================
2025-08-31 16:48:14,001 - batch_test_runner - INFO - Batch test runner initialized
2025-08-31 16:48:14,001 - batch_test_runner - INFO - Configuration: debug=False, silent=False, adaptive=False
2025-08-31 16:48:14,002 - batch_test_runner - INFO - Log file: logs/batch_test_20250831_164813.log
2025-08-31 16:48:14,002 - batch_test_runner - INFO - ============================================================
2025-08-31 16:48:14,002 - batch_test_runner - INFO - Running 35 tests with 2 workers, QPS limit: None
2025-08-31 16:48:14,002 - batch_test_runner - INFO - Initializing test components...
2025-08-31 16:48:14,509 - batch_test_runner - INFO - Found pre-generated workflows, will use them to save memory
2025-08-31 16:48:14,510 - batch_test_runner - INFO - âš¡ [OPTIMIZATION] Using MDPWorkflowGenerator with SKIP_MODEL_LOADING=true
2025-08-31 16:48:14,510 - batch_test_runner - INFO - âš¡ This saves ~350MB memory while keeping all functionality intact
2025-08-31 16:48:14,511 - api_client_manager - INFO - Loaded configuration from config/config.json
2025-08-31 16:48:14,512 - batch_test_runner - INFO - Found pre-generated workflows, will use them to save memory
2025-08-31 16:48:14,512 - batch_test_runner - INFO - âš¡ [OPTIMIZATION] Using MDPWorkflowGenerator with SKIP_MODEL_LOADING=true
2025-08-31 16:48:14,512 - batch_test_runner - INFO - âš¡ This saves ~350MB memory while keeping all functionality intact
2025-08-31 16:48:14,513 - api_client_manager - INFO - Loaded configuration from config/config.json
2025-08-31 16:48:14,532 - batch_test_runner - INFO - Found pre-generated workflows, will use them to save memory
2025-08-31 16:48:14,534 - batch_test_runner - INFO - âš¡ [OPTIMIZATION] Using MDPWorkflowGenerator with SKIP_MODEL_LOADING=true
2025-08-31 16:48:14,534 - batch_test_runner - INFO - âš¡ This saves ~350MB memory while keeping all functionality intact
2025-08-31 16:48:14,535 - api_client_manager - INFO - Loaded configuration from config/config.json
2025-08-31 16:48:15,077 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:48:15,084 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:48:15,135 - httpx - INFO - HTTP Request: POST https://archer222arc.openai.azure.com/openai/deployments/text-embedding-3-large/embeddings?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:48:15,210 - mcp_embedding_manager - INFO - Loading embedding cache from .mcp_embedding_cache/embedding_cache.pkl (size: 175022829 bytes)
2025-08-31 16:48:15,210 - mcp_embedding_manager - INFO - Loading embedding cache from .mcp_embedding_cache/embedding_cache.pkl (size: 175022829 bytes)
2025-08-31 16:48:15,210 - mcp_embedding_manager - INFO - Loading embedding cache from .mcp_embedding_cache/embedding_cache.pkl (size: 175022829 bytes)
2025-08-31 16:48:15,611 - mcp_embedding_manager - INFO - Loaded 7100 cached embeddings
2025-08-31 16:48:15,612 - mcp_embedding_manager - INFO - Loaded 7100 cached embeddings
2025-08-31 16:48:15,613 - mcp_embedding_manager - INFO - Loaded 7100 cached embeddings
2025-08-31 16:48:16,109 - mcp_embedding_manager - INFO - Loaded search cache with 3 entries
2025-08-31 16:48:16,110 - mcp_embedding_manager - INFO - Initialized operation mappings with 149 terms
2025-08-31 16:48:16,113 - mcp_embedding_manager - INFO - Loaded search cache with 3 entries
2025-08-31 16:48:16,113 - mcp_embedding_manager - INFO - Initialized operation mappings with 149 terms
2025-08-31 16:48:16,113 - mcp_embedding_manager - INFO - Loaded search cache with 3 entries
2025-08-31 16:48:16,113 - mcp_embedding_manager - INFO - Initialized operation mappings with 149 terms
2025-08-31 16:48:16,199 - mcp_embedding_manager - INFO - Loading embedding cache from .mcp_embedding_cache/embedding_cache.pkl (size: 175022829 bytes)
2025-08-31 16:48:16,204 - mcp_embedding_manager - INFO - Loading embedding cache from .mcp_embedding_cache/embedding_cache.pkl (size: 175022829 bytes)
2025-08-31 16:48:16,208 - mcp_embedding_manager - INFO - Loading embedding cache from .mcp_embedding_cache/embedding_cache.pkl (size: 175022829 bytes)
2025-08-31 16:48:16,336 - mcp_embedding_manager - INFO - Loaded 7100 cached embeddings
2025-08-31 16:48:16,338 - mcp_embedding_manager - INFO - Loaded 7100 cached embeddings
2025-08-31 16:48:16,341 - mcp_embedding_manager - INFO - Loaded 7100 cached embeddings
2025-08-31 16:48:17,165 - mcp_embedding_manager - INFO - [Cache] Loaded 9650 entries from file
2025-08-31 16:48:17,165 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:48:17,168 - mcp_embedding_manager - INFO - [Cache] Loaded 9650 entries from file
2025-08-31 16:48:17,168 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:48:17,176 - mcp_embedding_manager - INFO - [Cache] Loaded 9650 entries from file
2025-08-31 16:48:17,177 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:48:17,543 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:48:17,544 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:48:17,544 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:48:17,548 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:48:17,548 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:48:17,548 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:48:17,551 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:48:17,551 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:48:17,551 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:48:17,563 - mdp_workflow_generator - INFO - Embedding index loaded successfully
2025-08-31 16:48:17,565 - mdp_workflow_generator - INFO - Embedding index loaded successfully
2025-08-31 16:48:17,565 - mdp_workflow_generator - INFO - Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
2025-08-31 16:48:17,565 - mdp_workflow_generator - INFO - Loading tools from mcp_generated_library/tool_registry_consolidated.json
2025-08-31 16:48:17,566 - mdp_workflow_generator - INFO - Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
2025-08-31 16:48:17,566 - mdp_workflow_generator - INFO - Loading tools from mcp_generated_library/tool_registry_consolidated.json
2025-08-31 16:48:17,567 - mdp_workflow_generator - INFO - Embedding index loaded successfully
2025-08-31 16:48:17,568 - mdp_workflow_generator - INFO - Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
2025-08-31 16:48:17,568 - mdp_workflow_generator - INFO - Loading tools from mcp_generated_library/tool_registry_consolidated.json
2025-08-31 16:48:17,574 - mdp_workflow_generator - INFO - Loaded 30 tools
2025-08-31 16:48:17,575 - mdp_workflow_generator - INFO - Loaded 30 tools
2025-08-31 16:48:17,575 - mdp_workflow_generator - INFO - Loaded 30 tools
2025-08-31 16:48:17,575 - mdp_workflow_generator - INFO - Default state_dim calculated: 345
2025-08-31 16:48:17,576 - mdp_workflow_generator - INFO - Default action_dim calculated: 31
2025-08-31 16:48:17,576 - mdp_workflow_generator - INFO - Model loading skipped for memory optimization (SKIP_MODEL_LOADING=true)
2025-08-31 16:48:17,576 - mdp_workflow_generator - INFO - Default state_dim calculated: 345
2025-08-31 16:48:17,576 - mdp_workflow_generator - INFO - Default state_dim calculated: 345
2025-08-31 16:48:17,576 - mdp_workflow_generator - INFO - Default action_dim calculated: 31
2025-08-31 16:48:17,576 - mdp_workflow_generator - INFO - Default action_dim calculated: 31
2025-08-31 16:48:17,576 - mdp_workflow_generator - INFO - Model loading skipped for memory optimization (SKIP_MODEL_LOADING=true)
2025-08-31 16:48:17,576 - mdp_workflow_generator - INFO - Model loading skipped for memory optimization (SKIP_MODEL_LOADING=true)
2025-08-31 16:48:20,472 - unified_training_manager - INFO - Using device: cpu
2025-08-31 16:48:20,472 - unified_training_manager - INFO - Using device: cpu
2025-08-31 16:48:20,472 - unified_training_manager - INFO - Using device: cpu
2025-08-31 16:48:21,350 - unified_training_manager - INFO - Task filtering results:
2025-08-31 16:48:21,350 - unified_training_manager - INFO -   Total: 5040 -> 5040
2025-08-31 16:48:21,350 - unified_training_manager - INFO -   simple_task: 320 -> 320
2025-08-31 16:48:21,350 - unified_training_manager - INFO -   data_pipeline: 1520 -> 1520
2025-08-31 16:48:21,350 - unified_training_manager - INFO -   api_integration: 1360 -> 1360
2025-08-31 16:48:21,350 - unified_training_manager - INFO -   basic_task: 1200 -> 1200
2025-08-31 16:48:21,350 - unified_training_manager - INFO -   multi_stage_pipeline: 640 -> 640
2025-08-31 16:48:21,350 - unified_training_manager - INFO - Loaded 5040 tasks from mcp_generated_library/task_library_all_difficulties.json
2025-08-31 16:48:21,354 - unified_training_manager - INFO - Organized tasks: 5 types, 3 complexity levels, 5 difficulty levels
2025-08-31 16:48:21,356 - unified_training_manager - INFO - Task filtering results:
2025-08-31 16:48:21,357 - unified_training_manager - INFO -   Total: 5040 -> 5040
2025-08-31 16:48:21,357 - unified_training_manager - INFO -   simple_task: 320 -> 320
2025-08-31 16:48:21,357 - unified_training_manager - INFO -   data_pipeline: 1520 -> 1520
2025-08-31 16:48:21,357 - unified_training_manager - INFO -   api_integration: 1360 -> 1360
2025-08-31 16:48:21,357 - unified_training_manager - INFO -   basic_task: 1200 -> 1200
2025-08-31 16:48:21,357 - unified_training_manager - INFO -   multi_stage_pipeline: 640 -> 640
2025-08-31 16:48:21,357 - unified_training_manager - INFO - Loaded 5040 tasks from mcp_generated_library/task_library_all_difficulties.json
2025-08-31 16:48:21,360 - mdp_workflow_generator - INFO - MDPWorkflowGenerator initialized successfully
2025-08-31 16:48:21,361 - batch_test_runner - INFO - âœ… MDPWorkflowGenerator initialized successfully:
2025-08-31 16:48:21,361 - batch_test_runner - INFO -   - task_manager: âœ“
2025-08-31 16:48:21,361 - unified_training_manager - INFO - Organized tasks: 5 types, 3 complexity levels, 5 difficulty levels
2025-08-31 16:48:21,361 - batch_test_runner - INFO -   - output_verifier: âœ“
2025-08-31 16:48:21,361 - batch_test_runner - INFO -   - embedding_manager: âœ“
2025-08-31 16:48:21,361 - batch_test_runner - INFO -   - tool_capabilities: 30 tools
2025-08-31 16:48:21,361 - batch_test_runner - INFO -   - neural network: skipped (saving 350MB)
2025-08-31 16:48:21,362 - unified_training_manager - INFO - Task filtering results:
2025-08-31 16:48:21,362 - unified_training_manager - INFO -   Total: 5040 -> 5040
2025-08-31 16:48:21,362 - unified_training_manager - INFO -   simple_task: 320 -> 320
2025-08-31 16:48:21,362 - unified_training_manager - INFO -   data_pipeline: 1520 -> 1520
2025-08-31 16:48:21,362 - unified_training_manager - INFO -   api_integration: 1360 -> 1360
2025-08-31 16:48:21,362 - unified_training_manager - INFO -   basic_task: 1200 -> 1200
2025-08-31 16:48:21,362 - unified_training_manager - INFO -   multi_stage_pipeline: 640 -> 640
2025-08-31 16:48:21,362 - unified_training_manager - INFO - Loaded 5040 tasks from mcp_generated_library/task_library_all_difficulties.json
2025-08-31 16:48:21,365 - mdp_workflow_generator - INFO - MDPWorkflowGenerator initialized successfully
2025-08-31 16:48:21,365 - batch_test_runner - INFO - âœ… MDPWorkflowGenerator initialized successfully:
2025-08-31 16:48:21,365 - batch_test_runner - INFO -   - task_manager: âœ“
2025-08-31 16:48:21,365 - batch_test_runner - INFO -   - output_verifier: âœ“
2025-08-31 16:48:21,365 - batch_test_runner - INFO -   - embedding_manager: âœ“
2025-08-31 16:48:21,365 - batch_test_runner - INFO -   - tool_capabilities: 30 tools
2025-08-31 16:48:21,365 - batch_test_runner - INFO -   - neural network: skipped (saving 350MB)
2025-08-31 16:48:21,367 - unified_training_manager - INFO - Organized tasks: 5 types, 3 complexity levels, 5 difficulty levels
2025-08-31 16:48:21,371 - mdp_workflow_generator - INFO - MDPWorkflowGenerator initialized successfully
2025-08-31 16:48:21,371 - batch_test_runner - INFO - âœ… MDPWorkflowGenerator initialized successfully:
2025-08-31 16:48:21,371 - batch_test_runner - INFO -   - task_manager: âœ“
2025-08-31 16:48:21,371 - batch_test_runner - INFO -   - output_verifier: âœ“
2025-08-31 16:48:21,371 - batch_test_runner - INFO -   - embedding_manager: âœ“
2025-08-31 16:48:21,371 - batch_test_runner - INFO -   - tool_capabilities: 30 tools
2025-08-31 16:48:21,371 - batch_test_runner - INFO -   - neural network: skipped (saving 350MB)
2025-08-31 16:48:21,397 - focused_ai_classifier - INFO - Focused AI classifier initialized with gpt-5-nano (parameter-filtered)
2025-08-31 16:48:21,403 - focused_ai_classifier - INFO - Focused AI classifier initialized with gpt-5-nano (parameter-filtered)
2025-08-31 16:48:21,404 - result_collector - INFO - ResultCollectoråˆå§‹åŒ–ï¼Œä¸´æ—¶ç›®å½•: temp_results
2025-08-31 16:48:21,404 - result_merger - INFO - ResultMergeråˆå§‹åŒ–å®Œæˆ
2025-08-31 16:48:21,407 - merger_lock - INFO - è·å¾—åˆå¹¶å™¨é” (PID: 44588)
2025-08-31 16:48:21,407 - result_merger - INFO - ğŸš€ å¯åŠ¨ResultMergerï¼Œåˆå¹¶é—´éš”: 10ç§’
2025-08-31 16:48:21,410 - result_collector - INFO - ResultCollectoråˆå§‹åŒ–ï¼Œä¸´æ—¶ç›®å½•: temp_results
2025-08-31 16:48:21,410 - result_merger - INFO - ResultMergeråˆå§‹åŒ–å®Œæˆ
2025-08-31 16:48:21,411 - result_merger - WARNING - å¦ä¸€ä¸ªåˆå¹¶å™¨å·²åœ¨è¿è¡Œ (PID: -1)
2025-08-31 16:48:21,414 - result_merger - INFO - ResultMergerå¼€å§‹è¿è¡Œï¼Œæ™ºèƒ½åœæ­¢é˜ˆå€¼: 3è½®
2025-08-31 16:48:21,415 - result_merger - INFO - âœ… ResultMergeråå°çº¿ç¨‹å·²å¯åŠ¨ï¼Œæ”¯æŒæ™ºèƒ½åœæ­¢æœºåˆ¶
2025-08-31 16:48:21,421 - focused_ai_classifier - INFO - Focused AI classifier initialized with gpt-5-nano (parameter-filtered)
2025-08-31 16:48:21,424 - result_collector - INFO - ResultCollectoråˆå§‹åŒ–ï¼Œä¸´æ—¶ç›®å½•: temp_results
2025-08-31 16:48:21,424 - result_merger - INFO - ResultMergeråˆå§‹åŒ–å®Œæˆ
2025-08-31 16:48:21,425 - result_merger - WARNING - å¦ä¸€ä¸ªåˆå¹¶å™¨å·²åœ¨è¿è¡Œ (PID: -1)
2025-08-31 16:48:21,449 - batch_test_runner - INFO - Loading task library with pre-generated workflows: task_library_enhanced_v3_easy_with_workflows.json
2025-08-31 16:48:21,449 - batch_test_runner - INFO - Using partial loading: 20 tasks per type
2025-08-31 16:48:21,450 - batch_test_runner - INFO - Loading task library with pre-generated workflows: task_library_enhanced_v3_easy_with_workflows.json
2025-08-31 16:48:21,450 - batch_test_runner - INFO - Using partial loading: 20 tasks per type
2025-08-31 16:48:21,451 - batch_test_runner - INFO - Loading task library with pre-generated workflows: task_library_enhanced_v3_easy_with_workflows.json
2025-08-31 16:48:21,451 - batch_test_runner - INFO - Using partial loading: 20 tasks per type
2025-08-31 16:48:22,031 - batch_test_runner - INFO - Partially loaded 100 tasks (vs 630 total)
2025-08-31 16:48:22,031 - batch_test_runner - INFO - Estimated memory saving: 84.1%
2025-08-31 16:48:22,041 - batch_test_runner - INFO - Partially loaded 100 tasks (vs 630 total)
2025-08-31 16:48:22,041 - batch_test_runner - INFO - Estimated memory saving: 84.1%
2025-08-31 16:48:22,042 - batch_test_runner - INFO - Partially loaded 100 tasks (vs 630 total)
2025-08-31 16:48:22,042 - batch_test_runner - INFO - Estimated memory saving: 84.1%
2025-08-31 16:48:22,096 - batch_test_runner - INFO - Initialization complete
2025-08-31 16:48:22,102 - batch_test_runner - INFO - Initialization complete
2025-08-31 16:48:22,104 - batch_test_runner - INFO - Initialization complete
2025-08-31 16:48:22,189 - batch_test_runner - INFO - Starting batch test with 35 tasks, 2 workers
2025-08-31 16:48:22,189 - batch_test_runner - INFO - Each task timeout: 10 minutes, Total batch timeout: 20 minutes
2025-08-31 16:48:22,190 - batch_test_runner - INFO - Batch timeout set to 3600s (60.0 minutes) for 35 tasks
2025-08-31 16:48:22,191 - smart_model_router - INFO - Using idealab for qwen2.5-72b-instruct
2025-08-31 16:48:22,196 - batch_test_runner - INFO - Starting batch test with 35 tasks, 2 workers
2025-08-31 16:48:22,197 - batch_test_runner - INFO - Each task timeout: 10 minutes, Total batch timeout: 20 minutes
2025-08-31 16:48:22,198 - batch_test_runner - INFO - Batch timeout set to 3600s (60.0 minutes) for 35 tasks
2025-08-31 16:48:22,199 - smart_model_router - INFO - Using idealab for qwen2.5-72b-instruct
2025-08-31 16:48:22,200 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:48:22,200 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:48:22,201 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:48:22,201 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:48:22,206 - batch_test_runner - INFO - Starting batch test with 30 tasks, 2 workers
2025-08-31 16:48:22,206 - batch_test_runner - INFO - Each task timeout: 10 minutes, Total batch timeout: 20 minutes
2025-08-31 16:48:22,207 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:48:22,208 - smart_model_router - INFO - Using idealab for qwen2.5-72b-instruct
2025-08-31 16:48:22,208 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:48:22,209 - batch_test_runner - INFO - Batch timeout set to 3600s (60.0 minutes) for 30 tasks
2025-08-31 16:48:22,210 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:48:22,211 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:48:22,218 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:48:22,218 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:48:22,219 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:48:22,219 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:48:22,248 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:48:22,248 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:48:22,248 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:48:22,262 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:48:22,262 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:48:22,262 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:48:22,270 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:48:22,270 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:48:22,271 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:48:22,271 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:48:22,271 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:48:22,272 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:48:22,272 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:48:22,272 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:48:22,275 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:48:22,275 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:48:22,275 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:48:22,271 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:48:23,517 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:23,538 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:23,590 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:23,606 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:23,942 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:24,396 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:24,396 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:24,811 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:24,920 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:25,076 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:25,442 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:25,467 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:25,579 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:25,967 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:25,967 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:26,903 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:26,906 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:26,915 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:27,345 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:27,437 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:27,876 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:27,907 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:27,929 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:28,354 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:28,839 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:29,744 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:29,787 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:29,958 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:30,239 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:30,520 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:30,707 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:30,783 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:30,975 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:31,205 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:31,215 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:31,736 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:31,880 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:31,900 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:32,252 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:32,632 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:33,530 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:33,723 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:34,607 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:34,611 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:34,890 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:35,057 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:35,419 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
[INFO] âš¡ SKIP_MODEL_LOADING=true - Skipping torch imports
[INFO] ä½¿ç”¨JSONå­˜å‚¨æ ¼å¼
[INFO] ä½¿ç”¨JSONå­˜å‚¨æ ¼å¼
[INFO] ä½¿ç”¨JSONå­˜å‚¨æ ¼å¼
[INFO] ä½¿ç”¨JSONå­˜å‚¨æ ¼å¼

============================================================
æ™ºèƒ½æ‰¹æµ‹è¯•: qwen2.5-72b-instruct (idealab)
Prompt types: ['baseline']
éš¾åº¦: easy
ç›®æ ‡: æ¯ç§é…ç½® 6 ä¸ªå®ä¾‹
============================================================
â—‹ simple_task         :   0/  6 å·²å®Œæˆ (éœ€è¦è¡¥å…… 6 ä¸ª)
â—‹ basic_task          :   0/  6 å·²å®Œæˆ (éœ€è¦è¡¥å…… 6 ä¸ª)
â—‹ data_pipeline       :   0/  6 å·²å®Œæˆ (éœ€è¦è¡¥å…… 6 ä¸ª)
â—‹ api_integration     :   0/  6 å·²å®Œæˆ (éœ€è¦è¡¥å…… 6 ä¸ª)
â—‹ multi_stage_pipeline:   0/  6 å·²å®Œæˆ (éœ€è¦è¡¥å…… 6 ä¸ª)

â³ éœ€è¦è¿è¡Œ 30 ä¸ªæ–°æµ‹è¯•

â–¶ å‡†å¤‡ simple_task (6 ä¸ªå®ä¾‹)...

â–¶ å‡†å¤‡ basic_task (6 ä¸ªå®ä¾‹)...

â–¶ å‡†å¤‡ data_pipeline (6 ä¸ªå®ä¾‹)...

â–¶ å‡†å¤‡ api_integration (6 ä¸ªå®ä¾‹)...

â–¶ å‡†å¤‡ multi_stage_pipeline (6 ä¸ªå®ä¾‹)...

â–¶ å¼€å§‹æ‰§è¡Œ 30 ä¸ªæµ‹è¯•...
ğŸ“Š è‡ªé€‚åº”checkpoint_interval: 20
ğŸ“¦ æ‰¹é‡æäº¤æ¨¡å¼ï¼šæ¯20ä¸ªæµ‹è¯•ä¿å­˜ä¸€æ¬¡
âš ï¸  æ£€æµ‹åˆ°idealab APIï¼Œè°ƒæ•´å¹¶å‘: workers=2, qps=None
ğŸ§  å¯ç”¨SmartResultCollectoræ¨¡å¼ï¼Œæ™ºèƒ½æ•°æ®ç®¡ç†
[AI_DEBUG] AIåˆ†ç±»å™¨åˆå§‹åŒ–æˆåŠŸ: <txt_based_ai_classifier.TxtBasedAIClassifier object at 0x10a3e5c20>
[DEBUG] BatchTestRunner initialized with save_logs=False, enable_database_updates=False, use_ai_classification=True, checkpoint_interval=20
[DEBUG] Creating new ToolCapabilityManager instance
[OperationEmbeddingIndex] Initializing with unified API client manager
['config/config.json', 'config/api_keys.json', 'config/api-keys.json']
[OperationEmbeddingIndex] OpenAI client initialized with model: gpt-4o-mini
[OperationEmbeddingIndex] Using embedding model: text-embedding-3-large
[OperationEmbeddingIndex] Detecting actual embedding dimension...
[OperationEmbeddingIndex] Detected embedding dimension: 3072
[INFO] Loaded 4150 embeddings from persistent cache
[OperationEmbeddingIndex] Initialized with 4150 cached embeddings
[INFO] Loaded 15 LLM-enhanced operation definitions from cache
[INFO] Found cached operation index at .mcp_operation_cache/operation_index.pkl
[INFO] Loading operation index from .mcp_operation_cache/operation_index.pkl
[INFO] Successfully loaded FAISS index with dimension 3072
[INFO] Operation index loaded successfully from .mcp_operation_cache/operation_index.pkl
[INFO] Loaded 15 operations with dimension 3072
[INFO] Successfully loaded cached index
[INFO] Operation semantic index initialized
[INFO] âš¡ SKIP_MODEL_LOADING=true - No device initialization
[INFO] Initialized tool success tracking attributes
[INFO] Initializing embedding manager for enhanced tool selection
[MCPEmbeddingManager] Creating new singleton instance
[MCPEmbeddingManager] Initializing with unified API client manager
[MCPEmbeddingManager] Using embedding model: text-embedding-3-large
[MCPEmbeddingManager] Client initialized successfully
[MCPEmbeddingManager] Singleton created with 0 cached embeddings
[INFO] Loading embedding index from .mcp_embedding_cache/tool_index.pkl
[SUCCESS] Loaded 30 tool embeddings
[SUCCESS] Embedding manager initialized with 30 tools
[INFO] Initialized task types: ['simple_task', 'data_pipeline', 'api_integration', 'basic_task', 'multi_stage_pipeline']
[INFO] Loading full MCP protocol registry...
[INFO] Loaded full tool registry with 30 tools
[INFO] Loading tools from mcp_generated_library/tool_registry_consolidated.json
[INFO] Embedding manager ready with 30 tools
[WARNING] Embedding manager exists but has no embeddings
[INFO] Consider rebuilding index with: embedding_manager.build_index(tools_path)
[INFO] Tool file_operations_reader: semantic operations = ['read', 'write', 'parse', 'transform']
[INFO] Tool file_operations_scanner: semantic operations = ['read', 'write', 'parse', 'transform']
[INFO] Tool network_fetcher: semantic operations = ['read', 'write', 'validate', 'integrate']
[INFO] Tool integration_authenticator: semantic operations = ['integrate', 'transform', 'validate']
[INFO] Tool utility_logger: semantic operations = ['cache', 'process']
[INFO] Tool data_processing_parser: semantic operations = ['validate', 'transform', 'filter', 'aggregate']
[INFO] Tool data_processing_transformer: semantic operations = ['validate', 'transform', 'filter', 'aggregate']
[INFO] Tool data_processing_validator: semantic operations = ['validate', 'transform', 'filter', 'aggregate']
[INFO] Tool network_validator: semantic operations = ['read', 'write', 'validate', 'integrate']
[INFO] Tool computation_calculator: semantic operations = ['compute', 'aggregate', 'transform']
[INFO] Tool data_processing_filter: semantic operations = ['validate', 'transform', 'filter', 'aggregate']
[INFO] Tool data_processing_aggregator: semantic operations = ['validate', 'transform', 'filter', 'aggregate']
[INFO] Tool file_operations_converter: semantic operations = ['read', 'write', 'parse', 'transform']
[INFO] Tool file_operations_compressor: semantic operations = ['read', 'write', 'parse', 'transform']
[INFO] Tool file_operations_writer: semantic operations = ['read', 'write', 'parse', 'transform']
[INFO] Tool network_poster: semantic operations = ['read', 'write', 'validate', 'integrate']
[INFO] Tool network_monitor: semantic operations = ['read', 'write', 'validate', 'integrate']
[INFO] Tool network_router: semantic operations = ['read', 'write', 'validate', 'integrate']
[INFO] Tool computation_predictor: semantic operations = ['compute', 'aggregate', 'transform']
[INFO] Tool computation_analyzer: semantic operations = ['compute', 'aggregate', 'transform']
[INFO] Tool computation_simulator: semantic operations = ['compute', 'aggregate', 'transform']
[INFO] Tool computation_optimizer: semantic operations = ['compute', 'aggregate', 'transform']
[INFO] Tool integration_mapper: semantic operations = ['integrate', 'transform', 'validate']
[INFO] Tool integration_queue: semantic operations = ['integrate', 'transform', 'validate']
[INFO] Tool integration_scheduler: semantic operations = ['integrate', 'transform', 'validate']
[INFO] Tool integration_connector: semantic operations = ['integrate', 'transform', 'validate']
[INFO] Tool utility_cache: semantic operations = ['cache', 'process']
[INFO] Tool utility_tracker: semantic operations = ['cache', 'process']
[INFO] Tool utility_helper: semantic operations = ['cache', 'process']
[INFO] Tool utility_notifier: semantic operations = ['cache', 'process']
[INFO] Setting default state_dim based on loaded tools
[INFO] state_dim set to 345 (tools=30, base=340, task_types=5)
[INFO] Setting default action_dim based on loaded tools
[INFO] action_dim set to 31 (tools=30 + NO_OP)
[INFO] âš¡ SKIP_MODEL_LOADING=true - Skipping neural network model loading
[INFO] âš¡ Memory optimization: Saving ~350MB by not loading model
[INFO] âš¡ Will use pre-generated workflows or random policy
[INFO] Initializing TaskManager...
[TaskManager] Difficulty level 'easy': 1096 tasks
[TaskManager] Difficulty level 'very_easy': 856 tasks
[TaskManager] Difficulty level 'medium': 1136 tasks
[TaskManager] Difficulty level 'hard': 1096 tasks
[TaskManager] Difficulty level 'very_hard': 856 tasks
[INFO] TaskManager initialized with 5040 tasks
[INFO] Task types available: ['basic_task', 'data_pipeline', 'multi_stage_pipeline', 'simple_task', 'api_integration']
[INFO] Initializing ToolCallVerifier...
[INFO] ToolCallVerifier initialized with 30 tools
[INFO] Output tools identified: 1
[INFO] Component initialization status:
  - embedding_manager: initialized
  - task_manager: initialized
  - output_verifier: initialized
  - tool_capability_manager: initialized
  - tool_success_rates: initialized with 0 entries
[INFO] MDPWorkflowGenerator initialization complete
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[FlawedWorkflowGenerator] Initialized with 30 tools
[FlawedWorkflowGenerator] RAG support: disabled
[INFO] Enhanced manager: AIé”™è¯¯åˆ†ç±»ç³»ç»Ÿå·²å¯ç”¨
[INFO] æ£€æµ‹åˆ°å¹¶å‘ç¯å¢ƒï¼Œä½¿ç”¨å®‰å…¨å­˜å‚¨æ¨¡å¼ï¼ˆResultCollectorï¼‰
[INFO] åå°åˆå¹¶è¿›ç¨‹å·²å¯åŠ¨ï¼ˆæ¯10ç§’åˆå¹¶ä¸€æ¬¡ï¼‰2025-08-31 16:48:35,458 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
[INFO] âš¡ SKIP_MODEL_LOADING=true - Skipping torch imports
[INFO] ä½¿ç”¨JSONå­˜å‚¨æ ¼å¼
[INFO] ä½¿ç”¨JSONå­˜å‚¨æ ¼å¼
[INFO] ä½¿ç”¨JSONå­˜å‚¨æ ¼å¼
[INFO] ä½¿ç”¨JSONå­˜å‚¨æ ¼å¼

============================================================
æ™ºèƒ½æ‰¹æµ‹è¯•: qwen2.5-72b-instruct (idealab)
Prompt types: ['baseline']
éš¾åº¦: easy
ç›®æ ‡: æ¯ç§é…ç½® 7 ä¸ªå®ä¾‹
============================================================
â—‹ simple_task         :   0/  7 å·²å®Œæˆ (éœ€è¦è¡¥å…… 7 ä¸ª)
â—‹ basic_task          :   0/  7 å·²å®Œæˆ (éœ€è¦è¡¥å…… 7 ä¸ª)
â—‹ data_pipeline       :   0/  7 å·²å®Œæˆ (éœ€è¦è¡¥å…… 7 ä¸ª)
â—‹ api_integration     :   0/  7 å·²å®Œæˆ (éœ€è¦è¡¥å…… 7 ä¸ª)
â—‹ multi_stage_pipeline:   0/  7 å·²å®Œæˆ (éœ€è¦è¡¥å…… 7 ä¸ª)

â³ éœ€è¦è¿è¡Œ 35 ä¸ªæ–°æµ‹è¯•

â–¶ å‡†å¤‡ simple_task (7 ä¸ªå®ä¾‹)...

â–¶ å‡†å¤‡ basic_task (7 ä¸ªå®ä¾‹)...

â–¶ å‡†å¤‡ data_pipeline (7 ä¸ªå®ä¾‹)...

â–¶ å‡†å¤‡ api_integration (7 ä¸ªå®ä¾‹)...

â–¶ å‡†å¤‡ multi_stage_pipeline (7 ä¸ªå®ä¾‹)...

â–¶ å¼€å§‹æ‰§è¡Œ 35 ä¸ªæµ‹è¯•...
ğŸ“Š è‡ªé€‚åº”checkpoint_interval: 20
ğŸ“¦ æ‰¹é‡æäº¤æ¨¡å¼ï¼šæ¯20ä¸ªæµ‹è¯•ä¿å­˜ä¸€æ¬¡
âš ï¸  æ£€æµ‹åˆ°idealab APIï¼Œè°ƒæ•´å¹¶å‘: workers=2, qps=None
ğŸ§  å¯ç”¨SmartResultCollectoræ¨¡å¼ï¼Œæ™ºèƒ½æ•°æ®ç®¡ç†
[AI_DEBUG] AIåˆ†ç±»å™¨åˆå§‹åŒ–æˆåŠŸ: <txt_based_ai_classifier.TxtBasedAIClassifier object at 0x10ea41080>
[DEBUG] BatchTestRunner initialized with save_logs=False, enable_database_updates=False, use_ai_classification=True, checkpoint_interval=20
[DEBUG] Creating new ToolCapabilityManager instance
[OperationEmbeddingIndex] Initializing with unified API client manager
['config/config.json', 'config/api_keys.json', 'config/api-keys.json']
[OperationEmbeddingIndex] OpenAI client initialized with model: gpt-4o-mini
[OperationEmbeddingIndex] Using embedding model: text-embedding-3-large
[OperationEmbeddingIndex] Detecting actual embedding dimension...
[OperationEmbeddingIndex] Detected embedding dimension: 3072
[INFO] Loaded 4150 embeddings from persistent cache
[OperationEmbeddingIndex] Initialized with 4150 cached embeddings
[INFO] Loaded 15 LLM-enhanced operation definitions from cache
[INFO] Found cached operation index at .mcp_operation_cache/operation_index.pkl
[INFO] Loading operation index from .mcp_operation_cache/operation_index.pkl
[INFO] Successfully loaded FAISS index with dimension 3072
[INFO] Operation index loaded successfully from .mcp_operation_cache/operation_index.pkl
[INFO] Loaded 15 operations with dimension 3072
[INFO] Successfully loaded cached index
[INFO] Operation semantic index initialized
[INFO] âš¡ SKIP_MODEL_LOADING=true - No device initialization
[INFO] Initialized tool success tracking attributes
[INFO] Initializing embedding manager for enhanced tool selection
[MCPEmbeddingManager] Creating new singleton instance
[MCPEmbeddingManager] Initializing with unified API client manager
[MCPEmbeddingManager] Using embedding model: text-embedding-3-large
[MCPEmbeddingManager] Client initialized successfully
[MCPEmbeddingManager] Singleton created with 0 cached embeddings
[INFO] Loading embedding index from .mcp_embedding_cache/tool_index.pkl
[SUCCESS] Loaded 30 tool embeddings
[SUCCESS] Embedding manager initialized with 30 tools
[INFO] Initialized task types: ['simple_task', 'data_pipeline', 'api_integration', 'basic_task', 'multi_stage_pipeline']
[INFO] Loading full MCP protocol registry...
[INFO] Loaded full tool registry with 30 tools
[INFO] Loading tools from mcp_generated_library/tool_registry_consolidated.json
[INFO] Embedding manager ready with 30 tools
[WARNING] Embedding manager exists but has no embeddings
[INFO] Consider rebuilding index with: embedding_manager.build_index(tools_path)
[INFO] Tool file_operations_reader: semantic operations = ['read', 'write', 'parse', 'transform']
[INFO] Tool file_operations_scanner: semantic operations = ['read', 'write', 'parse', 'transform']
[INFO] Tool network_fetcher: semantic operations = ['read', 'write', 'validate', 'integrate']
[INFO] Tool integration_authenticator: semantic operations = ['integrate', 'transform', 'validate']
[INFO] Tool utility_logger: semantic operations = ['cache', 'process']
[INFO] Tool data_processing_parser: semantic operations = ['validate', 'transform', 'filter', 'aggregate']
[INFO] Tool data_processing_transformer: semantic operations = ['validate', 'transform', 'filter', 'aggregate']
[INFO] Tool data_processing_validator: semantic operations = ['validate', 'transform', 'filter', 'aggregate']
[INFO] Tool network_validator: semantic operations = ['read', 'write', 'validate', 'integrate']
[INFO] Tool computation_calculator: semantic operations = ['compute', 'aggregate', 'transform']
[INFO] Tool data_processing_filter: semantic operations = ['validate', 'transform', 'filter', 'aggregate']
[INFO] Tool data_processing_aggregator: semantic operations = ['validate', 'transform', 'filter', 'aggregate']
[INFO] Tool file_operations_converter: semantic operations = ['read', 'write', 'parse', 'transform']
[INFO] Tool file_operations_compressor: semantic operations = ['read', 'write', 'parse', 'transform']
[INFO] Tool file_operations_writer: semantic operations = ['read', 'write', 'parse', 'transform']
[INFO] Tool network_poster: semantic operations = ['read', 'write', 'validate', 'integrate']
[INFO] Tool network_monitor: semantic operations = ['read', 'write', 'validate', 'integrate']
[INFO] Tool network_router: semantic operations = ['read', 'write', 'validate', 'integrate']
[INFO] Tool computation_predictor: semantic operations = ['compute', 'aggregate', 'transform']
[INFO] Tool computation_analyzer: semantic operations = ['compute', 'aggregate', 'transform']
[INFO] Tool computation_simulator: semantic operations = ['compute', 'aggregate', 'transform']
[INFO] Tool computation_optimizer: semantic operations = ['compute', 'aggregate', 'transform']
[INFO] Tool integration_mapper: semantic operations = ['integrate', 'transform', 'validate']
[INFO] Tool integration_queue: semantic operations = ['integrate', 'transform', 'validate']
[INFO] Tool integration_scheduler: semantic operations = ['integrate', 'transform', 'validate']
[INFO] Tool integration_connector: semantic operations = ['integrate', 'transform', 'validate']
[INFO] Tool utility_cache: semantic operations = ['cache', 'process']
[INFO] Tool utility_tracker: semantic operations = ['cache', 'process']
[INFO] Tool utility_helper: semantic operations = ['cache', 'process']
[INFO] Tool utility_notifier: semantic operations = ['cache', 'process']
[INFO] Setting default state_dim based on loaded tools
[INFO] state_dim set to 345 (tools=30, base=340, task_types=5)
[INFO] Setting default action_dim based on loaded tools
[INFO] action_dim set to 31 (tools=30 + NO_OP)
[INFO] âš¡ SKIP_MODEL_LOADING=true - Skipping neural network model loading
[INFO] âš¡ Memory optimization: Saving ~350MB by not loading model
[INFO] âš¡ Will use pre-generated workflows or random policy
[INFO] Initializing TaskManager...
[TaskManager] Difficulty level 'easy': 1096 tasks
[TaskManager] Difficulty level 'very_easy': 856 tasks
[TaskManager] Difficulty level 'medium': 1136 tasks
[TaskManager] Difficulty level 'hard': 1096 tasks
[TaskManager] Difficulty level 'very_hard': 856 tasks
[INFO] TaskManager initialized with 5040 tasks
[INFO] Task types available: ['basic_task', 'data_pipeline', 'multi_stage_pipeline', 'simple_task', 'api_integration']
[INFO] Initializing ToolCallVerifier...
[INFO] ToolCallVerifier initialized with 30 tools
[INFO] Output tools identified: 1
[INFO] Component initialization status:
  - embedding_manager: initialized
  - task_manager: initialized
  - output_verifier: initialized
  - tool_capability_manager: initialized
  - tool_success_rates: initialized with 0 entries
[INFO] MDPWorkflowGenerator initialization complete
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[FlawedWorkflowGenerator] Initialized with 30 tools
[FlawedWorkflowGenerator] RAG support: disabled
[INFO] Enhanced manager: AIé”™è¯¯åˆ†ç±»ç³»ç»Ÿå·²å¯ç”¨
[INFO] æ£€æµ‹åˆ°å¹¶å‘ç¯å¢ƒï¼Œä½¿ç”¨å®‰å…¨å­˜å‚¨æ¨¡å¼ï¼ˆResultCollectorï¼‰
[INFO] åå°åˆå¹¶è¿›ç¨‹å·²å¯åŠ¨ï¼ˆæ¯10ç§’åˆå¹¶ä¸€æ¬¡ï¼‰2025-08-31 16:48:35,472 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:48:35,473 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:48:35,500 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:48:35,500 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:48:35,500 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:48:35,518 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
[INFO] âš¡ SKIP_MODEL_LOADING=true - Skipping torch imports
[INFO] ä½¿ç”¨JSONå­˜å‚¨æ ¼å¼
[INFO] ä½¿ç”¨JSONå­˜å‚¨æ ¼å¼
[INFO] ä½¿ç”¨JSONå­˜å‚¨æ ¼å¼
[INFO] ä½¿ç”¨JSONå­˜å‚¨æ ¼å¼

============================================================
æ™ºèƒ½æ‰¹æµ‹è¯•: qwen2.5-72b-instruct (idealab)
Prompt types: ['baseline']
éš¾åº¦: easy
ç›®æ ‡: æ¯ç§é…ç½® 7 ä¸ªå®ä¾‹
============================================================
â—‹ simple_task         :   0/  7 å·²å®Œæˆ (éœ€è¦è¡¥å…… 7 ä¸ª)
â—‹ basic_task          :   0/  7 å·²å®Œæˆ (éœ€è¦è¡¥å…… 7 ä¸ª)
â—‹ data_pipeline       :   0/  7 å·²å®Œæˆ (éœ€è¦è¡¥å…… 7 ä¸ª)
â—‹ api_integration     :   0/  7 å·²å®Œæˆ (éœ€è¦è¡¥å…… 7 ä¸ª)
â—‹ multi_stage_pipeline:   0/  7 å·²å®Œæˆ (éœ€è¦è¡¥å…… 7 ä¸ª)

â³ éœ€è¦è¿è¡Œ 35 ä¸ªæ–°æµ‹è¯•

â–¶ å‡†å¤‡ simple_task (7 ä¸ªå®ä¾‹)...

â–¶ å‡†å¤‡ basic_task (7 ä¸ªå®ä¾‹)...

â–¶ å‡†å¤‡ data_pipeline (7 ä¸ªå®ä¾‹)...

â–¶ å‡†å¤‡ api_integration (7 ä¸ªå®ä¾‹)...

â–¶ å‡†å¤‡ multi_stage_pipeline (7 ä¸ªå®ä¾‹)...

â–¶ å¼€å§‹æ‰§è¡Œ 35 ä¸ªæµ‹è¯•...
ğŸ“Š è‡ªé€‚åº”checkpoint_interval: 20
ğŸ“¦ æ‰¹é‡æäº¤æ¨¡å¼ï¼šæ¯20ä¸ªæµ‹è¯•ä¿å­˜ä¸€æ¬¡
âš ï¸  æ£€æµ‹åˆ°idealab APIï¼Œè°ƒæ•´å¹¶å‘: workers=2, qps=None
ğŸ§  å¯ç”¨SmartResultCollectoræ¨¡å¼ï¼Œæ™ºèƒ½æ•°æ®ç®¡ç†
[AI_DEBUG] AIåˆ†ç±»å™¨åˆå§‹åŒ–æˆåŠŸ: <txt_based_ai_classifier.TxtBasedAIClassifier object at 0x10bb0eaa0>
[DEBUG] BatchTestRunner initialized with save_logs=False, enable_database_updates=False, use_ai_classification=True, checkpoint_interval=20
[DEBUG] Creating new ToolCapabilityManager instance
[OperationEmbeddingIndex] Initializing with unified API client manager
['config/config.json', 'config/api_keys.json', 'config/api-keys.json']
[OperationEmbeddingIndex] OpenAI client initialized with model: gpt-4o-mini
[OperationEmbeddingIndex] Using embedding model: text-embedding-3-large
[OperationEmbeddingIndex] Detecting actual embedding dimension...
[OperationEmbeddingIndex] Detected embedding dimension: 3072
[INFO] Loaded 4150 embeddings from persistent cache
[OperationEmbeddingIndex] Initialized with 4150 cached embeddings
[INFO] Loaded 15 LLM-enhanced operation definitions from cache
[INFO] Found cached operation index at .mcp_operation_cache/operation_index.pkl
[INFO] Loading operation index from .mcp_operation_cache/operation_index.pkl
[INFO] Successfully loaded FAISS index with dimension 3072
[INFO] Operation index loaded successfully from .mcp_operation_cache/operation_index.pkl
[INFO] Loaded 15 operations with dimension 3072
[INFO] Successfully loaded cached index
[INFO] Operation semantic index initialized
[INFO] âš¡ SKIP_MODEL_LOADING=true - No device initialization
[INFO] Initialized tool success tracking attributes
[INFO] Initializing embedding manager for enhanced tool selection
[MCPEmbeddingManager] Creating new singleton instance
[MCPEmbeddingManager] Initializing with unified API client manager
[MCPEmbeddingManager] Using embedding model: text-embedding-3-large
[MCPEmbeddingManager] Client initialized successfully
[MCPEmbeddingManager] Singleton created with 0 cached embeddings
[INFO] Loading embedding index from .mcp_embedding_cache/tool_index.pkl
[SUCCESS] Loaded 30 tool embeddings
[SUCCESS] Embedding manager initialized with 30 tools
[INFO] Initialized task types: ['simple_task', 'data_pipeline', 'api_integration', 'basic_task', 'multi_stage_pipeline']
[INFO] Loading full MCP protocol registry...
[INFO] Loaded full tool registry with 30 tools
[INFO] Loading tools from mcp_generated_library/tool_registry_consolidated.json
[INFO] Embedding manager ready with 30 tools
[WARNING] Embedding manager exists but has no embeddings
[INFO] Consider rebuilding index with: embedding_manager.build_index(tools_path)
[INFO] Tool file_operations_reader: semantic operations = ['read', 'write', 'parse', 'transform']
[INFO] Tool file_operations_scanner: semantic operations = ['read', 'write', 'parse', 'transform']
[INFO] Tool network_fetcher: semantic operations = ['read', 'write', 'validate', 'integrate']
[INFO] Tool integration_authenticator: semantic operations = ['integrate', 'transform', 'validate']
[INFO] Tool utility_logger: semantic operations = ['cache', 'process']
[INFO] Tool data_processing_parser: semantic operations = ['validate', 'transform', 'filter', 'aggregate']
[INFO] Tool data_processing_transformer: semantic operations = ['validate', 'transform', 'filter', 'aggregate']
[INFO] Tool data_processing_validator: semantic operations = ['validate', 'transform', 'filter', 'aggregate']
[INFO] Tool network_validator: semantic operations = ['read', 'write', 'validate', 'integrate']
[INFO] Tool computation_calculator: semantic operations = ['compute', 'aggregate', 'transform']
[INFO] Tool data_processing_filter: semantic operations = ['validate', 'transform', 'filter', 'aggregate']
[INFO] Tool data_processing_aggregator: semantic operations = ['validate', 'transform', 'filter', 'aggregate']
[INFO] Tool file_operations_converter: semantic operations = ['read', 'write', 'parse', 'transform']
[INFO] Tool file_operations_compressor: semantic operations = ['read', 'write', 'parse', 'transform']
[INFO] Tool file_operations_writer: semantic operations = ['read', 'write', 'parse', 'transform']
[INFO] Tool network_poster: semantic operations = ['read', 'write', 'validate', 'integrate']
[INFO] Tool network_monitor: semantic operations = ['read', 'write', 'validate', 'integrate']
[INFO] Tool network_router: semantic operations = ['read', 'write', 'validate', 'integrate']
[INFO] Tool computation_predictor: semantic operations = ['compute', 'aggregate', 'transform']
[INFO] Tool computation_analyzer: semantic operations = ['compute', 'aggregate', 'transform']
[INFO] Tool computation_simulator: semantic operations = ['compute', 'aggregate', 'transform']
[INFO] Tool computation_optimizer: semantic operations = ['compute', 'aggregate', 'transform']
[INFO] Tool integration_mapper: semantic operations = ['integrate', 'transform', 'validate']
[INFO] Tool integration_queue: semantic operations = ['integrate', 'transform', 'validate']
[INFO] Tool integration_scheduler: semantic operations = ['integrate', 'transform', 'validate']
[INFO] Tool integration_connector: semantic operations = ['integrate', 'transform', 'validate']
[INFO] Tool utility_cache: semantic operations = ['cache', 'process']
[INFO] Tool utility_tracker: semantic operations = ['cache', 'process']
[INFO] Tool utility_helper: semantic operations = ['cache', 'process']
[INFO] Tool utility_notifier: semantic operations = ['cache', 'process']
[INFO] Setting default state_dim based on loaded tools
[INFO] state_dim set to 345 (tools=30, base=340, task_types=5)
[INFO] Setting default action_dim based on loaded tools
[INFO] action_dim set to 31 (tools=30 + NO_OP)
[INFO] âš¡ SKIP_MODEL_LOADING=true - Skipping neural network model loading
[INFO] âš¡ Memory optimization: Saving ~350MB by not loading model
[INFO] âš¡ Will use pre-generated workflows or random policy
[INFO] Initializing TaskManager...
[TaskManager] Difficulty level 'easy': 1096 tasks
[TaskManager] Difficulty level 'very_easy': 856 tasks
[TaskManager] Difficulty level 'medium': 1136 tasks
[TaskManager] Difficulty level 'hard': 1096 tasks
[TaskManager] Difficulty level 'very_hard': 856 tasks
[INFO] TaskManager initialized with 5040 tasks
[INFO] Task types available: ['basic_task', 'data_pipeline', 'multi_stage_pipeline', 'simple_task', 'api_integration']
[INFO] Initializing ToolCallVerifier...
[INFO] ToolCallVerifier initialized with 30 tools
[INFO] Output tools identified: 1
[INFO] Component initialization status:
  - embedding_manager: initialized
  - task_manager: initialized
  - output_verifier: initialized
  - tool_capability_manager: initialized
  - tool_success_rates: initialized with 0 entries
[INFO] MDPWorkflowGenerator initialization complete
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[FlawedWorkflowGenerator] Initialized with 30 tools
[FlawedWorkflowGenerator] RAG support: disabled
[INFO] Enhanced manager: AIé”™è¯¯åˆ†ç±»ç³»ç»Ÿå·²å¯ç”¨
[INFO] æ£€æµ‹åˆ°å¹¶å‘ç¯å¢ƒï¼Œä½¿ç”¨å®‰å…¨å­˜å‚¨æ¨¡å¼ï¼ˆResultCollectorï¼‰
[INFO] åå°åˆå¹¶è¿›ç¨‹å·²å¯åŠ¨ï¼ˆæ¯10ç§’åˆå¹¶ä¸€æ¬¡ï¼‰2025-08-31 16:48:35,962 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:36,036 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:36,592 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:36,664 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:36,808 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:36,854 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:36,863 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:48:36,863 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:48:36,928 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:48:36,928 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:48:36,928 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:48:37,207 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:37,511 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:37,517 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:37,567 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:37,749 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:38,651 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:39,081 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:39,176 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:39,587 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:39,771 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:39,985 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:40,112 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:40,292 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:40,637 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:40,927 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:41,160 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:41,160 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:41,479 - result_merger - INFO - ğŸ›‘ è¿ç»­3è½®æ— æ–°æ–‡ä»¶ï¼Œè‡ªåŠ¨åœæ­¢åˆå¹¶å™¨é˜²æ­¢hangä½
2025-08-31 16:48:41,480 - result_merger - INFO - ğŸ ResultMergeråˆå¹¶å¾ªç¯å·²ç»“æŸ
2025-08-31 16:48:41,820 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:42,054 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:42,310 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:42,424 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:48:42,581 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:42,733 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:42,917 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:43,163 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:43,870 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:43,985 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:44,304 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:44,389 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:44,829 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:44,842 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:48:44,842 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:48:44,893 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:48:44,893 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:48:44,893 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:48:45,353 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:45,462 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:45,585 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:46,055 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:46,401 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:46,480 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:46,925 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:46,925 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:48:46,955 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:47,179 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:47,449 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:47,973 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:48,061 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:48,333 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:48,497 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:48,585 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:49,021 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:49,573 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:49,587 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:48:49,587 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:48:49,706 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:48:49,706 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:48:49,706 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:48:49,923 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:48:50,124 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:50,170 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:50,642 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:51,119 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:51,341 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:51,473 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

DEBUG: Checking generator attributes
  - has tool_capabilities: True
  - has tool_capability_manager: True
  - has task_manager: True
[INFO] Loaded 30 tools from generator
[INFO] Tool names sample: ['computation_analyzer', 'computation_calculator', 'computation_optimizer', 'computation_predictor', 'computation_simulator']...
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Initializing LLM client using APIClientManager
[INFO] Using Azure OpenAI client
[DEBUG] Checking if generator has tool_capability_manager attribute
[DEBUG] Creating FlawedWorkflowGenerator with correct parameters
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[FlawedWorkflowGenerator] Initialized with 30 tools
[FlawedWorkflowGenerator] RAG support: enabled
[INFO] FlawedWorkflowGenerator initialized successfully
[INFO] Initializing StableScorer for Phase 2 scoring
<tool_capability_manager.ToolCapabilityManager object at 0x12b991140>
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Loaded tool success history for 0 tools
[INFO] StableScorer initialized with semantic capability
[INFO] StableScorer initialized successfully
[INFO] Loading task instances...
[INFO] Loading task instances from mcp_generated_library/difficulty_versions/task_library_enhanced_v3_easy.json
[INFO] Loaded 630 task instances
[INFO] Task instances loaded: ['basic_task', 'data_pipeline', 'multi_stage_pipeline', 'simple_task', 'api_integration']
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"2228cc42-d85b-9928-9819-8c5e776255e3"}, traceId: 213e001317566733026941414e0a87'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.5s before retry (not counting as turn)...
  [SEARCH] Query: data validation compliance

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"1cd6b730-00a9-9e9a-a03b-b39bd1d95512"}, traceId: 213e007417566733043171914eef6b'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"724bbd62-9be3-96fb-8b41-46efb7420540"}, traceId: 213e007517566733048266653ef08e'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: data validation parser

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"e58893c7-c7ff-9948-b787-75b69baba08a"}, traceId: 213e064617566733075924528e09cc'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"5837f0e9-8ab7-9869-a685-8437b322211c"}, traceId: 213e01f617566733095228693e154b'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"14500cf6-6dc4-994f-a4ec-d253cf011410"}, traceId: 213e007317566733109583413ee522'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"2a37630e-79e4-9079-88b7-d1e8e1581c28"}, traceId: 213e065c17566733115015443e80d9'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.8s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"8aab4965-8a30-9a5e-809f-2a38c1248229"}, traceId: 2150435d17566733147838058e2004'}2025-08-31 16:48:52,165 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"

DEBUG: Checking generator attributes
  - has tool_capabilities: True
  - has tool_capability_manager: True
  - has task_manager: True
[INFO] Loaded 30 tools from generator
[INFO] Tool names sample: ['computation_analyzer', 'computation_calculator', 'computation_optimizer', 'computation_predictor', 'computation_simulator']...
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Initializing LLM client using APIClientManager
[INFO] Using Azure OpenAI client
[DEBUG] Checking if generator has tool_capability_manager attribute
[DEBUG] Creating FlawedWorkflowGenerator with correct parameters
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[FlawedWorkflowGenerator] Initialized with 30 tools
[FlawedWorkflowGenerator] RAG support: enabled
[INFO] FlawedWorkflowGenerator initialized successfully
[INFO] Initializing StableScorer for Phase 2 scoring
<tool_capability_manager.ToolCapabilityManager object at 0x128bbcb40>
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Loaded tool success history for 0 tools
[INFO] StableScorer initialized with semantic capability
[INFO] StableScorer initialized successfully
[INFO] Loading task instances...
[INFO] Loading task instances from mcp_generated_library/difficulty_versions/task_library_enhanced_v3_easy.json
[INFO] Loaded 630 task instances
[INFO] Task instances loaded: ['basic_task', 'data_pipeline', 'multi_stage_pipeline', 'simple_task', 'api_integration']
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json

[TURN 1/10]
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"c4531467-42f5-92fe-80a8-b97ab66e2041"}, traceId: 213e03d917566733026987310e1f2b'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
  [SEARCH] Query: data validation parser

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"103fa082-a7d8-9d28-af22-1801d1200196"}, traceId: 213e007617566733047166351e1381'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"e24fac12-7259-937b-8c56-2bbe3356ddaa"}, traceId: 213e04ea17566733067022119e371b'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.5s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"13cf2203-82b2-969a-95d9-831f399a7682"}, traceId: 213e081017566733072075855e0bc8'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"1f612d0e-4fc5-926d-a924-f7f60629d151"}, traceId: 213e069217566733104661531e81ef'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.2s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"2bb831b3-aa2d-96c8-9657-81dd30435de3"}, traceId: 213e06b617566733119162960e8126'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"46df77c4-4904-9d7a-935e-936b094bf173"}, traceId: 2150421317566733147281366e360c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[DEBUG] Got result for task: has_result=True, save_logs=False2025-08-31 16:48:52,211 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"

DEBUG: Checking generator attributes
  - has tool_capabilities: True
  - has tool_capability_manager: True
  - has task_manager: True
[INFO] Loaded 30 tools from generator
[INFO] Tool names sample: ['computation_analyzer', 'computation_calculator', 'computation_optimizer', 'computation_predictor', 'computation_simulator']...
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Initializing LLM client using APIClientManager
[INFO] Using Azure OpenAI client
[DEBUG] Checking if generator has tool_capability_manager attribute
[DEBUG] Creating FlawedWorkflowGenerator with correct parameters
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[FlawedWorkflowGenerator] Initialized with 30 tools
[FlawedWorkflowGenerator] RAG support: enabled
[INFO] FlawedWorkflowGenerator initialized successfully
[INFO] Initializing StableScorer for Phase 2 scoring
<tool_capability_manager.ToolCapabilityManager object at 0x129f5c540>
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Loaded tool success history for 0 tools
[INFO] StableScorer initialized with semantic capability
[INFO] StableScorer initialized successfully
[INFO] Loading task instances...
[INFO] Loading task instances from mcp_generated_library/difficulty_versions/task_library_enhanced_v3_easy.json
[INFO] Loaded 630 task instances
[INFO] Task instances loaded: ['basic_task', 'data_pipeline', 'multi_stage_pipeline', 'simple_task', 'api_integration']
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: data processing validator

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: data validation parser

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"fd5088f5-69e0-9257-b765-7947e1984999"}, traceId: 213e007e17566733047061487eee51'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"c0f96d25-a530-946a-9342-f4d4ace696f6"}, traceId: 213e007d17566733066698980eef5d'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.1s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"ab8465b8-22be-93ce-bafe-f8355d584a04"}, traceId: 213e06c817566733071851729e8245'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"b8e5ece1-c5d1-9cae-85e1-a1160117dda9"}, traceId: 213e006017566733098025815ee2dc'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"299688a0-a347-92d4-927b-6aa6b12d138a"}, traceId: 213e068217566733116654044e77e2'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.7s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"75906550-3081-9021-8208-6a09200a6dc1"}, traceId: 213e007a17566733111552779ee273'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"faa630e4-f6c3-9da0-9e90-0f2af609238e"}, traceId: 213e007317566733139003316ee41a'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...2025-08-31 16:48:52,536 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:52,546 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:48:52,546 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:48:52,575 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:48:52,575 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:48:52,575 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:48:53,090 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:53,215 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:53,215 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:53,922 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:53,999 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:54,039 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:54,263 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:48:54,381 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:54,390 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:48:54,390 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:48:54,415 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:48:54,415 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:48:54,415 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:48:54,999 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:55,002 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:55,439 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:55,836 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:55,836 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:55,978 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:56,715 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:56,885 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:57,192 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:57,235 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:57,809 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:57,932 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:58,779 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:58,867 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:59,507 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:48:59,873 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:59,884 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:48:59,885 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:48:59,886 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:48:59,910 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:48:59,910 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:48:59,910 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:49:00,369 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:49:00,425 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:00,433 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:49:00,433 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:49:00,455 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:49:00,455 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:49:00,455 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:49:00,555 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:00,568 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:49:00,568 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:49:00,593 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:49:00,593 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:49:00,593 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:49:00,720 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:01,145 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:01,243 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:01,620 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:01,659 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:01,673 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:02,126 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:02,129 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:02,135 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:49:02,135 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:49:02,159 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:02,159 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:49:02,159 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:49:02,159 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:49:02,651 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:03,490 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:03,973 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:04,022 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:04,051 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:04,389 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:04,971 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:05,007 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:05,455 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:49:05,856 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:06,000 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:06,136 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:49:06,356 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:06,426 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:06,500 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:06,855 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:06,947 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:07,012 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:07,392 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 3205
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=3205
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"3db594f5-e4a7-934f-904b-2819a0d72b74"}, traceId: 213e06c217566733159647925e8294'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"1a87c0b7-a77f-9ce2-95be-41a4192ad883"}, traceId: 213e066c17566733173697873e7990'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
  [SEARCH] Query: data validation parser

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: data validation parser

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"b5388355-7e3e-98d2-b296-ef3898113a99"}, traceId: 2150417c17566733195968017ef017'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
  [INFO] Tool info request: data_processing_validator

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"2aa9a524-cf05-909b-970a-9ca978dc62bc"}, traceId: 215041de17566733213432705e363d'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_selection_errors, confidence=0.84
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22661
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22661
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"2e9d4d3e-2869-96fd-954d-0a1075a29a4e"}, traceId: 2150429e17566733218736397e2273'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"b9e8f66a-3bdc-9105-afad-22c8478de2f4"}, traceId: 2150434117566733236742190e1ea5'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"af02bb01-bc59-9746-b200-94d9f8d12958"}, traceId: 2150415c17566733247302776eeb93'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.7s before retry (not counting as turn)...
  [INFO] Tool info request: data_processing_parser

[TURN 4/10]
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"d829ea44-c8a5-9b6d-a695-7b72a0298608"}, traceId: 2150436a17566733276277897e24da'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.3s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns2025-08-31 16:49:07,406 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:07,615 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:49:07,732 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:07,904 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:08,418 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"2bcf21fa-054e-9e1d-bf8d-c3db79447824"}, traceId: 2150430d17566733167726511e97b0'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"ca9ba7c5-76d0-9c03-a3c5-7bbaceb0dddf"}, traceId: 215042f917566733172793958e1bba'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"48b5379f-2e16-9fb2-991e-12dbe1308fb9"}, traceId: 213e06a217566733187808389e81b8'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"23497608-535b-9558-86d5-c0bfc0246337"}, traceId: 2150436817566733216052181e1e79'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"40a10509-8f58-9074-888a-0deb53bd4644"}, traceId: 2150421317566733235755059e34a2'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.7s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22662
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22662
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: data validation parser

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"2063e93f-0302-99d2-82c6-bb192f7ac817"}, traceId: 2150415c17566733267466369eecf6'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"a2d9807d-5708-902e-8669-ead2c0204cb2"}, traceId: 2150417517566733273606037ee024'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.2s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"12032819-1593-9cbd-bc06-645db78efa0a"}, traceId: 2150430d17566733278781291e9624'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.9s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.7
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"db143af7-7de5-9adf-8b7a-2f64ca043c94"}, traceId: 2150429e17566733294731032e239c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.1s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"0daceaec-df6d-9288-845f-734b70c929c6"}, traceId: 2150416417566733307734108e10f5'}2025-08-31 16:49:08,642 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"


[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"92fe3ffd-8188-9de4-9199-767bcba4d783"}, traceId: 215041a817566733155961063e33f7'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"10c6b5ec-355e-9c85-b8cd-3c231de245d7"}, traceId: 213e007c17566733179298098ef242'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"b8b15d4c-0288-9dc4-9482-e11898f5f429"}, traceId: 2150419d17566733198631201e2d74'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"0391938e-14f5-9484-80c3-6c49b3d9febc"}, traceId: 2150434117566733203698312e2103'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"45b853d5-b573-97f2-a6bb-0d6a4b97da81"}, traceId: 2150430c17566733219577999e2185'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.7s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"8beaf347-f310-932d-a757-51c293add434"}, traceId: 215040b917566733245907741ef1e0'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"c7986361-8de7-9673-97b6-d83bcc8241b6"}, traceId: 2150417c17566733257703394eeeef'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"fde34335-2b6e-9c62-a249-99cfb7fc044a"}, traceId: 215040b917566733266198834ef285'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"65108ca8-4488-9201-b56b-310b431e6ea0"}, traceId: 215042ae17566733277528258e92ae'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22460
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22460
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"a31320e9-a03b-98af-a569-a55da41733ab"}, traceId: 2150409b17566733298913344e094b'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.8s before retry (not counting as turn)...
  [SEARCH] Query: data validation parser

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct2025-08-31 16:49:09,321 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:09,464 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:09,990 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:10,374 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:10,747 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:11,039 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:11,060 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:49:11,263 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:11,408 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:11,564 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:11,751 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:12,088 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:12,088 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:12,103 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:49:12,104 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:49:12,174 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:49:12,174 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:49:12,174 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:49:12,642 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:13,305 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:13,489 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:13,676 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:49:14,392 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:14,440 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:14,813 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:14,853 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:14,881 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:14,928 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:15,324 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:15,388 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:15,829 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:15,848 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:15,944 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:16,489 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:16,681 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:16,691 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:17,024 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:17,082 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:17,583 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:17,637 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:49:18,010 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:18,214 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:18,433 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:18,434 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:18,446 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:49:18,447 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:49:18,479 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:49:18,479 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:49:18,479 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:49:18,970 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:19,039 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:19,527 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:19,626 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:19,952 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:19,961 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:20,015 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:20,905 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:21,369 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:21,654 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:21,685 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:21,796 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:22,137 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:22,248 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:22,415 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:22,733 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:22,825 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:23,638 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"


[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"96e6c74e-1573-9d51-9e2d-ae59ebe81070"}, traceId: 2150417917566733318346452eef33'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 11254
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=11254
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: data validation schema compliance

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [INFO] Tool info request: data_processing_validator

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: data validation parser

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"2070562e-8c19-95e5-bb9f-8846a37f6923"}, traceId: 2150416317566733365418235e16be'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"a39e57b3-ec62-98b3-847f-98c36edc514f"}, traceId: 2150415b17566733387437331ee59c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.6s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_selection_errors, confidence=0.68
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 26053
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=26053
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"5fd22dee-be0f-9a53-9caa-1fc74288c9d6"}, traceId: 213e006717566733409084533edfcd'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"04fab326-42cd-90cb-b524-a5baf8b4cc79"}, traceId: 213e063817566733414252045e80ba'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.3s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"5cea8c35-6fb2-9251-8870-a9726681fbfb"}, traceId: 213e043517566733433587873e2e5a'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.7s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.7
  [INFO] Tool info request: data_processing_parser

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"886243f6-a01f-9998-94a4-6be8719041a5"}, traceId: 215045f517566733466932530e815a'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...2025-08-31 16:49:23,683 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:24,145 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:24,145 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:24,513 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:24,669 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:24,929 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:25,524 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:25,829 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:26,241 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:49:26,447 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:26,458 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:26,489 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:26,924 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"

[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.1s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"b92d26c2-6ac2-91f3-a68d-4d996e5727e6"}, traceId: 2150415d17566733324786212e11cc'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"dc6e998f-be4b-9b9f-b52b-a070bd6f4306"}, traceId: 215040ed17566733347408809ebe22'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 4.6s before retry (not counting as turn)...
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"bd4bc796-2651-97e9-903d-dfd189eed92a"}, traceId: 215040cc17566733364497351ed541'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.4s before retry (not counting as turn)...
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"105919ff-7363-91eb-9f3d-feb88288cfd6"}, traceId: 213e066e17566733398168115e8154'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 11420
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=11420
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: data validation parser

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"ffcb3a5a-a042-9df6-8078-e26a2a2859ad"}, traceId: 213e059717566733418605739e37bd'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
  [SEARCH] Query: data validation parser

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"91d4b1e4-4123-95c3-b2a2-2328b38afa50"}, traceId: 213e06ba17566733436783927e8a60'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"918b567c-7d16-9e2c-9214-240d9cc5e234"}, traceId: 213e007c17566733456572312ef284'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"fd8d16a5-e466-98ea-a6b9-e94a1ab237d7"}, traceId: 2150417d17566733472046980e144a'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=sequence_order_errors, confidence=0.72
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22864
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22864
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"20e9e962-a9b8-97af-809e-397633d60407"}, traceId: 2150457117566733477007844e786a'}2025-08-31 16:49:26,980 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:27,453 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:27,457 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:27,943 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:28,072 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:28,204 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"a0c8d7ab-4b10-954b-9dcf-27ebd0becf8a"}, traceId: 215040ed17566733337872180ebf34'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 4.8s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.88
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22728
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22728
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"e3ff6d2f-b5c8-981e-8610-a5403a5c7041"}, traceId: 2150414417566733397206572edb5d'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[ASSISTED] Task received 3 format helps, final result: failure
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: data validation parser

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"f3421a8e-069c-9d8d-be19-79fb933f1691"}, traceId: 215040cc17566733409627301ed628'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"e6173f6c-1187-9434-b9d6-94bed077eba5"}, traceId: 213e007517566733414306516eef4f'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"1af12061-4bd7-9b05-a446-6f34f6c8e966"}, traceId: 213e05ab17566733437592661e36fe'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
  [SEARCH] Query: data validation parser

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.86
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 18432
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=18432
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"84e8b61c-5847-9cf4-8707-79651fb93aa2"}, traceId: 215045ee17566733457088991e78bf'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.0s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"0925d5a3-4d06-9968-a477-d7747767573b"}, traceId: 2150416a17566733479197831edd8f'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...2025-08-31 16:49:28,864 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:29,251 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:29,400 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:29,528 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:29,935 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:30,437 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:30,437 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:30,450 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:49:30,451 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:49:30,491 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:49:30,491 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:49:30,491 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:49:30,960 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:31,045 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:31,055 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:49:31,056 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:49:31,086 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:49:31,086 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:49:31,086 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:49:31,353 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:31,485 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:31,623 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:32,009 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:32,206 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:32,914 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:33,082 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:33,092 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:49:33,092 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:49:33,114 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:49:33,114 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:49:33,114 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:49:33,946 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:34,107 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:34,120 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:49:34,121 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:49:34,147 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:49:34,147 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:49:34,147 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:49:34,252 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:34,523 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:34,895 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:34,905 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:49:34,905 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:49:34,931 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:49:34,931 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:49:34,931 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:49:35,212 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:35,394 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:35,420 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:49:35,862 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:36,048 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:36,333 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:36,407 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:49:36,420 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:36,876 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:36,937 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:37,336 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:37,913 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:38,300 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:38,303 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:38,431 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:38,826 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:38,826 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:38,826 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"090fc931-876c-95de-bbb2-78863fa07238"}, traceId: 213e007a17566733472054766ee507'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.7s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"d35f1b32-c3c1-9cb8-937c-1a3f23b6ead5"}, traceId: 213e006e17566733486865690e1315'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"3adc3291-5cd7-96bc-939d-b2f1cb184e7e"}, traceId: 213e007d17566733507061329ef0a7'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.1s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22716
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22716
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"27d968bd-7f67-9700-96c6-98107dd65092"}, traceId: 213e03e217566733526052122e1dcf'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: data validation parser

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"eaf356ff-c0c3-953a-a17e-e8c3b089b5ff"}, traceId: 213e007d17566733547243000ef275'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.7s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"5a13455b-23bc-9d63-a4c3-0a64a8136033"}, traceId: 215045af17566733557903743e7fa5'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"861827cf-10e8-9f87-bdc0-a29431baee69"}, traceId: 213e081017566733577162332e0aa9'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.8s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"00450022-acde-9b86-a5f2-37d34d177f6c"}, traceId: 215045c117566733591727296e7f86'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"04a39096-b58f-9a62-8640-d1e8b5ccd86b"}, traceId: 213e007417566733615584717eecf8'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.5s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"ab495f1a-f618-9e8d-9008-f244b6e39a6a"}, traceId: 213e007617566733629253333e1143'}2025-08-31 16:49:39,349 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:39,543 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:39,874 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:40,258 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:40,732 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:41,055 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:49:41,277 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.2s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"80a979be-7baa-932f-9e6f-383a3d8c88f7"}, traceId: 2150459f17566733486082522e8202'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.7s before retry (not counting as turn)...
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"fc94f671-d158-9de9-9a80-58bc23afc9db"}, traceId: 213e006d17566733508247171e12dd'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.4s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"bd901120-5392-9cfa-a446-6b18a7178409"}, traceId: 2150460e17566733513257497e7937'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.0s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"5328d6fd-6ed1-93ca-b639-522b55f3af77"}, traceId: 215045a817566733546336683e7e03'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.2s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"04c2dcff-11d6-9bc6-8c75-d5382e200b86"}, traceId: 2150452b17566733568782968e7b24'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"fa1e595e-6b93-99fb-b924-c09e881bb08c"}, traceId: 2150456317566733577085205e802f'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 11834
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=11834
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"d3399a9d-a1fa-9fda-8d3a-358bf124c642"}, traceId: 215045b817566733589247968e800c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: data validation parser

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"d00719d2-6f1a-9bbc-8c9f-c1117999dadd"}, traceId: 213e064d17566733614407553e83ae'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"c9d4de63-bee8-9372-91fb-064e54b0a69e"}, traceId: 2150459f17566733638071642e7fd3'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=sequence_order_errors, confidence=0.75
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct2025-08-31 16:49:41,734 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:49:41,806 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:41,849 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:42,343 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:42,673 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:43,200 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:49:43,211 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:43,370 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:43,695 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:43,708 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:43,871 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:44,254 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:44,431 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:44,592 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:44,708 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:44,830 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:45,116 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:45,547 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:45,558 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct

[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.7s before retry (not counting as turn)...
  [INFO] Tool info request: data_processing_validator

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"581b4ec9-bc0f-97d8-8b98-0c984025b174"}, traceId: 2150452b17566733505668595e7766'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"25527a76-90b0-9faa-9d8f-510ab2caaec3"}, traceId: 213e004f17566733519412733ee98e'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
  [INFO] Tool info request: data_processing_parser

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"376fe885-c404-9709-a249-d964aed936d6"}, traceId: 213e007517566733546815217ef218'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"a711f8f1-188e-96e5-85c4-2ba8849006b6"}, traceId: 2150458717566733568832745e7fb1'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.5s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"cd7a4d87-9c44-9891-bcf0-65220264f194"}, traceId: 2150454117566733588283600e7aac'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.2s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"c4706ff0-09a6-9af6-99f4-984172862cbb"}, traceId: 2150449017566733593158905e7f90'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"33e1ca45-dda0-933a-94ae-f31b4f030de5"}, traceId: 2150449017566733609528228e7f4e'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"3740f672-1f53-91a5-9cb7-8f672441ae33"}, traceId: 2150460817566733617111486e790f'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"805ead27-1491-95b0-8e69-6c125784d194"}, traceId: 215044da17566733634008551e7fb6'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"038dd1ed-25ac-9b9a-91eb-242440bab0c9"}, traceId: 213e059d17566733638926377e3496'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.6s before retry (not counting as turn)...
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"70da6826-7ff9-9ab2-a39d-b07c3a4de354"}, traceId: 213e006f17566733657548247ee541'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 5.1s before retry (not counting as turn)...
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"d12d6301-4eb6-91c2-b239-e82d123d64df"}, traceId: 2150443817566733675157494e8996'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching2025-08-31 16:49:45,559 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:49:45,602 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:49:45,602 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:49:45,602 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:49:45,779 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:46,030 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:46,168 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:46,691 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:46,692 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:47,099 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:47,216 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:47,942 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:48,553 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:48,856 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:48,934 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:49,017 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:49,096 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:49,354 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:49,521 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:49,530 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:49,869 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:50,053 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:50,999 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:51,499 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:51,561 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:51,570 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:49:51,571 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:49:51,594 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:49:51,594 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:49:51,594 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:49:51,931 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:51,932 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:49:52,365 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:52,861 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:52,980 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:53,171 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:53,348 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:53,505 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:53,821 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:53,868 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:54,721 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:54,818 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:55,078 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:55,646 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:55,717 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:55,773 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:55,843 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:56,224 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"363ca3eb-0de7-9d90-8dd3-f4e841dc4104"}, traceId: 2150415b17566733634383281ee36b'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"4f668ec8-ead4-98f8-bbbb-666b80642839"}, traceId: 213e06a117566733667518332e8a85'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"4c21ed06-8f64-93fd-93a2-0b6000deb6b3"}, traceId: 213e007617566733688163884e11c6'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 26148
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=26148
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"4e5c2869-3499-9d00-b146-ed292b7dfa24"}, traceId: 2150411817566733712057442e0a31'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.4s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"55c37b52-4634-904b-a468-abe435df2901"}, traceId: 2150411817566733709268886e0a35'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
  [SEARCH] Query: file reader

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: file reader

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"bdce16aa-5649-9533-9a1e-521839fcca0c"}, traceId: 2150436817566733756331432e1f8a'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22715
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22715
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"a5e378b5-4ebc-97f2-a881-fcf5f10dc50c"}, traceId: 2150458117566733762408713e7fee'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"d9c6fb3b-8317-9c73-946e-64bbbea25957"}, traceId: 2150417917566733781117217eeef1'}2025-08-31 16:49:56,823 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:56,895 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:56,907 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:49:56,908 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:49:56,932 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:49:56,932 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:49:56,932 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:49:57,059 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:57,173 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:49:57,416 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"948cf0b8-6661-9dc0-858f-d3447a4821c0"}, traceId: 213e007317566733667427247ee3b7'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22692
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22692
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"8e0e0a6d-b578-9372-86c0-52375cd41c89"}, traceId: 213e006917566733706585627edfd1'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"a4a6793a-98c4-9578-aa72-2af16d2e7f3f"}, traceId: 2150452b17566733715158983e7789'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"5ddd1fcb-0404-9ca1-bb1e-c5e065d890c1"}, traceId: 2150421317566733735562722e360d'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"3bbf7f45-c71f-913f-a8ba-4b177335d4bb"}, traceId: 2150443817566733738304573e8a5c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.88
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22673
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22673
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"8b40c1dc-c1ee-9ab5-a84c-e3071f22c2ca"}, traceId: 215045b017566733757294698e8461'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
  [SEARCH] Query: data validation parser

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: file reader

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"80f1d9f5-b1c5-90c2-a637-afca23c19286"}, traceId: 215045b417566733786117108e810d'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.92
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"05f03684-2ff9-9e1e-a62b-cd808614ab0c"}, traceId: 2150436817566733805845949e1e84'}2025-08-31 16:49:57,557 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:57,566 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:49:57,567 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:49:57,591 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:49:57,591 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:49:57,591 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:49:57,801 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:58,005 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:58,633 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:58,830 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:58,856 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:59,283 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:49:59,689 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:59,795 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:49:59,795 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:00,047 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:00,092 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:01,214 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:01,579 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:01,612 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:50:01,612 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:50:01,658 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:50:01,658 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:50:01,658 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:50:01,670 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[RETRY] 400 error detected, waiting 1.7s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"8593fd1f-701e-98ab-b87b-0e59f016e947"}, traceId: 215041de17566733722137419e34b3'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"36bf9c2a-e175-9e55-bd21-4a17b4cb54b8"}, traceId: 2150439017566733719456395e262e'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[ASSISTED] Task received 2 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 20002
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=20002
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"c4aa4bd0-de32-9e84-bac7-e608e645c7a6"}, traceId: 2150439017566733746881944e281d'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"4d6e60b2-055a-9c71-b77b-3266f2b02e80"}, traceId: 2150443817566733739546245e8a7d'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.7s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"05b6e386-7966-925f-beb0-258e4cff3a55"}, traceId: 2150421317566733766364521e36d3'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"d59f55d9-f82a-9241-ab17-b2a1b8bfe26d"}, traceId: 2150429e17566733777252160e216b'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"bf9dbb03-4331-9f05-af9e-a8b7b3db4888"}, traceId: 2150449017566733788386046e81a1'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.3s before retry (not counting as turn)...
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"6dda4696-0057-93ed-b4ee-96f3ac630838"}, traceId: 215044da17566733816436663e809d'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.3s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.78
  [SEARCH] Query: data validation parser

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"3f5d64fb-ae30-974b-bcb0-8c8ac95d7b7a"}, traceId: 2150439017566733838942872e25aa'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"401aae38-c4c4-9f09-baae-ac23ba7e1561"}, traceId: 2150458717566733848343116e8287'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[ASSISTED] Task received 4 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 20571
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=20571
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct2025-08-31 16:50:02,111 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:02,118 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:02,118 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:02,158 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:02,724 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:03,465 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:50:03,466 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:03,629 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:04,238 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:04,255 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:04,267 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:50:04,267 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:50:04,293 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:50:04,293 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:50:04,293 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:50:04,514 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:50:05,054 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:05,332 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:05,581 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:05,952 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:06,154 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:06,407 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:06,491 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:07,035 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:07,145 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:07,244 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:07,254 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:50:07,254 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:50:07,277 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:50:07,278 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:50:07,278 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:50:07,550 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:07,660 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:07,875 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:08,055 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:08,442 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:08,445 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:08,445 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:08,708 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:09,266 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:09,599 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:09,799 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"540b88f3-9333-9e3f-85ac-ac226e5689af"}, traceId: 215042f917566733786117407e1c80'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.5s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"2252976c-ff1d-9461-aa6f-ee0f48dcffa1"}, traceId: 2150456317566733795674359e8180'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.9s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.7
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"94368bc1-c8d7-9840-afe8-8627e12e497f"}, traceId: 215045a817566733835527405e7dc1'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"0585e21a-c66f-94f1-8afc-0aae6e5b72d1"}, traceId: 2150436a17566733848984379e22a9'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"b5592349-c000-98bc-99e5-c07b82f21563"}, traceId: 215045af17566733854077220e806b'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.7s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"353f62df-1ee3-995c-b856-47d7f0969244"}, traceId: 2150434117566733859412922e219d'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"7dd3c57b-04e8-93a9-a455-17d13b1b37d1"}, traceId: 213e042b17566733888111237e1bf6'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"a6bd1b03-41b0-91d4-8e85-87649b41b635"}, traceId: 213e041717566733893173012e9795'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"94e325ad-f20c-9481-8964-a6bc334c916d"}, traceId: 213e006f17566733902867740ee76e'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"80988770-98cd-928d-8284-04be4e2c29d4"}, traceId: 213e060a17566733907923225e8c8c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"38ebda00-d7f9-9f4d-b403-05d25202fe96"}, traceId: 213e043517566733926507735e2d52'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.7s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"ddf53e45-6d6e-9390-bc5f-90c95419da51"}, traceId: 213e064617566733955243160e0ce5'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...2025-08-31 16:50:09,942 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:10,286 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:50:10,364 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:10,810 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:11,334 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:11,356 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:11,594 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:11,732 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:11,859 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:12,084 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:12,280 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:12,765 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:12,902 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:12,925 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:13,213 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:50:13,291 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:50:13,656 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:13,824 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:13,850 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:14,638 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:14,754 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:14,776 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:14,814 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:14,825 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct

[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.2s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"4fb760dd-a89d-9411-a399-27c52f9d869d"}, traceId: 2150449a17566733811073063e8221'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"df21ad19-c415-9aba-b2bf-0384a81e7157"}, traceId: 2150449a17566733837282297e82e8'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"1d4e77ae-3bd9-9ebd-b168-6893afc2debc"}, traceId: 2150458117566733859118727e7fee'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.9s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"d08f325e-30af-908e-9885-b9324f244ec4"}, traceId: 215045b017566733864248981e81c8'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.7s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"5148f7e5-948e-92e9-8a39-154d80ada8b1"}, traceId: 213e007617566733888293720e11e7'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"2fe917c0-d699-9ccf-9b4f-e90444d33786"}, traceId: 213e007e17566733893442755eefdd'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.7s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22716
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22716
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"057a1c78-f4b8-9294-9975-0db2d81c1a89"}, traceId: 213e066d17566733924671054e8291'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"235ce5ef-782c-9049-bbba-bd0ff16f1e5c"}, traceId: 213e060c17566733927447693e88df'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
  [SEARCH] Query: file reader

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"eed112cf-0712-915d-bd06-61cc63ee9b28"}, traceId: 213e060917566733955677944e702d'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.7s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"a6eda8bd-0a4e-906f-bedb-251b7e25143b"}, traceId: 213e060c17566733966518715e87cf'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...2025-08-31 16:50:14,826 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:50:14,854 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:50:14,854 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:50:14,854 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:50:15,527 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:15,739 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:16,213 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:16,263 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:16,641 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:16,769 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:16,801 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:50:16,801 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:50:16,832 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:50:16,832 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:50:16,832 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:50:17,125 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"0e08060c-e2af-9903-8ca0-428939f44dfd"}, traceId: 2150457a17566733853434061e0c43'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.9s before retry (not counting as turn)...
  [SEARCH] Query: file reader

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"fac7f4f5-24c4-9a1f-aed7-b28414b0dbbb"}, traceId: 213e00cd17566733886486138e9761'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"26e293f7-cce5-9b26-9cb0-425484c419b3"}, traceId: 213e007417566733891435426eecb6'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"e9025842-2231-9445-a12b-b7886cf39efb"}, traceId: 213e06c817566733926401845e8245'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.6s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"0e456a81-8d89-9dea-a235-df6f5a0bba21"}, traceId: 213e04ea17566733949132410e36b8'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.5s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"c470e2fb-03ef-93d8-9c49-f3c901b56a86"}, traceId: 213e06a117566733963558676e8b2a'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"389c5fce-2676-93d8-a4fe-9ccbf54895ea"}, traceId: 213e01f617566733968574242e14e8'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 11279
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=11279
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: file reader

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"a88d0fa7-3a29-9c52-a768-f6fffa810050"}, traceId: 213e007417566733985858642eee85'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.7s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"2612a5ad-4866-99a6-8159-875d206e6c0a"}, traceId: 213e001317566733990807804e0dc3'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"c6990849-8583-9a50-b0d9-b91a7aa69cb0"}, traceId: 213e007b17566734009391255eee59'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching2025-08-31 16:50:17,181 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:17,778 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:18,149 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:18,505 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:18,515 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:50:18,515 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:50:18,540 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:50:18,540 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:50:18,540 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:50:19,026 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:19,028 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:19,481 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:19,558 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:50:19,567 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:19,776 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:19,789 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:19,919 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:20,101 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:20,769 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:20,838 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:21,183 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:21,614 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:21,819 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:21,819 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:21,836 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:50:22,342 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:22,509 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:22,866 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:22,866 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:22,882 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:50:22,883 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:50:22,920 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:50:22,920 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:50:22,920 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:50:23,392 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:23,687 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:23,917 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:23,954 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:50:23,955 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:50:23,996 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:50:23,996 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:50:23,996 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:50:24,198 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:24,298 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:24,582 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:24,711 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:25,181 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:25,488 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:25,602 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:26,086 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:26,108 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:26,536 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:26,943 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:27,062 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:27,585 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:27,902 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:50:27,961 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:27,988 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:28,107 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:28,142 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:50:28,933 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:28,968 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:29,682 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.2s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"c0983eed-ae35-9f17-88bc-2fa5d5ecbe6b"}, traceId: 213e06c017566733998373580e81b4'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"5e742ff3-4cbb-95f1-b2c6-364cd8eb87ef"}, traceId: 213e042d17566734019125029e2276'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"258b6fbd-01f1-959f-ae72-f2cea4400ec7"}, traceId: 213e065e17566734014187467e7f50'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22650
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22650
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: file reader

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"d1175b9a-06d2-9f64-8adb-ec0e3066932c"}, traceId: 213e00cd17566734057912779e9568'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"923f5d56-c85c-9a16-b485-4d049c8dc225"}, traceId: 2150449a17566734082316939e817c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"245be93f-70a2-9987-895d-823a5c64acc3"}, traceId: 213e06bc17566734077298253e826d'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.8s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"62b79fdc-c433-9367-ae91-968bac1a6870"}, traceId: 2150458117566734106364088e81bb'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.2s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22650
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22650
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings2025-08-31 16:50:29,822 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 4.5s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22632
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22632
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: file_operations_reader

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"bb5bf44f-2b8d-98f4-b7e3-82a0bf50fd99"}, traceId: 213e006c17566733998821996e14f2'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"f79a1ac9-29ba-99ed-999a-f9c2fd646d11"}, traceId: 213e057b17566734008793358e405d'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"004f9049-69e6-9f92-b16a-c2bf6913d9b8"}, traceId: 213e007517566734013958810ef0ba'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.2s before retry (not counting as turn)...
  [SEARCH] Query: file reader

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 12100
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=12100
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"b51bf033-db10-98e1-bd78-d206e8f0a117"}, traceId: 215045f517566734029186480e7e85'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.5s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"b1c35c58-d318-90ac-92f4-d99b83445488"}, traceId: 213e065017566734048705319e7e46'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"02d52da0-5cdf-9e6f-9966-2ed8c5d8e1ff"}, traceId: 213e064b17566734068457902e78ed'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"3d2785f0-a3fe-96f6-a166-5d8caac6859a"}, traceId: 213e043b17566734073506543e238b'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"f9383ec8-c529-92ed-ab61-e82e4390bbad"}, traceId: 213e03e217566734085452434e1e74'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"7f0f2b38-e8e1-9b0a-88c5-67725681de57"}, traceId: 213e006d17566734095926926e1407'}2025-08-31 16:50:29,833 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:50:29,833 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:50:29,856 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:50:29,856 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:50:29,857 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:50:29,865 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:30,046 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:30,081 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:30,473 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:30,920 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:30,929 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:50:30,929 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:50:30,952 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:50:30,952 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:50:30,952 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:50:31,104 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:31,779 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:32,333 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:32,468 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:32,570 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:32,717 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:33,239 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:33,640 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"ccb39fae-473b-9205-8603-319aef1500c2"}, traceId: 213e060917566734014505416e6fea'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 4.2s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"d81fe0a5-0c59-9b9e-bf56-7a9d4102b846"}, traceId: 213e041917566734026895166e2af1'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.9s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=other_errors, confidence=0.72
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"7654af3d-d0be-9c5a-86ac-88ee70d2b491"}, traceId: 213e03d917566734065152253e1fd0'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[ASSISTED] Task received 2 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 16264
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=16264
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"555f5203-e5e2-9f5c-8d65-f9fb8aa24d46"}, traceId: 2150455f17566734077418175e7f79'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"1b38ee43-3eed-937d-aaa4-fc08f4b88969"}, traceId: 215045b717566734088988657e804c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
  [SEARCH] Query: file reader

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"0f4dc901-0157-9419-bb0f-eab2d7435553"}, traceId: 213e063817566734115615143e7f4d'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.82
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"0a8cc0ee-1718-9342-903c-cda4b751862d"}, traceId: 2150457a17566734129563479e0a44'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.7s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"8570bfc2-3d70-9cf2-bd67-9e5d8a2ef981"}, traceId: 215041e117566734139292909e3522'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"5003a2bc-af5c-92b4-b67e-19496735f4a4"}, traceId: 213e081017566734145572745e0a04'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"fc0d5b59-554e-9986-a4cf-352e97faa1dd"}, traceId: 213e062917566734159097961e808a'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.2s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"373d7637-a0a0-9488-9e5c-d13dcac7d186"}, traceId: 2150448717566734164283432e8192'}2025-08-31 16:50:33,676 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:34,109 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:34,581 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:34,994 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:35,306 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:35,310 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:35,712 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:35,723 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:50:35,723 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:50:35,748 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:50:35,748 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:50:35,748 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:50:36,122 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:36,125 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:36,209 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:37,034 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:37,104 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:37,711 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:37,852 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:37,947 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:37,966 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:50:38,077 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:38,314 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:38,449 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:38,470 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:50:38,474 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:50:38,549 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:50:38,549 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:50:38,549 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:50:38,777 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:38,970 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:39,279 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:50:39,644 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:39,738 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:39,738 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:40,039 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:40,258 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:40,692 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:40,693 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:40,828 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:41,598 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:41,616 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"5ea5772b-06ee-962c-ac7a-a2afcfbd0d3f"}, traceId: 2150423617566734108936235e0c4d'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.7s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=timeout_errors, confidence=0.64
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22678
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22678
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"9cd7eeee-5ce4-9b62-b92b-61798adcbc57"}, traceId: 213e068217566734164751482e799a'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.2s before retry (not counting as turn)...
  [SEARCH] Query: file reader

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"7f9b2653-e5e2-91d1-90aa-af77f34f8129"}, traceId: 213e006b17566734188601587eec1d'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.9s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"f53371a6-145d-9bc3-a463-c3251d50c5aa"}, traceId: 213e063717566734193996014e8c04'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.7s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.86
Progress: 10/35 (Success: 0)
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"d7264c4b-2f15-956e-99f3-ac4d39aa9e36"}, traceId: 213e006b17566734216228515eedb6'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.4s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"1416c112-29ec-9984-849b-2482d469f94c"}, traceId: 215045be17566734238786458e8281'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"e43c4fb8-c9bc-9005-a19c-95867f12c659"}, traceId: 213e080f17566734258983791e0b57'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.1s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"24f30383-5210-9e9e-a116-8b4ea52279ae"}, traceId: 2150438d17566734296261065e2cfe'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[DEBUG] Got result for task: has_result=True, save_logs=False2025-08-31 16:50:41,648 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:41,871 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:42,352 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:42,626 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:43,313 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:43,429 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:43,508 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:44,215 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:44,435 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:50:44,712 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:50:44,937 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:44,938 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:45,458 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:45,934 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:46,031 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:46,047 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:46,534 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:46,802 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:46,982 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:50:47,513 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:47,550 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:47,837 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:48,413 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:48,557 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:48,558 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"3ccb4d1b-c666-929a-bab2-1b09477f642f"}, traceId: 2150435d17566734147927122e1fa1'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.1s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"8397fa57-e2bc-9d36-ab59-2aa625e3bf68"}, traceId: 2150457a17566734155267034e0a68'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
  [SEARCH] Query: file reader

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"ec025f8b-ee32-9a4e-901c-affb9951af5f"}, traceId: 215041e117566734178064526e35e9'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"a416cd2d-782a-9652-84b0-343a044a6868"}, traceId: 213e007217566734192807679eed93'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 11388
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=11388
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
  [SEARCH] Query: file reader

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"53d04861-697f-93d4-88d8-3cb5c4af4a37"}, traceId: 213e006c17566734204894504e11da'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"4f31ecfa-ea80-9a6f-b009-4dec522e55b5"}, traceId: 213e007617566734218057326e14d9'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"5858e629-3000-9901-9386-03d2c42cc902"}, traceId: 2150448717566734235857884e7e7b'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"89d928dc-1530-99fc-a5f4-26fa6ada1bf0"}, traceId: 215044eb17566734249067695e811d'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"f2e9ae0f-3ef4-9fa7-8b16-87ae719a6e91"}, traceId: 213e007f17566734258165416eea03'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.1s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=other_errors, confidence=0.65
Progress: 10/35 (Success: 0)
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"569e17b8-4986-9669-9d52-cae291de387d"}, traceId: 215041d717566734288757724e367e'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...2025-08-31 16:50:48,574 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:50:48,575 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:50:48,606 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:50:48,606 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:50:48,606 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:50:48,705 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:48,916 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.2s before retry (not counting as turn)...
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"3a746fb5-3c80-9b6d-9a09-e5e7a207eecc"}, traceId: 213e006d17566734195762800e1193'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.6s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22636
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22636
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"37edf1c7-ac43-9ea9-8dfd-e60463fa4bfe"}, traceId: 213e063817566734236274056e815f'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: file reader

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: file reader

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"cbf0ce5a-6c6c-93f5-aa7b-eebe4d6a42e8"}, traceId: 2150454417566734247268326e8249'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.2s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"da8270bb-4537-9362-9d18-3ab0e3f54a3e"}, traceId: 2150455f17566734268636443e807f'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"01fecdeb-5987-9a15-a997-f422fb65bb0c"}, traceId: 2150430c17566734273604324e21c7'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.78
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 11558
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=11558
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"019ba7ca-2f28-9ab0-9f0b-4cb85ac4a24a"}, traceId: 213e06a117566734293447781e8aed'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"314d290f-10dd-97d0-8efd-2d98bd058e0b"}, traceId: 2150449a17566734297741274e7f8d'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.4s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"c3b3aa43-23ff-9a6c-93a3-67cfa3af12ce"}, traceId: 215042ae17566734329077039e91db'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...2025-08-31 16:50:49,770 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:50,355 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:50,665 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:50,834 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:51,030 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:51,046 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:51,523 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:51,917 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:52,081 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:52,234 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:52,611 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:52,771 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:52,870 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:53,646 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:53,652 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:53,657 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:50:53,658 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:50:53,683 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:50:53,683 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:50:53,683 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:50:54,212 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:54,848 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:54,978 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:55,230 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:55,653 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:55,906 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:55,918 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:55,936 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:50:55,938 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:50:55,984 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:50:55,984 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:50:55,984 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:50:56,146 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:57,107 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:57,611 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:57,637 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:57,996 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:50:58,227 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:58,522 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:50:58,533 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:50:58,533 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:50:58,566 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:50:58,566 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:50:58,566 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:50:59,043 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:59,180 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:50:59,735 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:00,092 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:00,092 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:00,100 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:51:00,101 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:51:00,127 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:51:00,127 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:51:00,127 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:51:00,462 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 12008
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=12008
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"5ab96bfe-8ad2-9a62-9400-b16302cd0118"}, traceId: 2150417717566734309144335edca7'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
  [SEARCH] Query: file_operations_reader

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: file reader

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"e4b92d8c-48f6-9090-aefe-794f5c4d4b73"}, traceId: 213e06c817566734325281903e8245'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"02dda202-a7ed-97c8-948c-c56c57ea53ab"}, traceId: 213e066417566734338656950e8109'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"e9a4ad2c-03c9-9d24-b464-f76ce6d7018b"}, traceId: 213e065417566734359064108e8211'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"5e7bf456-2743-902a-8212-5a3d47eadc9d"}, traceId: 213e00cd17566734376054113e9547'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=sequence_order_errors, confidence=0.75
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22552
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22552
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"7fbc8024-0a2a-9219-b654-fefd72ec6145"}, traceId: 213e043517566734395448368e2cce'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"0d4fe07c-c99d-9e53-a71e-f3f02d18634e"}, traceId: 213e007517566734408864808ef181'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"6bd037eb-6aa4-92d8-b331-f79bdc4c3f4e"}, traceId: 213e043a17566734414062006e1d54'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching2025-08-31 16:51:00,645 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:00,919 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:01,043 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:01,140 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:51:01,401 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:01,762 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:02,618 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:02,713 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:03,237 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 5.1s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"e47bd0bc-3500-95f6-9089-236a77ff2539"}, traceId: 213e04ea17566734293844663e371c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"16d669da-667f-922f-869b-2048e3610a15"}, traceId: 2150454117566734317662944e7983'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.1s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"e95b5c51-0124-9e56-b0b1-539ffb13a30b"}, traceId: 213e043b17566734347897266e21dd'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.7s before retry (not counting as turn)...
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"8cb2cc07-cc96-912d-bb06-29879264f45d"}, traceId: 213e065117566734355195175e81b1'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 11784
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=11784
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: file reader

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"f95c320a-e433-911f-851d-364abdd62978"}, traceId: 213e059617566734372358164e3e4a'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"7b2fe93b-bf53-9618-a686-9012ae6619fe"}, traceId: 213e007517566734378754048eef92'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.9s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"8fa376b9-cce0-9dda-84ec-d39fbad23423"}, traceId: 213e00cd17566734394035738e9526'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.1s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"1f223df4-297a-9152-8c29-95039759c258"}, traceId: 213e060a17566734419263613e8995'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.9s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"7bce80a8-ca2a-9d92-ba1f-b37ad6fd4b80"}, traceId: 213e007f17566734435146179eebb0'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=sequence_order_errors, confidence=0.72
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"2da6f81c-b35c-9d2c-aadf-0fc4d7c0aa24"}, traceId: 213e007b17566734447441304eee59'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.6s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"68e48907-da18-9977-9086-34aee34019c3"}, traceId: 213e007c17566734457936951ef263'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.7s before retry (not counting as turn)...
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"0a303b62-4ce8-969f-93e7-1db419cfe9a9"}, traceId: 213e007b17566734477742355eeba3'}2025-08-31 16:51:03,245 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:51:03,245 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:51:03,276 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:51:03,276 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:51:03,276 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:51:03,388 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:03,446 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:03,663 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"

[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"980a939b-ff3d-96b9-b202-44d3040e6fc5"}, traceId: 2150436a17566734334135001e2180'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.3s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"1228101d-ca42-9cfe-ac2b-94f0a6295ade"}, traceId: 213e065017566734355061512e80ba'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_selection_errors, confidence=0.72
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"9b1fb92f-0fb3-93aa-8616-e28a180883f9"}, traceId: 213e06bc17566734377331127e824c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 11382
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=11382
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: file reader

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"a14221e0-ce31-9dbc-b5fa-18b4f1116523"}, traceId: 213e042b17566734393143453e1cbb'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.7s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"f180d559-e48a-9f1c-9694-47e46a6f7fcd"}, traceId: 213e043117566734399276000e21ec'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"fcf741fa-35c6-99e2-af37-201c46321e62"}, traceId: 213e060b17566734409504346e8a1e'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"1f63fcd5-9b3c-968c-bf14-4773efc48521"}, traceId: 213e007b17566734416477789eed0f'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.7s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"f32458fa-2992-9fca-86d4-8774b2f4ca93"}, traceId: 213e007c17566734427313569ef15b'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.9s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=other_errors, confidence=0.75
Progress: 10/30 (Success: 0)
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"47927e5d-b8b8-96b4-b45a-6982b7fb5172"}, traceId: 213e007c17566734477138473ef17c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"880a0c8a-dc36-9d16-bbd5-b4ffb6cb429b"}, traceId: 213e001317566734482188904e0b2e'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching2025-08-31 16:51:03,761 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:03,862 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:03,865 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:03,914 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:04,660 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:04,809 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:04,878 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:05,228 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:51:05,551 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:05,691 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:05,715 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:05,864 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:06,056 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:06,692 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:06,761 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:07,594 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:07,595 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:07,656 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:08,352 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:51:08,545 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:08,730 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:09,037 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:09,051 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:51:09,052 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:51:09,076 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:51:09,076 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:51:09,076 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:51:09,528 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:09,628 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:09,790 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:10,206 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:10,244 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:10,578 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:10,845 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:10,856 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:51:10,856 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:51:10,887 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:51:10,887 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:51:10,887 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:51:10,900 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:11,101 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:11,498 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:11,625 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:11,801 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:11,910 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:11,973 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:12,747 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:12,856 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:13,314 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:13,402 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:13,723 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:13,855 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:13,919 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:14,625 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:14,840 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:51:14,882 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:15,079 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:15,142 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:15,583 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:51:15,874 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:15,886 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:51:15,887 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:51:15,911 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:51:15,912 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:51:15,912 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:51:15,952 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:16,448 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:16,564 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:16,868 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:16,869 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[RETRY] 400 error detected, waiting 2.8s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"508b2025-b4ce-94c6-a8e3-f7a7576c0acb"}, traceId: 213e007217566734428163076eeabe'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.6s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"55cab194-8c42-9b5f-b46d-50bbbc4e212a"}, traceId: 213e06a117566734453405749e895c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.2s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"8c5acc5e-5879-9235-9e26-833200d71b6f"}, traceId: 213e060b17566734458414350e8a1e'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"919964ca-becf-91b2-816b-b21a62ee97a4"}, traceId: 213e059717566734473512231e366d'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.2s before retry (not counting as turn)...
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"e6ebc505-4d9f-9130-89ce-cce1749d32d7"}, traceId: 213e006f17566734484904595ee479'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 4.0s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"f1944e92-f689-92db-a9d9-04bde53dfbe7"}, traceId: 213e01f617566734529363896e12b8'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[ASSISTED] Task received 1 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 14075
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=14075
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: file reader json

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"01c244e9-bf92-93f1-9a07-c64ec16ecac9"}, traceId: 213e081017566734545272498e0cdb'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"b9241a09-64b2-9aa5-9a9e-5a2045e02840"}, traceId: 213e06c117566734551702454e87f8'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.2s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"37acaf29-1d56-9068-b5d8-0574e86faadd"}, traceId: 213e06c317566734574065751e7908'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.1s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"c8363144-36c8-90e3-b499-ebc1a78af7de"}, traceId: 213e065a17566734569146822e839c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.9s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"97abda3c-c244-96cc-8f50-300707102d92"}, traceId: 213e007b17566734597625694eec07'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...2025-08-31 16:51:17,243 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:17,254 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"05b819ec-cd2a-9c9a-a7d7-d9276299e632"}, traceId: 213e007917566734482958305ee8d9'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.2s before retry (not counting as turn)...
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 11416
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=11416
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: file reader

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"d255c60c-09cf-97ad-91e9-135794e4cce9"}, traceId: 213e007c17566734508576963ef263'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.7s before retry (not counting as turn)...
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"e4999f39-a468-9eeb-a85c-31b2329b9f09"}, traceId: 213e058a17566734513744035e361c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.8s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"61aa3578-b30b-9d5b-822b-9e3aa172c903"}, traceId: 213e03d917566734518965441e1ff1'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.8s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"60b59584-461c-90e8-9f93-1b22a9ac113c"}, traceId: 213e06c817566734546235724e8425'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.5s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"32a2537d-2f3a-969e-8049-cb8720ddcf6d"}, traceId: 213e065a17566734564202553e8106'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=other_errors, confidence=0.72
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22688
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22688
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"baca0b43-4294-92cd-8735-bf1b4d545be9"}, traceId: 213e042f17566734575238939e257d'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 4.1s before retry (not counting as turn)...
  [SEARCH] Query: file reader json

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"199e2f41-06d7-9214-853b-a6b9221c0d3e"}, traceId: 213e007217566734593766386eec6a'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"4200d8b7-ea52-9c0d-9c4e-0580205d614a"}, traceId: 213e007f17566734608452674eedc0'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.7s before retry (not counting as turn)...
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"a2510736-85eb-9704-86f8-93154f029a13"}, traceId: 213e041717566734624862737e966d'}2025-08-31 16:51:18,759 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:18,862 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:51:19,061 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:19,324 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:19,666 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:19,859 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:20,013 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:20,270 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:20,416 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:51:20,537 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"9bbfe672-ff18-97b8-aa2c-7cb93559e973"}, traceId: 213e041917566734496371453e2bc0'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"0f8a0e36-d56e-9b8b-a5df-def99cd1c860"}, traceId: 213e080f17566734515352617e0a71'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.5s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"a8c26b99-0bb8-9538-9920-44f2374ed0dd"}, traceId: 213e081017566734529624463e0cfc'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.2s before retry (not counting as turn)...
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"88d4a40e-3fb2-9899-8149-4a60feadd817"}, traceId: 213e006817566734549572759ee479'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.9s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"ffa9653c-cfe6-9c79-8f56-7a61632483c6"}, traceId: 213e06c317566734554465388e7a0f'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.7s before retry (not counting as turn)...
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"04b32398-372d-9b65-8b9d-e63c66e69931"}, traceId: 213e064717566734577466366e87ce'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[ASSISTED] Task received 4 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 20659
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=20659
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: file reader json

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"29ba7897-fe67-9933-8d0a-8a84ad12cc5e"}, traceId: 213e065517566734599353284e81e7'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"2f02108f-50c5-945c-b210-789ec6d19fda"}, traceId: 213e068217566734606651518e799a'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"5a1bf94a-2758-9948-afe4-a2091d7afc4b"}, traceId: 213e059617566734627253784e3f20'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»2025-08-31 16:51:20,549 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:51:20,550 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:51:20,602 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:51:20,603 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:51:20,603 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:51:20,684 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:20,738 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:21,605 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:21,635 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:21,830 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:22,308 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:22,316 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:22,744 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:22,756 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:22,768 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:51:22,768 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:51:22,795 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:51:22,795 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:51:22,795 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:51:23,729 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:23,764 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:23,770 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:24,285 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:24,586 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:24,730 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:24,855 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:25,621 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:25,623 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:25,806 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:26,502 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:26,519 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:51:26,519 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:51:26,550 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:51:26,550 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:51:26,550 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:51:26,627 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:26,665 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:26,967 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:26,977 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:51:26,977 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:51:27,000 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:51:27,000 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:51:27,001 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:51:27,530 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:27,548 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:27,690 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:27,978 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:28,025 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:28,295 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:28,582 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:29,107 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:29,166 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:29,451 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:29,787 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:29,830 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:30,000 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:30,249 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:30,516 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:31,022 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:31,027 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:31,123 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:31,214 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:51:31,864 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:32,144 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:32,598 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"

[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"0a674baa-6992-9a73-a656-b890407abe13"}, traceId: 213e043117566734637105268e2126'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 11531
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=11531
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"d595e03b-a365-9e90-bcdf-a7293d3acc7a"}, traceId: 213e007317566734648524282ee311'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"aca0caa7-7733-90a0-bf73-7d0e8d5eeab6"}, traceId: 213e065e17566734653625191e7e47'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"b41856fa-9d75-940d-8b81-5d45f0ae4f0d"}, traceId: 213e004f17566734668751801ee756'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.7s before retry (not counting as turn)...
  [SEARCH] Query: file reader json

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"886bc931-f878-9567-9993-8ac6b72bc68a"}, traceId: 213e064e17566734687362647e8057'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.5s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"95b2eb75-5920-9663-823e-64f8c862a638"}, traceId: 213e007e17566734707663340eed8b'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"650a77bd-907d-904d-81eb-cacbe90aee6f"}, traceId: 213e007217566734727113292eea7b'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.2s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"d72235db-8dcf-9376-ac34-7cc1ba34cce5"}, traceId: 213e065a17566734732192208e839b'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"c014226a-ce67-96ba-864c-ea88522e71b3"}, traceId: 213e065017566734749558526e7f4e'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.2s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_selection_errors, confidence=0.62
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"3ff6f6ed-3373-9013-96ae-1bbf9892d83f"}, traceId: 213e06a017566734758474733e7992'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.2s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"743619d0-b32e-9ebf-8a46-88973917722a"}, traceId: 213e06b717566734765298529e783b'}2025-08-31 16:51:32,783 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:32,828 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:33,331 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:33,347 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:33,733 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:34,692 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.0s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.82
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"3783bc9d-a390-9912-9130-c2868788a376"}, traceId: 213e004f17566734626927184ee6b9'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 4.6s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"4459e820-1492-9e64-bc65-789280530483"}, traceId: 213e062917566734669557931e7efd'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.2s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22616
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22616
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"518290c4-0c25-9f5c-9a19-15d763eea560"}, traceId: 213e007e17566734697578731eeed5'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"6d0e206a-a8c1-9bfa-955b-6977fa53ab7f"}, traceId: 213e06ba17566734715952025e8893'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.9s before retry (not counting as turn)...
  [SEARCH] Query: file reader json

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"f7c10193-072b-90c9-9ebb-4bd05da444b6"}, traceId: 213e007017566734739167776eeb38'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 23226
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=23226
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
  [SEARCH] Query: file reader json

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"09540adc-0d76-9b8f-b0bc-d1d9ee43ea6c"}, traceId: 213e042f17566734761325328e253a'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"297881b2-cd27-91f8-b69e-4a97013b53e4"}, traceId: 213e065917566734766331066e7ff0'}2025-08-31 16:51:35,609 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:35,613 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:35,907 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:51:35,995 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:36,007 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:51:36,007 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:51:36,032 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:51:36,032 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:51:36,032 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:51:36,266 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:36,266 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:36,424 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22806
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22806
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"8912223e-2a86-9938-807c-e0682d8543ef"}, traceId: 213e065e17566734655208052e81a0'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.1s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"ca255656-49db-94d5-adbe-b17f0955af38"}, traceId: 213e06c217566734668853007e8129'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.88
  [SEARCH] Query: file reader json

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"62830176-8bf1-9813-812b-226cb230aa19"}, traceId: 213e011517566734689236320e9335'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.5s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"82f3cb69-1b40-942c-b20b-8301554b7499"}, traceId: 213e064b17566734703556071e7826'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"eb4e55f5-045f-9bc6-bacd-a687d986bf4b"}, traceId: 213e060b17566734708606540e8a60'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"87bbe87b-3ca6-9b94-be2e-5457121b5c00"}, traceId: 213e06c817566734726234433e8215'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.2s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 23181
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=23181
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"2178213b-c3a7-9751-9249-a1696980d82e"}, traceId: 213e065417566734757434871e7efa'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.1s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"a78c7e49-1742-92cc-82be-fea0a28c849c"}, traceId: 213e065417566734765341069e818d'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
  [SEARCH] Query: file reader json

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"1a27e7bf-2357-98a7-803e-8a2ab5a23ffb"}, traceId: 213e006917566734792523819ee19f'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.82
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"596b9085-8d20-9093-99ae-852ef412680f"}, traceId: 213e007617566734797785506e1148'}2025-08-31 16:51:36,435 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:51:36,435 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:51:36,466 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:51:36,466 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:51:36,466 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:51:36,627 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:36,644 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:37,217 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:37,444 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:37,522 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:37,725 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:37,839 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:37,977 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:51:38,104 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:38,141 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:38,959 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:38,964 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:39,141 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:39,753 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:39,995 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:40,652 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:40,791 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:40,984 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:51:41,262 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:41,353 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:41,365 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:51:41,365 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:51:41,389 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:51:41,389 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:51:41,389 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:51:41,681 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:41,781 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:42,109 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:42,238 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:42,627 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:42,722 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:42,955 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:43,168 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:43,190 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:43,611 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:43,872 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:43,882 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:51:43,883 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:51:43,907 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:51:43,907 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:51:43,907 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:51:44,290 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:44,522 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:44,820 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:44,896 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:45,179 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:45,179 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:45,189 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:51:45,189 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:51:45,211 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:51:45,211 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:51:45,211 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:51:45,845 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:46,046 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:46,634 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:46,843 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:46,881 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:47,276 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:47,377 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:51:47,690 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:47,800 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:48,022 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:48,590 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.1s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"41c01e66-3992-9af0-8808-8e7d29838a3d"}, traceId: 213e007417566734795848722eee85'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.1s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"714a19bb-9e2c-982f-b54b-527e7d5dabf3"}, traceId: 213e00cd17566734809306434e96f4'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"10fe9ca6-be24-9a7c-afce-2f548880336b"}, traceId: 213e065a17566734825667735e8230'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 11936
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=11936
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"7b6af1db-0f03-9074-b730-9ceceee9f225"}, traceId: 213e066317566734838711831e80fe'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: file reader

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"7e5a8b22-6926-94c4-be5c-f5d84f37d0ed"}, traceId: 213e006817566734868563824ee7b2'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"57fa93dd-ad09-93af-8e25-915eb10a6049"}, traceId: 213e006c17566734875975014e146e'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"87bbe091-378b-9eb6-a38b-b34e68f04c5a"}, traceId: 213e06b717566734886814161e787d'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.8s before retry (not counting as turn)...
  [SEARCH] Query: file reader json

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"37cf50ef-8f22-9b4a-82b9-a8869a66c57d"}, traceId: 213e065e17566734904401844e7e69'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"3eed7a59-e755-94fe-b359-d5b7f130fc43"}, traceId: 213e06c217566734914502193e8021'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.1s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_selection_errors, confidence=0.722025-08-31 16:51:48,641 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:48,849 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:51:48,906 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:51:48,908 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:49,443 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:49,540 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:49,898 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:50,054 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"445b4ab1-ebb4-9019-8ff9-3151d67862a8"}, traceId: 213e062917566734786282090e7e59'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.1s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"cfe5f4d9-7336-98c3-9f36-0a3e62af03f7"}, traceId: 213e042f17566734808758445e255b'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.2s before retry (not counting as turn)...
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"7d1e4d79-a18e-973c-88d2-50bdab53913c"}, traceId: 213e063817566734815915252e7f4e'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.3s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"553b8d63-299d-90d7-82e5-f00c90300b60"}, traceId: 213e066417566734849265753e8253'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"e1e53f06-446d-9873-a1fa-981f7e84195d"}, traceId: 213e06c817566734858002006e8191'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 11616
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=11616
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: file reader json

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"506af335-fcf3-94b3-9b94-7ef934b78cb6"}, traceId: 213e04ea17566734878855191e3613'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.5s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"34cd7079-8fe4-9868-99ad-b67ac30a7127"}, traceId: 213e057b17566734898184899e403c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"a74b207e-3623-9777-9ce1-81539feafb3e"}, traceId: 213e066417566734925762683e816c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"184a6535-4eb5-9151-8dc6-ee016f2efe1a"}, traceId: 213e065917566734939271890e82de'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching2025-08-31 16:51:50,066 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:51:50,066 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:51:50,088 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:51:50,088 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:51:50,088 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:51:50,846 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"

[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 11278
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=11278
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: file reader json

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"4536823e-ded1-9fc9-945f-577dcca3524b"}, traceId: 213e006b17566734816005964eeb43'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"9eb1663a-bfd6-9ba7-8baa-05b693bff77a"}, traceId: 213e006e17566734829615095e1509'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"33ce04cd-14c3-98c3-9a93-371ed99f6a58"}, traceId: 213e007d17566734835758468ef2b7'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"da726b70-4e33-9515-aede-f907e177d7c1"}, traceId: 213e007e17566734849284367eed49'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"a62b3836-176b-966d-af2c-b67ffe0cf929"}, traceId: 213e063717566734868212165e8b3e'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.7s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"f3dce6d1-c398-921a-a2a8-5aa7e02f085f"}, traceId: 213e062017566734873202264e81b3'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.2s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"7b3a7ccf-1663-9ebd-a779-3d1507c3b156"}, traceId: 213e066c17566734898137150e7a77'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 4.9s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"6284f947-32cd-97a5-a3ef-a335e3d4b1c7"}, traceId: 213e057b17566734903218762e3d66'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=other_errors, confidence=0.65
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"d48845be-4b23-984d-a2eb-9ad9e0a25449"}, traceId: 213e06c017566734921183034e8392'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.9s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"b4e6ffed-2b29-98cd-93f2-38014f15c120"}, traceId: 213e06c117566734956885440e8af1'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 11436
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=11436
  - task_model=qwen2.5-72b-instruct
2025-08-31 16:51:50,946 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:51,230 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:51,471 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:51,489 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:51,580 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:51,899 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:52,120 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:52,254 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:51:52,724 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:52,746 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:53,044 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:53,602 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:53,642 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:53,967 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:54,461 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:54,859 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:55,004 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:55,042 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:55,052 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:51:55,052 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:51:55,100 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:51:55,101 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:51:55,101 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:51:55,145 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:55,664 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:55,677 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:51:55,677 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:51:55,710 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:51:55,710 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:51:55,711 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:51:56,204 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:51:56,215 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:56,863 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:56,885 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:56,908 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:57,394 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:57,409 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:57,433 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:57,893 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:57,949 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:58,003 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:58,884 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:58,894 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:51:59,435 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:59,459 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:59,628 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:51:59,732 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:00,054 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:00,066 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:52:00,067 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:52:00,091 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:52:00,091 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:52:00,091 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:52:00,116 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:00,249 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:01,056 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:01,262 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:01,432 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:02,163 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:02,508 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:02,875 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:03,007 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:03,533 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:52:03,959 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:04,243 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:52:04,279 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:04,652 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=sequence_order_errors, confidence=0.78
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 23178
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=23178
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: file reader json

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"574f93d9-9364-9549-a084-2a138dde1535"}, traceId: 213e004f17566734967404224ee845'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"5c0e4694-b0db-9397-bdad-3a277d4cbe0e"}, traceId: 213e06bc17566734974062521e807e'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"1be01300-bb4c-9c27-8b88-79925cd96e13"}, traceId: 213e007917566734982682029eebef'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.1s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"7fbe3bc8-06c4-9bc9-bdae-460f7e4d83c2"}, traceId: 213e058a17566734997906554e35e4'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.8s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.92
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"84e13026-a7d0-9e7c-ad93-532b10df3195"}, traceId: 213e007a17566735024812991ee273'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 23178
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=23178
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"7eddff44-9fc8-96a7-984e-a764e9bc878b"}, traceId: 213e065017566735046944443e7e67'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 4.2s before retry (not counting as turn)...
  [SEARCH] Query: file reader json

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"75c1da77-cb1c-924c-8487-56da775e196b"}, traceId: 213e006a17566735088397903ee0cf'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"a3638409-102e-92b6-86b4-10c8807b4dca"}, traceId: 213e007917566735093416226eeb8d'}2025-08-31 16:52:04,670 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:04,863 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22916
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22916
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"8e1a8a85-c195-9244-b314-23b93ce1f412"}, traceId: 213e06c017566734926522347e82cc'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"698a36eb-9d1d-90db-8fcd-ad3b56406f16"}, traceId: 213e042b17566734955002849e1eab'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.8s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"cb2c83da-ebfb-91db-9a7c-b2887a0af13d"}, traceId: 213e03e217566734968227736e1b1a'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.5s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"03d9eac6-243b-9a0b-b48a-4f28f00c9cba"}, traceId: 213e007f17566734982552077eec13'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"0aac3e37-8789-917d-a69b-24ce75157fc5"}, traceId: 213e057b17566735006471321e3fb9'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 11398
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=11398
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"d05e97a8-e8d9-98e0-9e23-67b5489b10bf"}, traceId: 213e007b17566735019213232eed30'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"3fd0a673-4f76-9b9f-a6cd-485607ffdcf6"}, traceId: 213e05ab17566735022201318e35d5'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"0883b4ec-666a-9301-81cf-165f926733a9"}, traceId: 213e041917566735035931915e2e4b'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"f28ec7ea-4a45-92ad-a7ef-7c377475b74c"}, traceId: 213e068217566735046245813e7847'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
  [SEARCH] Query: file reader json

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"b2e1790c-e317-9d29-9615-ed068b505fef"}, traceId: 213e011517566735065272078e91eb'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.1s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"264608aa-2a54-947a-9e3a-ceb3bfffa140"}, traceId: 213e06a217566735078833422e8409'}2025-08-31 16:52:04,912 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:04,972 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:05,797 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:52:05,799 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673525798670.json
2025-08-31 16:52:05,799 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673525799166.json
2025-08-31 16:52:05,799 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673525799470.json
2025-08-31 16:52:05,799 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673525799716.json
2025-08-31 16:52:05,800 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673525799953.json
2025-08-31 16:52:05,800 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673525800171.json
2025-08-31 16:52:05,800 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673525800423.json
2025-08-31 16:52:05,800 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673525800645.json
2025-08-31 16:52:05,801 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673525800846.json
2025-08-31 16:52:05,801 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673525801227.json
2025-08-31 16:52:05,802 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673525801666.json
2025-08-31 16:52:05,802 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673525802340.json
2025-08-31 16:52:05,803 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673525802652.json
2025-08-31 16:52:05,803 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673525803342.json
2025-08-31 16:52:05,803 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673525803717.json
2025-08-31 16:52:05,804 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673525803953.json
2025-08-31 16:52:05,804 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673525804159.json
2025-08-31 16:52:05,804 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673525804349.json
2025-08-31 16:52:05,804 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673525804530.json
2025-08-31 16:52:05,804 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673525804739.json
2025-08-31 16:52:06,208 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:06,559 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: file reader json

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"42cede76-b798-930a-ba54-3f7024fcd4bb"}, traceId: 213e06c817566734979185295e83a1'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.7s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"0be28d21-8936-99db-a3d9-07befdfc495a"}, traceId: 213e060a17566734984313675e8995'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"98aa715e-13f6-9312-b989-a79298d4d220"}, traceId: 213e006817566734999507378ee70d'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"87afa997-b82f-940a-9e6d-507d0c82f094"}, traceId: 213e007017566735024697771ee9ac'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"7ee10018-d00e-9ebd-948a-79fa3f145386"}, traceId: 213e057b17566735038061115e3fb8'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.7s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"eb21c762-ef8b-97b2-97de-78c1370b9e06"}, traceId: 213e06b617566735056474052e805f'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"4a3bb916-9ae1-95c7-aeb1-f0481762a493"}, traceId: 213e04ea17566735059217449e3400'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=other_errors, confidence=0.7
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 23170
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=23170
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
  [SEARCH] Query: file reader

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"1e8e9c99-70c0-97fb-9b37-0f67f2c92998"}, traceId: 213e007517566735087201931ef0d0'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.2s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct2025-08-31 16:52:06,569 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:07,200 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:07,200 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:08,615 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:08,830 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:09,121 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:09,357 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:09,690 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:09,728 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:09,821 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:10,868 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:10,869 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:11,122 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:11,132 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:52:11,133 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:52:11,163 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:52:11,163 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:52:11,163 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:52:11,789 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:12,117 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:12,279 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:12,653 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:12,664 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:12,676 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:52:12,676 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:52:12,703 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:52:12,703 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:52:12,703 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:52:12,799 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:12,967 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:13,678 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:13,900 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:13,903 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:14,202 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:14,393 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:14,675 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:14,863 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:15,589 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:15,712 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:16,160 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:16,315 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:16,638 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:16,682 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:16,805 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:17,425 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:17,580 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:17,685 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:17,854 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:18,210 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:18,569 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:18,734 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:18,820 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:52:19,091 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:19,285 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_selection_errors, confidence=0.72
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"ecb2de6b-338b-9a48-b326-b5bca80712c4"}, traceId: 213e006817566735107633205ee750'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.9s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"ef87e17c-cc0b-9764-a81a-ffa7b95bf1a4"}, traceId: 213e006f17566735129155354ee7f3'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"c00a047a-6eae-93b4-87fa-b330f24a150e"}, traceId: 213e066417566735144236516e8040'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"93f4e76e-bbc0-9047-a121-5140dbff67d2"}, traceId: 213e06ba17566735149272064e8a3f'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 12022
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=12022
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: file reader json

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"661ada30-7684-9697-a0cd-1050f44aa708"}, traceId: 213e059d17566735166878999e34b7'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"b78bca48-ac7d-9124-8a91-d47cae2dfed8"}, traceId: 213e001317566735194197823e0cfc'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"f63b94dd-2c80-99e8-92d7-5f4920e0895a"}, traceId: 2150429e17566735199203143e24d7'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"767b3883-b2d8-98bb-a049-447679dee20c"}, traceId: 215040cc17566735208547581ed541'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"d868ebde-e7c5-96ca-8cdd-9a69df22dfcd"}, traceId: 2150419d17566735226846347e2b43'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.7
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"754d3bbb-8090-97d0-8bd8-d45bc7d546e6"}, traceId: 213e042f17566735246567465e22e9'}2025-08-31 16:52:19,613 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:19,955 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:20,373 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:20,474 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:52:20,610 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:20,843 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:21,355 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:21,368 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:21,528 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:21,882 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:22,013 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:22,706 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:22,928 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:23,505 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 11300
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=11300
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"39c441a6-a59e-98cf-a84e-b10d5e0d4793"}, traceId: 213e006017566735105214157ee1b1'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"9b68acce-1c74-9aa0-9654-6e6444e243fb"}, traceId: 213e060a17566735107943838e8ab5'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.8s before retry (not counting as turn)...
  [SEARCH] Query: file reader json

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"f42deffa-9b28-9563-a1c6-84c2450c6e5f"}, traceId: 213e04ea17566735129372508e36b8'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"db1f37d4-f1da-93ba-ba11-a5558ad21faa"}, traceId: 213e064e17566735141612116e8078'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"d65aeddd-c087-9c08-84b8-fb7a96ebb101"}, traceId: 213e058a17566735148033154e36a0'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=other_errors, confidence=0.62
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"2c2490ee-1bb7-93a8-a9c8-503eb6a4fe74"}, traceId: 2150417917566735166874849eeda8'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.2s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"6b3038ae-9b8e-9e96-a16a-cce605e8320c"}, traceId: 213e06a117566735187593652e8bea'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.9s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22877
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22877
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"83277e8d-d466-9c78-9431-990725e87397"}, traceId: 215041de17566735205703506e3516'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"ddb0c416-a0ac-9b16-84e9-1116b37e1555"}, traceId: 2150448717566735226854487e7ede'}2025-08-31 16:52:23,607 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:23,977 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:23,991 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:52:23,992 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:52:23,995 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:24,021 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:52:24,021 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:52:24,021 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:52:24,097 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:24,354 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"340e2289-4ebb-9d82-bd66-195b1d743b5d"}, traceId: 213e01f617566735108815212e135c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.6s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.86
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"3f79028e-96b3-983e-b3e7-15d7a36cc8ca"}, traceId: 213e065017566735123123342e7f90'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.5s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"4a226666-59e0-92e7-a8d8-21bc36f68fc6"}, traceId: 213e006e17566735137594031e14c5'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.6s before retry (not counting as turn)...
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"3f0e8dc2-b6d5-987a-b11a-fd421bd26033"}, traceId: 213e062917566735143381150e7e38'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 11800
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=11800
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: network api fetch

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"bc4b6ce3-df43-9af4-a4f6-38615a409d43"}, traceId: 213e065e17566735167412412e7e46'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"31663fb9-7fd7-9770-889c-a3a2fef3a45f"}, traceId: 213e06c217566735172418861e816b'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"f184661e-afff-93a3-bed1-17ad5cd92105"}, traceId: 2150415b17566735187096197ee641'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"ba7a9e57-a1c8-97e6-b2ba-358f63ab40d8"}, traceId: 213e03d917566735195222543e1faf'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"08251dbe-ed67-99dd-84b6-b8fa471c9dcf"}, traceId: 2150414417566735207277563edbda'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.6s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"230d2d75-cf5f-90a5-ac08-999ecd885e0e"}, traceId: 213e060c17566735217856946e881a'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.2s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_selection_errors, confidence=0.77
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"45df2602-b2d2-94fc-88e2-f44d9df84a73"}, traceId: 213e043a17566735242745972e20f0'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"ef7ef0e9-1f41-9fd3-977c-390fb9a288bd"}, traceId: 2150423617566735258596272e0bc2'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...2025-08-31 16:52:25,055 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:25,550 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:26,074 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:26,089 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:26,597 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:26,990 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:27,122 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:27,141 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:52:27,141 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:52:27,182 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:52:27,182 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:52:27,182 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:52:27,400 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:27,498 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:27,510 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:27,531 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:52:27,532 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:52:27,571 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:52:27,571 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:52:27,571 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:52:27,830 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:28,310 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:28,747 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:28,921 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:29,053 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:29,647 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:52:29,744 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:29,775 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:29,825 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:30,121 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:30,407 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:30,686 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:30,826 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:30,943 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:31,316 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:31,524 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:31,841 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:31,841 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:32,036 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:32,385 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:52:32,387 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673552386662.json
2025-08-31 16:52:32,388 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673552387554.json
2025-08-31 16:52:32,388 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673552388206.json
2025-08-31 16:52:32,389 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673552388556.json
2025-08-31 16:52:32,389 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673552389132.json
2025-08-31 16:52:32,390 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673552389617.json
2025-08-31 16:52:32,390 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673552390236.json
2025-08-31 16:52:32,391 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673552390647.json
2025-08-31 16:52:32,391 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673552391188.json
2025-08-31 16:52:32,391 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673552391768.json
2025-08-31 16:52:32,392 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673552392001.json
2025-08-31 16:52:32,392 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673552392218.json
2025-08-31 16:52:32,392 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673552392430.json
2025-08-31 16:52:32,392 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673552392633.json
2025-08-31 16:52:32,393 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673552392842.json
2025-08-31 16:52:32,393 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673552393090.json
2025-08-31 16:52:32,393 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673552393316.json
2025-08-31 16:52:32,393 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673552393540.json
2025-08-31 16:52:32,394 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673552393887.json
2025-08-31 16:52:32,394 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673552394232.json
2025-08-31 16:52:32,687 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:32,736 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:33,079 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:33,632 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:34,069 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:34,136 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:34,522 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:34,987 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:34,994 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:35,005 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:52:35,005 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:52:35,030 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:52:35,030 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:52:35,030 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:52:35,553 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:52:35,631 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:35,912 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:36,158 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:36,180 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:36,790 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:36,857 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:37,652 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:37,911 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:38,041 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:38,184 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:38,291 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:38,301 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:52:38,301 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:52:38,332 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:52:38,332 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:52:38,332 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools

[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.6s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"69cc7f06-5cd9-90f8-9d92-22cc1f57d51b"}, traceId: 2150434117566735244693887e1f6a'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
ğŸ’¾ æ™ºèƒ½Checkpoint: ä¿å­˜20ä¸ªç»“æœ...
   è§¦å‘åŸå› : æ•°é‡=20, æ—¶é—´=0.0s, å¼ºåˆ¶=False
âœ… Checkpointå®Œæˆ: æˆåŠŸä¿å­˜ 20/20 ä¸ªç»“æœ
Progress: 20/35 (Success: 0)
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"e166bc0e-5bf9-9dc2-a9b6-aaa67c4a0c10"}, traceId: 213e041717566735254978079e96f0'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"3d54fb54-24f6-92f9-837c-396f580b67e9"}, traceId: 213e04ea17566735264337471e3400'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.7s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: network api fetch

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"d6ebc70a-8e16-97f1-86c7-56699d49205f"}, traceId: 213e065117566735295684910e7f5d'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"4a594008-de05-9e52-9961-745a7e8e0328"}, traceId: 213e066e17566735315802577e7e5b'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"248c193f-8a2f-9b05-80dd-a757de557900"}, traceId: 215040cc17566735334981146ed3d6'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.6s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"0c09c7a9-ae6a-9cdd-ad91-a0547f9034aa"}, traceId: 213e064617566735348698221e0ad6'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"90d7d328-4954-946c-8e6a-cd299c33ab21"}, traceId: 213e006f17566735367235865ee477'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.0s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"95222c09-a8e7-9eaa-a0a3-68c2178c956f"}, traceId: 213e06a117566735389285369e8aae'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"28a513f2-1028-9322-81c2-296c74f0d0ab"}, traceId: 213e065c17566735396531185e81e2'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.2s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"93042179-765a-95a7-a313-30aaca4dc7af"}, traceId: 2150452b17566735406593226e7b24'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.7s before retry (not counting as turn)...
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"5d257e6b-7e0a-93b7-935d-6556cafe4706"}, traceId: 213e007c17566735427957808eeff0'}2025-08-31 16:52:38,691 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"

[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"1c61bb9f-e025-9909-a78b-07f2718afe18"}, traceId: 213e00cd17566735241507433e9460'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"9250a8bd-a6f9-9a1f-badd-84631e8fc058"}, traceId: 2150417517566735258792582edef3'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"580ce06c-813b-9176-82a1-f09c454b64a6"}, traceId: 213e06b717566735263998915e7733'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"9ce939c9-e333-9cd2-8c97-8fb8e834332e"}, traceId: 213e007517566735279123214eee4c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.2s before retry (not counting as turn)...
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"a850c9c7-2257-9eeb-80fd-ac87ea1018cb"}, traceId: 213e062917566735286532597e7df4'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.8s before retry (not counting as turn)...
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"01a95fcf-2530-96e6-9b6d-1b360b100881"}, traceId: 215040be17566735319614114ede8f'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.2s before retry (not counting as turn)...
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"13652f68-769b-9112-9d23-41c1fd436261"}, traceId: 2150448717566735324698765e8028'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[ASSISTED] Task received 2 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 16604
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=16604
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"e9acdeb5-d281-9ffe-aad3-930863cd52b4"}, traceId: 213e043b17566735331952374e2262'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"bb155434-a4ad-9e81-aa38-551e84bc8df1"}, traceId: 213e03d917566735354711103e1f8e'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"71225a1f-2ea0-92db-a18f-621fd56d685a"}, traceId: 213e007a17566735368766601ee252'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"d7c6fd0f-12ca-94a7-8d46-bcd0d2e0d9be"}, traceId: 2150430c17566735374435202e20bf'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"b19fbf59-7918-9a80-a104-17c1acc80615"}, traceId: 213e006e17566735385721368e1462'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...2025-08-31 16:52:38,914 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:38,924 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:52:38,924 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:52:38,951 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:52:38,951 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:52:38,951 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:52:38,965 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:39,538 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:39,574 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:39,716 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:39,810 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:52:39,812 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44588_1756673559811288.json
2025-08-31 16:52:39,812 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44588_1756673559812527.json
2025-08-31 16:52:39,813 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44588_1756673559813080.json
2025-08-31 16:52:39,813 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44588_1756673559813389.json
2025-08-31 16:52:39,814 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44588_1756673559813880.json
2025-08-31 16:52:39,814 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44588_1756673559814200.json
2025-08-31 16:52:39,814 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44588_1756673559814434.json
2025-08-31 16:52:39,814 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44588_1756673559814665.json
2025-08-31 16:52:39,815 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44588_1756673559814884.json
2025-08-31 16:52:39,815 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44588_1756673559815090.json
2025-08-31 16:52:39,815 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44588_1756673559815312.json
2025-08-31 16:52:39,816 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44588_1756673559815653.json
2025-08-31 16:52:39,816 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44588_1756673559816475.json
2025-08-31 16:52:39,817 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44588_1756673559816906.json
2025-08-31 16:52:39,817 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44588_1756673559817124.json
2025-08-31 16:52:39,817 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44588_1756673559817329.json
2025-08-31 16:52:39,817 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44588_1756673559817545.json
2025-08-31 16:52:39,817 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44588_1756673559817743.json
2025-08-31 16:52:39,818 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44588_1756673559817950.json
2025-08-31 16:52:39,818 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44588_1756673559818166.json
2025-08-31 16:52:40,088 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:40,229 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:40,753 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:41,106 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:41,641 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:41,802 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:41,930 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:42,164 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 4.2s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"fc494444-9f82-9dec-a1ce-7770532152b7"}, traceId: 213e059717566735309225217e3695'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[ASSISTED] Task received 1 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 14401
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=14401
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"071dfe56-41b4-96e2-bc63-937a823ccd17"}, traceId: 215045b817566735314238154e800c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
  [SEARCH] Query: network api fetch

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"bc8a1b7e-2f2a-9621-95b7-807a41db92a4"}, traceId: 213e006817566735336823276ee689'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"47a78d3c-519c-9d79-8104-19fa9afdbc4d"}, traceId: 213e041917566735356264481e2aae'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.6s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"2f5cda24-630e-9191-83dc-5bdf32d738fc"}, traceId: 215045b717566735378747939e80af'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.72
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"b1dca155-ca84-9cd4-be19-6ad85a029dbb"}, traceId: 2150416717566735406491904eeefb'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.8s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 23224
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=23224
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"c6ca5d64-3c56-99b2-9ace-ed4f770e126b"}, traceId: 2150448717566735436495304e7fbb'}2025-08-31 16:52:42,398 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:42,413 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:42,760 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:42,884 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:43,617 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:43,900 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:43,900 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:44,102 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:44,577 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:44,827 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:44,948 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:52:45,081 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:45,814 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:45,826 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:52:45,826 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:52:45,859 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:52:45,859 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:52:45,859 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:52:45,997 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:46,524 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:46,726 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:46,937 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:47,122 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:47,127 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:47,272 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:47,280 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:52:47,280 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:52:47,309 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:52:47,309 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:52:47,309 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:52:47,620 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:47,802 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:47,838 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:48,617 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:48,618 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:48,759 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:48,979 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:49,500 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:49,670 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:50,191 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:50,241 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:50,252 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:52:50,252 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:52:50,276 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:52:50,276 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:52:50,276 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:52:50,718 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:50,778 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:52:50,891 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:52:50,917 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:51,402 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:51,450 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:51,602 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:51,764 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:52,083 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:52,171 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:52,646 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:52,931 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:53,164 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:53,176 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:52:53,177 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:52:53,203 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:52:53,203 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:52:53,203 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools

[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.9s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"06fd5893-72da-9143-8e72-ab6ebe337b3c"}, traceId: 213e060a17566735462908097e8c5b'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.7s before retry (not counting as turn)...
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"e043df49-e5d2-9467-9ad4-157ede8d378b"}, traceId: 2150430917566735468086908e1f19'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[ASSISTED] Task received 1 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 14948
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=14948
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: network api fetch

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"3bb71a32-c7cf-9cf2-8680-61de978f325f"}, traceId: 213e058a17566735483622691e33ca'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"0f795bf2-a3aa-9b01-bc0a-dfccb4250867"}, traceId: 215045ee17566735505411235e78bf'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.7s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"48de7811-fe09-918f-967a-be624c08ed20"}, traceId: 2150449a17566735528777069e817c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"78ddb0cd-e2e5-93e0-87ee-90b7314920ca"}, traceId: 2150454417566735533947442e826b'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.0s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.68
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"e75f5ea3-5ba7-9e4b-b106-7d6e5616433b"}, traceId: 2150457917566735575684226e8b05'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[ASSISTED] Task received 4 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 20737
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=20737
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json2025-08-31 16:52:53,407 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:53,483 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:53,575 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:53,749 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:54,005 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:54,385 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:54,629 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.7s before retry (not counting as turn)...
  [SEARCH] Query: file reader json

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"134b0b26-58ed-955a-8b3f-b4d63658c6bb"}, traceId: 213e041717566735408058770e983a'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"b3ec9aff-72fc-968a-b69c-9226ec2e28b2"}, traceId: 213e06c817566735428995412e8170'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"e547569e-645e-9efc-932a-5343b3922027"}, traceId: 213e06c217566735434007901e81ef'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.1s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22898
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22898
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"eb303c77-b1d3-9d5a-878a-19e42b1bd90e"}, traceId: 213e063717566735467962540e8b80'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"a28c83b9-fbbc-920b-9719-ecceb8c1d3e0"}, traceId: 213e007617566735476113026e1353'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
  [SEARCH] Query: network api fetch

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"5f775351-b455-90c1-b456-8948cef3a343"}, traceId: 2150417917566735497178267eed87'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 4.4s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
ğŸ’¾ æ™ºèƒ½Checkpoint: ä¿å­˜20ä¸ªç»“æœ...
   è§¦å‘åŸå› : æ•°é‡=20, æ—¶é—´=0.0s, å¼ºåˆ¶=False
âœ… Checkpointå®Œæˆ: æˆåŠŸä¿å­˜ 20/20 ä¸ªç»“æœ
Progress: 20/35 (Success: 0)
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"ccc39a55-4612-9c8b-9dc8-f1c97c7597a8"}, traceId: 215045b817566735529278438e823c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"f8740907-40ac-94e4-8ed5-e75f015e4a92"}, traceId: 215045b017566735554833506e83d4'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"12b3e251-7a79-941c-9181-91a16874d001"}, traceId: 2150455f17566735574861331e7f77'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.3s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...2025-08-31 16:52:55,276 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:55,542 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:56,140 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:56,368 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:56,615 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.1s before retry (not counting as turn)...
  [SEARCH] Query: network api fetch

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"c7b9d81e-165a-9c93-b970-0ea1d590b5d8"}, traceId: 213e006717566735466427738ee1bd'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.1s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"cc52e7ec-9359-97fc-a466-28c0c4dfb95d"}, traceId: 213e007017566735489413420eead5'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.5s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"bcd49d88-f3ba-9813-8abc-25b4ef84d5f1"}, traceId: 2150459f17566735508318751e8124'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.7s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"feedc06d-5a47-9bb2-8ef8-f83dcf564d51"}, traceId: 2150454417566735513302980e8310'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"774e2415-cc21-9d10-a15d-2a02c8067620"}, traceId: 215045ee17566735538198485e7901'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.4s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 23203
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=23203
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"a637bc32-2210-9091-a434-f1fbfcfdd2e0"}, traceId: 2150439017566735554628455e2778'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"d1d5c506-698c-9957-b09e-2b8c7509ce02"}, traceId: 215045be17566735573552152e7fc3'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: network api fetch

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.82
ğŸ’¾ æ™ºèƒ½Checkpoint: ä¿å­˜20ä¸ªç»“æœ...
   è§¦å‘åŸå› : æ•°é‡=20, æ—¶é—´=0.0s, å¼ºåˆ¶=False
âœ… Checkpointå®Œæˆ: æˆåŠŸä¿å­˜ 20/20 ä¸ªç»“æœ
Progress: 20/30 (Success: 0)
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"58c0f5cf-f842-9dd5-8995-2cb7d17ed60b"}, traceId: 215045b017566735595228130e8446'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.5s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"5cd74541-a056-9bb6-89d1-c583c7a1fd41"}, traceId: 215041a817566735609464538e366b'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"8841c54a-c5dd-951c-ad4b-56fb9471ad9f"}, traceId: 2150457917566735614726651e8a81'}2025-08-31 16:52:56,649 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:57,206 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:57,232 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:57,234 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:52:57,611 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:57,791 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:58,211 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:58,221 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:52:58,222 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:52:58,246 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:52:58,246 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:52:58,247 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:52:58,294 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:58,365 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:52:58,805 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:58,935 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:58,990 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:52:59,457 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:52:59,670 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:59,838 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:52:59,950 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:00,197 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:00,262 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:00,328 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:00,586 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:00,726 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:01,343 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:01,562 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:01,634 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:02,121 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:02,133 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:03,087 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:03,595 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:03,683 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:03,686 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:53:03,823 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:03,915 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:03,932 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:04,528 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:04,595 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:05,218 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:05,823 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:05,957 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:05,962 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:06,276 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:06,450 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:06,968 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:06,968 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:06,981 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:53:06,981 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:53:07,006 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:53:07,006 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:53:07,006 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:53:07,064 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:08,017 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:08,054 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:08,060 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:08,065 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:53:08,065 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:53:08,094 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:53:08,094 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:53:08,094 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:53:08,152 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:08,541 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:08,937 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:09,198 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:09,471 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:09,742 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:10,113 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:10,488 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:10,502 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:10,636 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"


[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"8136ceef-6e20-95e7-9341-ad8b61ba68ba"}, traceId: 2150411617566735588823689eeabf'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"e809e9d4-c305-90b6-8b45-4f4b98fba2ed"}, traceId: 2150456117566735617094833e7f3a'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.7s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 23002
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=23002
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: network api fetch

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"3d2c2a8a-f26e-92ed-8fef-ee4f0dcecfcc"}, traceId: 215041de17566735665671059e3450'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"c1f21455-0845-93e4-a705-9d92f503ba7b"}, traceId: 2150457a17566735678581182e0ca6'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"17fe8c24-e2b2-9498-b231-db6f087f5499"}, traceId: 2150435d17566735688081886e210b'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.5s before retry (not counting as turn)...
  [SEARCH] Query: network api fetch

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.9
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 12699
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=12699
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"c83a9acf-9d53-924b-9592-61837f86bef1"}, traceId: 2150454117566735707573230e79c5'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"60bdd70e-2cba-9e7d-b466-1bb1b91819d9"}, traceId: 2150411817566735727797662e0a31'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.8s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"012e34d0-f3cb-9371-9be1-350fec49bf7b"}, traceId: 215040be17566735739405581edebb'}2025-08-31 16:53:10,649 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:53:10,651 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:53:10,689 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:53:10,689 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:53:10,689 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:53:11,012 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:11,162 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:11,318 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"e1dda1dc-d53b-9137-9f9a-4e42d30817d9"}, traceId: 2150415b17566735588451367ee518'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"6e839779-f16a-9184-afa9-a8443e098c96"}, traceId: 215041de17566735593816362e34d4'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
  [SEARCH] Query: network api fetch

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: network api fetch

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"c27001ce-9428-9057-966e-4e946feb65a6"}, traceId: 2150443817566735617197734e8996'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"839ec1ae-61ed-9a7a-ad15-fe68fe50c37d"}, traceId: 2150417d17566735638927221e106c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.82
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 23209
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=23209
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"23fb8f54-0bed-97d4-ab64-b9c22679ea9b"}, traceId: 215045a817566735657813253e7f2e'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.3s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"ef62efee-87b6-94aa-9912-fd860cdf9d29"}, traceId: 2150457917566735689527666e8c6f'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"91f50a92-ad77-9de8-862d-d20a8850daf3"}, traceId: 2150458117566735709117364e8260'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 23191
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=23191
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct2025-08-31 16:53:11,566 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:11,846 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:11,940 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:12,215 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:53:12,595 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:12,736 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:12,837 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:13,526 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:13,635 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:14,130 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:14,157 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:14,832 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:53:14,843 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:15,034 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:15,246 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:15,356 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"b5fc66f2-a0d5-9ddd-b4a9-c0eb67e9a4ab"}, traceId: 2150448717566735629092642e8130'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.2s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"bb0c0071-adde-99e2-9dd7-5e037a808a26"}, traceId: 2150458717566735634125053e813d'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.9s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"4d4ee535-8d1b-9687-89bd-fea999ba6a1f"}, traceId: 2150456317566735669174658e815f'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"58bee2d8-b900-9968-8189-678d41867c56"}, traceId: 2150430d17566735674208411e993a'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.7s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"3fa8f054-2d33-91a3-89fd-632b9d95b2f3"}, traceId: 2150438d17566735682875617e2d82'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"8b08a177-9eef-9bcc-9a62-9b6fc542054e"}, traceId: 2150458717566735695445062e813d'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 12875
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=12875
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"06c48009-ab86-9ced-bc5a-ec2888b94bbd"}, traceId: 215045ee17566735707044138e7773'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"6ee7b651-1677-9810-a02c-a9acc78c5ea8"}, traceId: 215041de17566735719527643e3537'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"7b252c67-28f3-94f2-8b99-42ff100dceae"}, traceId: 2150439017566735727186375e27ba'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"1d9894ee-34d1-9d49-b9aa-3da5c581394d"}, traceId: 215045c117566735738105569e7f65'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.7s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"d18cd2a7-e66d-9a9f-be7f-1e10f0fb6583"}, traceId: 215045af17566735748613863e7e5d'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.5s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"50f3465b-973b-9474-8dbe-a3596813a4ee"}, traceId: 2150438d17566735759112564e2b0f'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching2025-08-31 16:53:15,564 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:15,881 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:16,019 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:16,052 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:16,782 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:17,011 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:17,059 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:53:17,293 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:17,663 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:17,860 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:17,883 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:18,167 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:18,503 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:18,808 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:18,913 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:19,295 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:19,719 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:19,796 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:19,823 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:20,699 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:20,982 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:20,988 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:21,004 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:53:21,004 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:53:21,030 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:53:21,030 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:53:21,030 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:53:21,485 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:21,897 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:22,038 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:22,264 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:22,455 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:22,697 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:23,048 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:24,081 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:24,093 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:53:24,094 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:53:24,119 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:53:24,119 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:53:24,119 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:53:24,683 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:25,043 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:25,193 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:25,318 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:25,843 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"

[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_selection_errors, confidence=0.62
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"de6cdcfa-3c48-9619-a765-bbe158881efb"}, traceId: 2150430c17566735769102027e1f35'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 23191
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=23191
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"daf39a70-a5a5-9be1-91d9-2f4f39338369"}, traceId: 215040b917566735787674362ef264'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"7159d596-9167-95af-bf6d-b850542beae7"}, traceId: 2150411817566735801427553e0cf1'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"7e18509a-c131-9b7d-8a1d-a33469902fdf"}, traceId: 2150411617566735806538831ee85a'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.9s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"c5ae3fcb-eda3-9fcb-afc0-893070237f54"}, traceId: 2150416417566735814295393e13eb'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"e6726356-cf02-922b-914c-ed26556e496d"}, traceId: 215044fd17566735832271720e7faa'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"e976a4a8-c8f8-9da0-9a0a-938069441c9a"}, traceId: 215045b717566735837226591e81b7'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.6s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"b3abf3d3-5c4a-903e-80e9-326177e5ec38"}, traceId: 2150415b17566735857601390ee518'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.3s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"c6edb39f-dd04-9545-9021-9967f100a677"}, traceId: 215040cc17566735862527892ed6ac'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"c4c505dd-b94f-9336-8fff-8369b8893b91"}, traceId: 2150460e17566735878111512e7958'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.1s before retry (not counting as turn)...
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"1c4e4291-3d39-9b31-a99f-342a1c70ac9d"}, traceId: 2150458117566735899317382e8260'}2025-08-31 16:53:26,495 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:53:26,496 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:53:26,523 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:53:26,523 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:53:26,523 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:53:26,648 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:53:26,900 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:26,928 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:27,763 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:27,844 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:28,021 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:28,326 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:28,338 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:28,464 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:28,630 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:53:28,800 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:28,876 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:28,988 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:29,664 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:30,037 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:30,260 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:30,563 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:31,086 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:31,155 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:31,735 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:31,991 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:32,169 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:32,658 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:32,753 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:33,183 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:33,183 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:33,183 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"

[RETRY] 400 error detected, waiting 2.2s before retry (not counting as turn)...
  [SEARCH] Query: network api fetch

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_selection_errors, confidence=0.72
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"377f8ffb-daec-9dc9-8af0-db8ba6de32f3"}, traceId: 2150436a17566735795131683e2246'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.5s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"b47a561d-95ee-9d29-89de-c21f4926bbd8"}, traceId: 2150423617566735809293554e0b60'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.2s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"b21f448f-e492-951e-b839-e6bf42338197"}, traceId: 2150458717566735814225167e7ec9'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.1s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"b88b6751-33ab-9e1b-bb69-8dc42caf5e48"}, traceId: 215040c017566735855803654edefa'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.2s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22991
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22991
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: network api fetch

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"3af4a0b9-0a26-972a-82ec-b3a50144a3f4"}, traceId: 2150416a17566735897748081edeb4'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"1936f66e-eb16-9044-ae9c-8d25b3a8ead6"}, traceId: 2150435d17566735903126345e1fc2'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"bae9b716-f4c1-9b9a-8d64-7fcc1ac44019"}, traceId: 213e043b17566735913526983e2430'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.9s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.86
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"11e15a02-edee-9ffb-a473-0f2427a12be1"}, traceId: 213e011517566735918786065e9188'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"920507ca-a695-9294-8103-09bc6a7571bc"}, traceId: 213e041917566735946596666e2af2'}
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"155f6c72-08cc-9632-bff6-09137b43df5b"}, traceId: 215040b917566735728824500eef2c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.9s before retry (not counting as turn)...
  [SEARCH] Query: network api fetch

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"217c060a-092c-9e9c-a495-ee9587256f5a"}, traceId: 2150417d17566735745761414e1132'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"edb1e2ba-1ea5-9344-b892-a197967e43e0"}, traceId: 2150429e17566735775705057e21ad'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"55d6fe9d-6d2b-9741-a62e-8be9a408874e"}, traceId: 2150436a17566735795661271e2225'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.9s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"cc4db1c9-e4ee-947c-8f4e-77b07121ec70"}, traceId: 2150415b17566735808655348ee620'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"f0984dc8-202c-9278-abc6-cd3120de1acf"}, traceId: 2150409517566735833901936eeab8'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"32fb5b84-fbca-975d-834a-d71dd24b5b5a"}, traceId: 2150457917566735838963738e8c2e'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"3d7d2d8d-2193-9f2f-a9e7-b7102ebb03eb"}, traceId: 215044da17566735852753697e8100'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.2s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 23217
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=23217
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"162590af-93b1-9962-8f07-1daabfb8cf90"}, traceId: 2150434117566735877597558e213a'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
  [SEARCH] Query: network api fetch

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"3230f1fc-4fa7-9af4-8ffb-ef52900e62dc"}, traceId: 2150438d17566735906165529e2c9b'}2025-08-31 16:53:33,197 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:53:33,197 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:53:33,241 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:53:33,241 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:53:33,241 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:53:33,597 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:34,126 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:34,260 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:34,410 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:34,599 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:34,843 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:34,910 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:35,135 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:35,482 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:35,997 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:36,381 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:36,570 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:36,852 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:36,958 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:37,066 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:37,399 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:37,932 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:38,008 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:38,019 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:53:38,019 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:53:38,046 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:53:38,046 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:53:38,046 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:53:38,255 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:53:38,580 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:38,690 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:39,008 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:53:39,236 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:39,349 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:39,839 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:40,394 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:40,747 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:40,869 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:40,935 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:41,150 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:41,695 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:41,904 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:42,146 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 3215
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=3215
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"8c5f0a7d-5c73-9120-bd79-f23628282a67"}, traceId: 2150416417566735904312885e1367'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.7s before retry (not counting as turn)...
  [SEARCH] Query: network api fetch

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"320635ac-a4d9-97ce-ac52-bcf9a5d80558"}, traceId: 213e065517566735929257317e80f0'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"6529d091-f037-9181-a4ee-1321445d1048"}, traceId: 213e007a17566735948337551ee549'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.2s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"041f4e0c-8a62-95b6-b21e-0f7f8a7720c1"}, traceId: 213e062917566735953481503e7df5'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_selection_errors, confidence=0.65
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"d8de435d-f64b-9375-9bf9-b62b09707f93"}, traceId: 213e007217566735969597108eeabd'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.6s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"7556b18b-b175-9959-926c-7dde365e5271"}, traceId: 213e064e17566735974626829e830c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.8s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"5f763253-175b-9721-9bdc-6980761deb3b"}, traceId: 213e043517566736007834471e2bc6'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 5.0s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 23290
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=23290
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"66d7121e-dd31-9d90-9fdb-43aeb78f7d81"}, traceId: 213e060a17566736045515654e89b6'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
[LLM_ERROR] Attempt 5/5: Connection error.
[ERROR] Max retries reached after 5 attempts2025-08-31 16:53:42,405 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:53:42,620 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:42,750 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:42,864 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:43,144 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:43,158 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:53:43,158 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:53:43,182 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:53:43,182 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:53:43,183 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:53:43,668 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:43,742 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:43,815 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:44,301 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:44,560 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:44,720 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:44,875 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:44,885 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:53:44,885 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:53:44,909 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:53:44,909 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:53:44,909 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:53:45,494 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:45,507 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:53:45,507 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:53:45,535 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:53:45,535 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:53:45,535 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:53:45,633 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:46,310 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:46,439 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:46,647 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:46,706 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:47,145 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:47,375 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.5s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"12d4dcd9-d04d-99ea-8704-f54599bc5051"}, traceId: 213e06bc17566735965934512e81ea'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"cd49fc6f-44d0-91bf-9e6c-03b595005e0b"}, traceId: 213e006f17566735995896683ee605'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 23203
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=23203
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"b1bcd7a4-ddaa-9e73-beb1-b4da82cdfc7e"}, traceId: 213e06a217566736015467542e81f6'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"53f2b863-625f-9d20-86a0-e5e65487951e"}, traceId: 213e06c117566736017385552e8a8e'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
  [SEARCH] Query: file reader json

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"4cd17acf-877f-9fe9-b4ea-74dd7f669f5e"}, traceId: 213e007017566736044591165ee949'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.1s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"e375ca29-a448-9eeb-94c7-fed310179ee4"}, traceId: 213e080f17566736076061193e0b9a'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.5s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"792fc6b8-9c95-9947-980b-4c556880c185"}, traceId: 213e06ba17566736098596823e8870'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"2ca0834f-7fb1-97f9-add7-510ed86a645c"}, traceId: 213e059d17566736119311775e359e'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.7s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"780b9f7d-7261-91f4-b613-7cb5c9834e20"}, traceId: 213e058a17566736124423248e36a0'}2025-08-31 16:53:47,582 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:48,029 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:48,127 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:48,536 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:48,554 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:48,951 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:49,194 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:53:49,289 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"3d1d6dbc-7a94-9a00-a7b6-f90a6e7733e8"}, traceId: 213e063817566735921328217e81e3'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"e2092fd9-1aa8-93e0-b2a5-394a882a2215"}, traceId: 213e007b17566735928184352eec49'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.6s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.82
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"f160b6b1-2edd-955e-b8e5-b7cb1926910b"}, traceId: 213e05ab17566735945288529e34cd'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.9s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"c4fdf8c4-9132-9c3e-8c98-1400f92069f5"}, traceId: 213e041717566735977314060e9921'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.6s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"c74bf3f5-0783-908c-9b0e-363e6f713106"}, traceId: 213e007d17566735999141256eef5d'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"c0534466-c312-9599-a12d-4863562a761a"}, traceId: 213e042b17566736013282675e1d3f'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 4.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"f8dff90c-636d-9f3b-a55f-ac023f0a8d39"}, traceId: 213e03e217566736019181291e1e95'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.9s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"473d04ab-3e03-9476-97dc-16622d4382ec"}, traceId: 213e064717566736048354400e89bd'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.3s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"44ad5ec1-4d87-9cea-9e7e-5193ac30bcd2"}, traceId: 213e062b17566736076291462e7f6e'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.1s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"2873d225-6d94-9ae8-ab7e-82fc125ac1db"}, traceId: 213e006817566736081378950ee6cb'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.7s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"d91c98a9-74b7-9d64-990f-46e629370d13"}, traceId: 213e062917566736098141516e7df5'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.2s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 23405
[AI_DEBUG] _ai_classify_with_txt_content called:2025-08-31 16:53:49,545 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:49,967 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:50,111 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:53:50,484 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:50,822 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:50,860 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:53:50,865 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:51,405 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:51,533 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:51,648 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:51,768 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:52,264 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:52,582 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:52,582 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:52,582 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:53,482 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:53,482 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:54,678 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:55,203 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:55,207 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:55,499 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:55,518 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:55,727 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:55,798 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:56,391 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:56,641 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:56,646 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:56,983 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:57,415 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:57,528 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:57,970 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:57,982 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:53:57,983 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:53:58,008 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:53:58,008 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:53:58,009 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:53:58,018 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: network api fetch

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"1bdea5c6-b8e5-9102-a037-06f4d0cffe21"}, traceId: 213e059d17566736076728282e3709'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.88
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 12371
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=12371
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"e0805696-8349-9573-9164-5ce2f1f83e3b"}, traceId: 213e062b17566736082157038e811b'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"85e3ea43-0199-9c83-98ac-5c2651b4e769"}, traceId: 213e065117566736095734458e81e7'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.6s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"e3f8eaaa-6e19-9277-805e-1abf83d1d3ae"}, traceId: 213e05ab17566736125517186e34ee'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
  [SEARCH] Query: network api fetch

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"5307cbb2-1fc2-9be9-98e8-a937f310d727"}, traceId: 213e007217566736138977134eeabd'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"183d8fe8-90c5-9a0c-aeb6-d83d2ef52adc"}, traceId: 213e066417566736144201117e8295'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.5s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"29c306f2-0798-9daf-b914-06b9e2b92b0d"}, traceId: 213e03d917566736158574029e1e44'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.6s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_selection_errors, confidence=0.68
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"4a9ca36c-2cd7-962d-8891-a54bb54a75ae"}, traceId: 213e065e17566736186298605e80bb'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.7s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"a06d13d6-043b-90fa-b921-010146704a71"}, traceId: 213e06a017566736209424856e7992'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.7s before retry (not counting as turn)...
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"fe07ca6f-e317-9cd0-a748-6b2775ac1d32"}, traceId: 213e006d17566736214432057e12bc'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching2025-08-31 16:53:58,026 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:58,140 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:58,873 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:59,200 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:53:59,610 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:53:59,687 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:00,149 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:00,183 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:00,298 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:00,688 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:01,083 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:01,576 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:01,587 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:01,629 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:02,631 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:03,315 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:54:03,591 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:03,939 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:04,116 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:04,215 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:04,269 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:04,869 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:05,165 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:05,275 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:05,395 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:05,826 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:05,900 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:06,123 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:06,609 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:06,758 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:07,300 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:07,625 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:07,785 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:07,819 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:08,171 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:08,514 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:08,834 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"

[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"35074bcb-de48-9341-a9e6-7d9072619f21"}, traceId: 213e007a17566736147636683ee252'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.7s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"4d790a27-8302-94ff-a3d5-2ae57bed3a67"}, traceId: 213e065917566736166976510e7f4c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.1s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22951
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22951
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"b901dd64-85c7-9bbf-baab-71fd01a6871a"}, traceId: 213e043a17566736184896059e20f0'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"99bc8578-d4f1-9a71-9bdf-ce3f380013ca"}, traceId: 213e01f617566736196702908e1296'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.5s before retry (not counting as turn)...
  [SEARCH] Query: file reader json

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.88
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"5f81d6bc-db2e-9746-9042-03aee9bcbfcb"}, traceId: 213e06a117566736218892687e8896'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"540f39e7-b89c-967f-a84a-2fb29f76153f"}, traceId: 2150436817566736235862385e1d19'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 23203
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=23203
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"c2c3b2e3-59cc-9704-a2ab-9bdd45aeb983"}, traceId: 213e057b17566736256268933e401b'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.3s before retry (not counting as turn)...
  [SEARCH] Query: data parser

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"ee80e55c-2342-9a39-b681-007dae00e9d2"}, traceId: 215042ae17566736266318130e9394'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...2025-08-31 16:54:08,847 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:54:08,847 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:54:08,874 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:54:08,874 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:54:08,874 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:54:09,403 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:09,413 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:54:09,413 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:54:09,437 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:54:09,437 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:54:09,437 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:54:09,592 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:09,605 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:54:09,605 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:54:09,635 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:54:09,635 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:54:09,635 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools

  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=23405
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"708045d3-21dd-91f3-a3d0-8c8d9750db31"}, traceId: 213e06b717566736128634550e7a4b'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.1s before retry (not counting as turn)...
  [SEARCH] Query: network api fetch

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"966e0028-f0b4-9376-9567-c1d3116fd6a1"}, traceId: 213e043b17566736146047480e23cd'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"71148602-9b5d-9af5-be21-7f63653f8eb0"}, traceId: 213e007d17566736156856010ef2f9'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.9s before retry (not counting as turn)...
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"fab36d53-f571-98e1-a382-bc9264c2434b"}, traceId: 213e006717566736168662732edf8b'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.9s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"67549e42-d811-94db-a2fb-cf146f92f5e5"}, traceId: 213e041917566736185115496e2cbf'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.6s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 23245
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=23245
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"826d179d-3bd9-9794-bdd8-6b551085f68b"}, traceId: 2150417917566736238574927eee8f'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"892b5cc1-1d1d-9d3d-8611-76b4da2110c1"}, traceId: 213e062b17566736249158253e80b8'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
  [SEARCH] Query: file reader json

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"b565bf28-7c16-9cc2-bfe3-ab59d069d403"}, traceId: 215041de17566736269003282e3348'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"6ed1cbd4-9513-965e-8829-ca18f574db43"}, traceId: 215041e117566736274073958e32ce'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.7s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"23624c69-b5f9-9203-b16b-491f2a4721b6"}, traceId: 213e060b17566736285765230e87cd'}2025-08-31 16:54:09,716 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:09,984 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:10,123 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:10,282 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:10,799 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:10,931 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:11,165 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:11,174 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:54:11,174 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:54:11,205 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:54:11,205 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:54:11,205 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:54:11,739 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:11,886 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:12,356 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:12,575 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:12,621 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:13,029 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:13,086 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:13,905 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:13,984 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:14,078 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:14,461 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[RETRY] 400 error detected, waiting 4.1s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"089e3622-eb65-9dfd-962b-8904d4f33948"}, traceId: 215041a817566736229498992e347b'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 23191
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=23191
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"73ec00cd-e264-9f4c-afec-fb0e8863f54d"}, traceId: 213e057b17566736259636817e3f76'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.5s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"229c6421-e32c-908e-8068-52f569cccbf7"}, traceId: 213e001317566736278383603e0b4f'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.9s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"9593203a-4be3-97a3-869b-95486191c0aa"}, traceId: 213e042f17566736283528971e236c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.2s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.82
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"02627fc2-f09a-92ca-a83c-8695a7bde300"}, traceId: 213e043a17566736306705735e1f64'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.9s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"e4963bd8-21fd-9b17-a045-9065141a8bcd"}, traceId: 213e066e17566736327847924e80f0'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
  [SEARCH] Query: file reader json

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"45106d40-d73e-966b-923b-20b92930cb50"}, traceId: 213e066d17566736356924014e8215'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.2s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 23245
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=23245
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"2bf60bed-9a30-9d46-b517-332437394893"}, traceId: 2150417717566736378198733ed9ed'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.2s before retry (not counting as turn)...
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json2025-08-31 16:54:14,472 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:14,605 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:15,126 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:15,137 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:54:15,138 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673655138280.json
2025-08-31 16:54:15,139 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673655138767.json
2025-08-31 16:54:15,139 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673655139394.json
2025-08-31 16:54:15,140 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673655139737.json
2025-08-31 16:54:15,140 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673655140157.json
2025-08-31 16:54:15,140 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673655140620.json
2025-08-31 16:54:15,141 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673655140943.json
2025-08-31 16:54:15,141 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673655141265.json
2025-08-31 16:54:15,530 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:15,650 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:54:16,173 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:16,624 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:54:16,913 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:17,223 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:17,249 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:17,302 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:17,342 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:17,358 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:54:17,359 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:54:17,392 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:54:17,392 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:54:17,392 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:54:17,747 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:18,012 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:18,321 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:18,625 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:18,641 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:18,969 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:19,216 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:19,962 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:20,043 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:20,209 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:54:20,456 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:20,641 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:20,891 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:21,079 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:21,091 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:54:21,092 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:54:21,123 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:54:21,124 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:54:21,124 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:54:21,417 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:21,791 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:21,941 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:22,020 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:54:22,310 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:22,603 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:22,881 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:23,025 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:23,171 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"

[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.7s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"6f186467-60fb-94f0-b0e0-a299a841174d"}, traceId: 213e011517566736288533096e91ca'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 4.3s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"17075fe6-38f3-9c9c-9e37-42fc108cd618"}, traceId: 213e043b17566736318405702e22c6'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"60275f5a-9bde-91b8-aabb-7f9cb9a918b3"}, traceId: 213e00cd17566736345051322e96d3'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"a1ec7ec5-4d9a-92ec-8dda-29ad0c0a5a0f"}, traceId: 213e06a117566736359217926e8a01'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"dd222f55-9c59-986e-836c-df1b1dc3f57d"}, traceId: 215045b417566736367046179e80ab'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"8202388f-69a0-943e-8b6f-d5b67e5a3523"}, traceId: 2150454417566736379171546e7ff8'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"ae4a107c-d304-9290-8140-a78237648db3"}, traceId: 213e006f17566736395843253ee49b'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.6s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"ee6b6402-0e46-95e9-b5b7-9bccf3806c3f"}, traceId: 213e006d17566736408798517e127a'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"a68270d5-63d8-9389-948c-e24bd721df44"}, traceId: 2150449017566736446393026e7f0a'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"15981623-f12b-9414-8d2f-8a1a9081870b"}, traceId: 213e007217566736474661892eed51'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 23072
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True2025-08-31 16:54:23,513 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:23,611 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:23,649 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:24,067 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:24,272 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:24,562 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:24,937 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:25,085 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:25,107 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:54:25,109 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:54:25,177 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:54:25,177 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:54:25,177 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools

[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.9s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.92
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"f876c873-5c97-9d69-b266-b59cc1df4bb5"}, traceId: 213e060917566736315687609e700b'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.7s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"25e5b616-0aba-952b-8863-a2b5b1d4fe36"}, traceId: 213e007f17566736327875363eeb8f'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"2925ce0a-a34c-9435-a081-909a9039b1cc"}, traceId: 213e065517566736344675705e8239'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"2618eecb-99e0-939d-b1c5-891a6ce4c06e"}, traceId: 2150411817566736368333584e0b3d'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.2s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"b30fcaae-73b7-9a38-8500-fa4d236479d6"}, traceId: 2150460817566736389146867e7b72'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 5.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"fd9c9c23-0547-9693-b1c5-f259e5171088"}, traceId: 2150415b17566736394187860ee3ef'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"9bdde383-276a-9dd6-8363-794eb281dc09"}, traceId: 213e059617566736409257196e3cae'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.8s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"328de6a1-fc08-9fc9-816c-e0cc2c40108e"}, traceId: 213e060b17566736466027716e8726'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"a2345489-ccd3-9af5-b35d-1039d41b73eb"}, traceId: 213e007e17566736471107079eed07'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 23300
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=23300
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized2025-08-31 16:54:25,808 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:25,856 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:54:26,001 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:26,335 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:26,343 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:26,660 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:26,809 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:27,320 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:27,709 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:28,328 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:28,620 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:29,044 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:29,360 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:30,060 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:30,330 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:30,338 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:30,439 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:30,451 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:54:30,451 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:54:30,476 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:54:30,476 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:54:30,476 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:54:30,735 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:30,854 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:30,854 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:30,966 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:31,377 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:54:31,642 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:31,659 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:31,788 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:32,053 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:32,723 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:33,103 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:33,196 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:33,752 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"

[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: file reader json

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"ac1cb8aa-a2a6-9b56-8577-5d24126ffa62"}, traceId: 213e007d17566736403786949ef33b'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.7s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"b58e3d0b-ecb4-9bf8-917a-621e5c50a846"}, traceId: 2150423617566736408806730e0aba'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.1s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.88
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"9b97dabf-86c8-9fe8-b2bb-99266698cd55"}, traceId: 213e057b17566736428264639e3d87'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"c7d8d84c-2101-9acc-9d4f-f04b286a5ddb"}, traceId: 2150455217566736438463174e8082'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 4.4s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"c85e3d90-d4ad-9ef6-b1d3-eff94eba2dc4"}, traceId: 215045be17566736445436591e7f44'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"767d1f2e-af7f-9387-b48c-65821357f4f3"}, traceId: 213e065117566736478197234e82fb'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.7s before retry (not counting as turn)...
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"5457ca64-a5be-9344-8031-88ae6cbf6d07"}, traceId: 213e043117566736486663061e2147'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 11166
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=11166
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"70a3e38e-8249-9314-a1d8-abb614b59b57"}, traceId: 213e006c17566736494191613e14d1'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.0s before retry (not counting as turn)...
  [SEARCH] Query: file reader json

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"59b86577-6048-96ad-9412-3169aada7cef"}, traceId: 213e064e17566736519322193e815f'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"5c256271-67c0-9cab-9004-8468cf4440c9"}, traceId: 213e006717566736532821825ee094'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"e5e0222b-b6af-9c5f-b650-c069bba91a88"}, traceId: 213e065017566736542665103e8057'}2025-08-31 16:54:34,000 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:34,000 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:34,203 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:34,524 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:34,818 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:34,829 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:54:34,830 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:54:34,843 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:34,853 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:54:34,853 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:54:34,853 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:54:35,724 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:35,805 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:35,976 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:36,275 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:36,310 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:36,490 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:36,501 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:36,502 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:54:36,502 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:54:36,530 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:54:36,530 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:54:36,530 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:54:36,623 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:37,023 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:37,396 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:37,812 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:38,068 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:38,074 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:38,779 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:38,779 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:39,013 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"

  - txt_content_len=23072
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"ce65a50c-a4a1-9395-94b2-6c234e4dd754"}, traceId: 213e05ab17566736493028348e3460'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"380c4287-0c62-902c-b29d-daa31535adae"}, traceId: 213e059717566736495905967e377c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.8s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"26b07204-2dee-9def-9f88-afc224f0444d"}, traceId: 213e006a17566736516674444ee29c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"f5effac8-e278-97a8-9aad-2aa9dc4137bf"}, traceId: 213e057b17566736533021327e3e8f'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.7s before retry (not counting as turn)...
  [SEARCH] Query: data parser

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"7a0bc5ac-7cbf-9273-916f-ccfcbadcb4b2"}, traceId: 213e065417566736547992377e7e76'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"fa5ed1ea-61ac-9f8d-a147-32a4f367fd42"}, traceId: 213e060917566736566153247e6e7e'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"42107458-3a46-9793-bf22-47fe799b1df0"}, traceId: 213e06c317566736571304384e7a30'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[ASSISTED] Task received 3 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 17582
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=17582
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"f9dc0025-6c64-9461-99f2-f135b86d9b22"}, traceId: 213e064d17566736579363403e8411'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"e0c19587-00f2-9b35-ac48-912e2b0d0e45"}, traceId: 213e065017566736585265624e7e46'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.7s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"371455f2-1079-934a-8fda-30e210cb55a0"}, traceId: 213e01f617566736599431744e154c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.2s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"7943c334-b7f8-96e5-99f6-8b87941f0dd4"}, traceId: 213e06bc17566736624854250e828e'}2025-08-31 16:54:39,321 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:39,335 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:39,833 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:40,150 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:40,901 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:54:40,903 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673680902269.json
2025-08-31 16:54:40,903 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673680903164.json
2025-08-31 16:54:40,904 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673680903758.json
2025-08-31 16:54:40,904 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673680904268.json
2025-08-31 16:54:40,905 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673680904857.json
2025-08-31 16:54:40,905 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673680905290.json
2025-08-31 16:54:40,906 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673680905974.json
2025-08-31 16:54:40,906 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673680906350.json
2025-08-31 16:54:40,907 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673680906726.json
2025-08-31 16:54:40,908 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673680907467.json
2025-08-31 16:54:40,908 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673680908475.json
2025-08-31 16:54:40,916 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:40,973 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:41,235 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:41,864 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:41,864 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:42,388 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:42,996 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:42,996 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:43,005 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct


[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: file reader json

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"7d09838c-dc5b-9bdf-8d48-e50f8586bb68"}, traceId: 213e007317566736518706610ee68d'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"e3287c3c-ea7f-9d19-8c7f-50805abfcb2f"}, traceId: 213e007317566736537635923ee62a'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.9s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"b59d7b99-ad49-9ea2-aeef-1edc7360f2a2"}, traceId: 213e063817566736542668275e82a9'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
ğŸ’¾ æ™ºèƒ½Checkpoint: ä¿å­˜8ä¸ªç»“æœ...
   è§¦å‘åŸå› : æ•°é‡=8, æ—¶é—´=129.3s, å¼ºåˆ¶=False
âœ… Checkpointå®Œæˆ: æˆåŠŸä¿å­˜ 8/8 ä¸ªç»“æœ
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 23007
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=23007
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"e7c45664-3b70-9197-a3ab-d434f0631916"}, traceId: 213e066017566736565377666e87a9'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.8s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"0b6f80cf-43d8-903c-a667-758b4e24e703"}, traceId: 213e043517566736579422667e2bc4'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
  [SEARCH] Query: file reader

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.82
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"ff3b7ed5-783f-9d27-9879-8941aa55deda"}, traceId: 213e065e17566736606681639e7f92'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.2s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"17bcb58d-27ad-9c9d-9ee9-c1df8c3847a9"}, traceId: 213e066017566736627938872e8b23'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.0s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22782
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22782
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized2025-08-31 16:54:43,006 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:54:43,032 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:54:43,032 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:54:43,032 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:54:43,961 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:44,171 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:44,486 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:44,535 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:54:44,537 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44588_1756673684536305.json
2025-08-31 16:54:44,537 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44588_1756673684537274.json
2025-08-31 16:54:44,538 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44588_1756673684537566.json
2025-08-31 16:54:44,538 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44588_1756673684538097.json
2025-08-31 16:54:44,539 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44588_1756673684538778.json
2025-08-31 16:54:44,539 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44588_1756673684539184.json
2025-08-31 16:54:44,539 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44588_1756673684539451.json
2025-08-31 16:54:44,539 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44588_1756673684539677.json
2025-08-31 16:54:44,540 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44588_1756673684539919.json
2025-08-31 16:54:44,779 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:45,696 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:45,735 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:45,787 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:54:45,942 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:46,105 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:46,631 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:47,172 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:47,813 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:48,201 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:48,212 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:54:48,212 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:54:48,237 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:54:48,237 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:54:48,237 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:54:48,533 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:48,680 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:48,680 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"

[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=other_errors, confidence=0.72
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"a47fd893-eecd-923e-8d07-71c2e925a77d"}, traceId: 213e06c217566736576121442e83fe'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.2s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"4ed743f4-4dac-9288-92e8-80ecfee62542"}, traceId: 213e06bc17566736597504032e7f34'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22914
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22914
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"4301cce2-35a9-9f05-96c7-dc2130728344"}, traceId: 213e06b617566736616066331e7f9b'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.5s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: file reader json

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"13512348-fbd1-91cd-b66a-e734e11fb1a2"}, traceId: 213e06c817566736635416280e82dc'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
Progress: 30/35 (Success: 0)
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"3cf36ea9-e757-90c8-9966-045333ed507e"}, traceId: 213e00cd17566736656444751e96fb'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.1s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"9ce549fe-3bfa-98af-8190-8511f14631be"}, traceId: 2150458117566736679022163e83aa'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"1ce790e5-51d8-92f0-85ea-eef541f11d33"}, traceId: 213e006f17566736705641777ee69f'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns2025-08-31 16:54:49,235 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:49,384 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:49,727 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:49,964 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:50,128 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:50,657 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:50,856 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:54:50,997 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:52,073 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:52,706 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:53,043 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:53,399 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:53,399 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:54:53,476 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:53,973 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:54,083 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:54,790 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:54,972 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:54,992 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:55,686 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:55,894 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:56,119 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:56,578 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:56,866 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:56,876 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:54:56,876 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:54:56,904 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:54:56,904 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:54:56,904 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:54:57,223 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:57,232 - api_client_manager - INFO - Created idealab client for model qwen2.5-72b-instruct
2025-08-31 16:54:57,233 - mcp_embedding_manager - INFO - Loading index from .mcp_embedding_cache/tool_index.pkl (size: 3723608 bytes)
2025-08-31 16:54:57,256 - mcp_embedding_manager - INFO - FAISS index loaded
2025-08-31 16:54:57,256 - mcp_embedding_manager - INFO - Updated dimension to 3072
2025-08-31 16:54:57,256 - mcp_embedding_manager - INFO - Index loaded successfully: 30 tools
2025-08-31 16:54:57,954 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:58,117 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:58,386 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:58,487 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:58,881 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:58,991 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:59,236 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:59,689 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:54:59,732 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:54:59,855 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:55:00,213 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:55:00,626 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:55:00,749 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:55:01,619 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:55:01,642 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"


[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"db6e522b-fb6b-9163-b00e-7eb5b539a531"}, traceId: 213e058a17566736656434032e363d'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"1d90ad99-b236-9369-8b5c-69a1e61c306e"}, traceId: 213e060c17566736659117396e8753'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.8s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"21103339-7105-9465-88e5-ae135c3c7adc"}, traceId: 2150436817566736676286882e200e'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"47a52cd3-2d2d-96e6-af32-fb11870c0a82"}, traceId: 213e06b717566736696328109e7754'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 4.7s before retry (not counting as turn)...
  [SEARCH] Query: file reader

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.92
Progress: 30/35 (Success: 0)
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"d9241fef-3de3-991b-a6d5-e0fd29eec961"}, traceId: 2150455f17566736709305169e82b1'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"29ada605-d85c-94a1-9ded-c7f93119c084"}, traceId: 2150421317566736757717808e364f'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 11144
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=11144
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"41977914-4992-9b16-86fe-3f13bc28f8c3"}, traceId: 2150417917566736766963384eeb54'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.5s before retry (not counting as turn)...
  [SEARCH] Query: file reader json

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"bfe24264-0c33-9b19-81b7-71335ede136f"}, traceId: 215041a817566736786416086e3629'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"aaad3f0f-87a2-9b3e-a34c-9bc0218d11c8"}, traceId: 213e059717566736805292375e34b6'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.8s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)2025-08-31 16:55:02,019 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:55:02,413 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:55:02,618 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:55:02,863 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:55:03,007 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:55:03,358 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:55:03,480 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:55:03,753 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:55:03,884 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:55:03,960 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:55:04,802 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:55:04,959 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:55:05,752 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:55:05,859 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"


[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"1ea172c1-a74f-9607-b939-4505e190e3be"}, traceId: 2150458717566736734976423e80b9'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 23320
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=23320
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: file reader

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"8ab9c029-d8aa-9db6-a857-ccefb754a716"}, traceId: 215040b917566736758218328ef17d'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"0b92c8f6-3d29-9302-9890-910f3484d2f6"}, traceId: 215040ed17566736763198183ebd3c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"ce5c9801-9c0a-9aa7-8f7b-b483e5c2bb5e"}, traceId: 2150457a17566736778585483e0d2a'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"595bb5d5-ff1f-9fc2-838c-708f2d3cb285"}, traceId: 2150417d17566736785611583e127c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
ğŸ’¾ æ™ºèƒ½Checkpoint: ä¿å­˜11ä¸ªç»“æœ...
   è§¦å‘åŸå› : æ•°é‡=11, æ—¶é—´=128.5s, å¼ºåˆ¶=False
âœ… Checkpointå®Œæˆ: æˆåŠŸä¿å­˜ 11/11 ä¸ªç»“æœ
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"adc7aab8-914f-9a3f-9391-771e370955ab"}, traceId: 213e062917566736816741852e7fc4'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.6s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"aefe0b77-ff47-9735-95be-0781b1061488"}, traceId: 213e043a17566736858911359e1fa6'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22773
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22773
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"355398fb-873e-9af9-91a1-10c507aebd66"}, traceId: 213e004f17566736878728383ee71c'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...2025-08-31 16:55:05,870 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:55:06,643 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:55:06,656 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:55:07,050 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:55:07,698 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:55:08,210 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:55:08,360 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:55:08,816 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:55:09,201 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:55:09,370 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:55:09,741 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:55:09,791 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:55:10,217 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:55:10,748 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:55:10,789 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:55:11,413 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:55:11,849 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:55:12,799 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:55:13,736 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:55:13,848 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:55:14,661 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 400 Bad Request"
2025-08-31 16:55:14,924 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:55:14,926 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44588_1756673714925412.json
2025-08-31 16:55:14,926 - batch_test_runner - INFO - Batch writing 30 records to database (qwen2.5-72b-instruct:30)
2025-08-31 16:55:14,928 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 30 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44588_1756673714926658.json
2025-08-31 16:55:14,928 - batch_test_runner - INFO - Successfully wrote 30/30 records (qwen2.5-72b-instruct:30)
2025-08-31 16:55:14,978 - batch_test_runner - INFO - Database saved successfully
2025-08-31 16:55:14,978 - batch_test_runner - INFO - Statistics saved to: /Users/ruicheng/Documents/GitHub/WorkflowBench/pilot_bench_cumulative_results/master_database.json
2025-08-31 16:55:14,978 - batch_test_runner - INFO - ============================================================
2025-08-31 16:55:14,978 - batch_test_runner - INFO - Batch test completed at 2025-08-31T16:55:14.978567
2025-08-31 16:55:14,978 - batch_test_runner - INFO - Summary:
2025-08-31 16:55:14,978 - batch_test_runner - INFO -   - Total tests: 30
2025-08-31 16:55:14,978 - batch_test_runner - INFO -   - Successful: 0
2025-08-31 16:55:14,978 - batch_test_runner - INFO -   - Failed: 30
2025-08-31 16:55:14,978 - batch_test_runner - INFO -   - Success rate: 0.0%
2025-08-31 16:55:14,978 - batch_test_runner - INFO -   - Log file: logs/batch_test_20250831_164813.log
2025-08-31 16:55:14,978 - batch_test_runner - INFO - ============================================================
2025-08-31 16:55:14,978 - batch_test_runner - INFO - ğŸ§¹ æ­£åœ¨æ¸…ç†å­˜å‚¨é€‚é…å™¨èµ„æº...
2025-08-31 16:55:14,979 - result_merger - INFO - åˆå¹¶çº¿ç¨‹å·²ç»åœæ­¢ï¼Œæ— éœ€é‡å¤æ“ä½œ
2025-08-31 16:55:14,979 - result_merger - INFO - å‘ç°90ä¸ªæ–°çš„ç»“æœæ–‡ä»¶
2025-08-31 16:55:15,012 - focused_ai_classifier - INFO - Focused AI classifier initialized with gpt-5-nano (parameter-filtered)
2025-08-31 16:55:15,012 - result_merger - INFO - [MERGER_PROTECTION] ä½¿ç”¨managerå†…ç½®çš„å®‰å…¨åˆå¹¶æœºåˆ¶
2025-08-31 16:55:17,020 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:55:17,716 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:55:17,950 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:55:18,044 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:55:18,047 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673718046200.json
2025-08-31 16:55:18,047 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673718047170.json
2025-08-31 16:55:18,047 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673718047598.json
2025-08-31 16:55:18,048 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673718048036.json
2025-08-31 16:55:18,049 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673718048658.json
2025-08-31 16:55:18,049 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673718049331.json
2025-08-31 16:55:18,050 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673718049763.json
2025-08-31 16:55:18,050 - batch_test_runner - INFO - Batch writing 35 records to database (qwen2.5-72b-instruct:35)
2025-08-31 16:55:18,059 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 35 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44586_1756673718050622.json
2025-08-31 16:55:18,059 - batch_test_runner - INFO - Successfully wrote 35/35 records (qwen2.5-72b-instruct:35)
2025-08-31 16:55:18,114 - batch_test_runner - INFO - Database saved successfully
2025-08-31 16:55:18,114 - batch_test_runner - INFO - Statistics saved to: /Users/ruicheng/Documents/GitHub/WorkflowBench/pilot_bench_cumulative_results/master_database.json
2025-08-31 16:55:18,114 - batch_test_runner - INFO - ============================================================
2025-08-31 16:55:18,114 - batch_test_runner - INFO - Batch test completed at 2025-08-31T16:55:18.114481
2025-08-31 16:55:18,114 - batch_test_runner - INFO - Summary:
2025-08-31 16:55:18,114 - batch_test_runner - INFO -   - Total tests: 35
2025-08-31 16:55:18,114 - batch_test_runner - INFO -   - Successful: 0
2025-08-31 16:55:18,114 - batch_test_runner - INFO -   - Failed: 35
2025-08-31 16:55:18,114 - batch_test_runner - INFO -   - Success rate: 0.0%
2025-08-31 16:55:18,114 - batch_test_runner - INFO -   - Log file: logs/batch_test_20250831_164813.log
2025-08-31 16:55:18,114 - batch_test_runner - INFO - ============================================================
2025-08-31 16:55:18,114 - batch_test_runner - INFO - ğŸ§¹ æ­£åœ¨æ¸…ç†å­˜å‚¨é€‚é…å™¨èµ„æº...
2025-08-31 16:55:18,115 - result_merger - INFO - åˆå¹¶çº¿ç¨‹å·²ç»åœæ­¢ï¼Œæ— éœ€é‡å¤æ“ä½œ
2025-08-31 16:55:18,115 - result_merger - INFO - å‘ç°8ä¸ªæ–°çš„ç»“æœæ–‡ä»¶
2025-08-31 16:55:18,135 - focused_ai_classifier - INFO - Focused AI classifier initialized with gpt-5-nano (parameter-filtered)
2025-08-31 16:55:18,135 - result_merger - INFO - [MERGER_PROTECTION] ä½¿ç”¨managerå†…ç½®çš„å®‰å…¨åˆå¹¶æœºåˆ¶
2025-08-31 16:55:18,979 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:55:20,140 - httpx - INFO - HTTP Request: POST http://39.96.211.155:8000/proxy/api/openai/v1/chat/completions "HTTP/1.1 200 OK"
2025-08-31 16:55:21,805 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:55:21,807 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No evidence of a specific agent decision error. The task shows no tools were selected or executed and the error message is 'Unknown error', with empty tool
2025-08-31 16:55:25,820 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:55:25,825 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673725824654.json
2025-08-31 16:55:25,826 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673725825883.json
2025-08-31 16:55:25,826 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673725826388.json
2025-08-31 16:55:25,827 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 1 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673725826877.json
2025-08-31 16:55:25,828 - batch_test_runner - INFO - Batch writing 35 records to database (qwen2.5-72b-instruct:35)
2025-08-31 16:55:25,831 - result_collector - INFO - ğŸ“¤ å·²æäº¤ qwen2.5-72b-instruct çš„ 35 ä¸ªç»“æœåˆ°æ”¶é›†å™¨: qwen2.5-72b-instruct_44587_1756673725829270.json
2025-08-31 16:55:25,831 - batch_test_runner - INFO - Successfully wrote 35/35 records (qwen2.5-72b-instruct:35)
2025-08-31 16:55:25,908 - batch_test_runner - INFO - Database saved successfully
2025-08-31 16:55:25,908 - batch_test_runner - INFO - Statistics saved to: /Users/ruicheng/Documents/GitHub/WorkflowBench/pilot_bench_cumulative_results/master_database.json
2025-08-31 16:55:25,909 - batch_test_runner - INFO - ============================================================
2025-08-31 16:55:25,909 - batch_test_runner - INFO - Batch test completed at 2025-08-31T16:55:25.909056
2025-08-31 16:55:25,909 - batch_test_runner - INFO - Summary:
2025-08-31 16:55:25,909 - batch_test_runner - INFO -   - Total tests: 35
2025-08-31 16:55:25,909 - batch_test_runner - INFO -   - Successful: 0
2025-08-31 16:55:25,909 - batch_test_runner - INFO -   - Failed: 35
2025-08-31 16:55:25,909 - batch_test_runner - INFO -   - Success rate: 0.0%
2025-08-31 16:55:25,909 - batch_test_runner - INFO -   - Log file: logs/batch_test_20250831_164813.log
2025-08-31 16:55:25,909 - batch_test_runner - INFO - ============================================================
2025-08-31 16:55:25,909 - batch_test_runner - INFO - ğŸ§¹ æ­£åœ¨æ¸…ç†å­˜å‚¨é€‚é…å™¨èµ„æº...
2025-08-31 16:55:25,910 - result_merger - INFO - åˆå¹¶çº¿ç¨‹å·²ç»åœæ­¢ï¼Œæ— éœ€é‡å¤æ“ä½œ
2025-08-31 16:55:25,912 - result_merger - INFO - å‘ç°5ä¸ªæ–°çš„ç»“æœæ–‡ä»¶
2025-08-31 16:55:25,936 - focused_ai_classifier - INFO - Focused AI classifier initialized with gpt-5-nano (parameter-filtered)
2025-08-31 16:55:25,936 - result_merger - INFO - [MERGER_PROTECTION] ä½¿ç”¨managerå†…ç½®çš„å®‰å…¨åˆå¹¶æœºåˆ¶
2025-08-31 16:55:26,021 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:55:26,024 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "dependency_errors",
  "reason": "In a multi_stage_pipeline, complete failure without any tools executed indicates prerequisites or dependencies were not satisfied or triggered. The ag
2025-08-31 16:55:27,387 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:55:27,388 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "Error presented as 'Unknown error' with no tool executions or defined task details. There is insufficient information to attribute a specific agent decisio

[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.3s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"1026aee3-b8ff-98b6-93a8-136fedc2ea30"}, traceId: 213e06b617566736638045047e801f'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"149cd178-7e4d-985f-8606-2c33cb2cb551"}, traceId: 213e042d17566736658105886e21d1'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.2s before retry (not counting as turn)...
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"2d238706-c0df-93a2-a0f5-8bd8887e26e9"}, traceId: 213e064b17566736666183869e7b60'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.9s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"3eabed5e-b6ed-9a03-a50a-7bc8fc96db11"}, traceId: 2150449017566736697394223e7fe0'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 3244
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=3244
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4997478192)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"b73f5cc4-21fb-9523-8e32-6aadad1f95f3"}, traceId: 213e066417566736702523267e81f0'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
  [SEARCH] Query: data parser

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"762e3463-2ffa-9c8f-8cdb-0071ed11efc5"}, traceId: 213e001317566736718507979e0c57'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"9fa4a89c-6abc-9c8d-822f-671104c11018"}, traceId: 2150457917566736733017029e8bcb'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"f0e16bda-9098-9c88-84c1-26e0a9a6c26c"}, traceId: 213e059717566736738005931e353e'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.8s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"fb6a99d4-55f5-9fd3-8ed0-0d6bdcdf6483"}, traceId: 215045af17566736755803512e8130'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"0bb18816-ea3c-92d7-a339-c013a4db4ea9"}, traceId: 213e006717566736760906161ee052'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.5s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_selection_errors, confidence=0.78
[DEBUG] Got result for task: has_result=True, save_logs=False2025-08-31 16:55:32,374 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:55:32,375 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The agent did not select or invoke any tool to perform the workflow (no tool execution). This is a tool-choice error in the initial decision step,
2025-08-31 16:55:34,359 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:55:34,359 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No agent-level decision error can be identified; the failure appears to be a tool/system-level unknown error (no tools executed, no parameters set). There 
2025-08-31 16:55:35,442 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:55:35,443 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "sequence_order_errors",
  "reason": "The task failed before any tool was invoked (0% coverage), indicating the agent did not proceed through the required multi-stage sequence. There i
2025-08-31 16:55:38,816 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:55:38,816 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No evidence of a mis-selection, misconfiguration, incorrect sequence, or missing dependencies. The error message is Unknown error with no tool execution de
2025-08-31 16:55:41,227 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:55:41,227 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "No tools were selected or executed for the multi-stage pipeline and no sequencing was established. The agent failed to choose/initialize the requi
2025-08-31 16:55:41,261 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:55:41,262 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "There were no required tools for this task, and no tool was executed. With no observable agent action or tool usage to critique, there is no demonstrable a

[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"3e0b51df-5169-93f5-8989-008a48d4b3b2"}, traceId: 213e03e217566736886821379e1e95'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.2s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"37f59458-07b1-9460-9e0c-15f7331a266c"}, traceId: 213e007617566736897584034e11e7'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"29453498-dfb4-9069-8972-f445d0d68fa1"}, traceId: 213e066e17566736907851099e800a'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"74a5e72f-c2f5-9c71-b8b4-6632cf327e69"}, traceId: 213e043517566736926723459e2ebc'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.8s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"ea4a7641-1c20-9172-9ed8-9e1cf1898b6a"}, traceId: 215040ed17566736942821678ebee8'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"c328b389-b488-99e0-9887-dd7a14e085b3"}, traceId: 215044eb17566736954283208e82a9'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.1s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22840
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22840
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 5057911616)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: file reader json

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: file reader json

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"4bbf25c4-70f7-9ff6-a1e9-ccc81bea5da6"}, traceId: 213e060917566736988946653e6e7f'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"cfb471a7-f3d0-90ab-bb89-a685cb5312fe"}, traceId: 213e006e17566736994325236e15b3'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"93b5836c-4349-9c15-8053-a0ed9c252154"}, traceId: 2150416317566737009271553e1301'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.9s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"164f357c-9766-9ba3-b7e2-53c103d60b22"}, traceId: 213e059617566737037535635e3d30'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.1s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"6908401a-2c1a-9490-a126-9972f4c24f40"}, traceId: 213e064d17566737051547786e80fa'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"4db03cdd-8779-9f33-80b0-5476f3640018"}, traceId: 213e059d17566737105488616e34b8'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22923
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22923
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"9fb80ffc-2d10-9927-a05b-8febcbc7ad63"}, traceId: 213e066d17566737139431356e8291'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.88
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22911
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22911
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
ğŸ’¾ æ™ºèƒ½Checkpoint: ä¿å­˜4ä¸ªç»“æœ...
   è§¦å‘åŸå› : æ•°é‡=4, æ—¶é—´=44.9s, å¼ºåˆ¶=True
âœ… Checkpointå®Œæˆ: æˆåŠŸä¿å­˜ 4/4 ä¸ªç»“æœ

[INFO] Batch writing 35 records to database (qwen2.5-72b-instruct:35)
[INFO] Successfully wrote 35/35 records (qwen2.5-72b-instruct:35)
[INFO] Runtime error report saved to: runtime_reports/runtime_error_report_20250831_165525.json
[SAVE_ENHANCED] å¼€å§‹å¢å¼ºä¿å­˜ï¼Œæ—¶é—´: 16:55:25
[SAVE_ENHANCED] ä½¿ç”¨æ–‡ä»¶é”æœºåˆ¶ä¿å­˜
[SAVE_ENHANCED] æ–‡ä»¶é”ä¿å­˜æˆåŠŸ

ğŸ“Š Statistics saved to: /Users/ruicheng/Documents/GitHub/WorkflowBench/pilot_bench_cumulative_results/master_database.json
[INFO] æ­£åœ¨åœæ­¢ResultMerger...
[INFO] æ‰§è¡Œæœ€ç»ˆåˆå¹¶...
[INFO] Enhanced manager: AIé”™è¯¯åˆ†ç±»ç³»ç»Ÿå·²å¯ç”¨
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct multi_stage_pipeline: sequence_order_errors (confidence: 0.62) - The task failed before any tool was invoked (0% co
[V3_UPDATE] åˆ›å»ºæ–°promptç±»å‹ç»“æ„: qwen2.5-72b-instruct -> baseline
[V3_UPDATE] åˆ›å»ºæ–°å·¥å…·æˆåŠŸç‡ç»“æ„: qwen2.5-72b-instruct -> baseline -> 0.8
[V3_UPDATE] åˆ›å»ºæ–°éš¾åº¦ç»“æ„: qwen2.5-72b-instruct -> baseline -> 0.8 -> easy
[V3_UPDATE] åˆ›å»ºæ–°ä»»åŠ¡ç±»å‹ç»“æ„: qwen2.5-72b-instruct -> baseline -> 0.8 -> easy -> multi_stage_pipeline
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 1)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] åˆ›å»ºæ–°ä»»åŠ¡ç±»å‹ç»“æ„: qwen2.5-72b-instruct -> baseline -> 0.8 -> easy -> simple_task
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 1)
[AI-CLASSIFY-EXISTING] Using existing AI classification: sequence_order_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 2)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct multi_stage_pipeline: tool_selection_errors (confidence: 0.85)
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'sequence_order_errors' -> 'sequence_order_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: sequence_order_errors -> sequence_order_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 5)
[AI-CLASSIFY-EXISTING] Using existing AI classification: sequence_order_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 6)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 7)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'sequence_order_errors' -> 'sequence_order_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: sequence_order_errors -> sequence_order_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors2025-08-31 16:55:44,785 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:55:44,787 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "No tools were selected or invoked to initiate the multi_stage_pipeline; the agent failed to choose an appropriate tool to run the required stages,
2025-08-31 16:55:49,386 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:55:49,387 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "Agent did not select or initialize any tool or workflow steps for the given (unknown) task; no tool usage occurred, indicating a failure at the to
2025-08-31 16:55:49,618 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:55:49,620 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No explicit agent decision error could be identified: there were no tools executed (no tool selection, parameter config, sequence, or dependency decisions 
2025-08-31 16:55:51,071 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:55:51,072 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "sequence_order_errors",
  "reason": "The agent failed to establish or follow the required multi-stage pipeline sequence; no stages were executed and the correct Aâ†’Bâ†’C workflow was not
2025-08-31 16:55:55,900 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:55:55,900 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "Primary agent decision error: The agent failed to select or initiate any tool(s) for the multi-stage pipeline. No tools were executed, indicating 
2025-08-31 16:55:57,253 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:55:57,254 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "No data_pipeline tooling was selected or invoked; the agent failed to initiate the required workflow by choosing and activating any tool. This lac
2025-08-31 16:55:59,611 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:55:59,612 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "dependency_errors",
  "reason": "The pipeline did not execute any tools, leaving no prerequisites (data loading/initialization) satisfied. This indicates the agent failed to establish
2025-08-31 16:55:59,984 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:55:59,986 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No evidence of an incorrect tool choice, parameterization, sequence, or dependency handling by the agent. The failure appears to be an unknown/system-level
2025-08-31 16:56:01,979 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:56:01,980 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No agent decision error identified: no tools were selected or executed, and the error message is Unknown error. The failure appears to be environmental/sys
2025-08-31 16:56:06,474 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:56:06,476 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No agent-driven decision can be identified: no tools were executed (0% tool coverage) and the error message is unknown. Without any tool usage, there is no
2025-08-31 16:56:06,803 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:56:06,805 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "Insufficient information to determine a specific agent decision error. The error message is Unknown error with no tools executed or details about tool choi
2025-08-31 16:56:07,884 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:56:07,885 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "There is no evidence of a wrong agent decision (no tools were invoked; required tools coverage is 0%), and the failure is reported as a generic Unknown err
2025-08-31 16:56:11,916 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:56:11,916 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No identifiable agent decision error: there is no evidence of incorrect tool selection, wrong parameters, incorrect sequence, or unmet dependencies. The er
2025-08-31 16:56:12,719 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:56:12,720 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "The task ended with an Unknown error and no tools were executed, making it impossible to attribute the failure to any specific agent decision (tool selecti
2025-08-31 16:56:14,474 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:56:14,475 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "dependency_errors",
  "reason": "In a multi-stage pipeline, stages rely on outputs from previous steps. The execution shows 0% tool coverage and no tools were run, indicating the agen
2025-08-31 16:56:18,120 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:56:18,121 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "No tools were selected or executed for the multi_stage_pipeline task. This constitutes a wrong tool decision/selection by the agent, as the task r
2025-08-31 16:56:18,223 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:56:18,223 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "sequence_order_errors",
  "reason": "No tools were invoked and no workflow steps were executed; the agent did not establish or follow a valid multi-stage pipeline sequence, effectivel
2025-08-31 16:56:19,012 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:56:19,013 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "No tools were selected or invoked for the multi_stage_pipeline task, resulting in 0% coverage. The agent effectively ignored the required tool set
2025-08-31 16:56:23,159 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:56:23,159 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "sequence_order_errors",
  "reason": "The agent failed to initiate or correctly sequence the multi-stage pipeline steps (Aâ†’Bâ†’C). No stages were executed or configured, effectively skip
2025-08-31 16:56:23,577 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:56:23,578 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The agent did not select any tool or begin with an inappropriate/absent tool for a data_pipeline task, resulting in no tools being executed and no
2025-08-31 16:56:23,888 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:56:23,888 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "sequence_order_errors",
  "reason": "No stages/tools were selected or executed, effectively skipping the required multi-stage pipeline order. The agent failed to initiate the Aâ†’Bâ†’C se
2025-08-31 16:56:23,938 - result_merger - INFO - æ¨¡å‹qwen2.5-72b-instructä¿å­˜39/39æ¡è®°å½•
2025-08-31 16:56:23,939 - result_merger - INFO - åˆå¹¶å®Œæˆï¼Œå…±å¤„ç†5ä¸ªæ–‡ä»¶ï¼Œä¿å­˜39æ¡è®°å½•
2025-08-31 16:56:23,940 - batch_test_runner - INFO - âœ… å­˜å‚¨é€‚é…å™¨èµ„æºæ¸…ç†å®Œæˆ
2025-08-31 16:56:23,940 - batch_test_runner - INFO - ğŸ”š å­è¿›ç¨‹æµ‹è¯•å®Œæˆï¼Œä¸»åŠ¨é€€å‡º

[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 11)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 12)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] åˆ›å»ºæ–°ä»»åŠ¡ç±»å‹ç»“æ„: qwen2.5-72b-instruct -> baseline -> 0.8 -> easy -> basic_task
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 1)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 3)
[AI-CLASSIFY-EXISTING] Using existing AI classification: other_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 4)
[AI-CLASSIFY-EXISTING] Using existing AI classification: sequence_order_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 5)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'other_errors' -> 'sequence_order_errors' (score: 0.78)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: other_errors -> sequence_order_errors
[FUZZY_MATCH] 'sequence_order_errors' -> 'sequence_order_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: sequence_order_errors -> sequence_order_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.05s
[AI-CLASSIFY-EXISTING] Using existing AI classification: other_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 9)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 10)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_selection_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 11)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'other_errors' -> 'sequence_order_errors' (score: 0.78)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: other_errors -> sequence_order_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_selection_errors' -> 'tool_selection_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_selection_errors -> tool_selection_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_selection_errors
[V3_UPDATE] åˆ›å»ºæ–°ä»»åŠ¡ç±»å‹ç»“æ„: qwen2.5-72b-instruct -> baseline -> 0.8 -> easy -> data_pipeline
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 1)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 2)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_selection_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 3)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'tool_selection_errors' -> 'tool_selection_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_selection_errors -> tool_selection_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_selection_errors' -> 'tool_selection_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_selection_errors -> tool_selection_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 7)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 8)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 9)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 13)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_selection_errors
[V3_UPDATE] åˆ›å»ºæ–°ä»»åŠ¡ç±»å‹ç»“æ„: qwen2.5-72b-instruct -> baseline -> 0.8 -> easy -> api_integration
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 1)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 2)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_selection_errors' -> 'tool_selection_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_selection_errors -> tool_selection_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.05s
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_selection_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 5)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 6)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_selection_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 7)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'tool_selection_errors' -> 'tool_selection_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_selection_errors -> tool_selection_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_selection_errors' -> 'tool_selection_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_selection_errors -> tool_selection_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.05s
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 11)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 12)
[AI-CLASSIFY-EXISTING] Using existing AI classification: other_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 3)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'other_errors' -> 'sequence_order_errors' (score: 0.78)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: other_errors -> sequence_order_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 5)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 6)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 7)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 11)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 12)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 13)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct multi_stage_pipeline: tool_selection_errors (confidence: 0.85) - Agent did not select or initialize any tool or wor
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 17)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct multi_stage_pipeline: dependency_errors (confidence: 0.62) - The pipeline did not execute any tools, leaving no
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 18)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct multi_stage_pipeline: other_errors (confidence: 0.72) - No agent-driven decision can be identified: no too
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 19)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct multi_stage_pipeline: dependency_errors (confidence: 0.85)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct multi_stage_pipeline: tool_selection_errors (confidence: 0.72)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct multi_stage_pipeline: sequence_order_errors (confidence: 0.85)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.05s
[INFO] æœ€ç»ˆåˆå¹¶å®Œæˆ: 5 ä¸ªæ–‡ä»¶
2025-08-31 16:56:24,040 - smart_result_collector - INFO - SmartResultCollector æ­£åœ¨å…³é—­...
2025-08-31 16:56:24,040 - smart_result_collector - INFO - SmartResultCollector å·²å…³é—­
2025-08-31 16:56:27,474 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:56:27,475 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "dependency_errors",
  "reason": "The agent did not establish or respect the required multi-stage pipeline dependencies; no tools were executed, indicating prerequisites or workflow se
2025-08-31 16:56:28,722 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:56:28,723 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "Agent did not select or invoke any tool to perform the basic_task; no action taken, effectively a tool choice/initiative failure (no tool usage). 
2025-08-31 16:56:32,399 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:56:32,400 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "Insufficient information to identify a specific agent decision error: no tools were executed and the error is reported as Unknown error. Unable to determin
2025-08-31 16:56:37,384 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:56:37,385 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "No tools were selected or invoked for the multi_stage_pipeline, effectively a failure to choose the requisite tools (e.g., pdf_reader and downstre

[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"0894a981-9d8c-9e70-8000-2b0dda2aa3ce"}, traceId: 213e068217566736834657668e7635'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"b479a884-8b88-9879-9d0c-5f40ff3e7b84"}, traceId: 213e007d17566736837421062ef1af'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.7s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_selection_errors, confidence=0.7
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22900
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22900
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"584b2a85-b9ba-9795-b4a9-053d1db640ab"}, traceId: 215045f517566736852337215e7ec5'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 2.2s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"5228d0aa-5f61-9c46-8cca-2310065041c6"}, traceId: 213e065517566736885102635e8132'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.2s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"c3d57ab1-cc19-994b-9827-156328ae47dd"}, traceId: 213e007217566736899224305ee9b4'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.3s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"4ee3a026-2b58-91bd-b253-c79d377dd186"}, traceId: 215045c117566736927726135e7f64'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 4.1s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22782
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22782
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[InteractiveExecutor] No LLM client provided, initializing from api_client_manager
[InteractiveExecutor] Initialized client with model: qwen2.5-72b-instruct
[InteractiveExecutor] Using prompt type: baseline for API key selection
[InteractiveExecutor] API model name: qwen2.5-72b-instruct
[MCPEmbeddingManager] Reusing existing singleton instance (id: 4984431360)
[MCPEmbeddingManager] Current cache size: 30 embeddings
[INFO] Tool embedding index loaded successfully
[INFO] Loaded full tool registry from mcp_generated_library/tool_registry_consolidated.json
[INFO] Operation semantic index initialized

[TURN 1/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [SEARCH] Query: file reader json

[TURN 2/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 5/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"360bbd38-f9ef-9e9f-a985-ca7b3d9fbd35"}, traceId: 2150456317566736977804790e815f'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[ERROR] Max retries reached after 5 attempts
[API_FAILURE] All retries exhausted
  [API_FAILURE] API failed (timeout or max retries)
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"dc7f0fe8-f682-92cd-908c-9515540044e1"}, traceId: 213e006717566737009407803ee15a'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.5s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 3247
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=3247
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"ee06bbb5-97eb-97f2-8b8c-a8fe4da9c099"}, traceId: 213e041917566737068498378e2d86'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"401b8e4d-6e44-9b61-97fe-0198a7b86b6c"}, traceId: 213e007d17566737086238441ef044'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.1s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_selection_errors, confidence=0.67
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"731f3075-5afe-946d-8e3e-fe361121c681"}, traceId: 213e011517566737106684196e94a1'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.4s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 22959
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=22959
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
ğŸ’¾ æ™ºèƒ½Checkpoint: ä¿å­˜7ä¸ªç»“æœ...
   è§¦å‘åŸå› : æ•°é‡=7, æ—¶é—´=62.9s, å¼ºåˆ¶=True
âœ… Checkpointå®Œæˆ: æˆåŠŸä¿å­˜ 7/7 ä¸ªç»“æœ

[INFO] Batch writing 35 records to database (qwen2.5-72b-instruct:35)
[INFO] Successfully wrote 35/35 records (qwen2.5-72b-instruct:35)
[INFO] Runtime error report saved to: runtime_reports/runtime_error_report_20250831_165518.json
[SAVE_ENHANCED] å¼€å§‹å¢å¼ºä¿å­˜ï¼Œæ—¶é—´: 16:55:18
[SAVE_ENHANCED] ä½¿ç”¨æ–‡ä»¶é”æœºåˆ¶ä¿å­˜
[SAVE_ENHANCED] æ–‡ä»¶é”ä¿å­˜æˆåŠŸ

ğŸ“Š Statistics saved to: /Users/ruicheng/Documents/GitHub/WorkflowBench/pilot_bench_cumulative_results/master_database.json
[INFO] æ­£åœ¨åœæ­¢ResultMerger...
[INFO] æ‰§è¡Œæœ€ç»ˆåˆå¹¶...
[INFO] Enhanced manager: AIé”™è¯¯åˆ†ç±»ç³»ç»Ÿå·²å¯ç”¨
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct multi_stage_pipeline: dependency_errors (confidence: 0.56) - In a multi_stage_pipeline, complete failure withou
[V3_UPDATE] åˆ›å»ºæ–°promptç±»å‹ç»“æ„: qwen2.5-72b-instruct -> baseline
[V3_UPDATE] åˆ›å»ºæ–°å·¥å…·æˆåŠŸç‡ç»“æ„: qwen2.5-72b-instruct -> baseline -> 0.8
[V3_UPDATE] åˆ›å»ºæ–°éš¾åº¦ç»“æ„: qwen2.5-72b-instruct -> baseline -> 0.8 -> easy
[V3_UPDATE] åˆ›å»ºæ–°ä»»åŠ¡ç±»å‹ç»“æ„: qwen2.5-72b-instruct -> baseline -> 0.8 -> easy -> multi_stage_pipeline
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 1)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct multi_stage_pipeline: tool_selection_errors (confidence: 0.82) - The agent did not select or invoke any tool to per
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 2)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct multi_stage_pipeline: other_errors (confidence: 0.85) - No evidence of a mis-selection, misconfiguration, 
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 3)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct multi_stage_pipeline: tool_selection_errors (confidence: 0.75)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct multi_stage_pipeline: sequence_order_errors (confidence: 0.72)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct multi_stage_pipeline: tool_selection_errors (confidence: 0.80)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct multi_stage_pipeline: other_errors (confidence: 0.75) - No evidence of an incorrect tool choice, parameter
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 7)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct multi_stage_pipeline: other_errors (confidence: 0.85) - There is no evidence of a wrong agent decision (no
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 8)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct multi_stage_pipeline: other_errors (confidence: 0.75) - No identifiable agent decision error: there is no 
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 9)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct multi_stage_pipeline: tool_selection_errors (confidence: 0.62)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct multi_stage_pipeline: sequence_order_errors (confidence: 0.85)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct multi_stage_pipeline: dependency_errors (confidence: 0.85)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.05s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct multi_stage_pipeline: other_errors (confidence: 0.65) - Insufficient information to identify a specific ag
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 13)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_selection_errors
[V3_UPDATE] åˆ›å»ºæ–°ä»»åŠ¡ç±»å‹ç»“æ„: qwen2.5-72b-instruct -> baseline -> 0.8 -> easy -> simple_task
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 1)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 2)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct multi_stage_pipeline: tool_selection_errors (confidence: 0.85)2025-08-31 16:56:37,916 - result_merger - INFO - æ¨¡å‹qwen2.5-72b-instructä¿å­˜42/42æ¡è®°å½•
2025-08-31 16:56:37,917 - result_merger - INFO - åˆå¹¶å®Œæˆï¼Œå…±å¤„ç†8ä¸ªæ–‡ä»¶ï¼Œä¿å­˜42æ¡è®°å½•
2025-08-31 16:56:37,917 - batch_test_runner - INFO - âœ… å­˜å‚¨é€‚é…å™¨èµ„æºæ¸…ç†å®Œæˆ
2025-08-31 16:56:37,920 - batch_test_runner - INFO - ğŸ”š å­è¿›ç¨‹æµ‹è¯•å®Œæˆï¼Œä¸»åŠ¨é€€å‡º

[FUZZY_MATCH] 'tool_selection_errors' -> 'tool_selection_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_selection_errors -> tool_selection_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_selection_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 5)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 6)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 7)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'tool_selection_errors' -> 'tool_selection_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_selection_errors -> tool_selection_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 11)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 12)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] åˆ›å»ºæ–°ä»»åŠ¡ç±»å‹ç»“æ„: qwen2.5-72b-instruct -> baseline -> 0.8 -> easy -> basic_task
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 1)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-EXISTING] Using existing AI classification: timeout_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 3)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 4)
[AI-CLASSIFY-EXISTING] Using existing AI classification: sequence_order_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 5)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'timeout_errors' -> 'timeout_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: timeout_errors -> timeout_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'sequence_order_errors' -> 'sequence_order_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: sequence_order_errors -> sequence_order_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 9)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 10)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 11)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.06s
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] åˆ›å»ºæ–°ä»»åŠ¡ç±»å‹ç»“æ„: qwen2.5-72b-instruct -> baseline -> 0.8 -> easy -> data_pipeline
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 1)
[AI-CLASSIFY-EXISTING] Using existing AI classification: sequence_order_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 2)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 3)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'sequence_order_errors' -> 'sequence_order_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: sequence_order_errors -> sequence_order_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.05s
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 7)
[AI-CLASSIFY-EXISTING] Using existing AI classification: other_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 8)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 9)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'other_errors' -> 'sequence_order_errors' (score: 0.78)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: other_errors -> sequence_order_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] åˆ›å»ºæ–°ä»»åŠ¡ç±»å‹ç»“æ„: qwen2.5-72b-instruct -> baseline -> 0.8 -> easy -> api_integration
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 1)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 13)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 2)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 5)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 6)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 7)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.05s
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 11)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 12)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 15)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 17)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_selection_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 18)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 19)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_selection_errors' -> 'tool_selection_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_selection_errors -> tool_selection_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 23)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_selection_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 24)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 25)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_selection_errors' -> 'tool_selection_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_selection_errors -> tool_selection_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[INFO] æœ€ç»ˆåˆå¹¶å®Œæˆ: 8 ä¸ªæ–‡ä»¶
2025-08-31 16:56:38,017 - smart_result_collector - INFO - SmartResultCollector æ­£åœ¨å…³é—­...
2025-08-31 16:56:38,017 - smart_result_collector - INFO - SmartResultCollector å·²å…³é—­
2025-08-31 16:56:39,307 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:56:39,308 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "There was no actionable task or tool path to execute: required tools = 0 and the error is labeled Unknown error. No agent decision (tool selection, paramet
INFO:__main__:âœ… åˆ†ç‰‡1å®Œæˆ
INFO:__main__:ç­‰å¾…åˆ†ç‰‡2å®Œæˆï¼ˆ20å®ä¾‹Ã—50workersï¼Œæœ€å¤šç­‰å¾…50åˆ†é’Ÿï¼‰...
INFO:__main__:âœ… åˆ†ç‰‡2å®Œæˆ
INFO:__main__:ç­‰å¾…åˆ†ç‰‡3å®Œæˆï¼ˆ20å®ä¾‹Ã—50workersï¼Œæœ€å¤šç­‰å¾…50åˆ†é’Ÿï¼‰...
2025-08-31 16:56:45,076 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:56:45,076 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No tools were invoked and the error message is unknown. There is insufficient evidence of any agent decision (tool choice, parameters, sequence, or depende
2025-08-31 16:56:51,781 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:56:51,782 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "The task context is unknown and no tools were selected or executed. The error message 'Unknown error' appears to be a system-level failure rather than a mi
2025-08-31 16:56:55,854 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:56:55,855 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "Agent did not select or invoke any of the required tools for the multi_stage_pipeline task, indicating a tool selection decision error (missing/in
2025-08-31 16:57:02,475 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:57:02,475 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "sequence_order_errors",
  "reason": "The agent did not establish a valid sequence or plan for the API integration task (no steps were defined/executed), effectively failing to produce
2025-08-31 16:57:06,237 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:57:06,239 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The agent did not select or initialize the required API integration tools (no tools were executed). This omission indicates a tool-choice/initiali
2025-08-31 16:57:16,603 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:57:16,604 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The agent did not select or initiate any tool for an api_integration task, effectively making a wrong/toolless decision given the task context. Wi
2025-08-31 16:57:25,759 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:57:25,759 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "Agent did not select or initiate any tool to perform the task; there was no tool usage executed, indicating a tool-selection/initialization decisi
2025-08-31 16:57:29,346 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:57:29,347 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No evidence of a wrong agent decision (tool selection, parameter configuration, sequence, or dependency). The error appears to be an unspecified system/too
2025-08-31 16:57:32,905 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:57:32,906 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The agent did not select or invoke the required API integration tooling to perform the task; no tools were executed (0% coverage), indicating a fa
2025-08-31 16:57:38,879 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:57:38,880 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "Agent failed to select or initiate any tool to perform the required simple_task; no action was taken despite an action being expected, indicating 
2025-08-31 16:57:44,221 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:57:44,223 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "Agent failed to select and invoke any tool appropriate for the data_pipeline task; no tools were executed and no workflow steps were initiated, in
2025-08-31 16:57:49,827 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:57:49,828 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "The failure stems from an unknown/system error with no agent decision error: there were no required tools, no tools used, and no defined task details; thus
2025-08-31 16:57:57,804 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:57:57,805 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The agent failed to select and/or execute any tool. With 0/0 required tools documented, the complete absence of tool usage indicates a tool-select
2025-08-31 16:58:02,315 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:58:02,315 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No agent decision error identified: no tools were selected or executed, and the error message is a generic 'Unknown error' suggesting a system/environment 
2025-08-31 16:58:07,227 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:58:07,228 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "Agent did not select or initialize any tool to perform the api_integration task; no required tools were invoked, indicating a misstep in tool sele
2025-08-31 16:58:11,784 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:58:11,785 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The task required no tools (0 required, 0 executed). The agent attempted to engage tools anyway, indicating a wrong tool decision for a tool-less 
2025-08-31 16:58:18,395 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:58:18,396 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "Insufficient information to attribute to an agent decision error: no tools were executed, no explicit error message, and no observable wrong decision (tool
2025-08-31 16:58:23,369 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:58:23,370 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No tools were selected or executed and the error is labeled 'Unknown error'. There is no evidence of a wrong tool choice, incorrect parameters, wrong execu
2025-08-31 16:58:32,050 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:58:32,051 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "Cannot identify a concrete agent decision error: there were no tools selected/executed and the error is reported as Unknown error. With no tool usage to re
2025-08-31 16:58:38,318 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:58:38,318 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "No tool was selected or executed due to an unknown error, indicating a failure at the tool selection stage. With 0% tooling coverage, the primary 
2025-08-31 16:58:44,721 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:58:44,721 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "No tools were required (0/0 coverage), yet the agent did not initiate any tool or action. The decision to abstain/perform no operation represents 
2025-08-31 16:58:51,426 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:58:51,426 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The agent did not select or invoke any tool to perform the simple_task. This effectively represents a wrong tool decision (choosing to proceed wit
2025-08-31 16:59:00,549 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:59:00,549 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No agent decision evidence (no tool usage, no parameters set, no sequence executed). The task required no tools and should have completed by default; the c
2025-08-31 16:59:04,405 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:59:04,405 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No agent decision error detected: the task required no tools and no actions were performed; the error 'Unknown error' indicates an external/runtime issue o
2025-08-31 16:59:09,401 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:59:09,403 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No evidence of any agent decision (no tools executed, no parameters set, no sequence defined). The error message is Unknown error with zero tool usage, mak
2025-08-31 16:59:16,645 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:59:16,650 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "No tools were selected or executed due to an unknown error; there is insufficient information to identify whether the agent chose a wrong tool, mi
2025-08-31 16:59:21,195 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:59:21,196 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The agent did not select or invoke any tool to perform the required basic_task, effectively failing to initiate the workflow. This is a tool-selec
2025-08-31 16:59:26,855 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:59:26,856 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "No tools were selected or initialized to perform the data_pipeline task. The agent did not pick the required tool (or any tool) to execute the wor

[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 21905
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=21905
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[LLM_ERROR] Attempt 4/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"be6b7dcb-3446-91ff-82a0-5a04034555eb"}, traceId: 213e081017566736794622233e0aca'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 4.7s before retry (not counting as turn)...
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
ğŸ’¾ æ™ºèƒ½Checkpoint: ä¿å­˜9ä¸ªç»“æœ...
   è§¦å‘åŸå› : æ•°é‡=9, æ—¶é—´=124.7s, å¼ºåˆ¶=False
âœ… Checkpointå®Œæˆ: æˆåŠŸä¿å­˜ 9/9 ä¸ªç»“æœ
  [EARLY_EXIT] No actions taken, continuing...

[TURN 3/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"532ec915-e170-9628-9df6-671fecce5e9c"}, traceId: 213e041917566736859264334e2c9e'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.0s before retry (not counting as turn)...
[LLM_ERROR] Attempt 2/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"c596b04e-6bbc-9f01-b803-5f844b2c9cad"}, traceId: 213e060917566736878301453e6ec2'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.2s before retry (not counting as turn)...
[LLM_ERROR] Attempt 3/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"76b79400-b546-9d6f-91d0-762e0b2dcc26"}, traceId: 213e059717566736899601874e3821'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 3.3s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 4/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [EARLY_EXIT] No actions taken, continuing...

[TURN 5/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"8ec1b559-4f0b-95ff-b6d4-362a4af717b8"}, traceId: 2150448717566736958804338e7e5a'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.6s before retry (not counting as turn)...
  [EARLY_EXIT] No actions taken, continuing...

[TURN 6/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 6: No tools executed after multiple turns

[TURN 7/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
  [FORMAT_ISSUE] Turn 7: No tools executed after multiple turns

[TURN 8/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"1a912e79-b3f6-929f-a2cc-2c958182e889"}, traceId: 2150457917566736999266796e8a81'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.9s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 8: No tools executed after multiple turns

[TURN 9/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"b1541d6d-ee16-9502-bea7-aaa0a836dc80"}, traceId: 213e080f17566737026091120e0ab2'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 1.5s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 9: No tools executed after multiple turns

[TURN 10/10]
[LLM_CALL] Using model: qwen2.5-72b-instruct, API name: qwen2.5-72b-instruct
[LLM_ERROR] Attempt 1/5: Error code: 400 - {'success': False, 'message': 'æ¨¡å‹æä¾›æ–¹é™æµ', 'data': None, 'code': 'MPE-429', 'detailMessage': '{"code":"Throttling.RateQuota","message":"Requests rate limit exceeded, please try again later.","request_id":"f0b42d1f-b23e-9aea-9ad6-6725939dbd4c"}, traceId: 213e066e17566737059527993e80f0'}
[429_ERROR] 429 Too Many Requests detected, attempting deployment switch...
[429_ERROR] Client does not support deployment switching
[RETRY] 400 error detected, waiting 0.7s before retry (not counting as turn)...
  [FORMAT_ISSUE] Turn 10: No tools executed after multiple turns
[ASSISTED] Task received 5 format helps, final result: failure
[DEBUG] Got result for task: has_result=True, save_logs=False
[AI_DEBUG] æµ‹è¯•éå®Œå…¨æˆåŠŸ(failure)ï¼Œå‡†å¤‡è°ƒç”¨AIåˆ†ç±»
[AI_DEBUG] ç”Ÿæˆçš„txt_contenté•¿åº¦: 21984
[AI_DEBUG] _ai_classify_with_txt_content called:
  - use_ai_classification=True
  - ai_classifier=True
  - txt_content_len=21984
  - task_model=qwen2.5-72b-instruct
[AI_CLASSIFIER] ç›´æ¥ä½¿ç”¨AIåˆ†æå®Œæ•´äº¤äº’è®°å½•ï¼ˆè·³è¿‡è§„åˆ™åŒ¹é…ï¼‰
[AI_DEBUG] AIåˆ†ç±»ç»“æœ: category=tool_call_format_errors, confidence=0.85
Progress: 30/30 (Success: 0)
ğŸ’¾ æ™ºèƒ½Checkpoint: ä¿å­˜1ä¸ªç»“æœ...
   è§¦å‘åŸå› : æ•°é‡=1, æ—¶é—´=30.4s, å¼ºåˆ¶=True
âœ… Checkpointå®Œæˆ: æˆåŠŸä¿å­˜ 1/1 ä¸ªç»“æœ

[INFO] Batch writing 30 records to database (qwen2.5-72b-instruct:30)
[INFO] Successfully wrote 30/30 records (qwen2.5-72b-instruct:30)
[INFO] Runtime error report saved to: runtime_reports/runtime_error_report_20250831_165514.json
[SAVE_ENHANCED] å¼€å§‹å¢å¼ºä¿å­˜ï¼Œæ—¶é—´: 16:55:14
[SAVE_ENHANCED] ä½¿ç”¨æ–‡ä»¶é”æœºåˆ¶ä¿å­˜
[SAVE_ENHANCED] æ–‡ä»¶é”ä¿å­˜æˆåŠŸ

ğŸ“Š Statistics saved to: /Users/ruicheng/Documents/GitHub/WorkflowBench/pilot_bench_cumulative_results/master_database.json
[INFO] æ­£åœ¨åœæ­¢ResultMerger...
[INFO] æ‰§è¡Œæœ€ç»ˆåˆå¹¶...
[INFO] Enhanced manager: AIé”™è¯¯åˆ†ç±»ç³»ç»Ÿå·²å¯ç”¨
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct basic_task: other_errors (confidence: 0.58) - No evidence of a specific agent decision error. Th
[V3_UPDATE] åˆ›å»ºæ–°promptç±»å‹ç»“æ„: qwen2.5-72b-instruct -> baseline
[V3_UPDATE] åˆ›å»ºæ–°å·¥å…·æˆåŠŸç‡ç»“æ„: qwen2.5-72b-instruct -> baseline -> 0.8
[V3_UPDATE] åˆ›å»ºæ–°éš¾åº¦ç»“æ„: qwen2.5-72b-instruct -> baseline -> 0.8 -> easy
[V3_UPDATE] åˆ›å»ºæ–°ä»»åŠ¡ç±»å‹ç»“æ„: qwen2.5-72b-instruct -> baseline -> 0.8 -> easy -> basic_task
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 1)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct simple_task: other_errors (confidence: 0.55) - Error presented as 'Unknown error' with no tool ex2025-08-31 16:59:33,167 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:59:33,168 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "sequence_order_errors",
  "reason": "The task required no explicit tools, yet the agent did not execute any steps or produce output. This inaction indicates a faulty workflow decision
2025-08-31 16:59:39,198 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:59:39,199 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "Agent failed to select an appropriate tool for the api_integration task (no tools were executed). This represents a tool-selection decision error 
2025-08-31 16:59:45,109 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:59:45,110 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "No tools were selected or executed for the multi-stage task, indicating the agent failed to identify or choose the appropriate tools (tool-selecti
2025-08-31 16:59:52,367 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:59:52,368 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "parameter_config_errors",
  "reason": "The task description is unknown and required inputs/parameters were not provided or inferred, leading to no actionable steps. This reflects a fa
2025-08-31 16:59:56,780 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 16:59:56,781 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The agent did not select or activate any API integration tool despite the task requiring API interaction; zero tooling usage indicates a wrong or 
2025-08-31 17:00:03,944 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:00:03,945 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "For a multi_stage_pipeline task, the agent should select and orchestrate the appropriate tools in the required sequence. In this instance, no tool
2025-08-31 17:00:11,641 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:00:11,641 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The agent failed to select or invoke any tool/action for a task that requires an operational step, effectively making a wrong tool decision (choos
2025-08-31 17:00:17,200 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:00:17,200 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No tool decisions were possible because the task had no specified required tools or steps to execute. The error 'Unknown error' indicates an internal/envir
2025-08-31 17:00:24,622 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:00:24,623 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No agent decision could be evaluated because no tools were used and the error is labeled as 'Unknown error'. Insufficient information to attribute the fail
2025-08-31 17:00:29,192 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:00:29,192 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No tools were executed and the error is labeled as an unknown error. This suggests a system/tool-level failure rather than an agent decision (no evidence o
2025-08-31 17:00:38,379 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:00:38,380 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "Task had no required tools (0/0). No observable agent decision (no tool selection, parameters, or sequence) to attribute to a specific agent error type. Th
2025-08-31 17:00:46,272 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:00:46,274 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "sequence_order_errors",
  "reason": "The task requires performing a workflow, but no tools were invoked and no steps were executed. This indicates the agent did not follow or initiate
2025-08-31 17:00:51,386 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:00:51,387 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "Agent did not make any actionable decision: no tools were selected or executed, no parameters were set, and no workflow sequence was defined for the api_in
2025-08-31 17:00:57,445 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:00:57,446 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The task had no specified required tools and there is no evidence of any tool being chosen or clarification sought. The absence of a tool-selectio
2025-08-31 17:01:03,170 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:01:03,171 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No observable agent decision path to evaluate: no tools were executed, no parameters provided, and no sequence or dependency decisions can be assessed. The
2025-08-31 17:01:08,265 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:01:08,266 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No tools were selected or executed and there is insufficient information to identify any concrete agent decision error. The failure is attributed to an unk
2025-08-31 17:01:18,926 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:01:18,926 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No agent decision error: the task requires zero tools (0/0 coverage). There were no tool selections, parameter configurations, sequence steps, or dependenc
2025-08-31 17:01:25,227 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:01:25,227 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "dependency_errors",
  "reason": "Agent failed to handle pipeline dependencies: no tools were executed, suggesting prerequisites (e.g., data loading/initialization) were not completed 
2025-08-31 17:01:38,821 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:01:38,822 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "sequence_order_errors",
  "reason": "No actions were executed for a task that requires completing a simple operation; there was no tool usage, indicating the workflow did not proceed.
2025-08-31 17:01:48,643 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:01:48,651 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No agent-driven tool selection, parameter configuration, or sequencing occurred (no tools were executed). The error message appears to be a system-level Un
2025-08-31 17:01:53,113 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:01:53,114 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No tool usage or decision path to analyze. The failure is reported as 'Unknown error' with zero tool executions. There is no evidence of a wrong tool choic
2025-08-31 17:02:02,291 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:02:02,292 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "dependency_errors",
  "reason": "No tools were executed and no workflow steps were initiated. The agent did not establish or satisfy any prerequisites/dependencies for the api_integra
2025-08-31 17:02:07,094 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:02:07,095 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The agent did not select or invoke any required data pipeline tool, effectively failing to choose an appropriate tool (e.g., data_loader/data_read

[V3_UPDATE] åˆ›å»ºæ–°ä»»åŠ¡ç±»å‹ç»“æ„: qwen2.5-72b-instruct -> baseline -> 0.8 -> easy -> simple_task
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 1)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct data_pipeline: other_errors (confidence: 0.85) - No agent-level decision error can be identified; t
[V3_UPDATE] åˆ›å»ºæ–°ä»»åŠ¡ç±»å‹ç»“æ„: qwen2.5-72b-instruct -> baseline -> 0.8 -> easy -> data_pipeline
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 1)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct basic_task: other_errors (confidence: 0.68)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct simple_task: other_errors (confidence: 0.62)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct data_pipeline: tool_selection_errors (confidence: 0.85)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.05s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct multi_stage_pipeline: other_errors (confidence: 0.85) - No agent decision error identified: no tools were 
[V3_UPDATE] åˆ›å»ºæ–°ä»»åŠ¡ç±»å‹ç»“æ„: qwen2.5-72b-instruct -> baseline -> 0.8 -> easy -> multi_stage_pipeline
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 1)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct data_pipeline: other_errors (confidence: 0.65) - Insufficient information to determine a specific a
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 3)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct basic_task: other_errors (confidence: 0.72) - The task ended with an Unknown error and no tools 
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 3)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct multi_stage_pipeline: sequence_order_errors (confidence: 0.65)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct data_pipeline: tool_selection_errors (confidence: 0.85)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct basic_task: tool_selection_errors (confidence: 0.62)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.05s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct multi_stage_pipeline: other_errors (confidence: 0.62) - There was no actionable task or tool path to execu
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 3)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct api_integration: other_errors (confidence: 0.65) - No tools were invoked and the error message is unk
[V3_UPDATE] åˆ›å»ºæ–°ä»»åŠ¡ç±»å‹ç»“æ„: qwen2.5-72b-instruct -> baseline -> 0.8 -> easy -> api_integration
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 1)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct api_integration: other_errors (confidence: 0.65) - The task context is unknown and no tools were sele
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 2)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct multi_stage_pipeline: tool_selection_errors (confidence: 0.85)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct api_integration: sequence_order_errors (confidence: 0.85)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct api_integration: tool_selection_errors (confidence: 0.75)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct api_integration: tool_selection_errors (confidence: 0.60) - The agent did not select or initiate any tool for 
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 5)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct simple_task: tool_selection_errors (confidence: 0.60) - Agent did not select or initiate any tool to perfo
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 3)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct data_pipeline: other_errors (confidence: 0.85) - No evidence of a wrong agent decision (tool select
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 5)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct api_integration: tool_selection_errors (confidence: 0.85)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct simple_task: tool_selection_errors (confidence: 0.85)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct data_pipeline: tool_selection_errors (confidence: 0.75)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.08s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct api_integration: other_errors (confidence: 0.85) - The failure stems from an unknown/system error wit
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 7)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct simple_task: tool_selection_errors (confidence: 0.62) - The agent failed to select and/or execute any tool
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 5)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct api_integration: other_errors (confidence: 0.85) - No agent decision error identified: no tools were 
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 8)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct api_integration: tool_selection_errors (confidence: 0.85)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct simple_task: tool_selection_errors (confidence: 0.62)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct api_integration: other_errors (confidence: 0.75)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct basic_task: other_errors (confidence: 0.85) - No tools were selected or executed and the error i
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 5)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct simple_task: other_errors (confidence: 0.65) - Cannot identify a concrete agent decision error: t
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 7)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct simple_task: tool_selection_errors (confidence: 0.62) - No tool was selected or executed due to an unknown
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 8)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct basic_task: tool_selection_errors (confidence: 0.85)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct simple_task: tool_selection_errors (confidence: 0.80)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct simple_task: other_errors (confidence: 0.72)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.05s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct basic_task: other_errors (confidence: 0.85) - No agent decision error detected: the task require
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 7)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct data_pipeline: other_errors (confidence: 0.60) - No evidence of any agent decision (no tools execut
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 7)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct basic_task: tool_selection_errors (confidence: 0.40) - No tools were selected or executed due to an unkno
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 8)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct basic_task: tool_selection_errors (confidence: 0.85)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct data_pipeline: tool_selection_errors (confidence: 0.65)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct basic_task: sequence_order_errors (confidence: 0.72)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.05s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct api_integration: tool_selection_errors (confidence: 0.85) - Agent failed to select an appropriate tool for the
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 11)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct multi_stage_pipeline: tool_selection_errors (confidence: 0.85) - No tools were selected or executed for the multi-s
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 5)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct basic_task: parameter_config_errors (confidence: 0.75) - The task description is unknown and required input
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 11)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct api_integration: tool_selection_errors (confidence: 0.85)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct multi_stage_pipeline: tool_selection_errors (confidence: 0.62)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct basic_task: tool_selection_errors (confidence: 0.75)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct simple_task: other_errors (confidence: 0.85) - No tool decisions were possible because the task h
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 11)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct basic_task: other_errors (confidence: 0.85) - No agent decision could be evaluated because no to
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 13)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct api_integration: other_errors (confidence: 0.85) - No tools were executed and the error is labeled as
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 13)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct simple_task: other_errors (confidence: 0.85)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct basic_task: sequence_order_errors (confidence: 0.75)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct api_integration: other_errors (confidence: 0.85)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.05s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct simple_task: tool_selection_errors (confidence: 0.60) - The task had no specified required tools and there
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 13)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct multi_stage_pipeline: other_errors (confidence: 0.65) - No observable agent decision path to evaluate: no 
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 7)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct simple_task: other_errors (confidence: 0.85) - No tools were selected or executed and there is in
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 14)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct simple_task: other_errors (confidence: 0.60)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct multi_stage_pipeline: dependency_errors (confidence: 0.72)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct simple_task: sequence_order_errors (confidence: 0.60)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.11s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct api_integration: other_errors (confidence: 0.60) - No agent-driven tool selection, parameter configur
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 15)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct data_pipeline: other_errors (confidence: 0.85) - No tool usage or decision path to analyze. The fai
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 9)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 17)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct api_integration: dependency_errors (confidence: 0.85)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct data_pipeline: tool_selection_errors (confidence: 0.85)
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.05s
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 19)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 20)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 21)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 25)
[AI-CLASSIFY-EXISTING] Using existing AI classification: other_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 26)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 15)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'other_errors' -> 'sequence_order_errors' (score: 0.78)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: other_errors -> sequence_order_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.05s
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 17)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_selection_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 18)
[AI-CLASSIFY-EXISTING] Using existing AI classification: other_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 19)2025-08-31 17:02:13,578 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:02:13,579 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "Insufficient task context and absence of tool execution make it impossible to identify a concrete agent decision error. The failure appears to be an unknow
2025-08-31 17:02:21,566 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:02:21,568 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "sequence_order_errors",
  "reason": "The agent did not perform any steps in the required data_pipeline sequence (no tools were invoked), effectively breaking the intended execution fl
2025-08-31 17:02:27,543 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:02:27,543 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "Unknown system error occurred; there is no evidence of a wrong agent decision (no tools executed, no incorrect parameters, and no misordered sequence or un
2025-08-31 17:02:32,017 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:02:32,020 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "The failure appears to be a non-agent/system-level error (Unknown error) with no tool usage or explicit agent decision (no tool selection/parameters/sequen
2025-08-31 17:02:37,742 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:02:37,743 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The agent did not select or invoke any of the required API/integration tools, resulting in 0% tool coverage. This is a tool-selection decision err
2025-08-31 17:02:43,584 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:02:43,585 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "No tool was selected or executed for the required simple_task; the agent failed to choose an appropriate tool (effectively selecting 'none'), resu
2025-08-31 17:02:50,503 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:02:50,504 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "sequence_order_errors",
  "reason": "The agent did not perform any actions or follow any execution sequence (no tools used, no processing steps). This indicates a missing/incorrect de
2025-08-31 17:02:58,166 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:02:58,166 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The agent did not select or invoke any API integration tool; effectively an omission in tool selection/planning, resulting in no execution. This i
2025-08-31 17:03:03,152 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:03:03,156 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No tool usage occurred and the error is reported as an unknown/system-level issue. There is no evidence of incorrect tool selection, misconfigured paramete
2025-08-31 17:03:09,248 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:03:09,249 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "In a multi-stage pipeline with an unknown task, no tools were selected or executed. The agent failed to choose an appropriate starting tool (and t
2025-08-31 17:03:13,441 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:03:13,442 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No evidence of an agent decision error: no tool was selected, configured, or sequenced, and there were no dependency issues. The failure is an unknown/unde
2025-08-31 17:03:19,629 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:03:19,629 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "Agent failed to select any tool for the data_pipeline task (no tool was chosen or specified), leading to zero tool execution and no progress towar
2025-08-31 17:03:25,852 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:03:25,853 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The agent did not select or initialize any tools for the multi-stage pipeline (no tools executed), effectively skipping the required toolchain. Th
2025-08-31 17:03:33,355 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:03:33,356 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "sequence_order_errors",
  "reason": "The agent did not perform any action or follow any execution sequence for a simple_task with no required tools, effectively omitting steps in the 
2025-08-31 17:03:41,250 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:03:41,251 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "No tools were selected or workflow steps initialized for the task, indicating a failure at the decision level to initiate the appropriate data_pip
2025-08-31 17:03:49,506 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:03:49,507 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The task reported an Unknown error with no tools executed and no required tools listed. The primary agent decision error appears to be a tool sele
2025-08-31 17:03:56,956 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:03:56,957 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No agent decision error detected: no tools were selected or executed and there is no information about required tools, parameters, or workflow sequence. Th
2025-08-31 17:04:00,812 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:04:00,813 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "Agent failed to select and initialize the required data ingestion/tooling for the data_pipeline task (no tools were chosen or executed), indicatin
2025-08-31 17:04:08,691 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:04:08,692 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "sequence_order_errors",
  "reason": "The agent produced complete inaction (no tools executed) and no error message, which indicates it did not follow or even begin the required data p
2025-08-31 17:04:12,685 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:04:12,685 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The agent did not select or initialize any tools for the multi_stage_pipeline, effectively halting the workflow before any stage could run. This r
2025-08-31 17:04:22,457 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:04:22,459 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The agent did not select or execute any tool because the task context was undefined ('Unknown task'). This represents a decision to abstain or fai
2025-08-31 17:04:30,547 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:04:30,548 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No agent decision was made: no tools were selected or actions taken due to an unknown internal error. This does not reflect a wrong tool choice/parameter/s
2025-08-31 17:04:35,091 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:04:35,092 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "Insufficient information to identify a specific agent decision error. No tools were executed and the error is labeled 'Unknown error,' suggesting an extern
2025-08-31 17:04:41,929 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:04:41,930 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The agent did not select or execute any actionable tool to address the simple_task. There were no required tools defined, yet no action or result 
2025-08-31 17:04:50,435 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:04:50,436 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "sequence_order_errors",
  "reason": "The agent did not execute any steps for the basic_task and did not initiate the required workflow. This omission indicates a failure to follow the
2025-08-31 17:04:55,179 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:04:55,180 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "Agent failed to select the required data_pipeline tool(s); no tools were executed (0% coverage), indicating an incorrect or missing initial tool c
2025-08-31 17:05:01,145 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:05:01,145 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "No tool was selected or executed for the (unknown) task; the agent made a premature/empty tool decision instead of choosing an appropriate tool to
2025-08-31 17:05:06,326 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:05:06,327 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "There is no evidence of any agent-level decision error. No tools were executed and the error is labeled 'Unknown error', implying a general/tool environmen
2025-08-31 17:05:11,157 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:05:11,158 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "There is no evidence of a concrete agent decision error (no tool usage, parameters, or sequence details). The error message 'Unknown error' suggests an ext
2025-08-31 17:05:17,452 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:05:17,453 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No agent decision error identified: the task requires no tools (0/0 coverage). There was no tool selection, parameter setting, sequencing, or dependency de
2025-08-31 17:05:23,797 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:05:23,800 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "sequence_order_errors",
  "reason": "Agent did not perform any actionable steps or invoke any tools for the basic_task. There is no execution sequence (Aâ†’Bâ†’C) completed, effectively s
2025-08-31 17:05:30,819 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:05:30,820 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "Agent failed to engage any tool or workflow appropriate for an api_integration task, effectively selecting 'no tool'. This is a wrong decision for
2025-08-31 17:05:36,263 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:05:36,264 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "No tool was selected or engaged for the api_integration task, leaving 0% tool coverage. The agent did not choose an appropriate tool (or request t
2025-08-31 17:05:43,022 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:05:43,025 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No agent-level decision or tool usage evidence to classify; the failure is described as an unknown error without indicating an incorrect tool choice, misco

[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_selection_errors' -> 'tool_selection_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_selection_errors -> tool_selection_errors
[FUZZY_MATCH] 'other_errors' -> 'sequence_order_errors' (score: 0.78)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: other_errors -> sequence_order_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.05s
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 23)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 24)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 11)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.05s
[AI-CLASSIFY-EXISTING] Using existing AI classification: other_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 13)
[AI-CLASSIFY-EXISTING] Using existing AI classification: other_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 14)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 15)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'other_errors' -> 'sequence_order_errors' (score: 0.78)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: other_errors -> sequence_order_errors
[FUZZY_MATCH] 'other_errors' -> 'sequence_order_errors' (score: 0.78)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: other_errors -> sequence_order_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_selection_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 19)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 20)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 17)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'tool_selection_errors' -> 'tool_selection_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_selection_errors -> tool_selection_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 19)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_selection_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 20)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 21)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_selection_errors' -> 'tool_selection_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_selection_errors -> tool_selection_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.05s
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 25)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 9)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 26)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.06s
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 11)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 12)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_selection_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 13)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
2025-08-31 17:05:52,967 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:05:52,968 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No tools were selected or executed and the error is reported as 'Unknown error' with 0% tool coverage. There is insufficient evidence of a specific agent d
2025-08-31 17:05:57,690 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:05:57,691 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "sequence_order_errors",
  "reason": "The agent did not initiate or follow the required api_integration workflow steps (no tool calls or actions taken). This indicates a breakdown in t
2025-08-31 17:06:07,979 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:06:07,979 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The task had zero required tools (0/0 coverage), yet the agent did not select or execute any tool, leading to a complete failure. The primary agen
2025-08-31 17:06:15,103 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:06:15,105 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "dependency_errors",
  "reason": "Agent failed to establish and execute the required pipeline steps; no tools were selected or executed, effectively ignoring dependencies and prerequis
2025-08-31 17:06:21,095 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:06:21,095 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No agent decision error detected: the task reported requires zero tools and no tools were executed. The failure appears to be an unknown/internal issue rat
2025-08-31 17:06:26,533 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:06:26,538 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No detectable agent decision error: no tool selections, parameters, or sequence were attempted; error log shows Unknown error with no executed tools, indic
2025-08-31 17:06:33,614 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:06:33,617 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No tools were executed and there is no observable agent decision path (no tool selection, parameterization, sequence, or dependency actions). The error mes
2025-08-31 17:06:39,687 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:06:39,688 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "dependency_errors",
  "reason": "No tools were selected or executed and no pipeline steps were performed, indicating the agent failed to establish or respect tool dependencies/sequenc
2025-08-31 17:06:46,584 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:06:46,585 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "sequence_order_errors",
  "reason": "No actions were executed for a task that should have followed a defined basic workflow; effectively a missing/incorrect sequence or steps in the w
2025-08-31 17:06:50,645 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:06:50,645 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The agent did not select and execute any tool appropriate for a data_pipeline task. No tools were invoked, indicating a wrong or missing tool choi
2025-08-31 17:06:54,972 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:06:54,972 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No agent decision errors were identifiable. The failure stems from an unknown runtime/environment error with no tools executed, so there was no wrong tool 
2025-08-31 17:06:59,860 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:06:59,861 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No tools were selected or executed and the error message is Unknown error; there is insufficient information to attribute failure to a specific agent decis
2025-08-31 17:07:04,573 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:07:04,574 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No tool usage data and the error reported is 'Unknown error'. There is insufficient information to attribute the failure to a specific agent decision (tool
2025-08-31 17:07:15,292 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:07:15,299 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No explicit agent decision or tool usage evidence to attribute to one of the four agent-error categories. The task shows complete failure with 0% required 
2025-08-31 17:07:22,927 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:07:22,928 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "sequence_order_errors",
  "reason": "The agent did not execute any steps or follow the expected workflow for a 'simple_task'. No actions were taken, which is an omission in the task s
2025-08-31 17:07:27,640 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:07:27,640 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The agent did not select or initialize any tool(s) to perform the api_integration task, effectively failing at the initial tool-selection/plan ste
2025-08-31 17:07:37,561 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:07:37,562 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The task reported 0 required tools (0/0), meaning there is no valid tool to select. The agent proceeded with an action path implying tooling despi
2025-08-31 17:07:42,424 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:07:42,424 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No agent decision can be identified: no tools were selected, parameters configured, or executed; the error message 'Unknown error' suggests a system-level 
2025-08-31 17:07:46,748 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:07:46,752 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The agent did not select or initiate any tool for the api_integration task, effectively choosing no tool at all when tools were required. This rep
2025-08-31 17:07:52,259 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:07:52,260 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "Agent did not select or invoke any tool for a task that requires action; abstaining from tool use indicates a wrong tool decision (no tool was cho
2025-08-31 17:07:55,820 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:07:55,821 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "Agent did not select or initialize any required tools for the multi_stage_pipeline task, resulting in 0% tool coverage. This indicates a wrong or 
2025-08-31 17:08:00,229 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:08:00,230 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "Agent failed to select and/or invoke an appropriate tool for the api_integration task; no tools were used, indicating a wrong initial tool choice 
2025-08-31 17:08:06,154 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:08:06,169 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "There is no evidence of any agent decision (no tools used, no tool outputs, and a generic 'Unknown error'). Without tool selection, parameter config, seque
2025-08-31 17:08:15,444 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:08:15,448 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No agent misdecision detected: there were no tools required or executed (Required Tools Coverage 0%), and no sequence or dependency decisions were made. Th
2025-08-31 17:08:22,086 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:08:22,086 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "dependency_errors",
  "reason": "The task context is unknown/undefined and no tools were executed; the agent failed to establish a workflow plan or clarify task requirements before pr
2025-08-31 17:08:27,238 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:08:27,239 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No agent decisions or tool usages were performed (0/0 required tools; no tools executed). Without any observable agent choices (tool selection, parameters,
2025-08-31 17:08:37,072 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:08:37,074 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No tools were required for the task (Required Tools Coverage: 0%). There is no evidence of an agent decision error (no tool selections, parameter settings,
2025-08-31 17:08:43,432 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:08:43,433 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No explicit error message and no tools executed; there is no evidence of a specific agent decision error (tool choice, parameters, sequence, or dependencie
2025-08-31 17:08:49,443 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:08:49,444 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No tool usage or agent decision details are available to attribution a specific agent error. The error message is a generic 'Unknown error' with 0% tool co
2025-08-31 17:08:58,361 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:08:58,362 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No evidence of an agent decision error: there were no tools executed and the error message is generic ('Unknown error'), making it impossible to attribute 
2025-08-31 17:09:05,189 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:09:05,191 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "No tool was selected or executed for the task (0% tool coverage). This indicates a misdecision at the tool-selection step, as the agent failed to 
2025-08-31 17:09:12,621 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:09:12,622 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "There were zero required tools for the task, and no actions were executed. The failure appears to be due to inaction rather than a definable agent decision
2025-08-31 17:09:18,003 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:09:18,010 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No agent decision error identifiable: the task required no tools and no steps were executed; therefore there is no wrong tool choice, parameter, sequence, 
2025-08-31 17:09:25,477 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:09:25,478 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "sequence_order_errors",
  "reason": "No actions were taken and no execution sequence was established or followed. The agent did not initiate any steps for the task, effectively skippi
2025-08-31 17:09:31,994 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:09:32,001 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "There is no evidence of a wrong agent decision (tool selection, parameter config, sequence, or dependencies). No tools were executed and the error is repor
2025-08-31 17:09:36,086 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:09:36,086 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "There were no tools selected or executed, and the error appears to be a system-level/unknown error rather than a decision-by-the-agent. Since there were no
2025-08-31 17:09:39,670 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:09:39,672 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "Insufficient information to determine a specific agent decision error; the error message is generic ('Unknown error') and there are no details about tool u
2025-08-31 17:09:43,407 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:09:43,408 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "Agent failed to select and invoke any appropriate tool for the api_integration task; no tools were executed, indicating a wrong tool selection/dec
2025-08-31 17:09:48,581 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:09:48,583 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "No API integration tool was selected or invoked for the task (no HTTP client/API tool used). The agent failed to choose an appropriate tool requir
2025-08-31 17:09:58,699 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:09:58,700 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "sequence_order_errors",
  "reason": "The agent did not perform any steps or execute any tools, effectively failing to follow any workflow sequence. With no actions taken, there is no 
2025-08-31 17:10:04,384 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:10:04,385 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "System-level unknown error prevented any tool usage or workflow decisions; there is no evidence of a wrong agent choice (tool selection, parameters, sequen
2025-08-31 17:10:11,984 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:10:11,985 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "There is no evidence of a concrete agent decision error (no tool selection, parameter config, sequencing, or dependency issue documented). Executed Tools a
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_selection_errors' -> 'tool_selection_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_selection_errors -> tool_selection_errors
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.05s
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 17)
[AI-CLASSIFY-EXISTING] Using existing AI classification: tool_call_format_errors
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 18)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct data_pipeline: other_errors (confidence: 0.72) - Insufficient task context and absence of tool exec
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 23)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[FUZZY_MATCH] 'tool_call_format_errors' -> 'tool_call_format_errors' (score: 1.00)
[TASK-AI-CLASSIFY] Using fuzzy-matched AI classification: tool_call_format_errors -> tool_call_format_errors
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct data_pipeline: sequence_order_errors (confidence: 0.60)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.08s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct simple_task: other_errors (confidence: 0.85) - Unknown system error occurred; there is no evidenc
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 29)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct simple_task: other_errors (confidence: 0.85) - The failure appears to be a non-agent/system-level
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 30)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct api_integration: tool_selection_errors (confidence: 0.72) - The agent did not select or invoke any of the requ
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 29)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct simple_task: tool_selection_errors (confidence: 0.85)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct simple_task: sequence_order_errors (confidence: 0.55)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct api_integration: tool_selection_errors (confidence: 0.66)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.06s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct data_pipeline: other_errors (confidence: 0.85) - No tool usage occurred and the error is reported a
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 25)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct multi_stage_pipeline: tool_selection_errors (confidence: 0.85) - In a multi-stage pipeline with an unknown task, no
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 21)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct simple_task: other_errors (confidence: 0.85) - No evidence of an agent decision error: no tool wa
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 33)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct data_pipeline: tool_selection_errors (confidence: 0.72)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct multi_stage_pipeline: tool_selection_errors (confidence: 0.85)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct simple_task: sequence_order_errors (confidence: 0.62)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.05s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct data_pipeline: tool_selection_errors (confidence: 0.58) - No tools were selected or workflow steps initializ
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 27)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct data_pipeline: tool_selection_errors (confidence: 0.58) - The task reported an Unknown error with no tools e
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 28)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct multi_stage_pipeline: other_errors (confidence: 0.78) - No agent decision error detected: no tools were se
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 23)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct data_pipeline: tool_selection_errors (confidence: 0.85)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct data_pipeline: sequence_order_errors (confidence: 0.60)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct multi_stage_pipeline: tool_selection_errors (confidence: 0.70)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.06s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct simple_task: tool_selection_errors (confidence: 0.62) - The agent did not select or execute any tool becau
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 35)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct basic_task: other_errors (confidence: 0.60) - No agent decision was made: no tools were selected
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 27)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct data_pipeline: other_errors (confidence: 0.60) - Insufficient information to identify a specific ag
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 31)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct simple_task: tool_selection_errors (confidence: 0.72)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct basic_task: sequence_order_errors (confidence: 0.65)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct data_pipeline: tool_selection_errors (confidence: 0.85)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.06s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct simple_task: tool_selection_errors (confidence: 0.72) - No tool was selected or executed for the (unknown)
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 37)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct basic_task: other_errors (confidence: 0.85) - There is no evidence of any agent-level decision e
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 29)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct api_integration: other_errors (confidence: 0.85) - There is no evidence of a concrete agent decision 
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 31)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct simple_task: other_errors (confidence: 0.78)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct basic_task: sequence_order_errors (confidence: 0.60)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct api_integration: tool_selection_errors (confidence: 0.62)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.05s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct api_integration: tool_selection_errors (confidence: 0.60) - No tool was selected or engaged for the api_integr
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 33)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct simple_task: other_errors (confidence: 0.62) - No agent-level decision or tool usage evidence to 
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 39)2025-08-31 17:10:19,163 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:10:19,164 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No tool was selected or executed, and there is no evidence of a decision-level mistake. The failure is described as an unknown error rather than a misselec
2025-08-31 17:10:24,341 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:10:24,341 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The agent did not select or invoke any required tool for the data_pipeline task, effectively failing to start the workflow. This indicates a misst
2025-08-31 17:10:35,030 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:10:35,031 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "sequence_order_errors",
  "reason": "No actions were executed for the basic_task, effectively ignoring the expected workflow and steps. This indicates a misdecision about the required
2025-08-31 17:10:39,479 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:10:39,479 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The agent did not select or configure any tools to perform the data_pipeline task (no tools executed). This represents an incorrect tooling decisi
2025-08-31 17:10:45,030 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:10:45,030 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "Insufficient information to attribute failure to a specific agent decision (no tools executed; error reported as Unknown). This appears external/systemic r
2025-08-31 17:10:52,110 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:10:52,110 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The task involves API integration but the agent did not select or execute any appropriate API tooling (or selected none/irrelevant tooling), leadi
2025-08-31 17:11:02,414 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:11:02,414 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No agent-level decision can be identified: the task had no required tools, and there is an unknown system-level error before any tool selection or executio
2025-08-31 17:11:06,175 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:11:06,175 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The agent did not select or activate the required data_pipeline tools (e.g., data_loader/reader components). With 0% tool coverage and no tools ex
2025-08-31 17:11:12,147 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:11:12,147 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The agent did not select or initialize any required API tooling for the integration task; no tools were executed, indicating a failure at the tool
2025-08-31 17:11:17,334 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:11:17,334 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "Agent failed to select and deploy any of the required tools for the api_integration task, resulting in 0% tool coverage and complete failure. This
2025-08-31 17:11:27,150 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:11:27,151 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "There is an unknown error with no executed tools or observable agent decisions to evaluate (no tool_selection_errors, parameter_config_errors, sequence_ord
2025-08-31 17:11:32,593 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:11:32,594 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "There is no evidence of any agent decision error (no tool was selected or executed). The failure message 'Unknown error' points to a system-level/unknown f
2025-08-31 17:11:38,010 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:11:38,010 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "Insufficient information to identify a specific agent decision error. The task context provides no details about which tools were chosen, parameters used, 
2025-08-31 17:11:42,883 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:11:42,884 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The agent failed to select or initialize any tools for the multi-stage pipeline; no tools were invoked and no workflow steps were defined, indicat
2025-08-31 17:11:47,452 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:11:47,452 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "Agent did not select or initialize any API integration tool/tools required for the task, effectively choosing no tool for an api_integration task,
2025-08-31 17:11:52,216 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:11:52,217 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "No tools were selected or invoked for the data_pipeline task; there is no executed tool, implying a wrong or missing tool choice at the decision s
2025-08-31 17:11:58,079 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:11:58,080 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "Agent failed to select or initialize any tool (no tools executed) because the task is reported as 'Unknown task'. This indicates a tool-selection/
2025-08-31 17:12:03,078 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:12:03,078 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No demonstrable agent decision error detected: the task shows an unknown/system-level failure with no evidence of incorrect tool selection, misconfigured p
2025-08-31 17:12:08,863 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:12:08,864 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No agent decision evidence; the task ended with an unknown error before any tool usage or workflow could be executed. This appears to be an external/unknow
2025-08-31 17:12:18,001 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:12:18,001 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "sequence_order_errors",
  "reason": "Agent did not perform any actions or follow any task sequence for the basic_task (no steps executed), effectively failing to initiate the required
2025-08-31 17:12:25,340 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:12:25,341 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "Agent did not select or initialize any tool or define a workflow for the api_integration task; no tools executed, indicating a poor decision/plan 
2025-08-31 17:12:30,664 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:12:30,664 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "sequence_order_errors",
  "reason": "The agent did not initiate or follow the required API integration workflow at all: no tools were invoked and no steps were executed, effectively b
2025-08-31 17:12:36,920 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:12:36,921 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "Insufficient task/context information to attribute the failure to any specific agent decision. No tools were selected or executed and the error message pro
2025-08-31 17:12:43,953 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:12:43,953 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "There is no evidence of an agent decision error (tool selection, parameter config, sequence order, or dependency handling). The error message is Unknown er
2025-08-31 17:12:52,245 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:12:52,246 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "No tools were selected or executed (Required Tools: 0; Executed Tools: 0) and the error message is generic 'Unknown error' with no evidence of a w
2025-08-31 17:12:56,652 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:12:56,652 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "No tool(s) were selected or used to perform the required basic_task. The agent did not choose an appropriate tool (or any tool) to complete the ta
2025-08-31 17:13:01,021 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:13:01,022 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "sequence_order_errors",
  "reason": "The agent did not perform any steps and did not follow the required data_pipeline sequence (no tools were executed, 0/0 coverage). This indicates 
2025-08-31 17:13:10,771 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:13:10,772 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "sequence_order_errors",
  "reason": "There is an implied execution sequence for completing the simple_task, but no tools were executed and no result was produced. The failure appears 
2025-08-31 17:13:16,389 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:13:16,389 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "The agent did not select or initialize any tool to progress the (multi-stage) task; no tools were executed and no plan was formed, indicating a wr
2025-08-31 17:13:22,124 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:13:22,124 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No tool usage or agent decision evidence available; the error message is Unknown, suggesting a system-level or unknown fault rather than a misstep in tool 
2025-08-31 17:13:31,995 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:13:31,996 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "tool_selection_errors",
  "reason": "No tools were required or executed for this task (coverage 0/0). With no tool choices to evaluate, there is insufficient evidence of a wrong tool 
2025-08-31 17:13:36,865 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:13:36,866 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "dependency_errors",
  "reason": "The agent did not establish or respect the required prerequisites/dependencies for the multi_stage_pipeline (no tools were invoked, implying prerequis
2025-08-31 17:13:44,151 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:13:44,152 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "sequence_order_errors",
  "reason": "Agent did not initiate or follow the required action sequence; no tools were invoked and no steps were executed, indicating a failure to start/use
2025-08-31 17:13:50,020 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:13:50,020 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "sequence_order_errors",
  "reason": "The agent did not perform any steps or establish a workflow for the basic_task, effectively yielding an undefined or empty sequence. There was no 
2025-08-31 17:13:54,644 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:13:54,645 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "No tools were selected or executed and the task details are unknown ('Unknown error'), so there is no identifiable agent decision (tool choice, parameters,
2025-08-31 17:14:00,073 - httpx - INFO - HTTP Request: POST https://85409-me3ofvov-eastus2.cognitiveservices.azure.com/openai/deployments/gpt-5-nano/chat/completions?api-version=2024-12-01-preview "HTTP/1.1 200 OK"
2025-08-31 17:14:00,073 - focused_ai_classifier - INFO - Raw AI response (first 200 chars): {
  "category": "other_errors",
  "reason": "There is insufficient evidence of any agent decision-making (no tools used, no task details). The error appears to be a system/unknown failure rather than 
2025-08-31 17:14:00,074 - result_merger - INFO - æ¨¡å‹qwen2.5-72b-instructä¿å­˜119/119æ¡è®°å½•
2025-08-31 17:14:00,075 - result_merger - INFO - åˆå¹¶å®Œæˆï¼Œå…±å¤„ç†90ä¸ªæ–‡ä»¶ï¼Œä¿å­˜119æ¡è®°å½•
2025-08-31 17:14:00,077 - batch_test_runner - INFO - âœ… å­˜å‚¨é€‚é…å™¨èµ„æºæ¸…ç†å®Œæˆ
2025-08-31 17:14:00,081 - batch_test_runner - INFO - ğŸ”š å­è¿›ç¨‹æµ‹è¯•å®Œæˆï¼Œä¸»åŠ¨é€€å‡º

[AI-CLASSIFY-NEW] qwen2.5-72b-instruct data_pipeline: other_errors (confidence: 0.65) - No tools were selected or executed and the error i
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 33)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct api_integration: sequence_order_errors (confidence: 0.72)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct simple_task: tool_selection_errors (confidence: 0.55)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct data_pipeline: dependency_errors (confidence: 0.65)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct data_pipeline: other_errors (confidence: 0.85) - No agent decision error detected: the task reporte
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 35)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct basic_task: other_errors (confidence: 0.72) - No detectable agent decision error: no tool select
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 31)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct data_pipeline: other_errors (confidence: 0.62) - No tools were executed and there is no observable 
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 36)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct data_pipeline: dependency_errors (confidence: 0.85)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct basic_task: sequence_order_errors (confidence: 0.55)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct data_pipeline: tool_selection_errors (confidence: 0.72)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.06s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct data_pipeline: other_errors (confidence: 0.85) - No agent decision errors were identifiable. The fa
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 39)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct simple_task: other_errors (confidence: 0.75) - No tools were selected or executed and the error m
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 41)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct api_integration: other_errors (confidence: 0.60) - No tool usage data and the error reported is 'Unkn
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 35)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct data_pipeline: other_errors (confidence: 0.60)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct simple_task: sequence_order_errors (confidence: 0.85)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct api_integration: tool_selection_errors (confidence: 0.68)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.05s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct basic_task: tool_selection_errors (confidence: 0.85) - The task reported 0 required tools (0/0), meaning 
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 33)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct multi_stage_pipeline: other_errors (confidence: 0.85) - No agent decision can be identified: no tools were
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 25)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct api_integration: tool_selection_errors (confidence: 0.85) - The agent did not select or initiate any tool for 
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 37)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct basic_task: tool_selection_errors (confidence: 0.85)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct multi_stage_pipeline: tool_selection_errors (confidence: 0.85)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct api_integration: tool_selection_errors (confidence: 0.70)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.08s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct simple_task: other_errors (confidence: 0.85) - There is no evidence of any agent decision (no too
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 43)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct basic_task: other_errors (confidence: 0.72) - No agent misdecision detected: there were no tools
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 35)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct data_pipeline: dependency_errors (confidence: 0.65) - The task context is unknown/undefined and no tools
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 41)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct simple_task: other_errors (confidence: 0.85)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct basic_task: other_errors (confidence: 0.65)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct data_pipeline: other_errors (confidence: 0.62)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.05s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct simple_task: other_errors (confidence: 0.65) - No tool usage or agent decision details are availa
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 45)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct basic_task: other_errors (confidence: 0.55) - No evidence of an agent decision error: there were
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 37)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct simple_task: tool_selection_errors (confidence: 0.85) - No tool was selected or executed for the task (0% 
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 46)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct simple_task: other_errors (confidence: 0.62)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct basic_task: other_errors (confidence: 0.85)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct simple_task: sequence_order_errors (confidence: 0.65)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.06s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct api_integration: other_errors (confidence: 0.72) - There is no evidence of a wrong agent decision (to
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 39)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct api_integration: other_errors (confidence: 0.82) - There were no tools selected or executed, and the 
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 40)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct simple_task: other_errors (confidence: 0.70) - Insufficient information to determine a specific a
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 49)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct api_integration: tool_selection_errors (confidence: 0.72)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct api_integration: tool_selection_errors (confidence: 0.60)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct simple_task: sequence_order_errors (confidence: 0.85)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct data_pipeline: other_errors (confidence: 0.72) - System-level unknown error prevented any tool usag
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 43)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct basic_task: other_errors (confidence: 0.72) - There is no evidence of a concrete agent decision 
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 39)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct data_pipeline: other_errors (confidence: 0.65) - No tool was selected or executed, and there is no 
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 44)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct data_pipeline: tool_selection_errors (confidence: 0.64)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct basic_task: sequence_order_errors (confidence: 0.65)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct data_pipeline: tool_selection_errors (confidence: 0.85)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct data_pipeline: other_errors (confidence: 0.62) - Insufficient information to attribute failure to a
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 47)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct api_integration: tool_selection_errors (confidence: 0.62) - The task involves API integration but the agent di
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 43)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct api_integration: other_errors (confidence: 0.72) - No agent-level decision can be identified: the tas
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 44)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct data_pipeline: tool_selection_errors (confidence: 0.62)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct api_integration: tool_selection_errors (confidence: 0.55)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct api_integration: tool_selection_errors (confidence: 0.60)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct multi_stage_pipeline: other_errors (confidence: 0.65) - There is an unknown error with no executed tools o
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 27)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct api_integration: other_errors (confidence: 0.85) - There is no evidence of any agent decision error (
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 47)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct data_pipeline: other_errors (confidence: 0.65) - Insufficient information to identify a specific ag
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 49)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct multi_stage_pipeline: tool_selection_errors (confidence: 0.68)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct api_integration: tool_selection_errors (confidence: 0.66)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct data_pipeline: tool_selection_errors (confidence: 0.75)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct basic_task: tool_selection_errors (confidence: 0.85) - Agent failed to select or initialize any tool (no 
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 41)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct api_integration: other_errors (confidence: 0.62) - No demonstrable agent decision error detected: the
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 49)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct api_integration: other_errors (confidence: 0.85) - No agent decision evidence; the task ended with an
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> api_integration (total: 50)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct basic_task: sequence_order_errors (confidence: 0.62)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct api_integration: tool_selection_errors (confidence: 0.72)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct api_integration: sequence_order_errors (confidence: 0.72)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct basic_task: other_errors (confidence: 0.60) - Insufficient task/context information to attribute
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 43)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct data_pipeline: other_errors (confidence: 0.72) - There is no evidence of an agent decision error (t
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> data_pipeline (total: 51)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct simple_task: tool_selection_errors (confidence: 0.55) - No tools were selected or executed (Required Tools
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> simple_task (total: 51)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct basic_task: tool_selection_errors (confidence: 0.75)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct data_pipeline: sequence_order_errors (confidence: 0.85)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct simple_task: sequence_order_errors (confidence: 0.55)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct multi_stage_pipeline: tool_selection_errors (confidence: 0.72) - The agent did not select or initialize any tool to
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> multi_stage_pipeline (total: 29)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct basic_task: other_errors (confidence: 0.65) - No tool usage or agent decision evidence available
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 45)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct basic_task: tool_selection_errors (confidence: 0.60) - No tools were required or executed for this task (
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 46)
[INFO] Buffer full (3 records), triggering flush...
[INFO] Flushing 3 records to database...
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct multi_stage_pipeline: dependency_errors (confidence: 0.70)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct basic_task: sequence_order_errors (confidence: 0.75)
[AI-CLASSIFY-TASK] qwen2.5-72b-instruct basic_task: sequence_order_errors (confidence: 0.66)
[INFO] All records processed, updating error metrics...
[INFO] Saving database...
[INFO] Database saved successfully in 0.04s
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct basic_task: other_errors (confidence: 0.62) - No tools were selected or executed and the task de
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 49)
[AI-CLASSIFY-NEW] qwen2.5-72b-instruct basic_task: other_errors (confidence: 0.60) - There is insufficient evidence of any agent decisi
[V3_UPDATE] æ›´æ–°ç»Ÿè®¡å®Œæˆ: qwen2.5-72b-instruct -> baseline -> basic_task (total: 50)
[INFO] æœ€ç»ˆåˆå¹¶å®Œæˆ: 90 ä¸ªæ–‡ä»¶
2025-08-31 17:14:00,200 - smart_result_collector - INFO - SmartResultCollector æ­£åœ¨å…³é—­...
2025-08-31 17:14:00,201 - smart_result_collector - INFO - SmartResultCollector å·²å…³é—­
INFO:__main__:âœ… åˆ†ç‰‡3å®Œæˆ
INFO:__main__:ğŸ“Š å¹¶å‘æ‰§è¡Œç»“æœ: 3/3 åˆ†ç‰‡æˆåŠŸ
INFO:__main__:âœ… Key0: å®Œæˆ qwen2.5-72b-instruct-easy
INFO:__main__:æœ€ç»ˆåˆ©ç”¨ç‡: 1.1%
=== æµ‹è¯•ç»“æŸæ—¶é—´: 2025å¹´ 8æœˆ31æ—¥ æ˜ŸæœŸæ—¥ 17æ—¶14åˆ†02ç§’ EDT ===
=== æµ‹è¯•ç”¨æ—¶: 1550ç§’ ===
