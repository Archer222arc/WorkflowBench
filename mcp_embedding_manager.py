#!/usr/bin/env python3
"""
MCP Tool Embedding Manager
==========================
ç‹¬ç«‹çš„å·¥å…·åµŒå…¥ç®¡ç†ç³»ç»Ÿï¼Œç”¨äºåŠ¨æ€æ£€ç´¢MCPå·¥å…·è§„èŒƒ

Features:
- å¤šç»´åº¦å·¥å…·åµŒå…¥ï¼ˆæè¿°ã€å‚æ•°ã€åŠŸèƒ½ï¼‰
- FAISSå‘é‡ç´¢å¼•
- æ™ºèƒ½ç›¸ä¼¼åº¦æœç´¢
- æ‰¹é‡å¤„ç†å’Œç¼“å­˜
- å‘½ä»¤è¡Œæ¥å£
"""

import json
import numpy as np
from typing import Dict, List, Any, Optional, Tuple, Union
from pathlib import Path
from dataclasses import dataclass, field, asdict
from collections import OrderedDict
import pickle
import logging
from datetime import datetime
import argparse
import os
from tqdm import tqdm
import hashlib
import threading
from api_client_manager import get_api_client, get_embedding_model


# Optional imports with fallback
import openai
HAS_OPENAI = True


try:
    import faiss
    HAS_FAISS = True
except ImportError:
    faiss = None
    HAS_FAISS = False


# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)



# æ–‡ä»¶ï¼šmcp_embedding_manager.py
# ä½ç½®ï¼šç¬¬48-150è¡Œ
# ç±»ï¼šPermanentSmartCacheï¼ˆæ”¹è¿›ç‰ˆï¼‰

class PermanentSmartCache:
    """æ°¸ä¹…æ™ºèƒ½ç¼“å­˜ç®¡ç†å™¨ - æ— è¿‡æœŸæœºåˆ¶ï¼Œä¿®å¤äº†è¿­ä»£é”™è¯¯"""
    
    def __init__(self, max_size: int = 1000, similarity_threshold: float = 0.95):
        self.max_size = max_size
        self.similarity_threshold = similarity_threshold
        self.cache = OrderedDict()
        self.embeddings = {}  # å­˜å‚¨æŸ¥è¯¢çš„embeddingç”¨äºç›¸ä¼¼åº¦æ¯”è¾ƒ
        self.access_counts = {}  # è®°å½•è®¿é—®é¢‘ç‡
        self._lock = threading.RLock()  # æ·»åŠ çº¿ç¨‹é”ä»¥é˜²å¹¶å‘é—®é¢˜
        
    def get(self, key: str, query_embedding: np.ndarray = None) -> Any:
        """æ™ºèƒ½è·å–ç¼“å­˜ï¼Œæ”¯æŒç›¸ä¼¼åº¦åŒ¹é…"""
        with self._lock:
            # 1. ç²¾ç¡®åŒ¹é…
            if key in self.cache:
                # æ›´æ–°è®¿é—®è®¡æ•°å’ŒLRUé¡ºåº
                self.access_counts[key] = self.access_counts.get(key, 0) + 1
                self.cache.move_to_end(key)
                return self.cache[key]
            
            # 2. ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆå¦‚æœæä¾›äº†embeddingï¼‰
            if query_embedding is not None:
                similar_key = self._find_similar_key(query_embedding)
                if similar_key:
                    logger.debug(f"[Cache] Similar match: '{key}' â†’ '{similar_key}'")
                    self.access_counts[similar_key] = self.access_counts.get(similar_key, 0) + 1
                    self.cache.move_to_end(similar_key)
                    return self.cache[similar_key]
            
            return None
    
    def put(self, key: str, value: Any, query_embedding: np.ndarray = None):
        """æ™ºèƒ½å­˜å‚¨ç¼“å­˜"""
        with self._lock:
            # æ£€æŸ¥ç›¸ä¼¼æ¡ç›®
            if query_embedding is not None:
                similar_key = self._find_similar_key(query_embedding)
                if similar_key:
                    logger.debug(f"[Cache] Merging similar: '{key}' â†’ '{similar_key}'")
                    # æ›´æ–°ç°æœ‰æ¡ç›®
                    self.cache[similar_key] = value
                    self.access_counts[similar_key] = self.access_counts.get(similar_key, 0) + 1
                    return
            
            # ç¼“å­˜å¤§å°ç®¡ç†
            while len(self.cache) >= self.max_size:
                self._evict_lfu()  # ä½¿ç”¨LFUï¼ˆæœ€å°‘é¢‘ç‡ä½¿ç”¨ï¼‰ç­–ç•¥
            
            # å­˜å‚¨æ–°æ¡ç›®
            self.cache[key] = value
            if query_embedding is not None:
                self.embeddings[key] = query_embedding
            self.access_counts[key] = 1
    
    def _find_similar_key(self, query_embedding: np.ndarray) -> Optional[str]:
        """æŸ¥æ‰¾ç›¸ä¼¼çš„ç¼“å­˜é”®"""
        max_similarity = 0
        similar_key = None
        
        # ä½¿ç”¨åˆ—è¡¨å‰¯æœ¬é¿å…è¿­ä»£æ—¶ä¿®æ”¹çš„é—®é¢˜
        for key in list(self.embeddings.keys()):
            if key not in self.cache:
                # æ¸…ç†è¿‡æœŸçš„embedding
                self.embeddings.pop(key, None)
                continue
                
            embedding = self.embeddings[key]
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            try:
                similarity = np.dot(query_embedding, embedding) / (
                    np.linalg.norm(query_embedding) * np.linalg.norm(embedding)
                )
                
                if similarity > self.similarity_threshold and similarity > max_similarity:
                    max_similarity = similarity
                    similar_key = key
            except Exception as e:
                logger.debug(f"Error calculating similarity for key {key}: {e}")
                continue
        
        return similar_key
    
    def _evict_lfu(self):
        """LFUæ·˜æ±°ç­–ç•¥ - ç§»é™¤è®¿é—®é¢‘ç‡æœ€ä½çš„æ¡ç›®"""
        if not self.cache:
            return
            
        # æ‰¾å‡ºè®¿é—®é¢‘ç‡æœ€ä½çš„é”®
        min_count = min(self.access_counts.values())
        
        # æ‰¾åˆ°æ‰€æœ‰æœ€ä½é¢‘ç‡çš„é”®
        lfu_candidates = [(k, self.access_counts[k]) for k in self.cache.keys() 
                          if self.access_counts.get(k, 0) == min_count]
        
        if lfu_candidates:
            # é€‰æ‹©æœ€æ—§çš„ï¼ˆç¬¬ä¸€ä¸ªï¼‰
            key_to_remove = lfu_candidates[0][0]
            
            # åˆ é™¤æ¡ç›®
            self.cache.pop(key_to_remove, None)
            self.embeddings.pop(key_to_remove, None)
            self.access_counts.pop(key_to_remove, None)
            logger.debug(f"[Cache] Evicted LFU entry: {key_to_remove} (count: {min_count})")
    
    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        with self._lock:
            self.cache.clear()
            self.embeddings.clear()
            self.access_counts.clear()
            logger.info("[Cache] Cleared all entries")
    
    def save_to_file(self, filepath: str):
        """ä¿å­˜ç¼“å­˜åˆ°æ–‡ä»¶"""
        with self._lock:
            save_data = {
                'cache': dict(self.cache),
                'embeddings': self.embeddings,
                'access_counts': self.access_counts
            }
            try:
                with open(filepath, 'wb') as f:
                    pickle.dump(save_data, f)
                logger.info(f"[Cache] Saved {len(self.cache)} entries to {filepath}")
            except Exception as e:
                logger.error(f"Failed to save cache: {e}")
    
    def load_from_file(self, filepath: str):
        """ä»æ–‡ä»¶åŠ è½½ç¼“å­˜"""
        with self._lock:
            try:
                with open(filepath, 'rb') as f:
                    save_data = pickle.load(f)
                
                self.cache = OrderedDict(save_data.get('cache', {}))
                self.embeddings = save_data.get('embeddings', {})
                self.access_counts = save_data.get('access_counts', {})
                
                logger.info(f"[Cache] Loaded {len(self.cache)} entries from file")
            except Exception as e:
                logger.warning(f"Failed to load cache: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        with self._lock:
            return {
                'size': len(self.cache),
                'max_size': self.max_size,
                'embeddings_count': len(self.embeddings),
                'total_accesses': sum(self.access_counts.values()),
                'average_access_count': sum(self.access_counts.values()) / len(self.access_counts) if self.access_counts else 0
            }
    
    def __len__(self):
        """è¿”å›ç¼“å­˜å¤§å°"""
        return len(self.cache)


# ===========================
# Data Classes
# ===========================

@dataclass
class ToolEmbedding:
    """å·¥å…·çš„å‘é‡è¡¨ç¤º"""
    tool_name: str
    category: str
    description: str
    description_embedding: np.ndarray
    parameter_embedding: np.ndarray
    functionality_embedding: np.ndarray
    combined_embedding: np.ndarray
    mcp_protocol: Dict[str, Any]
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸"""
        return {
            'tool_name': self.tool_name,
            'category': self.category,
            'description': self.description,
            'description_embedding': self.description_embedding.tolist(),
            'parameter_embedding': self.parameter_embedding.tolist(),
            'functionality_embedding': self.functionality_embedding.tolist(),
            'combined_embedding': self.combined_embedding.tolist(),
            'mcp_protocol': self.mcp_protocol,
            'metadata': self.metadata
        }
    
    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'ToolEmbedding':
        """ä»å­—å…¸åˆ›å»ºå®ä¾‹"""
        return cls(
            tool_name=data['tool_name'],
            category=data['category'],
            description=data['description'],
            description_embedding=np.array(data['description_embedding']),
            parameter_embedding=np.array(data['parameter_embedding']),
            functionality_embedding=np.array(data['functionality_embedding']),
            combined_embedding=np.array(data['combined_embedding']),
            mcp_protocol=data['mcp_protocol'],
            metadata=data.get('metadata', {})
        )


@dataclass
class EmbeddingConfig:
    """åµŒå…¥é…ç½®"""
    model: str = "text-embedding-3-large"  # <- ä¿®æ”¹ä¸ºä¾¿å®œçš„æ¨¡å‹
    dimension: int = 3072  # <- text-embedding-3-large ä¹Ÿæ˜¯ 1536 ç»´
    description_weight: float = 0.4
    parameter_weight: float = 0.3
    functionality_weight: float = 0.3
    batch_size: int = 100
    cache_dir: str = ".mcp_embedding_cache"

@dataclass
class SearchResult:
    """æœç´¢ç»“æœ"""
    tool_name: str
    score: float
    mcp_protocol: Dict[str, Any]
    category: str
    reason: str = ""


# ===========================
# Main Embedding Manager
# ===========================

class MCPEmbeddingManager:
    _instance = None
    _initialized = False
    _lock = threading.Lock()
    
    def __new__(cls, *args, **kwargs):
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self, config: Optional[EmbeddingConfig] = None, api_key: Optional[str] = None):
        """åˆå§‹åŒ–åµŒå…¥ç®¡ç†å™¨"""

        if MCPEmbeddingManager._initialized:
            return
            
        with MCPEmbeddingManager._lock:
            if MCPEmbeddingManager._initialized:
                return
            
            self.config = config or EmbeddingConfig()
            
            # ä½¿ç”¨ç»Ÿä¸€çš„ API å®¢æˆ·ç«¯ç®¡ç†å™¨
            from api_client_manager import get_api_client, get_embedding_model
            
            print("[MCPEmbeddingManager] Initializing with unified API client manager")
            
            # è·å– OpenAI/Azure OpenAI å®¢æˆ·ç«¯
            self.client = get_api_client()
            
            # è·å–å¹¶æ›´æ–°åµŒå…¥æ¨¡å‹åç§°
            embedding_model = get_embedding_model()
            self.config.model = embedding_model
            print(f"[MCPEmbeddingManager] Using embedding model: {embedding_model}")
            
            # Azure OpenAI çš„åµŒå…¥æ¨¡å‹é€šå¸¸æ˜¯ 1536 ç»´
            if embedding_model == "text-embedding-3-large":
                self.config.dimension = 3072
            else:
                self.config.dimension = 1536
            
            print(f"[MCPEmbeddingManager] Client initialized successfully")
            
            self.tool_embeddings: Dict[str, ToolEmbedding] = {}
            self.tool_names: List[str] = []
            self.index = None
            
            # Caches - ä¿®æ­£åˆå§‹åŒ–é¡ºåº
            if self.config.cache_dir is None:
                self.cache_dir = Path(".mcp_embedding_cache")
            else:
                self.cache_dir = Path(self.config.cache_dir)
            self.cache_dir.mkdir(exist_ok=True)
            
            # å…ˆå®šä¹‰ç¼“å­˜æ–‡ä»¶è·¯å¾„
            self.embedding_cache_file = self.cache_dir / "embedding_cache.pkl"  # <- æ·»åŠ è¿™è¡Œ
            self.search_cache_file = self.cache_dir / "search_cache.pkl"
            
            # ç„¶ååŠ è½½ç¼“å­˜
            self.embedding_cache = self._load_embedding_cache()
            self.search_cache = {}
            self._load_search_cache()
            
            # åˆå§‹åŒ–æ“ä½œæ˜ å°„ï¼ˆç”¨äºæŸ¥è¯¢è§„èŒƒåŒ–ï¼‰
            self.synonym_to_canonical = {}
            self._initialize_operation_mappings()

            self.search_cache = PermanentSmartCache(
                max_size=10000,  # æœç´¢ç¼“å­˜
                similarity_threshold=0.95
            )
            
            # Embeddingç¼“å­˜ - å®Œå…¨åŸºäºæ–‡ä»¶çš„æ°¸ä¹…å­˜å‚¨
            self.embedding_cache_file = self.cache_dir / "embedding_cache.pkl"
            self.embedding_cache = self._load_embedding_cache()
            
            # åŠ è½½æœç´¢ç¼“å­˜
            self.search_cache_file = self.cache_dir / "search_cache.pkl"
            if self.search_cache_file.exists():
                self.search_cache.load_from_file(str(self.search_cache_file))
            
            # æ‰¹é‡é¢„è®¡ç®—ç®¡ç†
            self.batch_queue = []
            self.batch_size = 20  # Azure OpenAIæ‰¹é‡å¤§å°é™åˆ¶
            self.last_batch_time = 0
            self.batch_interval = 1.0  # æ‰¹é‡è¯·æ±‚é—´éš”

            MCPEmbeddingManager._initialized = True

    
    def _load_config_file(self) -> Dict[str, Any]:
        """ä¸å†éœ€è¦è¿™ä¸ªæ–¹æ³•ï¼Œç”± APIClientManager ç»Ÿä¸€å¤„ç†"""
        # ä¿ç•™æ–¹æ³•ä»¥ä¿æŒå‘åå…¼å®¹ï¼Œä½†å®é™…ä¸Šä¸ä½¿ç”¨
        return {}
    
    def _build_faiss_index(self):
        """æ„å»ºFAISSç´¢å¼•ä»å½“å‰çš„tool_embeddings"""
        if not HAS_FAISS:
            logger.warning("FAISS not available, skipping index build")
            return
            
        if not self.tool_embeddings:
            logger.warning("No tool embeddings to index")
            return
            
        # æå–æ‰€æœ‰åµŒå…¥å‘é‡
        embeddings = []
        tool_names = []
        
        for tool_name, tool_emb in self.tool_embeddings.items():
            embeddings.append(tool_emb.combined_embedding)
            tool_names.append(tool_name)
        
        # åˆ›å»ºFAISSç´¢å¼•
        embeddings_matrix = np.vstack(embeddings).astype('float32')
        self.index = faiss.IndexFlatIP(self.config.dimension)
        
        # å½’ä¸€åŒ–å‘é‡ï¼ˆç”¨äºä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        faiss.normalize_L2(embeddings_matrix)
        self.index.add(embeddings_matrix)
        
        self.tool_names = tool_names
        logger.info(f"Built FAISS index with {len(tool_names)} tools")
    

    def _load_embedding_cache(self) -> Dict[str, np.ndarray]:
        """åŠ è½½embeddingç¼“å­˜ï¼Œå¢å¼ºé”™è¯¯å¤„ç†"""
        cache_file = Path(self.config.cache_dir) / "embedding_cache.pkl"
        
        if not cache_file.exists():
            logger.info("No embedding cache found, starting fresh")
            return {}
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = cache_file.stat().st_size
        if file_size == 0:
            logger.warning(f"Cache file {cache_file} is empty, removing and starting fresh")
            cache_file.unlink()  # åˆ é™¤ç©ºæ–‡ä»¶
            return {}
        
        logger.info(f"Loading embedding cache from {cache_file} (size: {file_size} bytes)")
        
        try:
            with open(cache_file, 'rb') as f:
                cache = pickle.load(f)
                
            # éªŒè¯ç¼“å­˜å†…å®¹
            if not isinstance(cache, dict):
                logger.warning(f"Invalid cache format (expected dict, got {type(cache)}), starting fresh")
                cache_file.unlink()
                return {}
                
            # éªŒè¯ç¼“å­˜æ¡ç›®
            valid_entries = 0
            for key, value in cache.items():
                if isinstance(value, np.ndarray):
                    valid_entries += 1
            
            logger.info(f"Loaded {valid_entries} cached embeddings")
            return cache
            
        except EOFError:
            logger.error(f"Cache file {cache_file} is corrupted (EOFError), removing and starting fresh")
            # å¤‡ä»½æŸåçš„æ–‡ä»¶ä»¥ä¾›è°ƒè¯•
            backup_path = cache_file.with_suffix('.pkl.corrupted')
            cache_file.rename(backup_path)
            logger.info(f"Corrupted cache backed up to {backup_path}")
            return {}
            
        except (pickle.UnpicklingError, ValueError) as e:
            logger.error(f"Failed to load cache: {e}, removing and starting fresh")
            cache_file.unlink()
            return {}
            
        except Exception as e:
            logger.error(f"Unexpected error loading cache: {e}")
            # ä¸åˆ é™¤æ–‡ä»¶ï¼Œä½†è¿”å›ç©ºç¼“å­˜
            return {}
    
    def _save_embedding_cache(self) -> None:
        """ä¿å­˜embeddingç¼“å­˜ï¼Œå¢åŠ åŸå­å†™å…¥ä¿æŠ¤"""
        if not self.embedding_cache:
            return
            
        cache_dir = Path(self.config.cache_dir)
        cache_dir.mkdir(exist_ok=True)
        cache_file = cache_dir / "embedding_cache.pkl"
        temp_file = cache_dir / "embedding_cache.pkl.tmp"
        
        try:
            # åŸå­å†™å…¥ï¼šå…ˆå†™åˆ°ä¸´æ—¶æ–‡ä»¶ï¼Œç„¶åé‡å‘½å
            logger.info(f"Saving {len(self.embedding_cache)} embeddings to cache")
            
            with open(temp_file, 'wb') as f:
                pickle.dump(self.embedding_cache, f)
            
            # éªŒè¯ä¸´æ—¶æ–‡ä»¶
            if temp_file.stat().st_size == 0:
                logger.error("Failed to write cache (file is empty)")
                temp_file.unlink()
                return
                
            # åŸå­é‡å‘½åï¼ˆåœ¨POSIXç³»ç»Ÿä¸Šæ˜¯åŸå­æ“ä½œï¼‰
            temp_file.replace(cache_file)
            logger.info("Embedding cache saved successfully")
            
        except Exception as e:
            logger.error(f"Failed to save embedding cache: {e}")
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_file.exists():
                temp_file.unlink()
    
    def _get_embedding(self, text: str) -> np.ndarray:
        """è·å–embedding - æ°¸ä¹…ç¼“å­˜"""
        # æ£€æŸ¥æ°¸ä¹…ç¼“å­˜
        text_hash = hashlib.md5(text.encode()).hexdigest()
        if text_hash in self.embedding_cache:
            return self.embedding_cache[text_hash]
        
        # APIè°ƒç”¨è·å–æ–°çš„embedding
        response = self.client.embeddings.create(
            model=self.config.model,
            input=text
        )
        embedding = np.array(response.data[0].embedding)
        
        # æ°¸ä¹…ä¿å­˜åˆ°ç¼“å­˜
        self.embedding_cache[text_hash] = embedding
        
        # æ¯50ä¸ªæ–°embeddingä¿å­˜ä¸€æ¬¡
        if len(self.embedding_cache) % 50 == 0:
            self._save_embedding_cache()
        
        return embedding
    
    def _process_batch_embeddings(self) -> np.ndarray:
        """æ‰¹é‡å¤„ç†embeddingè¯·æ±‚"""
        if not self.batch_queue:
            return None
            
        texts = [item[0] for item in self.batch_queue]
        hashes = [item[1] for item in self.batch_queue]
        
        print(f"[Batch] Processing {len(texts)} embeddings in batch")
        
        # æ‰¹é‡APIè°ƒç”¨
        response = self.client.embeddings.create(
            model=self.config.model,
            input=texts
        )
        
        # å¤„ç†ç»“æœ
        result_embedding = None
        for i, embedding_data in enumerate(response.data):
            embedding = np.array(embedding_data.embedding)
            
            # å­˜å‚¨åˆ°æ™ºèƒ½ç¼“å­˜
            self.smart_embedding_cache.put(
                hashes[i], 
                embedding,
                query_embedding=embedding
            )
            
            # è¿”å›ç¬¬ä¸€ä¸ªï¼ˆå½“å‰è¯·æ±‚çš„ï¼‰
            if i == 0:
                result_embedding = embedding
        
        # æ¸…ç©ºé˜Ÿåˆ—
        self.batch_queue.clear()
        self.last_batch_time = time.time()
        
        return result_embedding
    
    def _get_single_embedding(self, text: str, text_hash: str) -> np.ndarray:
        """è·å–å•ä¸ªembeddingï¼ˆfallbackï¼‰"""
        # é™é€Ÿ
        time.sleep(0.1)  # 100mså»¶è¿Ÿï¼Œé¿å…429é”™è¯¯
        
        response = self.client.embeddings.create(
            model=self.config.model,
            input=text
        )
        embedding = np.array(response.data[0].embedding)
        
        # å­˜å‚¨åˆ°æ™ºèƒ½ç¼“å­˜
        self.smart_embedding_cache.put(
            text_hash,
            embedding,
            query_embedding=embedding
        )
        
        return embedding
    
    def create_tool_embedding(self, tool_name: str, tool_spec: Dict[str, Any]) -> ToolEmbedding:
        """ä¸ºå•ä¸ªå·¥å…·åˆ›å»ºåµŒå…¥ï¼ŒåŒ…å«åŒºåˆ†åº¦ä¿¡æ¯"""
        # 1. æå–æè¿°ï¼ˆä½¿ç”¨å¢å¼ºçš„æè¿°ï¼‰
        description = tool_spec.get('description', '')
        desc_text = f"{tool_name}: {description}"
        desc_emb = self._get_embedding(desc_text)
        
        # 2. æå–å‚æ•°ä¿¡æ¯
        param_text = self._format_parameters(tool_spec.get('parameters', []))
        param_emb = self._get_embedding(param_text)
        
        # 3. æå–åŠŸèƒ½ä¿¡æ¯ï¼ˆåŒ…å«åŒºåˆ†åº¦ï¼‰
        func_text = self._format_functionality(tool_name, tool_spec)
        func_emb = self._get_embedding(func_text)
        
        # 4. ============ æ–°å¢ï¼šåŒºåˆ†åº¦åµŒå…¥ ============
        differentiation = tool_spec.get('differentiation', {})
        if differentiation:
            # åˆ›å»ºåŒºåˆ†åº¦æ–‡æœ¬
            diff_parts = []
            
            if 'unique_purpose' in differentiation:
                diff_parts.append(differentiation['unique_purpose'])
            
            if 'usage_keywords' in differentiation:
                diff_parts.append(f"Use for: {', '.join(differentiation['usage_keywords'])}")
            
            if 'key_differentiators' in differentiation:
                diff_parts.append(f"Features: {'; '.join(differentiation['key_differentiators'])}")
            
            if 'example_scenario' in differentiation:
                diff_parts.append(f"Example: {differentiation['example_scenario']}")
            
            if diff_parts:
                diff_text = f"{tool_name} differentiation: {'; '.join(diff_parts)}"
                diff_emb = self._get_embedding(diff_text)
                
                # è°ƒæ•´æƒé‡ï¼Œç»™åŒºåˆ†åº¦ä¿¡æ¯æ›´é«˜æƒé‡
                combined = (
                    0.25 * desc_emb +      # é™ä½æè¿°æƒé‡
                    0.20 * param_emb +     # é™ä½å‚æ•°æƒé‡
                    0.25 * func_emb +      # é™ä½åŠŸèƒ½æƒé‡
                    0.30 * diff_emb        # æ–°å¢åŒºåˆ†åº¦æƒé‡
                )
            else:
                # æ²¡æœ‰åŒºåˆ†åº¦ä¿¡æ¯æ—¶ä½¿ç”¨åŸæƒé‡
                combined = (
                    self.config.description_weight * desc_emb + 
                    self.config.parameter_weight * param_emb + 
                    self.config.functionality_weight * func_emb
                )
        else:
            # åŸæœ‰é€»è¾‘
            combined = (
                self.config.description_weight * desc_emb + 
                self.config.parameter_weight * param_emb + 
                self.config.functionality_weight * func_emb
            )
        # ============ ç»“æŸæ–°å¢ ============
        
        # å½’ä¸€åŒ–
        combined = combined / np.linalg.norm(combined)
        
        return ToolEmbedding(
            tool_name=tool_name,
            category=tool_spec.get('metadata', {}).get('category', 'general'),
            description=description,
            description_embedding=desc_emb,
            parameter_embedding=param_emb,
            functionality_embedding=func_emb,
            combined_embedding=combined,
            mcp_protocol=tool_spec,
            metadata={
                'created_at': datetime.now().isoformat(),
                'embedding_model': self.config.model,
                'has_differentiation': bool(differentiation)  # æ ‡è®°æ˜¯å¦æœ‰åŒºåˆ†åº¦ä¿¡æ¯
            }
        )
    
    def _format_parameters(self, parameters: List[Dict]) -> str:
        """æ ¼å¼åŒ–å‚æ•°ä¿¡æ¯"""
        if not parameters:
            return "No parameters required"
        
        parts = []
        for param in parameters:
            param_desc = f"{param.get('name', 'unnamed')} ({param.get('type', 'any')})"
            if param.get('description'):
                param_desc += f": {param['description']}"
            if param.get('required', True):
                param_desc += " [required]"
            parts.append(param_desc)
        
        return "Parameters: " + "; ".join(parts)
    
    def _format_functionality(self, tool_name: str, tool_spec: Dict) -> str:
        """æ ¼å¼åŒ–åŠŸèƒ½ä¿¡æ¯"""
        metadata = tool_spec.get('metadata', {})
        parts = []
        
        # åŸºæœ¬ä¿¡æ¯
        parts.append(f"Tool: {tool_name}")
        parts.append(f"Category: {metadata.get('category', 'general')}")
        parts.append(f"Operation: {metadata.get('operation', 'process')}")
        
        # ä¾èµ–å…³ç³»
        dependencies = tool_spec.get('dependencies', [])
        if dependencies:
            parts.append(f"Dependencies: {', '.join(dependencies)}")
        
        # è¿”å›ç±»å‹
        returns = tool_spec.get('returns', [])
        if returns:
            return_types = [r.get('type', 'any') for r in returns]
            parts.append(f"Returns: {', '.join(return_types)}")
        
        # é”™è¯¯å¤„ç†
        errors = tool_spec.get('errors', [])
        if errors:
            error_codes = [e.get('code', 'ERROR') for e in errors]
            parts.append(f"Possible errors: {', '.join(error_codes)}")
        
        return "; ".join(parts)
    
    def build_index(self, 
                tool_registry_path: Union[str, Path],
                force_rebuild: bool = False) -> Dict[str, Any]:
        """
        æ„å»ºå·¥å…·çš„å‘é‡ç´¢å¼•
        
        Args:
            tool_registry_path: å·¥å…·æ³¨å†Œè¡¨è·¯å¾„
            force_rebuild: æ˜¯å¦å¼ºåˆ¶é‡å»ºç´¢å¼•
            
        Returns:
            æ„å»ºç»Ÿè®¡ä¿¡æ¯
        """
        logger.info("Building tool embedding index...")
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡å»º
        index_file = self.cache_dir / "tool_index.pkl"
        if index_file.exists() and not force_rebuild:
            logger.info("Loading existing index...")
            self.load_index(index_file)
            return {'status': 'loaded', 'tools': len(self.tool_embeddings)}
        
        # åŠ è½½å·¥å…·æ³¨å†Œè¡¨
        with open(tool_registry_path, 'r') as f:
            tool_registry = json.load(f)
        
        # æ‰¹é‡åˆ›å»ºåµŒå…¥
        embeddings = []
        tool_names = []
        
        logger.info(f"Creating embeddings for {len(tool_registry)} tools...")
        for tool_name, tool_spec in tqdm(tool_registry.items()):
            tool_emb = self.create_tool_embedding(tool_name, tool_spec)
            self.tool_embeddings[tool_name] = tool_emb
            embeddings.append(tool_emb.combined_embedding)
            tool_names.append(tool_name)

        # æ„å»ºFAISSç´¢å¼•
        if embeddings:  # ç¡®ä¿æœ‰åµŒå…¥å‘é‡
            embeddings_matrix = np.vstack(embeddings).astype('float32')
            
            # åŠ¨æ€è·å–å®é™…çš„åµŒå…¥ç»´åº¦
            actual_dimension = embeddings_matrix.shape[1]
            
            # æ‰“å°è°ƒè¯•ä¿¡æ¯
            print(f"[MCPEmbeddingManager] Expected dimension: {self.config.dimension}")
            print(f"[MCPEmbeddingManager] Actual embedding dimension: {actual_dimension}")
            
            # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œæ›´æ–°é…ç½®
            if actual_dimension != self.config.dimension:
                logger.warning(f"Dimension mismatch: expected {self.config.dimension}, got {actual_dimension}")
                self.config.dimension = actual_dimension
                print(f"[MCPEmbeddingManager] Updated config dimension to: {self.config.dimension}")
            
            # ä½¿ç”¨å®é™…ç»´åº¦åˆ›å»ºFAISSç´¢å¼•
            self.index = faiss.IndexFlatIP(actual_dimension)
            faiss.normalize_L2(embeddings_matrix)
            self.index.add(embeddings_matrix)
        else:
            logger.error("No embeddings created, cannot build index")
            raise ValueError("No embeddings were created from the tool registry")
        
        self.tool_names = tool_names
        
        # ä¿å­˜ç¼“å­˜
        self._save_embedding_cache()
        self.save_index(index_file)
        
        stats = {
            'status': 'built',
            'tools': len(tool_names),
            'categories': len(set(e.category for e in self.tool_embeddings.values())),
            'index_type': 'faiss' if self.index else 'none',
            'embedding_dimension': self.config.dimension  # ç°åœ¨è¿™æ˜¯å®é™…ç»´åº¦
        }
        
        logger.info(f"Index built: {stats}")
        return stats
    def _initialize_operation_mappings(self):
        """ä»operation_embedding_indexåˆå§‹åŒ–æ“ä½œæ˜ å°„"""
        try:
            from operation_embedding_index import get_operation_index
            
            # è·å–æ“ä½œç´¢å¼•å®ä¾‹
            operation_index = get_operation_index()
            
            # æ„å»ºåŒä¹‰è¯æ˜ å°„
            self.synonym_to_canonical = {}
            
            # ä½¿ç”¨operation_definitionsä¸­çš„æ•°æ®
            for canonical, op_def in operation_index.operation_definitions.items():
                # è§„èŒƒè¯æ˜ å°„åˆ°è‡ªå·±
                self.synonym_to_canonical[canonical] = canonical
                
                # åŒä¹‰è¯æ˜ å°„åˆ°è§„èŒƒè¯
                if 'synonyms' in op_def:
                    for synonym in op_def['synonyms']:
                        self.synonym_to_canonical[synonym] = canonical
            
            logger.info(f"Initialized operation mappings with {len(self.synonym_to_canonical)} terms")
            
        except ImportError:
            logger.warning("operation_embedding_index not available, using basic normalization")
            # åŸºæœ¬çš„å¤‡ç”¨æ˜ å°„
            self.synonym_to_canonical = {}

    def _normalize_query(self, query: str) -> str:
        """
        è§„èŒƒåŒ–æŸ¥è¯¢å­—ç¬¦ä¸²ä»¥æé«˜ç¼“å­˜å‘½ä¸­ç‡
        
        Args:
            query: åŸå§‹æŸ¥è¯¢å­—ç¬¦ä¸²
            
        Returns:
            è§„èŒƒåŒ–åçš„æŸ¥è¯¢å­—ç¬¦ä¸²
        """
        # è½¬æ¢ä¸ºå°å†™
        normalized = query.lower()
        
        # æ›¿æ¢åŒä¹‰è¯ä¸ºè§„èŒƒå½¢å¼
        words = normalized.split()
        normalized_words = []
        
        for word in words:
            # æ£€æŸ¥æ˜¯å¦æ˜¯å·²çŸ¥çš„æ“ä½œåŒä¹‰è¯
            if hasattr(self, 'synonym_to_canonical') and word in self.synonym_to_canonical:
                normalized_words.append(self.synonym_to_canonical[word])
            else:
                normalized_words.append(word)
        
        # é‡æ–°ç»„åˆ
        normalized = ' '.join(normalized_words)
        
        # ç§»é™¤å¸¸è§çš„å˜åŒ–æ¨¡å¼
        # ä¾‹å¦‚ï¼š"for task_type_X" -> "for task_type"
        import re
        # æ›¿æ¢å…·ä½“çš„ä»»åŠ¡ç±»å‹ä¸ºé€šç”¨æ ‡è®°
        normalized = re.sub(r'for \w+_integration', 'for TASK_TYPE', normalized)
        normalized = re.sub(r'for \w+_processing', 'for TASK_TYPE', normalized)
        normalized = re.sub(r'for \w+_validation', 'for TASK_TYPE', normalized)
        normalized = re.sub(r'after \w+_\w+', 'after TOOL_NAME', normalized)
        
        # ç§»é™¤å¤šä½™çš„ç©ºæ ¼
        normalized = ' '.join(normalized.split())
        
        return normalized


    def _get_search_cache_key(self, query: str, k: int, filter_category: Optional[str], 
                            filter_tools: Optional[List[str]]) -> str:
        """ç”Ÿæˆæœç´¢ç¼“å­˜é”®"""
        # è§„èŒƒåŒ–æŸ¥è¯¢
        normalized_query = self._normalize_query(query)
        
        # æ„å»ºç¼“å­˜é”®
        key_parts = [
            normalized_query,
            str(k),
            filter_category or 'none',
            ','.join(sorted(filter_tools)) if filter_tools else 'none'
        ]
        
        return '|'.join(key_parts)


    def _load_search_cache(self):
        """åŠ è½½æœç´¢ç»“æœç¼“å­˜"""
        if self.search_cache_file.exists():
            try:
                with open(self.search_cache_file, 'rb') as f:
                    self.search_cache = pickle.load(f)
                logger.info(f"Loaded search cache with {len(self.search_cache)} entries")
            except Exception as e:
                logger.warning(f"Failed to load search cache: {e}")
                self.search_cache = {}
        else:
            self.search_cache = {}


    def _load_search_cache(self):
        """åŠ è½½æœç´¢ç»“æœç¼“å­˜"""
        if self.search_cache_file.exists():
            try:
                with open(self.search_cache_file, 'rb') as f:
                    cache_data = pickle.load(f)
                
                # å…¼å®¹æ€§å¤„ç†ï¼šåˆ¤æ–­åŠ è½½çš„æ˜¯å­—å…¸è¿˜æ˜¯å¯¹è±¡
                if isinstance(cache_data, dict):
                    # å¦‚æœæ˜¯å­—å…¸ï¼Œç›´æ¥ä½¿ç”¨
                    self.search_cache = cache_data
                    logger.info(f"Loaded search cache with {len(self.search_cache)} entries")
                else:
                    # å¦‚æœæ˜¯å…¶ä»–æ ¼å¼ï¼Œåˆ›å»ºæ–°çš„å­—å…¸ç¼“å­˜
                    self.search_cache = {}
                    logger.warning("Incompatible cache format, starting fresh")
                    
            except Exception as e:
                logger.warning(f"Failed to load search cache: {e}")
                self.search_cache = {}
        else:
            self.search_cache = {}

    # æ–‡ä»¶ï¼šmcp_embedding_manager.py
    # ä½ç½®ï¼šç¬¬850-950è¡Œ
    # å‡½æ•°ï¼šsearchï¼ˆä¿®å¤ç‰ˆæœ¬ï¼‰

    def search(self, 
            query: str, 
            k: int = 5,
            category_filter: Optional[str] = None,
            return_scores: bool = True,
            similarity_threshold: float = 0.0) -> List[SearchResult]:
        """
        æœç´¢ç›¸å…³å·¥å…·
        
        Args:
            query: æœç´¢æŸ¥è¯¢
            k: è¿”å›æ•°é‡
            category_filter: ç±»åˆ«è¿‡æ»¤
            return_scores: æ˜¯å¦è¿”å›åˆ†æ•°
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"{query}_{k}_{category_filter}_{similarity_threshold}"
        if hasattr(self.search_cache, 'get'):
            cached = self.search_cache.get(cache_key)
            if cached is not None:
                logger.debug(f"Search cache hit for: {query}")
                return cached
        
        if not self.index or not self.tool_embeddings:
            logger.warning("No index or embeddings available")
            return []
        
        # ç¡®ä¿ tool_names å’Œ tool_embeddings åŒæ­¥
        if not hasattr(self, 'tool_names') or len(self.tool_names) == 0:
            logger.warning("tool_names not initialized, rebuilding from embeddings")
            self.tool_names = list(self.tool_embeddings.keys())
        
        # è·å–æŸ¥è¯¢çš„åµŒå…¥
        query_embedding = self._get_embedding(query)
        query_embedding = query_embedding.reshape(1, -1).astype('float32')
        
        # å½’ä¸€åŒ–
        if HAS_FAISS:
            faiss.normalize_L2(query_embedding)
        
        # æœç´¢
        k_search = min(k * 3, len(self.tool_names))  # æœç´¢æ›´å¤šä»¥ä¾¿è¿‡æ»¤
        distances, indices = self.index.search(query_embedding, k_search)
        
        results = []
        for dist, idx in zip(distances[0], indices[0]):
            if dist < similarity_threshold:
                continue
            
            # è¾¹ç•Œæ£€æŸ¥
            if idx < 0 or idx >= len(self.tool_names):
                logger.warning(f"Invalid index {idx} (tool_names length: {len(self.tool_names)})")
                continue
                
            tool_name = self.tool_names[idx]
            
            # å…³é”®ä¿®å¤ï¼šæ£€æŸ¥å·¥å…·æ˜¯å¦åœ¨ embeddings ä¸­
            if tool_name not in self.tool_embeddings:
                logger.warning(f"Tool '{tool_name}' at index {idx} not found in tool_embeddings")
                # å°è¯•ä» embeddings é‡å»º tool_names
                if len(self.tool_names) != len(self.tool_embeddings):
                    logger.info("Detected mismatch, rebuilding tool_names from embeddings")
                    self.tool_names = list(self.tool_embeddings.keys())
                    # é‡å»º FAISS ç´¢å¼•
                    self._rebuild_faiss_index()
                    # é‡æ–°æœç´¢
                    return self.search(query, k, category_filter, return_scores, similarity_threshold)
                continue
                
            tool_emb = self.tool_embeddings[tool_name]
            
            # ç±»åˆ«è¿‡æ»¤
            if category_filter and tool_emb.category != category_filter:
                continue
            
            # åˆ†æç›¸å…³æ€§åŸå› 
            try:
                desc_sim = self._calculate_similarity(
                    query_embedding[0], 
                    tool_emb.description_embedding
                )
                param_sim = self._calculate_similarity(
                    query_embedding[0], 
                    tool_emb.parameter_embedding
                )
                func_sim = self._calculate_similarity(
                    query_embedding[0], 
                    tool_emb.functionality_embedding
                )
                
                # ç”ŸæˆåŸå› 
                reason_parts = []
                if desc_sim > 0.7:
                    reason_parts.append("description match")
                if param_sim > 0.7:
                    reason_parts.append("parameter match")
                if func_sim > 0.7:
                    reason_parts.append("functionality match")
                
                reason = " + ".join(reason_parts) if reason_parts else "general similarity"
            except Exception as e:
                logger.debug(f"Failed to calculate similarity reasons: {e}")
                reason = "general similarity"
            
            result = SearchResult(
                tool_name=tool_name,
                score=float(dist) if return_scores else 1.0,
                mcp_protocol=tool_emb.mcp_protocol,
                category=tool_emb.category,
                reason=reason
            )
            
            results.append(result)
            
            if len(results) >= k:
                break
        
        # ç¼“å­˜ç»“æœ
        if hasattr(self.search_cache, 'put'):
            self.search_cache.put(cache_key, results)
        elif isinstance(self.search_cache, dict):
            self.search_cache[cache_key] = results
        
        return results

    def _rebuild_faiss_index(self):
        """é‡å»º FAISS ç´¢å¼•ä»¥ç¡®ä¿åŒæ­¥"""
        if not HAS_FAISS or not self.tool_embeddings:
            return
        
        logger.info("Rebuilding FAISS index for synchronization")
        
        # ä» tool_embeddings é‡å»º
        embeddings = []
        tool_names = []
        
        for tool_name, tool_emb in self.tool_embeddings.items():
            embeddings.append(tool_emb.combined_embedding)
            tool_names.append(tool_name)
        
        if embeddings:
            embeddings_matrix = np.vstack(embeddings).astype('float32')
            
            # è·å–å®é™…ç»´åº¦
            actual_dimension = embeddings_matrix.shape[1]
            
            # åˆ›å»ºæ–°ç´¢å¼•
            self.index = faiss.IndexFlatIP(actual_dimension)
            faiss.normalize_L2(embeddings_matrix)
            self.index.add(embeddings_matrix)
            
            # æ›´æ–° tool_names
            self.tool_names = tool_names
            
            logger.info(f"Rebuilt FAISS index with {len(tool_names)} tools")

    def clear_cache(self, keep_embeddings: bool = True):
        """æ¸…ç†ç¼“å­˜"""
        print(f"ğŸ—‘ï¸ Clearing cache (keep_embeddings={keep_embeddings})...")
        
        # æ¸…ç†æœç´¢ç¼“å­˜
        self.search_cache = PermanentSmartCache(
            max_size=10000,
            similarity_threshold=0.95
        )
        if self.search_cache_file.exists():
            self.search_cache_file.unlink()
            print("  - Cleared search cache")
        
        # æ¸…ç†embeddingç¼“å­˜ï¼ˆå¯é€‰ï¼‰
        if not keep_embeddings:
            self.embedding_cache = {}
            if self.embedding_cache_file.exists():
                self.embedding_cache_file.unlink()
                print("  - Cleared embedding cache")
        else:
            print(f"  - Kept {len(self.embedding_cache)} embeddings")


    def get_cache_stats(self) -> Dict[str, Any]:
        """è·å–ç¼“å­˜ç»Ÿè®¡"""
        embedding_size_mb = 0
        if self.embedding_cache_file.exists():
            embedding_size_mb = self.embedding_cache_file.stat().st_size / (1024 * 1024)
        
        search_size_mb = 0
        if self.search_cache_file.exists():
            search_size_mb = self.search_cache_file.stat().st_size / (1024 * 1024)
        
        # è®¡ç®—æœ€å¸¸è®¿é—®çš„æŸ¥è¯¢
        top_queries = []
        if hasattr(self.search_cache, 'access_counts'):
            sorted_queries = sorted(
                self.search_cache.access_counts.items(),
                key=lambda x: x[1],
                reverse=True
            )[:5]
            top_queries = [
                {'query': k.split('|')[0], 'count': v}
                for k, v in sorted_queries
            ]
        
        return {
            'embedding_cache': {
                'entries': len(self.embedding_cache),
                'size_mb': embedding_size_mb,
                'type': 'permanent'
            },
            'search_cache': {
                'entries': len(self.search_cache.cache),
                'max_size': self.search_cache.max_size,
                'size_mb': search_size_mb,
                'type': 'permanent_lfu',
                'top_queries': top_queries
            },
            'total_size_mb': embedding_size_mb + search_size_mb
        }
    
    def _calculate_hit_rate(self, cache_manager: PermanentSmartCache) -> float:
        """è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡"""
        total_accesses = sum(cache_manager.access_counts.values())
        if total_accesses == 0:
            return 0.0
        
        # ä¼°ç®—å‘½ä¸­ç‡ï¼ˆè®¿é—®æ¬¡æ•°>1çš„æ¡ç›®ï¼‰
        hits = sum(count - 1 for count in cache_manager.access_counts.values() if count > 1)
        return hits / total_accesses if total_accesses > 0 else 0.0
       
    def get_tool_protocols(self, 
                          tool_names: List[str],
                          include_embeddings: bool = False) -> Dict[str, Dict[str, Any]]:
        """
        æ‰¹é‡è·å–å·¥å…·åè®®
        
        Args:
            tool_names: å·¥å…·åç§°åˆ—è¡¨
            include_embeddings: æ˜¯å¦åŒ…å«åµŒå…¥å‘é‡
            
        Returns:
            å·¥å…·åè®®å­—å…¸
        """
        protocols = {}
        for tool_name in tool_names:
            if tool_name in self.tool_embeddings:
                tool_emb = self.tool_embeddings[tool_name]
                protocol_data = {
                    'mcp_protocol': tool_emb.mcp_protocol,
                    'category': tool_emb.category,
                    'description': tool_emb.description
                }
                
                if include_embeddings:
                    protocol_data['embedding'] = tool_emb.combined_embedding.tolist()
                
                protocols[tool_name] = protocol_data
        
        return protocols
    
    def find_similar_tools(self, 
                        tool_name: str, 
                        k: int = 5,
                        same_category_only: bool = False) -> List[Tuple[str, float]]:
        """
        æŸ¥æ‰¾ç›¸ä¼¼å·¥å…·
        
        Args:
            tool_name: åŸºå‡†å·¥å…·åç§°
            k: è¿”å›æ•°é‡
            same_category_only: æ˜¯å¦åªè¿”å›åŒç±»åˆ«å·¥å…·
            
        Returns:
            ç›¸ä¼¼å·¥å…·åˆ—è¡¨
        """
        # ä½¿ç”¨çº¿ç¨‹é”ä¿æŠ¤æ•´ä¸ªæœç´¢è¿‡ç¨‹
        with MCPEmbeddingManager._lock:
            if tool_name not in self.tool_embeddings:
                logger.warning(f"Tool {tool_name} not found")
                return []
            
            tool_emb = self.tool_embeddings[tool_name]
            query_embedding = tool_emb.combined_embedding.reshape(1, -1).astype('float32')
            
            if not self.index or not HAS_FAISS:
                return []
            
            # åœ¨é”ä¿æŠ¤ä¸‹åˆ›å»ºæœ¬åœ°å‰¯æœ¬ï¼Œé¿å…æœç´¢è¿‡ç¨‹ä¸­æ•°æ®å˜åŒ–
            local_tool_names = list(self.tool_names)
            local_tool_embeddings_keys = set(self.tool_embeddings.keys())
            
            # æœç´¢ç›¸ä¼¼å·¥å…·
            distances, indices = self.index.search(query_embedding, k + 1)
            
            results = []
            for dist, idx in zip(distances[0], indices[0]):
                # æ·»åŠ è¾¹ç•Œæ£€æŸ¥
                if idx < 0 or idx >= len(local_tool_names):
                    print(f"[ERROR] Invalid index {idx} for tool_names length {len(local_tool_names)}")
                    continue
                    
                similar_tool = local_tool_names[idx]
                
                # è·³è¿‡è‡ªèº«
                if similar_tool == tool_name:
                    continue
                
                # å…³é”®ä¿®å¤ï¼šæ£€æŸ¥å·¥å…·æ˜¯å¦åœ¨ embeddings ä¸­
                if similar_tool not in local_tool_embeddings_keys:
                    print(f"[ERROR] Tool '{similar_tool}' at index {idx} not found in tool_embeddings")
                    print(f"[DEBUG] tool_names length: {len(local_tool_names)}, tool_embeddings keys: {len(local_tool_embeddings_keys)}")
                    print(f"[DEBUG] Thread ID: {threading.current_thread().ident}")
                    # ç›´æ¥è·³è¿‡è¿™ä¸ªå·¥å…·ï¼Œç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ª
                    continue
                
                # å†æ¬¡éªŒè¯ï¼ˆåŒé‡æ£€æŸ¥ï¼‰
                if similar_tool not in self.tool_embeddings:
                    print(f"[ERROR] Tool '{similar_tool}' disappeared during search")
                    continue
                    
                similar_emb = self.tool_embeddings[similar_tool]
                if same_category_only and similar_emb.category != tool_emb.category:
                    continue
                    
                results.append((similar_tool, float(dist)))
                
                if len(results) >= k:
                    break
            
            return results
    
    def _save_search_cache(self):
        """ä¿å­˜æœç´¢ç¼“å­˜åˆ°æ–‡ä»¶"""
        # ç›´æ¥è°ƒç”¨ PermanentSmartCache çš„ save_to_file æ–¹æ³•
        # ä½¿ç”¨å·²å®šä¹‰çš„ search_cache_file è·¯å¾„
        logger.debug(f" Saving search cache to {self.search_cache_file}")
        
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        self.search_cache_file.parent.mkdir(parents=True, exist_ok=True)
        
        # è°ƒç”¨ PermanentSmartCache çš„ save_to_file æ–¹æ³•
        self.search_cache.save_to_file(str(self.search_cache_file))
        
        # è®°å½•ä¿å­˜çš„æ¡ç›®æ•°é‡
        cache_entries = len(self.search_cache.cache)
        print(f"[INFO] Saved {cache_entries} search cache entries")
        logger.info(f"Search cache saved with {cache_entries} entries")

    def save_index(self, path: Union[str, Path]):
        """ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶"""
        # ä¿å­˜æœç´¢ç¼“å­˜
        self._save_search_cache()
        
        save_data = {
            'config': asdict(self.config),
            'tool_names': self.tool_names,
            'tool_embeddings': {
                name: emb.to_dict() 
                for name, emb in self.tool_embeddings.items()
            }
        }
        
        if HAS_FAISS and self.index:
            save_data['index'] = faiss.serialize_index(self.index)
        
        with open(path, 'wb') as f:
            pickle.dump(save_data, f)
        
        logger.info(f"Index saved to {path}")
    


    def load_index(self, index_file: str = None) -> None:
        """ä»æ–‡ä»¶åŠ è½½ç´¢å¼•ï¼Œå¢å¼ºé”™è¯¯å¤„ç†"""
        if index_file is None:
            index_file = Path(self.config.cache_dir) / "tool_index.pkl"
        else:
            index_file = Path(index_file)
        
        if not index_file.exists():
            logger.warning(f"Index file not found: {index_file}")
            return
        
        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = index_file.stat().st_size
        if file_size == 0:
            logger.error(f"Index file {index_file} is empty, cannot load")
            index_file.unlink()
            raise ValueError(f"Index file is empty: {index_file}")
        
        logger.info(f"Loading index from {index_file} (size: {file_size} bytes)")
        
        try:
            with open(index_file, 'rb') as f:
                data = pickle.load(f)
            
            # éªŒè¯æ•°æ®ç»“æ„
            if not isinstance(data, dict):
                raise ValueError(f"Invalid index format (expected dict, got {type(data)})")
            
            # ä¿®å¤ï¼šæ£€æŸ¥æ–°æ—§ä¸¤ç§å¯èƒ½çš„é”®å
            if 'tool_embeddings' in data:
                # æ–°æ ¼å¼ï¼šç›´æ¥ä½¿ç”¨ tool_embeddings
                embeddings_data = data['tool_embeddings']
            elif 'embeddings' in data:
                # æ—§æ ¼å¼ï¼šä½¿ç”¨ embeddings
                embeddings_data = data['embeddings']
            else:
                # éƒ½æ²¡æœ‰ï¼ŒæŠ¥é”™
                available_keys = list(data.keys())
                raise ValueError(f"Neither 'tool_embeddings' nor 'embeddings' found in index. Available keys: {available_keys}")
            
            # æ£€æŸ¥å…¶ä»–å¿…éœ€çš„é”®
            required_keys = ['tool_names', 'index', 'config']
            missing_keys = set(required_keys) - set(data.keys())
            if missing_keys:
                raise ValueError(f"Missing required keys in index: {missing_keys}")
            
            # æ¢å¤å·¥å…·åç§°
            self.tool_names = data['tool_names']
            
            # æ¢å¤åµŒå…¥æ•°æ®
            self.tool_embeddings = {}
            for name, emb_data in embeddings_data.items():
                if isinstance(emb_data, dict):
                    # ä»å­—å…¸æ¢å¤ ToolEmbedding å¯¹è±¡
                    self.tool_embeddings[name] = ToolEmbedding.from_dict(emb_data)
                else:
                    # å…¼å®¹æ—§æ ¼å¼
                    logger.warning(f"Old format detected for {name}, skipping")
            
            # æ¢å¤ FAISS ç´¢å¼•
            if 'index' in data and data['index'] is not None:
                self.index = faiss.deserialize_index(data['index'])
                logger.info(f"FAISS index loaded")
            else:
                self.index = None
                logger.warning("No FAISS index found in saved data")
            
            # æ¢å¤é…ç½®
            if 'config' in data:
                saved_config = data['config']
                # æ›´æ–°é…ç½®ä¸­çš„ç»´åº¦ä¿¡æ¯
                if 'dimension' in saved_config:
                    self.config.dimension = saved_config['dimension']
                    logger.info(f"Updated dimension to {self.config.dimension}")
            
            logger.info(f"Index loaded successfully: {len(self.tool_embeddings)} tools")
            
        except EOFError:
            logger.error(f"Index file {index_file} is corrupted (EOFError)")
            # å¤‡ä»½æŸåçš„æ–‡ä»¶
            backup_path = index_file.with_suffix('.pkl.corrupted')
            index_file.rename(backup_path)
            logger.info(f"Corrupted index backed up to {backup_path}")
            raise ValueError(f"Index file is corrupted: {index_file}")
            
        except Exception as e:
            logger.error(f"Failed to load index: {e}")
            raise
    def export_stats(self) -> Dict[str, Any]:
        """å¯¼å‡ºç»Ÿè®¡ä¿¡æ¯"""
        if not self.tool_embeddings:
            return {'status': 'empty'}
        
        categories = {}
        for tool_emb in self.tool_embeddings.values():
            cat = tool_emb.category
            if cat not in categories:
                categories[cat] = 0
            categories[cat] += 1
        
        return {
            'total_tools': len(self.tool_embeddings),
            'categories': categories,
            'index_type': 'faiss' if self.index else 'none',
            'embedding_model': self.config.model,
            'embedding_dimension': self.config.dimension,
            'cache_size': len(self.embedding_cache)
        }


# ===========================
# Convenience Functions
# ===========================

def create_manager(api_key: Optional[str] = None) -> MCPEmbeddingManager:
    """åˆ›å»ºåµŒå…¥ç®¡ç†å™¨çš„ä¾¿æ·å‡½æ•°"""
    return MCPEmbeddingManager(api_key=api_key)


def quick_search(query: str, 
                tool_registry_path: str = "mcp_generated_library/tool_registry_consolidated.json",
                k: int = 5) -> List[Dict[str, Any]]:
    """å¿«é€Ÿæœç´¢å·¥å…·çš„ä¾¿æ·å‡½æ•°"""
    manager = create_manager()
    
    # æ„å»ºæˆ–åŠ è½½ç´¢å¼•
    index_path = Path(".mcp_embedding_cache/tool_index.pkl")
    if index_path.exists():
        manager.load_index(index_path)
    else:
        manager.build_index(tool_registry_path)
    
    # æœç´¢
    results = manager.search(query, k=k)
    
    return [
        {
            'tool_name': r.tool_name,
            'score': r.score,
            'category': r.category,
            'description': r.mcp_protocol.get('description', '')
        }
        for r in results
    ]


# ===========================
# Command Line Interface
# ===========================

def main():
    """å‘½ä»¤è¡Œå…¥å£"""
    parser = argparse.ArgumentParser(
        description="MCP Tool Embedding Manager",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # æ„å»ºç´¢å¼•
  python mcp_embedding_manager.py build --registry tool_registry.json
  
  # æœç´¢å·¥å…·
  python mcp_embedding_manager.py search "validate json data" -k 5
  
  # æŸ¥æ‰¾ç›¸ä¼¼å·¥å…·
  python mcp_embedding_manager.py similar file_operations_reader -k 3
  
  # å¯¼å‡ºç»Ÿè®¡ä¿¡æ¯
  python mcp_embedding_manager.py stats
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='Commands')
    
    # Build command
    build_parser = subparsers.add_parser('build', help='Build embedding index')
    build_parser.add_argument('--registry', '-r', required=True, help='Tool registry JSON file')
    build_parser.add_argument('--force', '-f', action='store_true', help='Force rebuild')
    build_parser.add_argument('--api-key', help='OpenAI API key')
    
    # Search command
    search_parser = subparsers.add_parser('search', help='Search for tools')
    search_parser.add_argument('query', help='Search query')
    search_parser.add_argument('-k', type=int, default=5, help='Number of results')
    search_parser.add_argument('--category', help='Filter by category')
    search_parser.add_argument('--api-key', help='OpenAI API key')
    
    # Similar command
    similar_parser = subparsers.add_parser('similar', help='Find similar tools')
    similar_parser.add_argument('tool', help='Tool name')
    similar_parser.add_argument('-k', type=int, default=5, help='Number of results')
    similar_parser.add_argument('--same-category', action='store_true', help='Same category only')
    
    # Stats command
    stats_parser = subparsers.add_parser('stats', help='Show statistics')
    
    # Export command
    export_parser = subparsers.add_parser('export', help='Export tool protocols')
    export_parser.add_argument('tools', nargs='+', help='Tool names to export')
    export_parser.add_argument('--output', '-o', help='Output file')
    export_parser.add_argument('--embeddings', action='store_true', help='Include embeddings')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    # è®¾ç½®APIå¯†é’¥
    api_key = getattr(args, 'api_key', None) or os.getenv('OPENAI_API_KEY')
    
    # åˆ›å»ºç®¡ç†å™¨
    manager = MCPEmbeddingManager(api_key=api_key)
    
    # æ‰§è¡Œå‘½ä»¤
    if args.command == 'build':
        stats = manager.build_index(args.registry, force_rebuild=args.force)
        print(json.dumps(stats, indent=2))
        
    elif args.command == 'search':
        # åŠ è½½ç´¢å¼•
        index_path = Path(".mcp_embedding_cache/tool_index.pkl")
        if not index_path.exists():
            print("Error: Index not found. Run 'build' first.")
            return
        
        manager.load_index(index_path)
        
        # æœç´¢
        results = manager.search(
            args.query, 
            k=args.k, 
            filter_category=args.category
        )
        
        # æ˜¾ç¤ºç»“æœ
        print(f"\nSearch results for: '{args.query}'\n")
        for i, result in enumerate(results, 1):
            print(f"{i}. {result.tool_name} (score: {result.score:.3f})")
            print(f"   Category: {result.category}")
            print(f"   Description: {result.mcp_protocol.get('description', 'N/A')}")
            print()
    
    elif args.command == 'similar':
        # åŠ è½½ç´¢å¼•
        index_path = Path(".mcp_embedding_cache/tool_index.pkl")
        if not index_path.exists():
            print("Error: Index not found. Run 'build' first.")
            return
        
        manager.load_index(index_path)
        
        # æŸ¥æ‰¾ç›¸ä¼¼å·¥å…·
        results = manager.find_similar_tools(
            args.tool,
            k=args.k,
            same_category_only=args.same_category
        )
        
        if not results:
            print(f"Tool '{args.tool}' not found.")
            return
        
        print(f"\nTools similar to: '{args.tool}'\n")
        for i, (tool_name, score) in enumerate(results, 1):
            tool_emb = manager.tool_embeddings[tool_name]
            print(f"{i}. {tool_name} (similarity: {score:.3f})")
            print(f"   Category: {tool_emb.category}")
            print(f"   Description: {tool_emb.description}")
            print()
    
    elif args.command == 'stats':
        # åŠ è½½ç´¢å¼•
        index_path = Path(".mcp_embedding_cache/tool_index.pkl")
        if index_path.exists():
            manager.load_index(index_path)
        
        stats = manager.export_stats()
        print(json.dumps(stats, indent=2))
    
    elif args.command == 'export':
        # åŠ è½½ç´¢å¼•
        index_path = Path(".mcp_embedding_cache/tool_index.pkl")
        if not index_path.exists():
            print("Error: Index not found. Run 'build' first.")
            return
        
        manager.load_index(index_path)
        
        # è·å–åè®®
        protocols = manager.get_tool_protocols(
            args.tools,
            include_embeddings=args.embeddings
        )
        
        # è¾“å‡º
        output_data = json.dumps(protocols, indent=2)
        if args.output:
            with open(args.output, 'w') as f:
                f.write(output_data)
            print(f"Exported to {args.output}")
        else:
            print(output_data)

_global_embedding_manager = None

def get_embedding_manager(config: Optional[EmbeddingConfig] = None) -> MCPEmbeddingManager:
    """è·å–MCPEmbeddingManagerçš„å•ä¾‹å®ä¾‹
    
    Args:
        config: å¯é€‰çš„é…ç½®å¯¹è±¡ï¼Œä»…åœ¨é¦–æ¬¡åˆ›å»ºæ—¶ä½¿ç”¨
        
    Returns:
        MCPEmbeddingManagerçš„å…¨å±€å•ä¾‹å®ä¾‹
    """
    global _global_embedding_manager
    
    if _global_embedding_manager is None:
        print("[MCPEmbeddingManager] Creating new singleton instance")
        _global_embedding_manager = MCPEmbeddingManager(config=config)
        print(f"[MCPEmbeddingManager] Singleton created with {len(_global_embedding_manager.tool_embeddings)} cached embeddings")
    else:
        if config is not None:
            print("[MCPEmbeddingManager] Warning: Config ignored, using existing singleton instance")
        print(f"[MCPEmbeddingManager] Reusing existing singleton instance (id: {id(_global_embedding_manager)})")
        print(f"[MCPEmbeddingManager] Current cache size: {len(_global_embedding_manager.tool_embeddings)} embeddings")
    
    return _global_embedding_manager

def reset_embedding_manager():
    """é‡ç½®å•ä¾‹å®ä¾‹ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
    global _global_embedding_manager
    print("[MCPEmbeddingManager] Resetting singleton instance")
    _global_embedding_manager = None

def get_embedding_manager_info() -> dict:
    """è·å–å½“å‰embeddingç®¡ç†å™¨ä¿¡æ¯ï¼ˆè°ƒè¯•ç”¨ï¼‰"""
    global _global_embedding_manager
    
    if _global_embedding_manager is None:
        return {"status": "not_initialized"}
    
    return {
        "status": "initialized",
        "instance_id": id(_global_embedding_manager),
        "cached_embeddings": len(_global_embedding_manager.tool_embeddings),
        "index_loaded": _global_embedding_manager.index is not None,
        "model": _global_embedding_manager.config.model,
        "dimension": _global_embedding_manager.config.dimension
    }

if __name__ == "__main__":
    main()
